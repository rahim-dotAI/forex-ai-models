name: Smart Forex Brain Pipeline v17.5

on:
  workflow_dispatch:
  push:
    paths: ['colab_trigger.txt']
    branches: [main]
  schedule:
    - cron: '0 */2 * * 1-5'    # Weekdays: Every 2 hours
    - cron: '*/30 * * * 0,6'   # Weekends: Every 30 minutes

jobs:
  run-forex-brain:
    runs-on: ubuntu-latest
    timeout-minutes: 55

    env:
      FOREX_PAT: ${{ secrets.FOREX_PAT }}
      BROWSERLESS_TOKEN: ${{ secrets.BROWSERLESS_TOKEN }}
      ALPHA_VANTAGE_KEY: ${{ secrets.ALPHA_VANTAGE_KEY }}
      GMAIL_USER: ${{ secrets.GMAIL_USER }}
      GMAIL_APP_PASSWORD: ${{ secrets.GMAIL_APP_PASSWORD }}
      PYTHONIOENCODING: utf-8
      PYTHONUNBUFFERED: 1
      GITHUB_ACTIONS: "true"
      SINGLE_RUN_MODE: "true"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0
          token: ${{ secrets.FOREX_PAT }}

      - name: Fix Submodules
        run: |
          [ -f ".gitmodules" ] && rm -f .gitmodules && git rm --cached -r forex-alpha-models 2>/dev/null || true
          find . -mindepth 2 -name ".git" -type d -exec rm -rf {} + 2>/dev/null || true

      - name: Detect Mode
        id: mode
        run: |
          DAY=$(date +%u)
          echo "Day: $DAY ($(date +'%A'))"
          if [ $DAY -eq 6 ] || [ $DAY -eq 7 ]; then
            echo "mode=weekend" >> $GITHUB_OUTPUT
            echo "üèñÔ∏è WEEKEND MODE: Tagged cells only"
          else
            echo "mode=weekday" >> $GITHUB_OUTPUT
            echo "üíº WEEKDAY MODE: Full notebook"
          fi

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install Dependencies
        run: |
          pip install -q --no-cache-dir pandas numpy requests beautifulsoup4 scikit-learn \
            jupyter nbconvert nbformat ta yfinance mplfinance firebase-admin dropbox \
            pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm river scipy

      - name: Configure Git
        run: |
          git config --global user.name "Forex AI Bot"
          git config --global user.email "nakatonabira3@gmail.com"
          echo "https://${{ github.actor }}:${{ secrets.FOREX_PAT }}@github.com" > ~/.git-credentials
          git config --global credential.helper store

      - name: Repair Corrupted Pickle Files
        id: corruption_check
        run: |
          echo "üîß Checking pickle files for corruption..."
          
          cat > check_pickles.py << 'EOF'
          import os
          import pickle
          import pandas as pd
          from pathlib import Path
          
          def is_pickle_corrupted(filepath):
              """Quick check if pickle is readable"""
              try:
                  pd.read_pickle(filepath, compression='gzip')
                  return False
              except:
                  try:
                      pd.read_pickle(filepath, compression=None)
                      return False
                  except:
                      return True
          
          corrupted_count = 0
          total_count = 0
          
          # Check data/processed
          processed_dir = Path("data/processed")
          if processed_dir.exists():
              print(f"üìÇ Checking: {processed_dir.absolute()}")
              print(f"\nüìÑ All files in data/processed:")
              for pkl_file in processed_dir.glob("*.pkl"):
                  print(f"   - {pkl_file.name} ({pkl_file.stat().st_size} bytes)")
                  
                  if any(x in pkl_file.name for x in ['_model', '_sgd_', '_rf_']):
                      print(f"     ‚îî‚îÄ Skipped (model file)")
                      continue
                  
                  total_count += 1
                  if is_pickle_corrupted(pkl_file):
                      corrupted_count += 1
                      print(f"     ‚îî‚îÄ ‚ùå CORRUPTED")
                  else:
                      print(f"     ‚îî‚îÄ ‚úÖ Valid")
          
          print(f"\nüìä Summary:")
          print(f"   Data files: {total_count}")
          print(f"   Corrupted: {corrupted_count}")
          
          if total_count == 0:
              print("\n‚ö†Ô∏è No pickle files found - need to generate data")
              exit(2)  # Exit code 2 = no files
          elif corrupted_count == total_count:
              print(f"\n‚ö†Ô∏è ALL {total_count} files are corrupted")
              print("üßπ Will delete and regenerate all data")
              exit(1)  # Exit code 1 = all corrupted
          elif corrupted_count > 0:
              print(f"\n‚ö†Ô∏è {corrupted_count}/{total_count} files are corrupted")
              print("üßπ Will delete and regenerate all data")
              exit(1)  # Exit code 1 = some corrupted
          else:
              print("\n‚úÖ All pickle files are valid")
              exit(0)  # Exit code 0 = all good
          EOF
          
          mkdir -p data/processed database .github/run_history
          
          python check_pickles.py
          CHECK_EXIT=$?
          
          if [ $CHECK_EXIT -eq 0 ]; then
            echo "all_files_valid=true" >> $GITHUB_OUTPUT
            echo "need_full_run=false" >> $GITHUB_OUTPUT
            echo "‚úÖ All files are valid - can use weekend mode"
          elif [ $CHECK_EXIT -eq 2 ]; then
            echo "all_files_valid=false" >> $GITHUB_OUTPUT
            echo "need_full_run=true" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è No files found - forcing full notebook run"
          else
            echo "all_files_valid=false" >> $GITHUB_OUTPUT
            echo "need_full_run=true" >> $GITHUB_OUTPUT
            echo ""
            echo "üßπ Cleaning up corrupted files for fresh start..."
            find data/processed -name "*.pkl" -type f ! -name "*_model.pkl" -delete 2>/dev/null || true
            find rl_memory -name "*.pkl" -type f -delete 2>/dev/null || true
            rm -f database/memory_v85.db 2>/dev/null || true
            echo "‚úÖ Cleanup complete - will run full notebook"
          fi

      - name: Check Data
        id: data
        run: |
          PICKLE_COUNT=$(find data/processed -name "*.pkl" -type f ! -name "*_model.pkl" 2>/dev/null | wc -l)
          
          [ -f ".github/run_history/run_counter.txt" ] && RUN_NUM=$(cat .github/run_history/run_counter.txt) || RUN_NUM=0
          RUN_NUM=$((RUN_NUM + 1))
          echo $RUN_NUM > .github/run_history/run_counter.txt
          echo "run_number=$RUN_NUM" >> $GITHUB_OUTPUT
          
          echo "üìä Status Check:"
          echo "   Pickle files: $PICKLE_COUNT"
          echo "   Files valid: ${{ steps.corruption_check.outputs.all_files_valid }}"
          echo "   Need full run: ${{ steps.corruption_check.outputs.need_full_run }}"
          
          [ $((RUN_NUM % 10)) -eq 0 ] && echo "send_report=true" >> $GITHUB_OUTPUT || echo "send_report=false" >> $GITHUB_OUTPUT

      - name: Verify Notebook
        run: |
          if [ ! -f "AI_Forex_Brain_2.ipynb" ]; then
            echo "‚ùå ERROR: AI_Forex_Brain_2.ipynb not found!"
            ls -la *.ipynb 2>/dev/null || echo "No .ipynb files found"
            exit 1
          fi
          
          echo "‚úÖ Found AI_Forex_Brain_2.ipynb"
          
          # Check for tagged cells
          TAGGED_COUNT=$(python -c "
          import nbformat
          with open('AI_Forex_Brain_2.ipynb', 'r') as f:
              nb = nbformat.read(f, 4)
          tagged = sum(1 for c in nb.cells if c.cell_type=='code' and '#TAG: pipeline_main' in c.source)
          print(tagged)
          " 2>/dev/null || echo "0")
          
          echo "üìä Tagged cells found: $TAGGED_COUNT"
          
          if [ "$TAGGED_COUNT" = "0" ]; then
            echo "‚ö†Ô∏è WARNING: No tagged cells found!"
            echo "   Weekend mode will fail. Consider adding '#TAG: pipeline_main' to key cells."
          fi

      - name: Create Executors
        run: |
          cat > run_tagged.py << 'EOF'
          import nbformat, sys, time, re, json, os, traceback
          from nbconvert.preprocessors import ExecutePreprocessor
          from datetime import datetime
          
          class TaggedExecutor(ExecutePreprocessor):
              def __init__(self, *args, **kwargs):
                  super().__init__(*args, **kwargs)
                  self.tagged = []
                  self.summaries = []
              
              def preprocess(self, nb, resources=None, km=None):
                  self.tagged = [i for i, c in enumerate(nb.cells) if c.cell_type=='code' and '#TAG: pipeline_main' in c.source]
                  print(f"üìä Found {len(self.tagged)} tagged cells")
                  if len(self.tagged) == 0:
                      print("‚ö†Ô∏è WARNING: No tagged cells found! Looking for '#TAG: pipeline_main'")
                      print("Available code cells:", sum(1 for c in nb.cells if c.cell_type=='code'))
                  return super().preprocess(nb, resources, km)
              
              def preprocess_cell(self, cell, resources, idx):
                  if idx not in self.tagged: return cell, resources
                  n = self.tagged.index(idx) + 1
                  print(f"\n{'='*70}\nüîÑ CELL {n}/{len(self.tagged)} - Starting Execution\n{'='*70}")
                  
                  # Show full cell source (first 500 chars)
                  preview = cell.source[:500]
                  print(f"üìù Cell Source Preview:\n{preview}")
                  if len(cell.source) > 500:
                      print(f"... ({len(cell.source) - 500} more characters)")
                  print(f"\n{'‚îÄ'*70}")
                  
                  t = time.time()
                  try:
                      cell, resources = super().preprocess_cell(cell, resources, idx)
                      duration = time.time()-t
                      
                      # Show ALL cell outputs
                      print(f"\n{'‚îÄ'*70}")
                      print(f"üì§ CELL {n} OUTPUT:")
                      print(f"{'‚îÄ'*70}")
                      
                      if cell.outputs:
                          for out_idx, output in enumerate(cell.outputs, 1):
                              if output.output_type == 'stream':
                                  # Show full stream output (remove ANSI codes)
                                  text = re.sub(r'\x1b\[[0-9;]*m', '', output.text)
                                  lines = text.strip().split('\n')
                                  
                                  print(f"\n[Output Block {out_idx} - Stream - {len(lines)} lines]")
                                  # Show all lines (not truncated)
                                  for line in lines:
                                      print(line)
                                  
                              elif output.output_type == 'execute_result':
                                  print(f"\n[Output Block {out_idx} - Execute Result]")
                                  if 'text/plain' in output.data:
                                      print(output.data['text/plain'])
                                  
                              elif output.output_type == 'display_data':
                                  print(f"\n[Output Block {out_idx} - Display Data]")
                                  if 'text/plain' in output.data:
                                      print(output.data['text/plain'])
                                  if 'text/html' in output.data:
                                      print("[HTML output present]")
                                  
                              elif output.output_type == 'error':
                                  print(f"\n[Output Block {out_idx} - Error]")
                                  print(f"‚ùå {output.ename}: {output.evalue}")
                                  if hasattr(output, 'traceback') and output.traceback:
                                      print("\nFull Traceback:")
                                      for tb_line in output.traceback:
                                          # Remove ANSI codes from traceback
                                          clean_line = re.sub(r'\x1b\[[0-9;]*m', '', tb_line)
                                          print(clean_line)
                      else:
                          print("(No outputs generated)")
                      
                      print(f"\n{'‚îÄ'*70}")
                      print(f"‚úÖ Cell {n} completed in {duration:.1f}s")
                      print(f"{'='*70}\n")
                      
                      self.summaries.append({
                          'cell': n, 
                          'duration': duration, 
                          'status': 'success',
                          'output_blocks': len(cell.outputs) if cell.outputs else 0
                      })
                      
                  except Exception as e:
                      duration = time.time()-t
                      error_type = type(e).__name__
                      error_msg = str(e)[:500]
                      
                      print(f"\n{'‚îÄ'*70}")
                      print(f"‚ùå CELL {n} FAILED after {duration:.1f}s")
                      print(f"{'‚îÄ'*70}")
                      print(f"Error Type: {error_type}")
                      print(f"Error Message: {error_msg}")
                      
                      # Try to extract actual error from CellExecutionError
                      if hasattr(e, 'ename'):
                          print(f"\nActual Error: {e.ename}: {e.evalue}")
                      
                      # Print ALL outputs before error
                      if cell.outputs:
                          print(f"\n{'‚îÄ'*70}")
                          print(f"üì§ OUTPUTS BEFORE ERROR:")
                          print(f"{'‚îÄ'*70}")
                          
                          for out_idx, output in enumerate(cell.outputs, 1):
                              if output.output_type == 'stream':
                                  text = re.sub(r'\x1b\[[0-9;]*m', '', output.text)
                                  lines = text.strip().split('\n')
                                  print(f"\n[Output Block {out_idx} - {len(lines)} lines]")
                                  for line in lines:
                                      print(line)
                                      
                              elif output.output_type == 'error':
                                  print(f"\n[Output Block {out_idx} - Error]")
                                  print(f"‚ùå {output.ename}: {output.evalue}")
                                  if hasattr(output, 'traceback') and output.traceback:
                                      print("\nFull Traceback:")
                                      for tb_line in output.traceback:
                                          clean_line = re.sub(r'\x1b\[[0-9;]*m', '', tb_line)
                                          print(clean_line)
                      
                      print(f"\n{'='*70}\n")
                      
                      self.summaries.append({
                          'cell': n, 
                          'duration': duration, 
                          'status': 'failed',
                          'error': f"{error_type}: {error_msg[:200]}"
                      })
                      
                      # Don't raise - continue to next cell
                      print(f"‚ö†Ô∏è Continuing to next cell despite error...\n")
                      
                  return cell, resources
          
          if not os.path.exists('AI_Forex_Brain_2.ipynb'):
              print("‚ùå ERROR: AI_Forex_Brain_2.ipynb not found!")
              sys.exit(1)
          
          with open('AI_Forex_Brain_2.ipynb', 'r') as f:
              nb = nbformat.read(f, as_version=4)
          
          ep = TaggedExecutor(timeout=2400, kernel_name='python3', allow_errors=True)
          start = time.time()
          
          print("="*70)
          print("üöÄ STARTING TAGGED CELLS EXECUTION")
          print("="*70)
          
          try:
              ep.preprocess(nb, {'metadata': {'path': '.'}})
          except Exception as e:
              print(f"\n‚ö†Ô∏è Preprocess exception: {type(e).__name__}: {str(e)[:200]}")
          
          duration = time.time()-start
          success_count = sum(1 for s in ep.summaries if s['status']=='success')
          failed_count = sum(1 for s in ep.summaries if s['status']=='failed')
          
          print(f"\n{'='*70}")
          print(f"üìä FINAL EXECUTION SUMMARY")
          print(f"{'='*70}")
          print(f"Total cells executed: {len(ep.summaries)}")
          print(f"‚úÖ Successful: {success_count}")
          print(f"‚ùå Failed: {failed_count}")
          print(f"‚è±Ô∏è Total time: {duration:.1f}s")
          
          if ep.summaries:
              print(f"\nDetailed Results:")
              for s in ep.summaries:
                  status_icon = "‚úÖ" if s['status'] == 'success' else "‚ùå"
                  print(f"  {status_icon} Cell {s['cell']}: {s['duration']:.1f}s - {s['status']}")
                  if 'output_blocks' in s:
                      print(f"     ‚îî‚îÄ {s['output_blocks']} output blocks")
                  if 'error' in s:
                      print(f"     ‚îî‚îÄ {s['error']}")
          
          print(f"{'='*70}\n")
          
          report = {
              'timestamp': datetime.now().isoformat(), 
              'mode': 'tagged', 
              'duration': duration, 
              'cells': ep.summaries,
              'total': len(ep.summaries),
              'successful': success_count,
              'failed': failed_count
          }
          
          os.makedirs('.github/run_history', exist_ok=True)
          with open('.github/run_history/latest_run.json', 'w') as f: 
              json.dump(report, f, indent=2)
          
          # Exit with success if at least 1 cell succeeded, or if no cells were found
          if success_count > 0 or len(ep.summaries) == 0:
              print("‚úÖ Tagged cells execution completed successfully")
              sys.exit(0)
          else:
              print("‚ùå All tagged cells failed")
              sys.exit(1)
          EOF
          
          cat > run_full.py << 'EOF'
          import nbformat, sys, time, json, os, traceback, re
          from nbconvert.preprocessors import ExecutePreprocessor
          from datetime import datetime
          
          class VerboseExecutor(ExecutePreprocessor):
              def __init__(self, *args, **kwargs):
                  super().__init__(*args, **kwargs)
                  self.cell_count = 0
                  self.start_time = None
              
              def preprocess(self, nb, resources=None, km=None):
                  code_cells = sum(1 for c in nb.cells if c.cell_type=='code')
                  print(f"üìä Processing {len(nb.cells)} cells ({code_cells} code cells)...")
                  self.start_time = time.time()
                  return super().preprocess(nb, resources, km)
              
              def preprocess_cell(self, cell, resources, idx):
                  if cell.cell_type != 'code':
                      return cell, resources
                  
                  self.cell_count += 1
                  
                  # Show progress every cell
                  elapsed = time.time() - self.start_time
                  print(f"\n{'‚îÄ'*70}")
                  print(f"‚è≥ Cell {self.cell_count} ({elapsed:.0f}s elapsed)")
                  print(f"{'‚îÄ'*70}")
                  
                  cell, resources = super().preprocess_cell(cell, resources, idx)
                  
                  # Show key outputs from each cell
                  if cell.outputs:
                      for output in cell.outputs:
                          if output.output_type == 'stream':
                              text = re.sub(r'\x1b\[[0-9;]*m', '', output.text)
                              lines = text.strip().split('\n')
                              
                              # Show lines with important markers
                              important_lines = [l for l in lines if any(marker in l for marker in 
                                  ['‚úÖ', '‚ö†Ô∏è', '‚ùå', 'üí∞', 'üß†', 'üíæ', 'üìä', 'COMPLETE', 'ERROR', 
                                   'Win Rate', 'P&L', 'Iteration', 'Mode:', 'Total', 'Average'])]
                              
                              if important_lines:
                                  for line in important_lines[:10]:
                                      print(f"  {line}")
                              
                          elif output.output_type == 'error':
                              print(f"  ‚ùå Error: {output.ename}: {output.evalue}")
                  
                  return cell, resources
          
          if not os.path.exists('AI_Forex_Brain_2.ipynb'):
              print("‚ùå ERROR: AI_Forex_Brain_2.ipynb not found!")
              sys.exit(1)
          
          with open('AI_Forex_Brain_2.ipynb', 'r') as f:
              nb = nbformat.read(f, as_version=4)
          
          print("="*70)
          print("üöÄ STARTING FULL NOTEBOOK EXECUTION")
          print("="*70)
          
          ep = VerboseExecutor(timeout=2400, kernel_name='python3', allow_errors=True)
          start = time.time()
          
          try:
              ep.preprocess(nb, {'metadata': {'path': '.'}})
              duration = time.time() - start
              
              print(f"\n{'='*70}")
              print(f"‚úÖ EXECUTION COMPLETED")
              print(f"{'='*70}")
              print(f"Total code cells: {ep.cell_count}")
              print(f"Duration: {duration:.1f}s")
              print(f"{'='*70}\n")
              
              report = {
                  'timestamp': datetime.now().isoformat(), 
                  'mode': 'full', 
                  'duration': duration,
                  'cells_executed': ep.cell_count,
                  'status': 'success'
              }
              
          except Exception as e:
              duration = time.time() - start
              
              print(f"\n{'='*70}")
              print(f"‚ö†Ô∏è EXECUTION COMPLETED WITH ERRORS")
              print(f"{'='*70}")
              print(f"Error: {type(e).__name__}: {str(e)[:200]}")
              print(f"Duration: {duration:.1f}s")
              print(f"{'='*70}\n")
              
              report = {
                  'timestamp': datetime.now().isoformat(), 
                  'mode': 'full', 
                  'duration': duration,
                  'cells_executed': ep.cell_count,
                  'status': 'completed_with_errors',
                  'error': f"{type(e).__name__}: {str(e)[:200]}"
              }
          
          os.makedirs('.github/run_history', exist_ok=True)
          with open('.github/run_history/latest_run.json', 'w') as f: 
              json.dump(report, f, indent=2)
          
          print("‚úÖ Full notebook execution completed")
          EOF

      - name: Run Weekend Mode (if data is valid)
        if: steps.mode.outputs.mode == 'weekend' && steps.corruption_check.outputs.need_full_run == 'false'
        run: |
          echo "üèñÔ∏è Running weekend mode (tagged cells)..."
          python run_tagged.py
          EXIT_CODE=$?
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Tagged cells execution successful"
            
            # Check if notebook actually processed data
            if grep -q "No processed pickles found" .github/run_history/latest_run.json 2>/dev/null || \
               grep -q "No data loaded" .github/run_history/latest_run.json 2>/dev/null; then
              echo ""
              echo "‚ö†Ô∏è WARNING: Notebook couldn't find pickle files!"
              echo "üìã This means the files exist but notebook can't read them"
              echo "üîÑ Falling back to full notebook to regenerate data..."
              python run_full.py || exit 1
            fi
          else
            echo "‚ö†Ô∏è Tagged cells had errors (exit code: $EXIT_CODE)"
            echo "üìã Checking run report..."
            if [ -f ".github/run_history/latest_run.json" ]; then
              python -c "import json; r=json.load(open('.github/run_history/latest_run.json')); print(f\"Successful: {r.get('successful',0)}/{r.get('total',0)}\")" || true
            fi
            echo ""
            echo "üîÑ Falling back to full notebook execution..."
            python run_full.py || exit 1
          fi
        timeout-minutes: 50

      - name: Run Full Notebook (weekday or corrupted data)
        if: steps.mode.outputs.mode == 'weekday' || steps.corruption_check.outputs.need_full_run == 'true'
        run: |
          if [ "${{ steps.corruption_check.outputs.need_full_run }}" = "true" ]; then
            echo "üîÑ Running full notebook due to corrupted/missing data..."
          else
            echo "üíº Running full notebook (weekday mode)..."
          fi
          python run_full.py
        timeout-minutes: 50

      - name: Generate Email Report
        if: steps.data.outputs.send_report == 'true'
        run: |
          cat > report.py << 'EOF'
          import json, os
          from glob import glob
          from datetime import datetime
          
          runs = sorted(glob('.github/run_history/archive/run_*.json'))[-10:]
          if not runs:
              print("No runs to report")
              exit(0)
          
          data = [json.load(open(f)) for f in runs]
          success = sum(1 for r in data if r.get('failed', 0) == 0)
          avg = sum(r.get('duration', 0) for r in data) / len(data)
          
          html = f"""<!DOCTYPE html>
          <html><head><style>
          body{{font-family:Arial;padding:20px;background:#f5f5f5}}
          .container{{max-width:800px;margin:0 auto;background:#fff;padding:30px;border-radius:10px}}
          h1{{color:#2c3e50;border-bottom:3px solid #3498db}}
          .stats{{display:grid;grid-template-columns:repeat(2,1fr);gap:15px;margin:20px 0}}
          .stat{{background:linear-gradient(135deg,#667eea,#764ba2);padding:20px;border-radius:8px;color:#fff}}
          .stat h3{{margin:0;font-size:14px}}
          .stat .val{{font-size:32px;font-weight:bold}}
          </style></head><body><div class="container">
          <h1>ü§ñ Forex AI - 10 Run Report</h1>
          <div class="stats">
          <div class="stat"><h3>‚úÖ Success</h3><p class="val">{success}/{len(data)}</p></div>
          <div class="stat"><h3>‚è±Ô∏è Avg Time</h3><p class="val">{avg:.0f}s</p></div>
          </div></div></body></html>"""
          
          with open('report.html', 'w') as f: f.write(html)
          EOF
          python report.py

      - name: Send Email
        if: steps.data.outputs.send_report == 'true'
        run: |
          cat > email.py << 'EOF'
          import smtplib, os
          from email.mime.multipart import MIMEMultipart
          from email.mime.text import MIMEText
          from datetime import datetime
          
          sender = os.getenv('GMAIL_USER')
          password = os.getenv('GMAIL_APP_PASSWORD')
          if not sender or not password: exit(0)
          
          with open('report.html') as f: html = f.read()
          msg = MIMEMultipart('alternative')
          msg['Subject'] = f'ü§ñ Forex AI Report - {datetime.now().strftime("%Y-%m-%d")}'
          msg['From'] = msg['To'] = sender
          msg.attach(MIMEText(html, 'html'))
          
          try:
              s = smtplib.SMTP('smtp.gmail.com', 587)
              s.starttls()
              s.login(sender, password)
              s.send_message(msg)
              s.quit()
              print("‚úÖ Email sent")
          except Exception as e:
              print(f"‚ö†Ô∏è Email failed: {e}")
          EOF
          python email.py || true

      - name: Extract Metrics
        if: always()
        run: |
          [ ! -f "database/memory_v85.db" ] && echo '{"status":"no_db"}' > .github/run_history/metrics.json && exit 0
          python << 'EOF'
          import sqlite3, json, os
          try:
              conn = sqlite3.connect('database/memory_v85.db')
              c = conn.cursor()
              c.execute("SELECT name FROM sqlite_master WHERE type='table'")
              tables = [r[0] for r in c.fetchall()]
              
              metrics = {}
              if 'completed_trades' in tables:
                  c.execute("SELECT COUNT(*) FROM completed_trades")
                  metrics['trades'] = c.fetchone()[0]
              
              with open('.github/run_history/metrics.json', 'w') as f:
                  json.dump(metrics, f)
          except Exception as e:
              with open('.github/run_history/metrics.json', 'w') as f:
                  json.dump({'error': str(e)}, f)
          EOF

      - name: Commit Data Files
        if: success()
        run: |
          git add -f data/processed/*.pkl database/*.db rl_memory/* rl_models/* omega_state/* .github/run_history/* 2>/dev/null || true
          
          if git diff --cached --quiet; then
            echo "‚ÑπÔ∏è No changes"
          else
            RUN="${{ steps.data.outputs.run_number }}"
            MODE="${{ steps.mode.outputs.mode }}"
            TIME="$(date +'%Y-%m-%d %H:%M UTC')"
            CORRUPTED="${{ steps.corruption_check.outputs.need_full_run }}"
            
            if [ "$CORRUPTED" = "true" ]; then
              MSG="üîÑ Data Regenerated"
            elif [ "$MODE" = "weekend" ]; then
              MSG="üèñÔ∏è Weekend Learning"
            else
              MSG="üî¥ Live Trading"
            fi
            
            git commit -m "${MSG} - Run #${RUN} - ${TIME}" || true
            
            for i in {1..3}; do
              git push origin main && break || { 
                [ $i -lt 3 ] && sleep 3 && git pull --rebase origin main || exit 1
              }
            done
          fi

      - name: Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-${{ github.run_number }}
          path: |
            .github/run_history/*.json
            report.html
          retention-days: 7
          if-no-files-found: ignore

      - name: Summary
        if: always()
        run: |
          echo "=========================================="
          echo "üìä Run #${{ steps.data.outputs.run_number }}"
          echo "Mode: ${{ steps.mode.outputs.mode }}"
          echo "Data Status: ${{ steps.corruption_check.outputs.need_full_run == 'true' && 'Regenerated' || 'Valid' }}"
          echo "Time: $(date +'%Y-%m-%d %H:%M UTC')"
          [ "${{ steps.data.outputs.send_report }}" = "true" ] && echo "üìß Report: SENT" || echo "üìß Report: Pending"
          echo "‚úÖ Execution complete"
          echo "=========================================="
