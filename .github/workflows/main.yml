name: Multi-Source Learning System v20.3 - Pipedream Schedule + Enhanced Output

on:
  workflow_dispatch:  # Manual trigger
  schedule:
    # Weekdays - Every 2 hours at EVEN hours (12 runs/day) - SAME AS PIPEDREAM!
    - cron: '0 0,2,4,6,8,10,12,14,16,18,20,22 * * 1-5'
    
    # Weekends - Every 30 minutes (48 runs/day)
    - cron: '*/30 * * * 0,6'

jobs:
  run-learning-system:
    runs-on: ubuntu-latest
    timeout-minutes: 55
    concurrency:
      group: forex-learning-system
      cancel-in-progress: false

    env:
      FOREX_PAT: ${{ secrets.FOREX_PAT }}
      BROWSERLESS_TOKEN: ${{ secrets.BROWSERLESS_TOKEN }}
      ALPHA_VANTAGE_KEY: ${{ secrets.ALPHA_VANTAGE_KEY }}
      GMAIL_USER: ${{ secrets.GMAIL_USER }}
      GMAIL_APP_PASSWORD: ${{ secrets.GMAIL_APP_PASSWORD }}
      GIT_USER_NAME: "Forex AI Bot"
      GIT_USER_EMAIL: "nakatonabira3@gmail.com"
      PYTHONIOENCODING: utf-8
      PYTHONUNBUFFERED: 1
      GITHUB_ACTIONS: "true"
      SINGLE_RUN_MODE: "true"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0
          token: ${{ secrets.FOREX_PAT }}

      - name: Fix Submodules
        run: |
          [ -f ".gitmodules" ] && rm -f .gitmodules && git rm --cached -r forex-alpha-models 2>/dev/null || true
          find . -mindepth 2 -name ".git" -type d -exec rm -rf {} + 2>/dev/null || true

      - name: Detect Day Type and Schedule
        id: day_info
        run: |
          HOUR=$(date +%H)
          DAY=$(date +%u)
          DAY_NAME=$(date +'%A')
          
          echo "day=$DAY" >> $GITHUB_OUTPUT
          echo "day_name=$DAY_NAME" >> $GITHUB_OUTPUT
          echo "hour=$HOUR" >> $GITHUB_OUTPUT
          
          # Alpha Vantage runs ONLY at 00:00 UTC on weekdays (SAME AS PIPEDREAM!)
          if [ "$HOUR" = "00" ] && [ $DAY -ge 1 ] && [ $DAY -le 5 ]; then
            echo "is_alpha_run=true" >> $GITHUB_OUTPUT
            echo "SKIP_ALPHA_VANTAGE=false" >> $GITHUB_ENV
            echo "ðŸŒ™ ALPHA VANTAGE RUN (00:00 UTC = 3:00 AM EAT)"
          else
            echo "is_alpha_run=false" >> $GITHUB_OUTPUT
            echo "SKIP_ALPHA_VANTAGE=true" >> $GITHUB_ENV
            echo "âš¡ Regular run - Alpha Vantage SKIPPED"
          fi
          
          if [ $DAY -eq 6 ] || [ $DAY -eq 7 ]; then
            echo "is_weekend=true" >> $GITHUB_OUTPUT
            echo "ðŸ–ï¸ $DAY_NAME (Weekend - Intensive Learning)"
          else
            echo "is_weekend=false" >> $GITHUB_OUTPUT
            echo "ðŸ’¼ $DAY_NAME (Weekday - Live Trading)"
          fi
          
          echo ""
          echo "ðŸ“… Current: $DAY_NAME at $HOUR:00 UTC"
          echo "âš¡ Triggered by: GitHub Actions (Pipedream Schedule)"
          echo "ðŸ“Š Schedule: Weekdays EVEN hours (0,2,4,6,8,10,12,14,16,18,20,22) | Weekends every 30min"
          echo "ðŸ• Next weekday run: $(date -u -d "+2 hours" +'%H:00 UTC')"

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install Dependencies
        run: |
          pip install -q --no-cache-dir pandas numpy requests beautifulsoup4 scikit-learn \
            jupyter nbconvert nbformat ta yfinance mplfinance firebase-admin dropbox \
            pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm river scipy

      - name: Configure Git
        run: |
          git config --global user.name "Forex AI Bot"
          git config --global user.email "nakatonabira3@gmail.com"
          echo "https://${{ github.actor }}:${{ secrets.FOREX_PAT }}@github.com" > ~/.git-credentials
          git config --global credential.helper store

      - name: Check Data Status
        id: data
        run: |
          echo "ðŸ“Š Checking existing data files..."
          
          PICKLE_COUNT=$(find data/processed -name "*.pkl" -type f ! -name "*_model.pkl" 2>/dev/null | wc -l)
          echo "pickle_count=$PICKLE_COUNT" >> $GITHUB_OUTPUT
          
          if [ -f "rl_memory/experience_replay.json.gz" ]; then
            echo "rl_memory_exists=true" >> $GITHUB_OUTPUT
          else
            echo "rl_memory_exists=false" >> $GITHUB_OUTPUT
          fi
          
          if [ -f "learning_data/learning_outcomes.json" ]; then
            LEARNING_COUNT=$(python3 -c "import json; print(len(json.load(open('learning_data/learning_outcomes.json'))))" 2>/dev/null || echo "0")
            echo "learning_outcomes=$LEARNING_COUNT" >> $GITHUB_OUTPUT
          else
            echo "learning_outcomes=0" >> $GITHUB_OUTPUT
          fi
          
          [ -f ".github/run_history/run_counter.txt" ] && RUN_NUM=$(cat .github/run_history/run_counter.txt) || RUN_NUM=0
          RUN_NUM=$((RUN_NUM + 1))
          echo $RUN_NUM > .github/run_history/run_counter.txt
          echo "run_number=$RUN_NUM" >> $GITHUB_OUTPUT
          
          echo "ðŸ“Š Status:"
          echo "   Pickle files: $PICKLE_COUNT"
          echo "   RL Memory: $([ -f "rl_memory/experience_replay.json.gz" ] && echo "Yes" || echo "No")"
          echo "   Learning outcomes: $LEARNING_COUNT"
          echo "   Run number: $RUN_NUM"
          
          mkdir -p data/raw/yfinance data/raw/alpha_vantage data/processed data/quarantine \
                   database logs outputs omega_state rl_memory backups learning_data .github/run_history

      - name: Create Notebook Runner
        run: |
          cat > run_notebook.py << 'EOF'
          import nbformat
          from nbconvert.preprocessors import ExecutePreprocessor
          import sys
          import os
          from datetime import datetime
          
          # Load notebook
          print("Loading AI_Forex_Brain_2.ipynb...")
          with open('AI_Forex_Brain_2.ipynb', 'r') as f:
              nb = nbformat.read(f, as_version=4)
          
          # Check environment
          skip_av = os.environ.get('SKIP_ALPHA_VANTAGE', 'false').lower() == 'true'
          
          print("=" * 70)
          print("ðŸš€ STARTING FOREX AI BRAIN EXECUTION")
          print("=" * 70)
          print(f"ðŸ“… Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
          print(f"ðŸ”§ Alpha Vantage: {'SKIPPED â­ï¸' if skip_av else 'ACTIVE âœ…'}")
          print(f"ðŸ“Š Total cells: {len([c for c in nb.cells if c.cell_type == 'code'])}")
          print("=" * 70)
          print()
          
          # Execute notebook
          ep = ExecutePreprocessor(timeout=2400, kernel_name='python3', allow_errors=True)
          
          try:
              ep.preprocess(nb, {'metadata': {'path': '.'}})
              print("\n" + "=" * 70)
              print("âœ… EXECUTION COMPLETED SUCCESSFULLY")
              print("=" * 70)
              sys.exit(0)
          except Exception as e:
              print("\n" + "=" * 70)
              print("âŒ EXECUTION FAILED")
              print("=" * 70)
              print(f"Error: {str(e)}")
              print("=" * 70)
              sys.exit(1)
          EOF

      - name: Run Notebook Pipeline
        run: |
          echo "ðŸš€ STARTING EXECUTION"
          echo "===================================================="
          echo "âš¡ Triggered at: $(date +'%H:%M UTC')"
          echo "ðŸ“… Day: ${{ steps.day_info.outputs.day_name }}"
          echo "ðŸ”¢ Run: #${{ steps.data.outputs.run_number }}"
          echo "â° Schedule: EVEN hours (0,2,4,6,8,10,12,14,16,18,20,22)"
          echo ""
          python run_notebook.py
        timeout-minutes: 50

      - name: Extract Pipeline Metrics
        if: always()
        run: |
          cat > extract_metrics.py << 'EOF'
          import sqlite3
          import json
          import os
          from pathlib import Path
          
          metrics = {
              'status': 'no_data', 
              'version': 'v20.3-pipedream',
              'trigger': 'github_actions',
              'scheduler': 'pipedream_schedule',
              'hours': 'even (0,2,4,6,8,10,12,14,16,18,20,22)'
          }
          
          # Database metrics
          db_path = Path('database/memory_v85.db')
          if db_path.exists():
              try:
                  conn = sqlite3.connect(str(db_path))
                  c = conn.cursor()
                  c.execute("SELECT COUNT(*), SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) FROM completed_trades")
                  result = c.fetchone()
                  if result and result[0]:
                      metrics['pipeline_trades'] = result[0]
                      metrics['pipeline_wins'] = result[1] or 0
                      metrics['pipeline_win_rate'] = round((result[1] or 0) / result[0] * 100, 2)
                  conn.close()
                  metrics['status'] = 'active'
              except Exception as e:
                  metrics['db_error'] = str(e)[:100]
          
          # RL Memory
          rl_memory = Path('rl_memory/experience_replay.json.gz')
          if rl_memory.exists():
              metrics['rl_memory_size'] = rl_memory.stat().st_size
              metrics['rl_memory_mb'] = round(rl_memory.stat().st_size / 1024 / 1024, 2)
          
          # Learning stats
          stats_file = Path('rl_memory/learning_stats.json')
          if stats_file.exists():
              try:
                  with open(stats_file) as f:
                      rl_stats = json.load(f)
                  metrics['rl_trades'] = rl_stats.get('total_trades', 0)
                  metrics['rl_win_rate'] = round(rl_stats.get('win_rate', 0) * 100, 2)
                  metrics['pipeline_v6_learned'] = rl_stats.get('pipeline_v6_learned', 0)
                  metrics['live_validated'] = rl_stats.get('live_validated', 0)
              except Exception as e:
                  metrics['learning_stats_error'] = str(e)[:100]
          
          # Learning outcomes
          learning_db = Path('learning_data/learning_outcomes.json')
          if learning_db.exists():
              try:
                  with open(learning_db) as f:
                      outcomes = json.load(f)
                  metrics['learning_outcomes_count'] = len(outcomes)
                  
                  # Recent outcomes analysis
                  if outcomes:
                      recent = outcomes[-10:] if len(outcomes) >= 10 else outcomes
                      wins = sum(1 for o in recent if o.get('outcome') == 'win')
                      metrics['recent_win_rate'] = round(wins / len(recent) * 100, 2)
              except Exception as e:
                  metrics['outcomes_error'] = str(e)[:100]
          
          # Save metrics
          os.makedirs('.github/run_history', exist_ok=True)
          with open('.github/run_history/metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)
          
          # Print summary
          print("\n" + "="*70)
          print("ðŸ“Š EXTRACTED METRICS")
          print("="*70)
          for key, value in metrics.items():
              print(f"{key}: {value}")
          print("="*70)
          EOF
          python extract_metrics.py

      - name: Prepare Git for Commit
        if: success()
        run: |
          echo "ðŸ”„ Preparing git..."
          
          if ! git diff --quiet; then
            git stash push -m "Auto-stash - run ${{ steps.data.outputs.run_number }}"
            STASHED=true
          else
            STASHED=false
          fi
          
          git pull --rebase origin main || {
            git rebase --abort 2>/dev/null || true
            git pull --no-rebase origin main
          }
          
          if [ "$STASHED" = "true" ]; then
            git stash pop || echo "âš ï¸ Kept both versions"
          fi

      - name: Commit Everything
        if: success()
        run: |
          echo "ðŸ’¾ Committing changes..."
          git add -A
          
          if git diff --cached --quiet; then
            echo "â„¹ï¸ No changes to commit"
          else
            RUN="${{ steps.data.outputs.run_number }}"
            DAY="${{ steps.day_info.outputs.day_name }}"
            HOUR="${{ steps.day_info.outputs.hour }}"
            
            # Read metrics if available
            if [ -f ".github/run_history/metrics.json" ]; then
              TRADES=$(python3 -c "import json; d=json.load(open('.github/run_history/metrics.json')); print(d.get('learning_outcomes_count', 0))" 2>/dev/null || echo "0")
              git commit -m "ðŸŽ“ Run #${RUN} - ${DAY} ${HOUR}:00 UTC | ${TRADES} outcomes | Pipedream Schedule" || true
            else
              git commit -m "ðŸŽ“ Run #${RUN} - ${DAY} ${HOUR}:00 UTC | Pipedream Schedule" || true
            fi
            
            # Push with retry
            for i in {1..5}; do
              if git push origin main; then
                echo "âœ… Successfully pushed (attempt $i)"
                break
              else
                if [ $i -lt 5 ]; then
                  echo "âš ï¸ Push failed, retrying in 10s..."
                  sleep 10
                  git pull --rebase origin main || git pull --strategy-option=theirs origin main
                else
                  echo "âŒ Failed to push after 5 attempts"
                fi
              fi
            done
          fi

      - name: Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: learning-logs-run-${{ steps.data.outputs.run_number }}
          path: |
            .github/run_history/*.json
            logs/*.log
            outputs/*.json
            rl_memory/learning_stats.json
            learning_data/learning_outcomes.json
          retention-days: 7
          if-no-files-found: ignore

      - name: Final Summary
        if: always()
        run: |
          echo ""
          echo "=========================================="
          echo "ðŸŽ“ PIPEDREAM SCHEDULE RUN SUMMARY"
          echo "=========================================="
          echo "ðŸ“… Day: ${{ steps.day_info.outputs.day_name }}"
          echo "ðŸ”¢ Run: #${{ steps.data.outputs.run_number }}"
          echo "â° Time: $(date +'%Y-%m-%d %H:%M UTC')"
          echo "âš¡ Trigger: GitHub Actions (Pipedream Schedule)"
          echo "ðŸ“Š Schedule: Weekdays EVEN hours (0,2,4,6,8,10,12,14,16,18,20,22)"
          echo "            Weekends every 30min"
          echo ""
          echo "ðŸ“ˆ Data Status:"
          echo "   Pickle files: ${{ steps.data.outputs.pickle_count }}"
          echo "   RL Memory: ${{ steps.data.outputs.rl_memory_exists == 'true' && 'Yes' || 'No' }}"
          echo "   Learning outcomes: ${{ steps.data.outputs.learning_outcomes }}"
          echo ""
          
          if [ -f ".github/run_history/metrics.json" ]; then
            echo "ðŸ“Š Latest Metrics:"
            cat .github/run_history/metrics.json | python3 -m json.tool 2>/dev/null || cat .github/run_history/metrics.json
            echo ""
          fi
          
          echo "âœ… Run complete"
          echo "=========================================="
          echo ""
          echo "ðŸ’¡ Next scheduled runs (EAT = UTC+3):"
          CURRENT_HOUR=$(date -u +%H)
          if [ ${{ steps.day_info.outputs.day }} -ge 1 ] && [ ${{ steps.day_info.outputs.day }} -le 5 ]; then
            echo "   Weekday mode: Runs at EVEN hours"
            echo "   00:00 UTC = 3:00 AM EAT ðŸŒ™ (Alpha Vantage)"
            echo "   02:00 UTC = 5:00 AM EAT"
            echo "   04:00 UTC = 7:00 AM EAT"
            echo "   06:00 UTC = 9:00 AM EAT"
            echo "   08:00 UTC = 11:00 AM EAT"
            echo "   10:00 UTC = 1:00 PM EAT"
            echo "   12:00 UTC = 3:00 PM EAT"
            echo "   14:00 UTC = 5:00 PM EAT"
            echo "   16:00 UTC = 7:00 PM EAT"
            echo "   18:00 UTC = 9:00 PM EAT"
            echo "   20:00 UTC = 11:00 PM EAT"
            echo "   22:00 UTC = 1:00 AM EAT"
          else
            echo "   Weekend mode: Runs every 30 minutes"
          fi
          echo "=========================================="
