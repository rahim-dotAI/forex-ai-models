name: Smart Forex Brain Pipeline v18.3 - Notebook Compatible

on:
  workflow_dispatch:
  push:
    paths: ['colab_trigger.txt']
    branches: [main]
  schedule:
    # Main pipeline - Frequent runs with SKIP_ALPHA_VANTAGE
    - cron: '0 */2 * * 1-5'    # Weekdays: Every 2 hours (EXCLUDING midnight)
    - cron: '*/30 * * * 0,6'   # Weekends: Every 30 minutes
    
    # Alpha Vantage ONLY - Once daily at midnight UTC (separate job)
    - cron: '0 0 * * *'         # Daily midnight: Full run including Alpha Vantage

jobs:
  # Check if another run is in progress
  check-concurrent:
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
      is_alpha_run: ${{ steps.check.outputs.is_alpha_run }}
    steps:
      - name: Check for concurrent runs
        id: check
        run: |
          HOUR=$(date +%H)
          IS_ALPHA="false"
          
          # Detect if this is midnight Alpha Vantage run
          if [ "${{ github.event.schedule }}" = "0 0 * * *" ]; then
            IS_ALPHA="true"
            echo "is_alpha_run=true" >> $GITHUB_OUTPUT
            echo "ğŸŒ™ This is the MIDNIGHT Alpha Vantage run"
          else
            echo "is_alpha_run=false" >> $GITHUB_OUTPUT
            
            # If it's midnight hour but NOT the Alpha run, skip
            if [ "$HOUR" = "00" ]; then
              echo "â­ï¸ Skipping regular run at midnight to avoid conflict"
              echo "should_run=false" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          
          echo "should_run=true" >> $GITHUB_OUTPUT

  run-forex-brain:
    needs: check-concurrent
    if: needs.check-concurrent.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 55
    concurrency:
      group: forex-brain-pipeline
      cancel-in-progress: false

    env:
      FOREX_PAT: ${{ secrets.FOREX_PAT }}
      BROWSERLESS_TOKEN: ${{ secrets.BROWSERLESS_TOKEN }}
      ALPHA_VANTAGE_KEY: ${{ secrets.ALPHA_VANTAGE_KEY }}
      GMAIL_USER: ${{ secrets.GMAIL_USER }}
      GMAIL_APP_PASSWORD: ${{ secrets.GMAIL_APP_PASSWORD }}
      GIT_USER_NAME: "Forex AI Bot"
      GIT_USER_EMAIL: "nakatonabira3@gmail.com"
      PYTHONIOENCODING: utf-8
      PYTHONUNBUFFERED: 1
      GITHUB_ACTIONS: "true"
      SINGLE_RUN_MODE: "true"
      # Skip Alpha Vantage except at midnight
      SKIP_ALPHA_VANTAGE: ${{ needs.check-concurrent.outputs.is_alpha_run != 'true' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0
          token: ${{ secrets.FOREX_PAT }}

      - name: Fix Submodules
        run: |
          [ -f ".gitmodules" ] && rm -f .gitmodules && git rm --cached -r forex-alpha-models 2>/dev/null || true
          find . -mindepth 2 -name ".git" -type d -exec rm -rf {} + 2>/dev/null || true

      - name: Detect Day Type and Schedule
        id: day_info
        run: |
          DAY=$(date +%u)
          DAY_NAME=$(date +'%A')
          HOUR=$(date +%H)
          
          echo "day=$DAY" >> $GITHUB_OUTPUT
          echo "day_name=$DAY_NAME" >> $GITHUB_OUTPUT
          echo "hour=$HOUR" >> $GITHUB_OUTPUT
          
          # Detect if this is the midnight Alpha Vantage run
          if [ "${{ needs.check-concurrent.outputs.is_alpha_run }}" = "true" ]; then
            echo "is_alpha_run=true" >> $GITHUB_OUTPUT
            echo "ğŸŒ™ MIDNIGHT RUN - Alpha Vantage Data Fetch"
          else
            echo "is_alpha_run=false" >> $GITHUB_OUTPUT
            echo "â­ï¸ REGULAR RUN - Alpha Vantage Skipped"
          fi
          
          if [ $DAY -eq 6 ] || [ $DAY -eq 7 ]; then
            echo "is_weekend=true" >> $GITHUB_OUTPUT
            echo "ğŸ–ï¸ $DAY_NAME (Weekend - Learning Mode)"
          else
            echo "is_weekend=false" >> $GITHUB_OUTPUT
            echo "ğŸ’¼ $DAY_NAME (Weekday - Full Trading)"
          fi

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install Dependencies
        run: |
          pip install -q --no-cache-dir pandas numpy requests beautifulsoup4 scikit-learn \
            jupyter nbconvert nbformat ta yfinance mplfinance firebase-admin dropbox \
            pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm river scipy

      - name: Configure Git
        run: |
          git config --global user.name "Forex AI Bot"
          git config --global user.email "nakatonabira3@gmail.com"
          echo "https://${{ github.actor }}:${{ secrets.FOREX_PAT }}@github.com" > ~/.git-credentials
          git config --global credential.helper store

      - name: Check Data Status
        id: data
        run: |
          echo "ğŸ“Š Checking existing data files..."
          
          # Count pickle files
          PICKLE_COUNT=$(find data/processed -name "*.pkl" -type f ! -name "*_model.pkl" 2>/dev/null | wc -l)
          echo "pickle_count=$PICKLE_COUNT" >> $GITHUB_OUTPUT
          
          # Count RL experience replay
          if [ -f "rl_memory/experience_replay.json.gz" ]; then
            echo "rl_memory_exists=true" >> $GITHUB_OUTPUT
          else
            echo "rl_memory_exists=false" >> $GITHUB_OUTPUT
          fi
          
          # Run counter
          [ -f ".github/run_history/run_counter.txt" ] && RUN_NUM=$(cat .github/run_history/run_counter.txt) || RUN_NUM=0
          RUN_NUM=$((RUN_NUM + 1))
          echo $RUN_NUM > .github/run_history/run_counter.txt
          echo "run_number=$RUN_NUM" >> $GITHUB_OUTPUT
          
          # Email report check (every 10 runs)
          [ $((RUN_NUM % 10)) -eq 0 ] && echo "send_report=true" >> $GITHUB_OUTPUT || echo "send_report=false" >> $GITHUB_OUTPUT
          
          echo "ğŸ“Š Status:"
          echo "   Pickle files: $PICKLE_COUNT"
          echo "   RL Memory exists: $([ -f "rl_memory/experience_replay.json.gz" ] && echo "Yes" || echo "No")"
          echo "   Run number: $RUN_NUM"
          echo "   Alpha Vantage: ${{ steps.day_info.outputs.is_alpha_run == 'true' && 'ACTIVE' || 'SKIPPED' }}"
          
          mkdir -p data/raw/yfinance data/raw/alpha_vantage data/processed data/quarantine \
                   database logs outputs omega_state rl_memory backups .github/run_history

      - name: Verify Notebook
        run: |
          if [ ! -f "AI_Forex_Brain_2.ipynb" ]; then
            echo "âŒ ERROR: AI_Forex_Brain_2.ipynb not found!"
            ls -la *.ipynb 2>/dev/null || echo "No .ipynb files found"
            exit 1
          fi
          
          echo "âœ… Found AI_Forex_Brain_2.ipynb"
          
          # Check notebook structure
          CELLS=$(python -c "
          import nbformat
          with open('AI_Forex_Brain_2.ipynb', 'r') as f:
              nb = nbformat.read(f, 4)
          code_cells = sum(1 for c in nb.cells if c.cell_type=='code')
          print(code_cells)
          " 2>/dev/null || echo "0")
          
          echo "ğŸ“Š Code cells found: $CELLS"
          echo "â„¹ï¸  Pipeline: API Keys â†’ Environment â†’ GitHub Sync â†’ Alpha Vantage â†’ YFinance â†’ CSV Combiner â†’ ML Pipeline â†’ Trade Beacon"

      - name: Create Enhanced Notebook Executor
        run: |
          cat > run_full.py << 'EOFPYTHON'
          import nbformat, sys, time, json, os, re
          from nbconvert.preprocessors import ExecutePreprocessor
          from datetime import datetime
          
          class SmartExecutor(ExecutePreprocessor):
              def __init__(self, *args, **kwargs):
                  super().__init__(*args, **kwargs)
                  self.cell_count = 0
                  self.start_time = None
                  self.successful_cells = 0
                  self.failed_cells = 0
                  self.critical_errors = []
                  self.stage_timings = {}
                  self.current_stage = "Unknown"
              
              def preprocess(self, nb, resources=None, km=None):
                  code_cells = sum(1 for c in nb.cells if c.cell_type=='code')
                  print(f"ğŸ“Š Processing {len(nb.cells)} cells ({code_cells} code cells)...")
                  print(f"ğŸ–¥ï¸  Environment: GitHub Actions")
                  print(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
                  
                  skip_av = os.environ.get('SKIP_ALPHA_VANTAGE', 'false').lower() == 'true'
                  if skip_av:
                      print(f"â­ï¸  Alpha Vantage: SKIPPED (uses existing data)")
                  else:
                      print(f"ğŸ”½ Alpha Vantage: ACTIVE (fetching fresh data)")
                  
                  self.start_time = time.time()
                  return super().preprocess(nb, resources, km)
              
              def detect_stage(self, cell_source):
                  source_lower = cell_source.lower()
                  if 'api_keys' in source_lower or 'alpha_vantage_key' in source_lower:
                      return "1ï¸âƒ£ API Keys"
                  elif 'environment detection' in source_lower or 'in_colab' in source_lower:
                      return "2ï¸âƒ£ Environment"
                  elif 'github sync' in source_lower or 'repo_folder' in source_lower:
                      return "3ï¸âƒ£ GitHub Sync"
                  elif 'alpha vantage' in source_lower and 'fetcher' in source_lower:
                      return "4ï¸âƒ£ Alpha Vantage"
                  elif 'yfinance' in source_lower and 'fetcher' in source_lower:
                      return "5ï¸âƒ£ YFinance"
                  elif 'combiner' in source_lower or 'indicator' in source_lower:
                      return "6ï¸âƒ£ CSV Combiner"
                  elif 'ultra-persistent' in source_lower or 'pipeline' in source_lower:
                      return "7ï¸âƒ£ ML Pipeline"
                  elif 'trade beacon' in source_lower or 'deep q-learning' in source_lower:
                      return "8ï¸âƒ£ RL Agent"
                  return self.current_stage
              
              def preprocess_cell(self, cell, resources, idx):
                  if cell.cell_type != 'code':
                      return cell, resources
                  
                  self.cell_count += 1
                  
                  new_stage = self.detect_stage(cell.source)
                  if new_stage != self.current_stage:
                      stage_start = time.time()
                      if self.current_stage in self.stage_timings:
                          self.stage_timings[self.current_stage]['duration'] = stage_start - self.stage_timings[self.current_stage]['start']
                      self.current_stage = new_stage
                      self.stage_timings[new_stage] = {'start': stage_start, 'duration': 0}
                      print(f"\n{'='*70}")
                      print(f"ğŸš€ STAGE: {new_stage}")
                      print(f"{'='*70}")
                  
                  elapsed = time.time() - self.start_time
                  print(f"\nâ³ Cell {self.cell_count} | {self.current_stage} | {elapsed:.0f}s elapsed")
                  
                  try:
                      cell, resources = super().preprocess_cell(cell, resources, idx)
                      self.successful_cells += 1
                      
                      if cell.outputs:
                          for output in cell.outputs:
                              if output.output_type == 'stream':
                                  text = re.sub(r'\x1b\[[0-9;]*m', '', output.text)
                                  lines = text.strip().split('\n')
                                  
                                  important_markers = [
                                      'âœ…', 'âš ï¸', 'âŒ', 'ğŸ’°', 'ğŸ§ ', 'ğŸ’¾', 'ğŸ“Š', 'ğŸ–¥ï¸', 'ğŸ”„', 'â­ï¸',
                                      'COMPLETE', 'ERROR', 'FAILED', 'SUCCESS', 'SKIPPED',
                                      'Win Rate', 'P&L', 'Iteration', 'Mode:', 'Total', 'Average',
                                      'Loaded', 'Saved', 'Updated', 'Found', 'Processing',
                                      'Quality', 'Trades', 'Epsilon', 'Experience Replay',
                                      'Pipeline Stats', 'Database', 'Q-Network', 'Backtest',
                                      'API calls', 'Daily API usage', 'Alpha Vantage'
                                  ]
                                  
                                  important_lines = [l for l in lines if any(marker in l for marker in important_markers)]
                                  
                                  if important_lines:
                                      for line in important_lines[:15]:
                                          print(f"  {line}")
                              
                              elif output.output_type == 'error':
                                  error_msg = f"{output.ename}: {output.evalue}"
                                  print(f"  âŒ Error: {error_msg}")
                                  self.critical_errors.append({
                                      'cell': self.cell_count,
                                      'stage': self.current_stage,
                                      'error': error_msg
                                  })
                  
                  except Exception as e:
                      self.failed_cells += 1
                      error_summary = str(e)[:200]
                      print(f"  âš ï¸ Cell {self.cell_count} error: {error_summary}")
                      print(f"  ğŸ”„ Continuing to next cell...")
                      self.critical_errors.append({
                          'cell': self.cell_count,
                          'stage': self.current_stage,
                          'error': error_summary
                      })
                  
                  return cell, resources
          
          if not os.path.exists('AI_Forex_Brain_2.ipynb'):
              print("âŒ ERROR: AI_Forex_Brain_2.ipynb not found!")
              sys.exit(1)
          
          with open('AI_Forex_Brain_2.ipynb', 'r') as f:
              nb = nbformat.read(f, as_version=4)
          
          print("="*70)
          print("ğŸš€ FOREX AI BRAIN - FULL PIPELINE EXECUTION")
          print("="*70)
          
          ep = SmartExecutor(timeout=2400, kernel_name='python3', allow_errors=True)
          start = time.time()
          
          try:
              ep.preprocess(nb, {'metadata': {'path': '.'}})
              duration = time.time() - start
              
              print(f"\n{'='*70}")
              print(f"âœ… EXECUTION COMPLETED")
              print(f"{'='*70}")
              print(f"â±ï¸  Duration: {duration:.1f}s ({duration/60:.1f} min)")
              print(f"ğŸ“Š Cells: {ep.cell_count} total")
              print(f"âœ… Success: {ep.successful_cells}")
              print(f"âš ï¸  Failed: {ep.failed_cells}")
              
              if ep.stage_timings:
                  print(f"\nğŸ“Š Stage Timings:")
                  for stage, timing in ep.stage_timings.items():
                      duration_val = timing.get('duration', 0)
                      if duration_val > 0:
                          print(f"  {stage}: {duration_val:.1f}s")
              
              if ep.critical_errors:
                  print(f"\nâš ï¸  Critical Errors ({len(ep.critical_errors)}):")
                  for err in ep.critical_errors[:5]:
                      print(f"  Cell {err['cell']} ({err['stage']}): {err['error'][:100]}")
              
              print(f"{'='*70}\n")
              
              report = {
                  'timestamp': datetime.now().isoformat(), 
                  'mode': 'full_pipeline', 
                  'duration': duration,
                  'cells_executed': ep.cell_count,
                  'successful': ep.successful_cells,
                  'failed': ep.failed_cells,
                  'status': 'success',
                  'stage_timings': {k: v['duration'] for k, v in ep.stage_timings.items() if 'duration' in v},
                  'critical_errors': len(ep.critical_errors),
                  'alpha_vantage_active': os.environ.get('SKIP_ALPHA_VANTAGE', 'false').lower() != 'true'
              }
              
          except Exception as e:
              duration = time.time() - start
              
              print(f"\n{'='*70}")
              print(f"âš ï¸ EXECUTION COMPLETED WITH ERRORS")
              print(f"{'='*70}")
              print(f"âŒ Error: {type(e).__name__}: {str(e)[:200]}")
              print(f"â±ï¸  Duration: {duration:.1f}s ({duration/60:.1f} min)")
              print(f"âœ… Successful cells: {ep.successful_cells}")
              print(f"âš ï¸  Failed cells: {ep.failed_cells}")
              print(f"{'='*70}\n")
              
              report = {
                  'timestamp': datetime.now().isoformat(), 
                  'mode': 'full_pipeline', 
                  'duration': duration,
                  'cells_executed': ep.cell_count,
                  'successful': ep.successful_cells,
                  'failed': ep.failed_cells,
                  'status': 'completed_with_errors',
                  'error': f"{type(e).__name__}: {str(e)[:200]}",
                  'stage_timings': {k: v['duration'] for k, v in ep.stage_timings.items() if 'duration' in v},
                  'critical_errors': len(ep.critical_errors),
                  'alpha_vantage_active': os.environ.get('SKIP_ALPHA_VANTAGE', 'false').lower() != 'true'
              }
          
          os.makedirs('.github/run_history', exist_ok=True)
          with open('.github/run_history/latest_run.json', 'w') as f: 
              json.dump(report, f, indent=2)
          
          print("âœ… Full notebook execution completed")
          print("ğŸ“ Report saved to .github/run_history/latest_run.json")
          EOFPYTHON

      - name: Run Full Notebook Pipeline
        run: |
          echo "ğŸš€ STARTING FOREX AI BRAIN PIPELINE"
          echo "=================================="
          echo "ğŸ“… Day: ${{ steps.day_info.outputs.day_name }}"
          echo "ğŸ”¢ Run: #${{ steps.data.outputs.run_number }}"
          echo "â° Hour: ${{ steps.day_info.outputs.hour }}:00 UTC"
          echo "ğŸ““ Notebook: AI_Forex_Brain_2.ipynb"
          echo "ğŸ–¥ï¸  Environment: GitHub Actions"
          echo ""
          
          if [ "${{ steps.day_info.outputs.is_alpha_run }}" = "true" ]; then
            echo "ğŸŒ™ MIDNIGHT RUN - Full pipeline including Alpha Vantage"
            echo "ğŸ“Š Alpha Vantage: âœ… ACTIVE (4 API calls)"
          else
            echo "âš¡ REGULAR RUN - Alpha Vantage skipped"
            echo "ğŸ“Š Alpha Vantage: â­ï¸  SKIPPED (uses existing data)"
          fi
          
          echo ""
          echo "ğŸ“‹ Pipeline Stages:"
          echo "  1ï¸âƒ£  API Keys Configuration"
          echo "  2ï¸âƒ£  Environment Setup"
          echo "  3ï¸âƒ£  GitHub Repository Sync"
          
          if [ "${{ steps.day_info.outputs.is_alpha_run }}" = "true" ]; then
            echo "  4ï¸âƒ£  Alpha Vantage Data Fetch âœ…"
          else
            echo "  4ï¸âƒ£  Alpha Vantage Data Fetch â­ï¸  (skipped)"
          fi
          
          echo "  5ï¸âƒ£  YFinance Data Fetch"
          echo "  6ï¸âƒ£  CSV Combiner & Indicators"
          echo "  7ï¸âƒ£  ML Pipeline (Database Learning)"
          echo "  8ï¸âƒ£  RL Agent (Trade Beacon v20.1)"
          echo ""
          echo "ğŸ’¡ API Optimization:"
          echo "   Daily API usage: 4 calls (16% of 25 limit)"
          echo "   Savings: 44 calls/day vs hourly fetching"
          echo ""
          echo "â±ï¸  Estimated time: 30-50 minutes"
          echo "=================================="
          echo ""
          python run_full.py
        timeout-minutes: 50

      - name: Extract Pipeline Metrics
        if: always()
        run: |
          cat > extract_metrics.py << 'EOFPYTHON'
          import sqlite3, json, os
          from pathlib import Path
          
          metrics = {'status': 'no_data'}
          
          # Check database
          db_path = Path('database/memory_v85.db')
          if db_path.exists():
              try:
                  conn = sqlite3.connect(str(db_path))
                  c = conn.cursor()
                  
                  # Pipeline stats
                  c.execute("SELECT COUNT(*), SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) FROM completed_trades")
                  result = c.fetchone()
                  if result and result[0]:
                      metrics['pipeline_trades'] = result[0]
                      metrics['pipeline_wins'] = result[1] or 0
                      metrics['pipeline_win_rate'] = (result[1] / result[0] * 100) if result[0] else 0
                  
                  conn.close()
              except Exception as e:
                  metrics['db_error'] = str(e)
          
          # Check RL memory
          rl_memory = Path('rl_memory/experience_replay.json.gz')
          if rl_memory.exists():
              metrics['rl_memory_size'] = rl_memory.stat().st_size
          
          # Check learning stats
          stats_file = Path('rl_memory/learning_stats.json')
          if stats_file.exists():
              try:
                  with open(stats_file) as f:
                      rl_stats = json.load(f)
                  metrics['rl_trades'] = rl_stats.get('total_trades', 0)
                  metrics['rl_win_rate'] = rl_stats.get('win_rate', 0) * 100
                  metrics['rl_pnl'] = rl_stats.get('total_pnl', 0)
              except:
                  pass
          
          # Check Trade Beacon signals
          beacon_file = Path('outputs/omega_signals.json')
          if beacon_file.exists():
              try:
                  with open(beacon_file) as f:
                      beacon_data = json.load(f)
                  metrics['beacon_iteration'] = beacon_data.get('iteration', 0)
                  metrics['beacon_mode'] = beacon_data.get('mode', 'unknown')
                  metrics['active_signals'] = sum(1 for s in beacon_data.get('signals', {}).values() if s.get('direction') != 'HOLD')
              except:
                  pass
          
          # Add Alpha Vantage optimization info
          metrics['alpha_vantage_optimized'] = True
          metrics['daily_api_calls'] = 4
          metrics['api_usage_percent'] = 16
          
          # Save metrics
          os.makedirs('.github/run_history', exist_ok=True)
          with open('.github/run_history/metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)
          
          print("âœ… Metrics extracted:")
          for key, value in metrics.items():
              print(f"  {key}: {value}")
          EOFPYTHON
          python extract_metrics.py

      - name: Prepare Git for Commit
        if: success()
        run: |
          echo "ğŸ”„ Preparing git for commit..."
          
          # Check if there are unstaged changes and stash them if needed
          if ! git diff --quiet; then
            echo "ğŸ“¦ Stashing unstaged changes..."
            git stash push -m "Auto-stashed changes before pull - run ${{ steps.data.outputs.run_number }}"
            STASHED=true
          else
            STASHED=false
          fi
          
          # Pull latest changes
          echo "ğŸ”„ Pulling latest changes..."
          git pull --rebase origin main || {
            echo "âš ï¸ Rebase failed, using merge strategy..."
            git rebase --abort 2>/dev/null || true
            git pull --no-rebase origin main
          }
          
          # Restore stashed changes if any
          if [ "$STASHED" = "true" ]; then
            echo "ğŸ“¦ Restoring stashed changes..."
            git stash pop || {
              echo "âš ï¸ Stash pop had conflicts, keeping both versions"
              # Continue with both versions
            }
          fi
          
          echo "âœ… Repository ready for commit"

      - name: Commit Everything
        if: success()
        run: |
          echo "ğŸ’¾ COMMITTING EVERYTHING..."
          echo "=================================="
          
          # Add ALL files that exist in the repo
          git add -A
          
          # Check what's being committed
          echo "ğŸ“ Files to be committed:"
          git status --porcelain | head -20
          echo "..."
          
          if git diff --cached --quiet; then
            echo "â„¹ï¸ No changes to commit"
          else
            RUN="${{ steps.data.outputs.run_number }}"
            DAY="${{ steps.day_info.outputs.day_name }}"
            IS_WEEKEND="${{ steps.day_info.outputs.is_weekend }}"
            IS_ALPHA="${{ steps.day_info.outputs.is_alpha_run }}"
            
            # Get metrics for commit message
            if [ -f "rl_memory/learning_stats.json" ]; then
              RL_TRADES=$(python -c "import json; d=json.load(open('rl_memory/learning_stats.json')); print(d.get('total_trades', 0))" 2>/dev/null || echo "0")
              RL_WINRATE=$(python -c "import json; d=json.load(open('rl_memory/learning_stats.json')); print(round(d.get('win_rate', 0)*100, 1))" 2>/dev/null || echo "0")
            else
              RL_TRADES="0"
              RL_WINRATE="0"
            fi
            
            if [ -f "database/memory_v85.db" ]; then
              DB_SIZE=$(stat -f%z database/memory_v85.db 2>/dev/null || stat -c%s database/memory_v85.db 2>/dev/null || echo "0")
              DB_SIZE_MB=$((DB_SIZE / 1024 / 1024))
            else
              DB_SIZE_MB="0"
            fi
            
            # Build comprehensive commit message
            if [ "$IS_ALPHA" = "true" ]; then
              PREFIX="ğŸŒ™ MIDNIGHT"
            elif [ "$IS_WEEKEND" = "true" ]; then
              PREFIX="ğŸ–ï¸ WEEKEND"
            else
              PREFIX="ğŸ”´ LIVE"
            fi
            
            COMMIT_MSG="$PREFIX Run #${RUN} - ${DAY}
            
ğŸ“Š RL Agent: ${RL_TRADES} trades, ${RL_WINRATE}% WR
ğŸ’¾ Database: ${DB_SIZE_MB}MB
ğŸ”„ Alpha Vantage: ${IS_ALPHA}
ğŸ–ï¸ Weekend Mode: ${IS_WEEKEND}
â° $(date +'%Y-%m-%d %H:%M UTC')

ğŸ¤– Full pipeline execution completed
ğŸ“ˆ All data files committed including:
   â€¢ Raw price data (YFinance + Alpha Vantage)
   â€¢ Processed features and indicators  
   â€¢ RL experience replay and weights
   â€¢ Trade history database
   â€¢ Trading signals and outputs
   â€¢ Execution logs and metrics"

            echo "ğŸ“ Commit message:"
            echo "$COMMIT_MSG"
            echo ""
            
            git commit -m "$COMMIT_MSG" || {
              echo "âš ï¸ Commit failed, trying with simpler message..."
              git commit -m "$PREFIX Run #${RUN} - ${DAY} | RL: ${RL_TRADES} trades" || true
            }
            
            # Push with robust retry logic
            for i in {1..5}; do
              echo "ğŸš€ Pushing to GitHub (attempt $i/5)..."
              if git push origin main; then
                echo "âœ… Successfully committed and pushed ALL changes"
                break
              else
                if [ $i -lt 5 ]; then
                  echo "âš ï¸ Push failed, pulling latest and retrying in 10s..."
                  sleep 10
                  git pull --rebase origin main || git pull --strategy-option=theirs origin main
                else
                  echo "âŒ Failed to push after 5 attempts"
                  echo "ğŸ’¡ Changes are committed locally but not pushed to remote"
                fi
              fi
            done
          fi

      - name: Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-${{ github.run_number }}
          path: |
            .github/run_history/*.json
            logs/*.log
            outputs/*.json
            rl_memory/learning_stats.json
          retention-days: 7
          if-no-files-found: ignore

      - name: Final Summary
        if: always()
        run: |
          echo "=========================================="
          echo "ğŸ§  FOREX AI BRAIN PIPELINE SUMMARY"
          echo "=========================================="
          echo "ğŸ“… Day: ${{ steps.day_info.outputs.day_name }}"
          echo "ğŸ”¢ Run: #${{ steps.data.outputs.run_number }}"
          echo "â° Time: $(date +'%Y-%m-%d %H:%M UTC')"
          echo "ğŸ–¥ï¸  Env: GitHub Actions"
          echo ""
          
          if [ "${{ steps.day_info.outputs.is_alpha_run }}" = "true" ]; then
            echo "ğŸŒ™ Alpha Vantage: ACTIVE (4 API calls made)"
          else
            echo "â­ï¸  Alpha Vantage: SKIPPED (next run at midnight)"
          fi
          
          echo ""
          echo "ğŸ“‚ Data Files:"
          echo "   Processed pickles: ${{ steps.data.outputs.pickle_count }}"
          echo "   RL Memory: ${{ steps.data.outputs.rl_memory_exists }}"
          echo ""
          
          if [ -f ".github/run_history/metrics.json" ]; then
            echo "ğŸ“Š Latest Metrics:"
            cat .github/run_history/metrics.json | grep -E '"(rl_trades|rl_win_rate|pipeline_trades|beacon_iteration|active_signals)"' | head -10 || true
            echo ""
          fi
          
          if [ -f ".github/run_history/latest_run.json" ]; then
            echo "â±ï¸  Execution:"
            cat .github/run_history/latest_run.json | grep -E '"(status|duration|successful|failed|alpha_vantage_active)"' || true
            echo ""
          fi
          
          echo "ğŸ’¾ Commit Status: âœ… EVERYTHING COMMITTED"
          echo "   â€¢ Raw data files"
          echo "   â€¢ Processed features" 
          echo "   â€¢ RL learning progress"
          echo "   â€¢ Trading signals"
          echo "   â€¢ Logs and metrics"
          echo ""
          
          echo "ğŸ’¡ API Optimization:"
          echo "   Daily usage: 4/25 calls (16%)"
          echo "   Savings: 44 calls/day"
          echo ""
          
          echo "ğŸ“¦ Artifacts: Uploaded for this run"
          echo ""
          
          echo "âœ… Pipeline execution complete - ALL DATA COMMITTED"
          echo "=========================================="
