name: Multi-Source Learning System v20.2 - Pipeline v6.1 + Trade Beacon v20.2

on:
  workflow_dispatch:
  push:
    paths: ['colab_trigger.txt']
    branches: [main]
  schedule:
    # FIXED: Every 2 hours at EVEN hours (0,2,4,6,8,10,12,14,16,18,20,22)
    # Old schedule had ODD hours every 2 hours which caused faster runs
    - cron: '0 0,2,4,6,8,10,12,14,16,18,20,22 * * 1-5'  # Weekdays every 2 hours
    # Weekend schedule - Every 2 hours for backtest learning
    - cron: '0 0,2,4,6,8,10,12,14,16,18,20,22 * * 0,6'  # Weekends every 2 hours

jobs:
  check-concurrent:
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
      is_alpha_run: ${{ steps.check.outputs.is_alpha_run }}
    steps:
      - name: Check for concurrent runs
        id: check
        run: |
          HOUR=$(date +%H)
          DAY=$(date +%u)
          IS_ALPHA="false"
          
          # Alpha Vantage runs at 00:00 UTC daily (midnight)
          # Changed from 01:00 to 00:00 to align with 2-hour schedule
          if [ "$HOUR" = "00" ]; then
            IS_ALPHA="true"
            echo "is_alpha_run=true" >> $GITHUB_OUTPUT
            echo "üåô ALPHA VANTAGE RUN (00:00 UTC daily)"
          else
            echo "is_alpha_run=false" >> $GITHUB_OUTPUT
            echo "‚ö° Regular run - Alpha Vantage SKIPPED"
          fi
          
          echo "should_run=true" >> $GITHUB_OUTPUT
          echo "‚úÖ Run approved - Hour: $HOUR, Day: $DAY, Alpha: $IS_ALPHA"

  run-learning-system:
    needs: check-concurrent
    if: needs.check-concurrent.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 55
    concurrency:
      group: forex-learning-system
      cancel-in-progress: false

    env:
      FOREX_PAT: ${{ secrets.FOREX_PAT }}
      BROWSERLESS_TOKEN: ${{ secrets.BROWSERLESS_TOKEN }}
      ALPHA_VANTAGE_KEY: ${{ secrets.ALPHA_VANTAGE_KEY }}
      GMAIL_USER: ${{ secrets.GMAIL_USER }}
      GMAIL_APP_PASSWORD: ${{ secrets.GMAIL_APP_PASSWORD }}
      GIT_USER_NAME: "Forex AI Bot"
      GIT_USER_EMAIL: "nakatonabira3@gmail.com"
      PYTHONIOENCODING: utf-8
      PYTHONUNBUFFERED: 1
      GITHUB_ACTIONS: "true"
      SINGLE_RUN_MODE: "true"
      SKIP_ALPHA_VANTAGE: ${{ needs.check-concurrent.outputs.is_alpha_run != 'true' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0
          token: ${{ secrets.FOREX_PAT }}

      - name: Fix Submodules
        run: |
          [ -f ".gitmodules" ] && rm -f .gitmodules && git rm --cached -r forex-alpha-models 2>/dev/null || true
          find . -mindepth 2 -name ".git" -type d -exec rm -rf {} + 2>/dev/null || true

      - name: Detect Day Type and Schedule
        id: day_info
        run: |
          DAY=$(date +%u)
          DAY_NAME=$(date +'%A')
          HOUR=$(date +%H)
          
          echo "day=$DAY" >> $GITHUB_OUTPUT
          echo "day_name=$DAY_NAME" >> $GITHUB_OUTPUT
          echo "hour=$HOUR" >> $GITHUB_OUTPUT
          
          if [ "${{ needs.check-concurrent.outputs.is_alpha_run }}" = "true" ]; then
            echo "is_alpha_run=true" >> $GITHUB_OUTPUT
            echo "üåô ALPHA VANTAGE RUN - Fresh Daily Data (00:00 UTC)"
          else
            echo "is_alpha_run=false" >> $GITHUB_OUTPUT
            echo "‚è≠Ô∏è REGULAR RUN - Alpha Vantage Skipped"
          fi
          
          if [ $DAY -eq 6 ] || [ $DAY -eq 7 ]; then
            echo "is_weekend=true" >> $GITHUB_OUTPUT
            echo "üèñÔ∏è $DAY_NAME (Weekend - Intensive Learning Mode)"
          else
            echo "is_weekend=false" >> $GITHUB_OUTPUT
            echo "üíº $DAY_NAME (Weekday - Live Trading Mode)"
          fi
          
          echo ""
          echo "üìÖ Schedule Info:"
          echo "   Current: $DAY_NAME at $HOUR:00 UTC"
          echo "   Frequency: Every 2 hours (00,02,04,06,08,10,12,14,16,18,20,22)"
          echo "   Next run: In 2 hours at $(date -d '+2 hours' +'%H:00 UTC' 2>/dev/null || echo 'N/A')"

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install Dependencies
        run: |
          pip install -q --no-cache-dir pandas numpy requests beautifulsoup4 scikit-learn \
            jupyter nbconvert nbformat ta yfinance mplfinance firebase-admin dropbox \
            pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm river scipy

      - name: Configure Git
        run: |
          git config --global user.name "Forex AI Bot"
          git config --global user.email "nakatonabira3@gmail.com"
          echo "https://${{ github.actor }}:${{ secrets.FOREX_PAT }}@github.com" > ~/.git-credentials
          git config --global credential.helper store

      - name: Check Data Status
        id: data
        run: |
          echo "üìä Checking existing data files..."
          
          PICKLE_COUNT=$(find data/processed -name "*.pkl" -type f ! -name "*_model.pkl" 2>/dev/null | wc -l)
          echo "pickle_count=$PICKLE_COUNT" >> $GITHUB_OUTPUT
          
          if [ -f "rl_memory/experience_replay.json.gz" ]; then
            echo "rl_memory_exists=true" >> $GITHUB_OUTPUT
          else
            echo "rl_memory_exists=false" >> $GITHUB_OUTPUT
          fi
          
          # Check learning database
          if [ -f "learning_data/learning_outcomes.json" ]; then
            LEARNING_COUNT=$(python3 -c "import json; print(len(json.load(open('learning_data/learning_outcomes.json'))))" 2>/dev/null || echo "0")
            echo "learning_outcomes=$LEARNING_COUNT" >> $GITHUB_OUTPUT
          else
            echo "learning_outcomes=0" >> $GITHUB_OUTPUT
          fi
          
          [ -f ".github/run_history/run_counter.txt" ] && RUN_NUM=$(cat .github/run_history/run_counter.txt) || RUN_NUM=0
          RUN_NUM=$((RUN_NUM + 1))
          echo $RUN_NUM > .github/run_history/run_counter.txt
          echo "run_number=$RUN_NUM" >> $GITHUB_OUTPUT
          
          [ $((RUN_NUM % 10)) -eq 0 ] && echo "send_report=true" >> $GITHUB_OUTPUT || echo "send_report=false" >> $GITHUB_OUTPUT
          
          echo "üìä Status:"
          echo "   Pickle files: $PICKLE_COUNT"
          echo "   RL Memory exists: $([ -f "rl_memory/experience_replay.json.gz" ] && echo "Yes" || echo "No")"
          echo "   Learning outcomes: $LEARNING_COUNT"
          echo "   Run number: $RUN_NUM"
          echo "   Alpha Vantage: ${{ steps.day_info.outputs.is_alpha_run == 'true' && 'ACTIVE' || 'SKIPPED' }}"
          
          mkdir -p data/raw/yfinance data/raw/alpha_vantage data/processed data/quarantine \
                   database logs outputs omega_state rl_memory backups learning_data .github/run_history

      - name: Verify Notebook
        run: |
          if [ ! -f "AI_Forex_Brain_2.ipynb" ]; then
            echo "‚ùå ERROR: AI_Forex_Brain_2.ipynb not found!"
            ls -la *.ipynb 2>/dev/null || echo "No .ipynb files found"
            exit 1
          fi
          
          echo "‚úÖ Found AI_Forex_Brain_2.ipynb"
          
          CELLS=$(python -c "
          import nbformat
          with open('AI_Forex_Brain_2.ipynb', 'r') as f:
              nb = nbformat.read(f, 4)
          code_cells = sum(1 for c in nb.cells if c.cell_type=='code')
          print(code_cells)
          " 2>/dev/null || echo "0")
          
          echo "üìä Code cells found: $CELLS"
          echo "‚ÑπÔ∏è  Pipeline: API Keys ‚Üí Environment ‚Üí GitHub Sync ‚Üí Alpha Vantage ‚Üí YFinance ‚Üí CSV Combiner ‚Üí Pipeline v6.1 ‚Üí Trade Beacon v20.2"

      - name: Create Enhanced Notebook Executor
        run: |
          cat > run_full.py << 'EOF'
          import nbformat
          import sys
          import time
          import json
          import os
          import re
          from nbconvert.preprocessors import ExecutePreprocessor
          from datetime import datetime
          
          class SmartExecutor(ExecutePreprocessor):
              def __init__(self, *args, **kwargs):
                  super().__init__(*args, **kwargs)
                  self.cell_count = 0
                  self.start_time = None
                  self.successful_cells = 0
                  self.failed_cells = 0
                  self.critical_errors = []
                  self.stage_timings = {}
                  self.current_stage = "Unknown"
              
              def preprocess(self, nb, resources=None, km=None):
                  code_cells = sum(1 for c in nb.cells if c.cell_type == 'code')
                  print("Processing cells...")
                  print("Environment: GitHub Actions")
                  
                  skip_av = os.environ.get('SKIP_ALPHA_VANTAGE', 'false').lower() == 'true'
                  if skip_av:
                      print("Alpha Vantage: SKIPPED")
                  else:
                      print("Alpha Vantage: ACTIVE")
                  
                  self.start_time = time.time()
                  return super().preprocess(nb, resources, km)
              
              def detect_stage(self, cell_source):
                  source_lower = cell_source.lower()
                  if 'api_keys' in source_lower or 'alpha_vantage_key' in source_lower:
                      return "API Keys"
                  elif 'environment detection' in source_lower or 'in_colab' in source_lower:
                      return "Environment"
                  elif 'github sync' in source_lower or 'repo_folder' in source_lower:
                      return "GitHub Sync"
                  elif 'alpha vantage' in source_lower and 'fetcher' in source_lower:
                      return "Alpha Vantage"
                  elif 'yfinance' in source_lower and 'fetcher' in source_lower:
                      return "YFinance"
                  elif 'combiner' in source_lower or 'indicator' in source_lower:
                      return "CSV Combiner"
                  elif 'ultra-persistent' in source_lower or 'pipeline v6' in source_lower:
                      return "Pipeline v6.1"
                  elif 'trade beacon v20.2' in source_lower or 'multi-source learning' in source_lower:
                      return "Trade Beacon v20.2"
                  return self.current_stage
              
              def preprocess_cell(self, cell, resources, idx):
                  if cell.cell_type != 'code':
                      return cell, resources
                  
                  self.cell_count += 1
                  
                  new_stage = self.detect_stage(cell.source)
                  if new_stage != self.current_stage:
                      stage_start = time.time()
                      if self.current_stage in self.stage_timings:
                          self.stage_timings[self.current_stage]['duration'] = stage_start - self.stage_timings[self.current_stage]['start']
                      self.current_stage = new_stage
                      self.stage_timings[new_stage] = {'start': stage_start, 'duration': 0}
                      print("STAGE: " + new_stage)
                  
                  elapsed = time.time() - self.start_time
                  print("Cell " + str(self.cell_count) + " | " + self.current_stage + " | " + str(int(elapsed)) + "s elapsed")
                  
                  try:
                      cell, resources = super().preprocess_cell(cell, resources, idx)
                      self.successful_cells += 1
                      
                      if cell.outputs:
                          for output in cell.outputs:
                              if output.output_type == 'stream':
                                  text = re.sub(r'\x1b\[[0-9;]*m', '', output.text)
                                  lines = text.strip().split('\n')
                                  
                                  important_markers = [
                                      'COMPLETE', 'ERROR', 'FAILED', 'SUCCESS', 'SKIPPED',
                                      'Win Rate', 'P&L', 'Iteration', 'Mode:', 'Total', 'Average',
                                      'Loaded', 'Saved', 'Updated', 'Found', 'Processing',
                                      'Quality', 'Trades', 'Epsilon', 'Experience Replay',
                                      'Pipeline Stats', 'Database', 'Q-Network', 'Backtest',
                                      'API calls', 'Daily API usage', 'Alpha Vantage',
                                      'MULTI-SOURCE LEARNING', 'Pipeline v6', 'Trade Beacon',
                                      'Learning outcomes', 'Validated', 'learning_outcomes.json'
                                  ]
                                  
                                  important_lines = [l for l in lines if any(marker in l for marker in important_markers)]
                                  
                                  if important_lines:
                                      for line in important_lines[:20]:
                                          print("  " + line)
                              
                              elif output.output_type == 'error':
                                  error_msg = output.ename + ": " + output.evalue
                                  print("  Error: " + error_msg)
                                  self.critical_errors.append({
                                      'cell': self.cell_count,
                                      'stage': self.current_stage,
                                      'error': error_msg
                                  })
                  
                  except Exception as e:
                      self.failed_cells += 1
                      error_summary = str(e)[:200]
                      print("  Cell " + str(self.cell_count) + " error: " + error_summary)
                      print("  Continuing to next cell...")
                      self.critical_errors.append({
                          'cell': self.cell_count,
                          'stage': self.current_stage,
                          'error': error_summary
                      })
                  
                  return cell, resources
          
          if not os.path.exists('AI_Forex_Brain_2.ipynb'):
              print("ERROR: AI_Forex_Brain_2.ipynb not found!")
              sys.exit(1)
          
          with open('AI_Forex_Brain_2.ipynb', 'r') as f:
              nb = nbformat.read(f, as_version=4)
          
          print("=" * 70)
          print("MULTI-SOURCE LEARNING SYSTEM v20.2")
          print("=" * 70)
          
          ep = SmartExecutor(timeout=2400, kernel_name='python3', allow_errors=True)
          start = time.time()
          
          try:
              ep.preprocess(nb, {'metadata': {'path': '.'}})
              duration = time.time() - start
              
              print("\n" + "=" * 70)
              print("EXECUTION COMPLETED")
              print("=" * 70)
              print("Duration: " + str(round(duration, 1)) + "s (" + str(round(duration/60, 1)) + " min)")
              print("Cells: " + str(ep.cell_count) + " total")
              print("Success: " + str(ep.successful_cells))
              print("Failed: " + str(ep.failed_cells))
              
              if ep.stage_timings:
                  print("\nStage Timings:")
                  for stage, timing in ep.stage_timings.items():
                      duration_val = timing.get('duration', 0)
                      if duration_val > 0:
                          print("  " + stage + ": " + str(round(duration_val, 1)) + "s")
              
              if ep.critical_errors:
                  print("\nCritical Errors (" + str(len(ep.critical_errors)) + "):")
                  for err in ep.critical_errors[:5]:
                      print("  Cell " + str(err['cell']) + " (" + err['stage'] + "): " + err['error'][:100])
              
              print("=" * 70 + "\n")
              
              report = {
                  'timestamp': datetime.now().isoformat(), 
                  'mode': 'multi_source_learning', 
                  'duration': duration,
                  'cells_executed': ep.cell_count,
                  'successful': ep.successful_cells,
                  'failed': ep.failed_cells,
                  'status': 'success',
                  'stage_timings': {k: v['duration'] for k, v in ep.stage_timings.items() if 'duration' in v},
                  'critical_errors': len(ep.critical_errors),
                  'alpha_vantage_active': os.environ.get('SKIP_ALPHA_VANTAGE', 'false').lower() != 'true',
                  'version': 'Pipeline v6.1 + Trade Beacon v20.2'
              }
              
          except Exception as e:
              duration = time.time() - start
              
              print("\n" + "=" * 70)
              print("EXECUTION COMPLETED WITH ERRORS")
              print("=" * 70)
              print("Error: " + type(e).__name__ + ": " + str(e)[:200])
              print("Duration: " + str(round(duration, 1)) + "s (" + str(round(duration/60, 1)) + " min)")
              print("Successful cells: " + str(ep.successful_cells))
              print("Failed cells: " + str(ep.failed_cells))
              print("=" * 70 + "\n")
              
              report = {
                  'timestamp': datetime.now().isoformat(), 
                  'mode': 'multi_source_learning', 
                  'duration': duration,
                  'cells_executed': ep.cell_count,
                  'successful': ep.successful_cells,
                  'failed': ep.failed_cells,
                  'status': 'completed_with_errors',
                  'error': type(e).__name__ + ": " + str(e)[:200],
                  'stage_timings': {k: v['duration'] for k, v in ep.stage_timings.items() if 'duration' in v},
                  'critical_errors': len(ep.critical_errors),
                  'alpha_vantage_active': os.environ.get('SKIP_ALPHA_VANTAGE', 'false').lower() != 'true',
                  'version': 'Pipeline v6.1 + Trade Beacon v20.2'
              }
          
          os.makedirs('.github/run_history', exist_ok=True)
          with open('.github/run_history/latest_run.json', 'w') as f: 
              json.dump(report, f, indent=2)
          
          print("Full notebook execution completed")
          print("Report saved to .github/run_history/latest_run.json")
          EOF

      - name: Run Full Notebook Pipeline
        run: |
          echo "üöÄ MULTI-SOURCE LEARNING SYSTEM v20.2"
          echo "=================================="
          echo "üìÖ Day: ${{ steps.day_info.outputs.day_name }}"
          echo "üî¢ Run: #${{ steps.data.outputs.run_number }}"
          echo "‚è∞ Hour: ${{ steps.day_info.outputs.hour }}:00 UTC"
          echo "üìì Notebook: AI_Forex_Brain_2.ipynb"
          echo "üñ•Ô∏è  Environment: GitHub Actions"
          echo ""
          
          if [ "${{ steps.day_info.outputs.is_alpha_run }}" = "true" ]; then
            echo "üåô ALPHA VANTAGE RUN - Full pipeline with fresh daily data"
            echo "üìä Alpha Vantage: ‚úÖ ACTIVE (4 API calls)"
          else
            echo "‚ö° REGULAR RUN - Alpha Vantage skipped"
            echo "üìä Alpha Vantage: ‚è≠Ô∏è  SKIPPED (uses existing data)"
          fi
          
          echo ""
          echo "üìã Pipeline Stages:"
          echo "  1Ô∏è‚É£  API Keys Configuration"
          echo "  2Ô∏è‚É£  Environment Setup"
          echo "  3Ô∏è‚É£  GitHub Repository Sync"
          
          if [ "${{ steps.day_info.outputs.is_alpha_run }}" = "true" ]; then
            echo "  4Ô∏è‚É£  Alpha Vantage Data Fetch ‚úÖ"
          else
            echo "  4Ô∏è‚É£  Alpha Vantage Data Fetch ‚è≠Ô∏è  (skipped)"
          fi
          
          echo "  5Ô∏è‚É£  YFinance Data Fetch"
          echo "  6Ô∏è‚É£  CSV Combiner & Indicators"
          echo "  7Ô∏è‚É£  Ultra-Persistent Pipeline v6.1 (NEW)"
          echo "      ‚Ä¢ Evaluates previous predictions"
          echo "      ‚Ä¢ Generates new predictions"
          echo "      ‚Ä¢ Outputs learning_outcomes.json"
          echo "  8Ô∏è‚É£  Trade Beacon v20.2 (NEW)"
          echo "      ‚Ä¢ Multi-source learning active"
          echo "      ‚Ä¢ Learns from Pipeline v6.1"
          echo "      ‚Ä¢ Validates live signals"
          echo "      ‚Ä¢ Weekend backtest mode"
          echo ""
          echo "üéì Multi-Source Learning:"
          echo "   Source 1: Pipeline v6.1 outcomes"
          echo "   Source 2: Live signal validation"
          echo "   Source 3: Pipeline database"
          echo "   Source 4: Weekend backtest"
          echo ""
          echo "üí° API Optimization:"
          echo "   Daily API usage: 4 calls (16% of 25 limit)"
          echo "   Run frequency: Every 2 hours"
          echo "   Learning outcomes: ${{ steps.data.outputs.learning_outcomes }}"
          echo ""
          echo "‚è±Ô∏è  Estimated time: 30-50 minutes"
          echo "=================================="
          echo ""
          python run_full.py
        timeout-minutes: 50

      - name: Extract Pipeline Metrics
        if: always()
        run: |
          cat > extract_metrics.py << 'EOF'
          import sqlite3
          import json
          import os
          from pathlib import Path
          
          metrics = {'status': 'no_data', 'version': 'v20.2'}
          
          # Pipeline database
          db_path = Path('database/memory_v85.db')
          if db_path.exists():
              try:
                  conn = sqlite3.connect(str(db_path))
                  c = conn.cursor()
                  
                  c.execute("SELECT COUNT(*), SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) FROM completed_trades")
                  result = c.fetchone()
                  if result and result[0]:
                      metrics['pipeline_trades'] = result[0]
                      metrics['pipeline_wins'] = result[1] or 0
                      metrics['pipeline_win_rate'] = (result[1] / result[0] * 100) if result[0] else 0
                  
                  conn.close()
              except Exception as e:
                  metrics['db_error'] = str(e)
          
          # RL memory
          rl_memory = Path('rl_memory/experience_replay.json.gz')
          if rl_memory.exists():
              metrics['rl_memory_size'] = rl_memory.stat().st_size
          
          # RL stats
          stats_file = Path('rl_memory/learning_stats.json')
          if stats_file.exists():
              try:
                  with open(stats_file) as f:
                      rl_stats = json.load(f)
                  metrics['rl_trades'] = rl_stats.get('total_trades', 0)
                  metrics['rl_win_rate'] = rl_stats.get('win_rate', 0) * 100
                  metrics['rl_pnl'] = rl_stats.get('total_pnl', 0)
                  metrics['epsilon'] = rl_stats.get('epsilon_history', [0.7])[-1] if rl_stats.get('epsilon_history') else 0.7
                  
                  # Multi-source stats
                  metrics['pipeline_v6_learned'] = rl_stats.get('pipeline_v6_learned', 0)
                  metrics['live_validated'] = rl_stats.get('live_validated', 0)
                  metrics['backtest_learned'] = rl_stats.get('backtest_learned', 0)
                  metrics['total_learning_sources'] = rl_stats.get('total_learning_sources', 0)
              except:
                  pass
          
          # Trade Beacon signals
          beacon_file = Path('outputs/omega_signals.json')
          if beacon_file.exists():
              try:
                  with open(beacon_file) as f:
                      beacon_data = json.load(f)
                  metrics['beacon_iteration'] = beacon_data.get('iteration', 0)
                  metrics['beacon_mode'] = beacon_data.get('mode', 'unknown')
                  metrics['active_signals'] = sum(1 for s in beacon_data.get('signals', {}).values() if s.get('direction') != 'HOLD')
                  
                  # Learning summary
                  if 'learning_summary' in beacon_data:
                      metrics['learning_summary'] = beacon_data['learning_summary']
              except:
                  pass
          
          # Pipeline v6.1 learning outcomes
          learning_db = Path('learning_data/learning_outcomes.json')
          if learning_db.exists():
              try:
                  with open(learning_db) as f:
                      outcomes = json.load(f)
                  metrics['learning_outcomes_count'] = len(outcomes)
                  if outcomes:
                      wins = sum(1 for o in outcomes if o.get('was_correct', False))
                      metrics['learning_outcomes_wr'] = (wins / len(outcomes) * 100) if len(outcomes) else 0
              except:
                  pass
          
          # Pipeline v6.1 predictions
          pred_file = Path('learning_data/predictions_history.json')
          if pred_file.exists():
              try:
                  with open(pred_file) as f:
                      predictions = json.load(f)
                  metrics['predictions_pending'] = sum(1 for p in predictions if not p.get('evaluated', True))
              except:
                  pass
          
          metrics['alpha_vantage_optimized'] = True
          metrics['daily_api_calls'] = 4
          metrics['api_usage_percent'] = 16
          metrics['run_frequency'] = 'Every 2 hours'
          
          os.makedirs('.github/run_history', exist_ok=True)
          with open('.github/run_history/metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)
          
          print("Metrics extracted:")
          for key, value in metrics.items():
              print("  " + key + ": " + str(value))
          EOF
          python extract_metrics.py

      - name: Prepare Git for Commit
        if: success()
        run: |
          echo "üîÑ Preparing git for commit..."
          
          if ! git diff --quiet; then
            echo "üì¶ Stashing unstaged changes..."
            git stash push -m "Auto-stashed changes before pull - run ${{ steps.data.outputs.run_number }}"
            STASHED=true
          else
            STASHED=false
          fi
          
          echo "üîÑ Pulling latest changes..."
          git pull --rebase origin main || {
            echo "‚ö†Ô∏è Rebase failed, using merge strategy..."
            git rebase --abort 2>/dev/null || true
            git pull --no-rebase origin main
          }
          
          if [ "$STASHED" = "true" ]; then
            echo "üì¶ Restoring stashed changes..."
            git stash pop || {
              echo "‚ö†Ô∏è Stash pop had conflicts, keeping both versions"
            }
          fi
          
          echo "‚úÖ Repository ready for commit"

      - name: Commit Everything
        if: success()
        run: |
          echo "üíæ COMMITTING EVERYTHING..."
          echo "=================================="
          
          git add -A
          
          echo "üìÅ Files to be committed:"
          git status --porcelain | head -25
          echo "..."
          
          if git diff --cached --quiet; then
            echo "‚ÑπÔ∏è No changes to commit"
          else
            RUN="${{ steps.data.outputs.run_number }}"
            DAY="${{ steps.day_info.outputs.day_name }}"
            IS_WEEKEND="${{ steps.day_info.outputs.is_weekend }}"
            IS_ALPHA="${{ steps.day_info.outputs.is_alpha_run }}"
            
            if [ -f "rl_memory/learning_stats.json" ]; then
              RL_TRADES=$(python -c "import json; d=json.load(open('rl_memory/learning_stats.json')); print(d.get('total_trades', 0))" 2>/dev/null || echo "0")
              RL_WINRATE=$(python -c "import json; d=json.load(open('rl_memory/learning_stats.json')); print(round(d.get('win_rate', 0)*100, 1))" 2>/dev/null || echo "0")
              EPSILON=$(python -c "import json; d=json.load(open('rl_memory/learning_stats.json')); h=d.get('epsilon_history',[]); print(round(h[-1],3) if h else 0.7)" 2>/dev/null || echo "0.7")
            else
              RL_TRADES="0"
              RL_WINRATE="0"
              EPSILON="0.7"
            fi
            
            if [ -f "learning_data/learning_outcomes.json" ]; then
              LEARNING_COUNT=$(python -c "import json; print(len(json.load(open('learning_data/learning_outcomes.json'))))" 2>/dev/null || echo "0")
            else
              LEARNING_COUNT="0"
            fi
            
            if [ -f "database/memory_v85.db" ]; then
              DB_SIZE=$(stat -f%z database/memory_v85.db 2>/dev/null || stat -c%s database/memory_v85.db 2>/dev/null || echo "0")
              DB_SIZE_MB=$((DB_SIZE / 1024 / 1024))
            else
              DB_SIZE_MB="0"
            fi
            
            git commit -m "üéì Run #${RUN} - ${DAY} - Multi-Source Learning v20.2" \
                       -m "Trade Beacon: ${RL_TRADES} trades, ${RL_WINRATE}% WR, Œµ=${EPSILON}" \
                       -m "Pipeline v6.1: ${LEARNING_COUNT} learning outcomes" \
                       -m "Database: ${DB_SIZE_MB}MB" \
                       -m "Alpha Vantage: ${IS_ALPHA}" \
                       -m "Weekend Mode: ${IS_WEEKEND}" \
                       -m "Time: $(date +'%Y-%m-%d %H:%M UTC')" || {
              echo "‚ö†Ô∏è Commit failed, trying with simpler message..."
              git commit -m "Run #${RUN} - ${DAY}" || true
            }
            
            for i in {1..5}; do
              echo "üöÄ Pushing to GitHub (attempt $i/5)..."
              if git push origin main; then
                echo "‚úÖ Successfully committed and pushed ALL changes"
                break
              else
                if [ $i -lt 5 ]; then
                  echo "‚ö†Ô∏è Push failed, pulling latest and retrying in 10s..."
                  sleep 10
                  git pull --rebase origin main || git pull --strategy-option=theirs origin main
                else
                  echo "‚ùå Failed to push after 5 attempts"
                  echo "üí° Changes are committed locally but not pushed to remote"
                fi
              fi
            done
          fi

      - name: Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: learning-system-logs-${{ github.run_number }}
          path: |
            .github/run_history/*.json
            logs/*.log
            outputs/*.json
            rl_memory/learning_stats.json
            learning_data/learning_outcomes.json
            learning_data/predictions_history.json
          retention-days: 7
          if-no-files-found: ignore

      - name: Final Summary
        if: always()
        run: |
          echo "=========================================="
          echo "üéì MULTI-SOURCE LEARNING SYSTEM SUMMARY"
          echo "=========================================="
          echo "üìÖ Day: ${{ steps.day_info.outputs.day_name }}"
          echo "üî¢ Run: #${{ steps.data.outputs.run_number }}"
          echo "‚è∞ Time: $(date +'%Y-%m-%d %H:%M UTC')"
          echo "üñ•Ô∏è  Env: GitHub Actions"
          echo "üì¶ Version: Pipeline v6.1 + Trade Beacon v20.2"
          echo ""
          
          if [ "${{ steps.day_info.outputs.is_alpha_run }}" = "true" ]; then
            echo "üåô Alpha Vantage: ACTIVE (4 API calls made)"
          else
            echo "‚è≠Ô∏è  Alpha Vantage: SKIPPED (next run at 00:00 UTC)"
          fi
          
          echo ""
          echo "üìÇ Data Files:"
          echo "   Processed pickles: ${{ steps.data.outputs.pickle_count }}"
          echo "   RL Memory: ${{ steps.data.outputs.rl_memory_exists }}"
          echo "   Learning outcomes: ${{ steps.data.outputs.learning_outcomes }}"
          echo ""
          
          if [ -f ".github/run_history/metrics.json" ]; then
            echo "üìä Latest Metrics:"
            cat .github/run_history/metrics.json | python3 -m json.tool 2>/dev/null | grep -E '"(rl_trades|rl_win_rate|epsilon|pipeline_v6_learned|live_validated|backtest_learned|learning_outcomes_count|predictions_pending|beacon_iteration|active_signals)"' | head -15 || true
            echo ""
          fi
          
          if [ -f ".github/run_history/latest_run.json" ]; then
            echo "‚è±Ô∏è  Execution:"
            cat .github/run_history/latest_run.json | python3 -m json.tool 2>/dev/null | grep -E '"(status|duration|successful|failed|alpha_vantage_active|version)"' || true
            echo ""
          fi
          
          echo "üíæ Commit Status: ‚úÖ EVERYTHING COMMITTED"
          echo "   ‚Ä¢ Raw data files"
          echo "   ‚Ä¢ Processed features" 
          echo "   ‚Ä¢ RL learning progress"
          echo "   ‚Ä¢ Trading signals"
          echo "   ‚Ä¢ Pipeline v6.1 predictions"
          echo "   ‚Ä¢ Learning outcomes database"
          echo "   ‚Ä¢ Logs and metrics"
          echo ""
          
          echo "üéì Multi-Source Learning Status:"
          if [ -f "rl_memory/learning_stats.json" ]; then
            echo "   Pipeline v6.1: $(python3 -c "import json; print(json.load(open('rl_memory/learning_stats.json')).get('pipeline_v6_learned', 0))" 2>/dev/null || echo "0") outcomes learned"
            echo "   Live Validation: $(python3 -c "import json; print(json.load(open('rl_memory/learning_stats.json')).get('live_validated', 0))" 2>/dev/null || echo "0") signals validated"
            echo "   Pipeline DB: $(python3 -c "import json; print(json.load(open('rl_memory/learning_stats.json')).get('pipeline_trades_learned', 0))" 2>/dev/null || echo "0") trades learned"
            echo "   Backtest: $(python3 -c "import json; print(json.load(open('rl_memory/learning_stats.json')).get('backtest_learned', 0))" 2>/dev/null || echo "0") trades learned"
            echo "   Total: $(python3 -c "import json; print(json.load(open('rl_memory/learning_stats.json')).get('total_learning_sources', 0))" 2>/dev/null || echo "0") experiences this run"
          else
            echo "   (Stats not available yet)"
          fi
          echo ""
          
          echo "üí° API Optimization:"
          echo "   Daily usage: 4/25 calls (16%)"
          echo "   Run frequency: Every 2 hours (00,02,04,...,22)"
          echo "   Savings: 44 calls/day vs hourly"
          echo ""
          
          echo "‚è∞ Schedule Info:"
          echo "   Current: ${{ steps.day_info.outputs.hour }}:00 UTC"
          echo "   Next run: 2 hours from now"
          echo "   Weekday: Every 2 hours (learning + trading)"
          echo "   Weekend: Every 2 hours (intensive learning)"
          echo ""
          
          echo "üì¶ Artifacts: Uploaded for this run"
          echo ""
          
          echo "‚úÖ Multi-source learning cycle complete - ALL DATA COMMITTED"
          echo "=========================================="
