name: Smart Forex Brain Pipeline (Auto-Detect Mode)

on:
  workflow_dispatch:
  push:
    paths:
      - 'colab_trigger.txt'
    branches:
      - main

jobs:
  run-forex-brain:
    runs-on: ubuntu-latest
    timeout-minutes: 55

    env:
      FOREX_PAT: ${{ secrets.FOREX_PAT }}
      BROWSERLESS_TOKEN: ${{ secrets.BROWSERLESS_TOKEN }}
      ALPHA_VANTAGE_KEY: ${{ secrets.ALPHA_VANTAGE_KEY }}
      GIT_USER_NAME: "Forex AI Bot"
      GIT_USER_EMAIL: "nakatonabira3@gmail.com"
      GITHUB_USERNAME: "rahim-dotAI"
      GITHUB_REPO: "forex-ai-models"
      PYTHONIOENCODING: utf-8
      PYTHONUNBUFFERED: 1
      GITHUB_ACTIONS: "true"
      SINGLE_RUN_MODE: "true"

    steps:
      - name: Pre-cleanup
        run: |
          sudo rm -rf /content 2>/dev/null || true
          rm -f .gitmodules 2>/dev/null || true

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0
          lfs: false
          submodules: false
          clean: true

      - name: Verify secrets
        run: |
          MISSING_SECRETS=()
          [ -z "${FOREX_PAT}" ] && MISSING_SECRETS+=("FOREX_PAT")
          [ -z "${BROWSERLESS_TOKEN}" ] && MISSING_SECRETS+=("BROWSERLESS_TOKEN")
          [ -z "${ALPHA_VANTAGE_KEY}" ] && MISSING_SECRETS+=("ALPHA_VANTAGE_KEY")
          if [ ${#MISSING_SECRETS[@]} -gt 0 ]; then
            echo "Missing secrets: ${MISSING_SECRETS[*]}"
          else
            echo "All secrets configured"
          fi

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install --no-cache-dir \
            mplfinance firebase-admin dropbox requests beautifulsoup4 \
            pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm \
            joblib matplotlib alpha_vantage tqdm scikit-learn river \
            jupyter nbconvert

      - name: Create directory structure
        run: |
          sudo mkdir -p /content/forex-alpha-models/{csvs,pickles,logs,merged_data_pickles,temp_pickles,forex-ai-models}
          sudo chown -R $USER:$USER /content
          sudo chmod -R 755 /content
          mkdir -p $GITHUB_WORKSPACE/outputs/{csvs,pickles,logs,merged_data_pickles,temp_pickles}

      - name: Configure Git
        run: |
          git config --global user.name "${GIT_USER_NAME}"
          git config --global user.email "${GIT_USER_EMAIL}"
          git config --global advice.detachedHead false
          git config --global credential.helper store
          echo "https://${GITHUB_USERNAME}:${FOREX_PAT}@github.com" > ~/.git-credentials

      - name: Clone forex-ai-models repository
        run: |
          cd /content/forex-alpha-models
          if [ ! -d "forex-ai-models/.git" ]; then
            echo "üîÑ Cloning forex-ai-models repository..."
            git clone -b main https://${GITHUB_USERNAME}:${FOREX_PAT}@github.com/${GITHUB_USERNAME}/${GITHUB_REPO}.git forex-ai-models
          else
            echo "üì• Pulling latest changes..."
            cd forex-ai-models
            git pull origin main
          fi
          cd forex-ai-models
          git config user.name "${GIT_USER_NAME}"
          git config user.email "${GIT_USER_EMAIL}"
          
          echo ""
          echo "üìÇ Repository contents:"
          ls -la

      - name: Check dependencies and decide execution mode
        id: check_deps
        run: |
          echo "=========================================="
          echo "CHECKING DEPENDENCIES FOR MODE SELECTION"
          echo "=========================================="
          
          REPO_PATH="/content/forex-alpha-models/forex-ai-models"
          
          echo ""
          echo "üìä Checking for required files in $REPO_PATH..."
          echo ""
          
          # Check for CSV data files
          echo "1Ô∏è‚É£ Checking CSV files..."
          CSV_COUNT=$(find "$REPO_PATH" -maxdepth 1 -name "*.csv" -type f 2>/dev/null | wc -l)
          if [ $CSV_COUNT -gt 0 ]; then
            echo "   ‚úÖ Found $CSV_COUNT CSV files"
            ls -lh "$REPO_PATH"/*.csv 2>/dev/null | head -3
          else
            echo "   ‚ùå No CSV files found"
          fi
          
          # Check for pickle files WITH INDICATORS
          echo ""
          echo "2Ô∏è‚É£ Checking indicator pickle files..."
          INDICATOR_PKL_COUNT=$(find "$REPO_PATH" -maxdepth 1 -name "*_indicators.pkl" -type f 2>/dev/null | wc -l)
          if [ $INDICATOR_PKL_COUNT -gt 0 ]; then
            echo "   ‚úÖ Found $INDICATOR_PKL_COUNT indicator pickle files"
            ls -lh "$REPO_PATH"/*_indicators.pkl 2>/dev/null | head -3
          else
            echo "   ‚ùå No indicator pickle files found"
          fi
          
          # Check for merged 2244 pickle files
          echo ""
          echo "3Ô∏è‚É£ Checking merged pickle files (2244)..."
          MERGED_PKL_COUNT=$(find "$REPO_PATH" -maxdepth 1 -name "*_2244.pkl" -type f 2>/dev/null | wc -l)
          if [ $MERGED_PKL_COUNT -gt 0 ]; then
            echo "   ‚úÖ Found $MERGED_PKL_COUNT merged pickle files"
            ls -lh "$REPO_PATH"/*_2244.pkl 2>/dev/null | head -3
          else
            echo "   ‚ùå No merged pickle files found"
          fi
          
          # Check for database files
          echo ""
          echo "4Ô∏è‚É£ Checking database files..."
          DB_COUNT=$(find "$REPO_PATH" -maxdepth 1 -name "*.db" -type f -size +1k 2>/dev/null | wc -l)
          if [ $DB_COUNT -gt 0 ]; then
            echo "   ‚úÖ Found $DB_COUNT database files"
            ls -lh "$REPO_PATH"/*.db 2>/dev/null
          else
            echo "   ‚ö†Ô∏è  No database files (will be created)"
          fi
          
          # Check for ML model pickle files
          echo ""
          echo "5Ô∏è‚É£ Checking ML model files..."
          ML_PKL_COUNT=$(find "$REPO_PATH" -maxdepth 1 \( -name "*_sgd.pkl" -o -name "*_rf.pkl" -o -name "*_best_chrom.pkl" \) -type f 2>/dev/null | wc -l)
          if [ $ML_PKL_COUNT -gt 0 ]; then
            echo "   ‚úÖ Found $ML_PKL_COUNT ML model files"
            ls -lh "$REPO_PATH"/*_sgd.pkl "$REPO_PATH"/*_rf.pkl 2>/dev/null | head -3
          else
            echo "   ‚ö†Ô∏è  No ML model files (will be created)"
          fi
          
          echo ""
          echo "=========================================="
          echo "üìä DECISION LOGIC:"
          echo "   CSVs: $CSV_COUNT files"
          echo "   Indicator PKLs: $INDICATOR_PKL_COUNT files"
          echo "   Merged PKLs (2244): $MERGED_PKL_COUNT files"
          echo "   Databases: $DB_COUNT files"
          echo "   ML Models: $ML_PKL_COUNT files"
          echo "=========================================="
          echo ""
          
          # Decision: Need CSVs AND (indicator pickles OR merged pickles) for ultra mode
          if [ $CSV_COUNT -ge 4 ] && { [ $INDICATOR_PKL_COUNT -ge 4 ] || [ $MERGED_PKL_COUNT -ge 4 ]; }; then
            echo "mode=ultra" >> $GITHUB_OUTPUT
            echo "üü¢ DECISION: Running ULTRA-PERSISTENT mode"
            echo "   ‚úÖ Sufficient data files found in repository"
            echo "   ‚úÖ Will skip data fetching and use existing files"
          else
            echo "mode=full" >> $GITHUB_OUTPUT
            echo "üî¥ DECISION: Running FULL pipeline"
            echo "   ‚ùå Missing required data files"
            echo "   ‚ö†Ô∏è  Need: 4 CSVs + 4 indicator/merged pickles"
            echo "   ‚ö†Ô∏è  Found: $CSV_COUNT CSVs, $INDICATOR_PKL_COUNT indicators, $MERGED_PKL_COUNT merged"
          fi
          
          echo "=========================================="

      - name: Copy data files to working directories
        if: steps.check_deps.outputs.mode == 'ultra'
        run: |
          echo "üì¶ Copying data from repository to working directories..."
          
          REPO_PATH="/content/forex-alpha-models/forex-ai-models"
          
          # Create target directories
          mkdir -p /content/forex-alpha-models/{csvs,pickles,merged_data_pickles,logs}
          
          # Copy CSV files
          CSV_COUNT=0
          if ls "$REPO_PATH"/*.csv 1> /dev/null 2>&1; then
            for file in "$REPO_PATH"/*.csv; do
              cp -v "$file" /content/forex-alpha-models/csvs/
              CSV_COUNT=$((CSV_COUNT + 1))
            done
            echo "‚úÖ Copied $CSV_COUNT CSV files"
          fi
          
          # Copy merged pickle files (2244)
          PKL_2244_COUNT=0
          if ls "$REPO_PATH"/*_2244.pkl 1> /dev/null 2>&1; then
            for file in "$REPO_PATH"/*_2244.pkl; do
              cp -v "$file" /content/forex-alpha-models/merged_data_pickles/
              PKL_2244_COUNT=$((PKL_2244_COUNT + 1))
            done
            echo "‚úÖ Copied $PKL_2244_COUNT merged pickle files"
          fi
          
          # Copy indicator pickle files
          IND_COUNT=0
          if ls "$REPO_PATH"/*_indicators.pkl 1> /dev/null 2>&1; then
            for file in "$REPO_PATH"/*_indicators.pkl; do
              cp -v "$file" /content/forex-alpha-models/pickles/
              IND_COUNT=$((IND_COUNT + 1))
            done
            echo "‚úÖ Copied $IND_COUNT indicator pickle files"
          fi
          
          # Copy ML model pickle files
          ML_COUNT=0
          for pattern in "*_sgd.pkl" "*_rf.pkl" "*_rf_hist.pkl" "*_best_chrom.pkl" "*_population.pkl" "*_trade_memory.pkl" "*_gen_count.pkl" "*_ga_progress.pkl"; do
            if ls "$REPO_PATH"/$pattern 1> /dev/null 2>&1; then
              for file in "$REPO_PATH"/$pattern; do
                cp -v "$file" /content/forex-alpha-models/pickles/ 2>/dev/null || true
                ML_COUNT=$((ML_COUNT + 1))
              done
            fi
          done
          echo "‚úÖ Copied $ML_COUNT ML model files"
          
          # Database files - NO COPY needed (already in right place)
          echo "‚úÖ Database files already in place (no copy needed)"
          
          # Copy other important pickle files to working directory
          for file in learning_progress.pkl iteration_counter.pkl previous_signals.pkl monday_runs.pkl; do
            if [ -f "$REPO_PATH/$file" ]; then
              # These stay in repo folder (no copy needed)
              echo "‚úÖ Found $file in repo"
            fi
          done
          
          echo ""
          echo "=========================================="
          echo "üìä Working directory status:"
          echo "=========================================="
          echo "CSVs: $(ls -1 /content/forex-alpha-models/csvs/*.csv 2>/dev/null | wc -l) files"
          echo "Merged pickles (2244): $(ls -1 /content/forex-alpha-models/merged_data_pickles/*_2244.pkl 2>/dev/null | wc -l) files"
          echo "Indicator pickles: $(ls -1 /content/forex-alpha-models/pickles/*_indicators.pkl 2>/dev/null | wc -l) files"
          echo "ML model pickles: $(ls -1 /content/forex-alpha-models/pickles/*_{sgd,rf}.pkl 2>/dev/null | wc -l) files"
          echo "Databases: $(ls -1 /content/forex-alpha-models/forex-ai-models/*.db 2>/dev/null | wc -l) files"
          echo "=========================================="

      - name: Convert notebook to Python
        run: |
          jupyter nbconvert --to python "AI_Forex_Brain_2.ipynb" --output ai_forex_brain_2
          sed -i '/get_ipython()/d' ai_forex_brain_2.py
          sed -i '/^get_ipython/d' ai_forex_brain_2.py
          sed -i 's/\.mkdir(exist_ok=True)/.mkdir(parents=True, exist_ok=True)/g' ai_forex_brain_2.py

      - name: Apply path fixes
        run: |
          cat > path_fixes.sed << 'EOF'
          s|ROOT_DIR = Path("/content")|ROOT_DIR = Path("/content/forex-alpha-models")|g
          s|ROOT_PATH = ROOT_DIR / "forex-alpha-models"|ROOT_PATH = Path("/content/forex-alpha-models")|g
          s|BASE_DIR = Path(".")|BASE_DIR = Path("/content/forex-alpha-models")|g
          s|REPO_FOLDER = .* / "forex-ai-models"|REPO_FOLDER = Path("/content/forex-alpha-models/forex-ai-models")|g
          s|PICKLE_FOLDER = .* / "pickles"|PICKLE_FOLDER = Path("/content/forex-alpha-models/pickles")|g
          s|FINAL_PICKLE_FOLDER = .* / "merged_data_pickles"|FINAL_PICKLE_FOLDER = Path("/content/forex-alpha-models/merged_data_pickles")|g
          s|CSV_FOLDER = .* / "csvs"|CSV_FOLDER = Path("/content/forex-alpha-models/csvs")|g
          s|LOG_FOLDER = .* / "logs"|LOG_FOLDER = Path("/content/forex-alpha-models/logs")|g
          EOF
          sed -i -f path_fixes.sed ai_forex_brain_2.py

      - name: Extract THREE Ultra-Persistent Cells
        if: steps.check_deps.outputs.mode == 'ultra'
        run: |
          cat > extract_three_cells.py << 'PYTHON_EOF'
          import re
          
          with open("ai_forex_brain_2.py", "r", encoding="utf-8") as f:
              content = f.read()
          
          # Define the THREE cell markers in order
          markers = [
              {
                  'name': 'Ultra-Persistent Pipeline (VERSION 3.6)',
                  'pattern': r'# ={60,}\n# VERSION 3\.6 ‚Äì Unified Loader \+ Merge Pickles'
              },
              {
                  'name': 'Unified Pipeline (FX CSV Combine)',
                  'pattern': r'# ={60,}\n# FX CSV Combine \+ Incremental Indicators Pipeline'
              },
              {
                  'name': 'Hybrid Pipeline v7.5',
                  'pattern': r'#!/usr/bin/env python3\n"""\nUltimate Hybrid Forex Pipeline v7\.5'
              }
          ]
          
          print("=" * 80)
          print("EXTRACTING THREE ULTRA-PERSISTENT CELLS")
          print("=" * 80)
          
          cell_positions = []
          
          # Find all three cells
          for i, marker_info in enumerate(markers, 1):
              match = re.search(marker_info['pattern'], content)
              if match:
                  pos = match.start()
                  cell_positions.append({
                      'index': i,
                      'name': marker_info['name'],
                      'position': pos
                  })
                  print(f"\n‚úÖ Cell {i} Found: {marker_info['name']}")
                  print(f"   Position: {pos:,}")
                  # Show first line of content
                  first_line = content[pos:pos+100].split('\n')[0]
                  print(f"   First line: {first_line[:70]}...")
              else:
                  print(f"\n‚ùå Cell {i} NOT FOUND: {marker_info['name']}")
          
          if len(cell_positions) < 3:
              print("\n" + "=" * 80)
              print("‚ö†Ô∏è  WARNING: Not all 3 cells found!")
              print(f"   Found: {len(cell_positions)}/3 cells")
              print("=" * 80)
              
              # Fallback: extract from first found cell to end
              if cell_positions:
                  start_pos = min(c['position'] for c in cell_positions)
                  print(f"\nüîÑ Fallback: Extracting from position {start_pos:,} to END")
                  extracted_code = content[start_pos:]
              else:
                  print("\n‚ùå No cells found! Using full notebook")
                  extracted_code = content
          else:
              # Extract from FIRST cell to END of file
              start_pos = cell_positions[0]['position']
              extracted_code = content[start_pos:]
              
              print("\n" + "=" * 80)
              print("‚úÖ EXTRACTION SUCCESSFUL")
              print("=" * 80)
              print(f"\nExtracting from Cell 1 ({cell_positions[0]['name']}) to END")
              print(f"Start position: {start_pos:,}")
              print(f"Extracted length: {len(extracted_code):,} characters")
              print(f"Total lines: {len(extracted_code.split(chr(10))):,}")
          
          # Write extracted code
          with open("ai_forex_brain_2_ultra.py", "w", encoding="utf-8") as f:
              f.write(extracted_code)
          
          # Verify extraction
          lines = extracted_code.split('\n')
          print("\n" + "=" * 80)
          print("EXTRACTION SUMMARY")
          print("=" * 80)
          print(f"Total lines: {len(lines):,}")
          print(f"\nFirst 5 lines:")
          for i, line in enumerate(lines[:5], 1):
              print(f"  {i:3d}: {line[:75]}")
          print(f"\nLast 5 lines:")
          for i, line in enumerate(lines[-5:], len(lines)-4):
              print(f"  {i:3d}: {line[:75]}")
          
          # Count cells included
          cell_markers_found = sum([
              1 for m in markers 
              if re.search(m['pattern'], extracted_code)
          ])
          print(f"\nüìã Cells included in extraction: {cell_markers_found}/3")
          
          print("\n‚úÖ Ultra-persistent script ready: ai_forex_brain_2_ultra.py")
          print("=" * 80)
          PYTHON_EOF
          
          python extract_three_cells.py

      - name: Run Pipeline (Full Mode)
        if: steps.check_deps.outputs.mode == 'full'
        run: |
          echo "=========================================="
          echo "üî¥ RUNNING FULL PIPELINE MODE"
          echo "=========================================="
          echo "This will execute ALL cells in the notebook:"
          echo "  1. Data fetching from APIs"
          echo "  2. Indicator calculation"
          echo "  3. Pickle creation"
          echo "  4. Model training"
          echo "  5. Signal generation"
          echo "=========================================="
          python ai_forex_brain_2.py 2>&1 | tee pipeline_output.log
          EXIT_CODE=${PIPESTATUS[0]}
          if [ $EXIT_CODE -ne 0 ]; then
            echo "‚ùå Pipeline failed with exit code $EXIT_CODE"
            tail -n 50 pipeline_output.log
            exit $EXIT_CODE
          fi
        timeout-minutes: 50

      - name: Run Pipeline (Ultra-Persistent Mode - 3 Cells)
        if: steps.check_deps.outputs.mode == 'ultra'
        run: |
          echo "=========================================="
          echo "üü¢ RUNNING ULTRA-PERSISTENT MODE (3 CELLS)"
          echo "=========================================="
          echo "Executing in order:"
          echo "  1. VERSION 3.6 - Ultra-Persistent Pipeline"
          echo "  2. FX CSV Combine + Incremental Indicators"
          echo "  3. Ultimate Hybrid Forex Pipeline v7.5"
          echo "=========================================="
          python ai_forex_brain_2_ultra.py 2>&1 | tee pipeline_output.log
          EXIT_CODE=${PIPESTATUS[0]}
          if [ $EXIT_CODE -ne 0 ]; then
            echo "‚ùå Pipeline failed with exit code $EXIT_CODE"
            tail -n 100 pipeline_output.log
            exit $EXIT_CODE
          fi
        timeout-minutes: 50

      - name: Copy outputs back to repository
        run: |
          echo "üì¶ Copying outputs back to repository..."
          
          REPO_DIR="/content/forex-alpha-models/forex-ai-models"
          
          # Copy CSV files from working dir to repo
          CSV_COUNT=0
          if ls /content/forex-alpha-models/csvs/*.csv 1> /dev/null 2>&1; then
            for file in /content/forex-alpha-models/csvs/*.csv; do
              filename=$(basename "$file")
              if [ ! -f "$REPO_DIR/$filename" ] || ! cmp -s "$file" "$REPO_DIR/$filename"; then
                cp -v "$file" "$REPO_DIR/"
                CSV_COUNT=$((CSV_COUNT + 1))
              fi
            done
            echo "‚úÖ Updated $CSV_COUNT CSV files"
          fi
          
          # Copy merged pickle files (2244) - CRITICAL!
          PKL_2244_COUNT=0
          if ls /content/forex-alpha-models/merged_data_pickles/*_2244.pkl 1> /dev/null 2>&1; then
            for file in /content/forex-alpha-models/merged_data_pickles/*_2244.pkl; do
              filename=$(basename "$file")
              if [ ! -f "$REPO_DIR/$filename" ] || ! cmp -s "$file" "$REPO_DIR/$filename"; then
                cp -v "$file" "$REPO_DIR/"
                PKL_2244_COUNT=$((PKL_2244_COUNT + 1))
              fi
            done
            echo "‚úÖ Updated $PKL_2244_COUNT merged pickle files (2244)"
          fi
          
          # Copy indicator pickle files
          IND_COUNT=0
          if ls /content/forex-alpha-models/pickles/*_indicators.pkl 1> /dev/null 2>&1; then
            for file in /content/forex-alpha-models/pickles/*_indicators.pkl; do
              filename=$(basename "$file")
              if [ ! -f "$REPO_DIR/$filename" ] || ! cmp -s "$file" "$REPO_DIR/$filename"; then
                cp -v "$file" "$REPO_DIR/"
                IND_COUNT=$((IND_COUNT + 1))
              fi
            done
            echo "‚úÖ Updated $IND_COUNT indicator pickle files"
          fi
          
          # Copy ML model files
          ML_COUNT=0
          for pattern in "*_sgd.pkl" "*_rf.pkl" "*_rf_hist.pkl" "*_best_chrom.pkl" "*_population.pkl" "*_trade_memory.pkl" "*_gen_count.pkl" "*_ga_progress.pkl"; do
            if ls /content/forex-alpha-models/pickles/$pattern 1> /dev/null 2>&1; then
              for file in /content/forex-alpha-models/pickles/$pattern; do
                filename=$(basename "$file")
                if [ ! -f "$REPO_DIR/$filename" ] || ! cmp -s "$file" "$REPO_DIR/$filename"; then
                  cp -v "$file" "$REPO_DIR/"
                  ML_COUNT=$((ML_COUNT + 1))
                fi
              done
            fi
          done
          echo "‚úÖ Updated $ML_COUNT ML model files"
          
          # Database files - already in correct location (skip)
          echo "‚úÖ Database files already in repository (no copy needed)"
          
          # Copy JSON files
          JSON_COUNT=0
          if ls "$REPO_DIR"/*.json 1> /dev/null 2>&1; then
            JSON_COUNT=$(ls -1 "$REPO_DIR"/*.json 2>/dev/null | wc -l)
            echo "‚úÖ JSON files: $JSON_COUNT (already in repo)"
          fi
          
          # Other state files - already in correct location (skip)
          echo "‚úÖ State files (learning_progress, iteration_counter, etc.) already in repo"
          
          echo ""
          echo "=========================================="
          echo "üìä Repository now contains:"
          echo "=========================================="
          echo "CSV files: $(ls -1 "$REPO_DIR"/*.csv 2>/dev/null | wc -l)"
          echo "Merged pickles (2244): $(ls -1 "$REPO_DIR"/*_2244.pkl 2>/dev/null | wc -l)"
          echo "Indicator pickles: $(ls -1 "$REPO_DIR"/*_indicators.pkl 2>/dev/null | wc -l)"
          echo "ML SGD/RF pickles: $(ls -1 "$REPO_DIR"/*_{sgd,rf}.pkl 2>/dev/null | wc -l)"
          echo "ML state pickles: $(ls -1 "$REPO_DIR"/*_{best_chrom,population,trade_memory}.pkl 2>/dev/null | wc -l)"
          echo "Databases: $(ls -1 "$REPO_DIR"/*.db 2>/dev/null | wc -l)"
          echo "JSON files: $(ls -1 "$REPO_DIR"/*.json 2>/dev/null | wc -l)"
          echo "=========================================="
          
          echo "‚úÖ All outputs copied successfully (skipped identical files)"

      - name: Commit and push changes
        run: |
          cd /content/forex-alpha-models/forex-ai-models
          
          git add .
          
          if git diff --cached --quiet; then
            echo "‚ÑπÔ∏è  No changes to commit"
          else
            git commit -m "ü§ñ Auto-update: Ultra-Persistent Pipeline - $(date +'%Y-%m-%d %H:%M:%S UTC')"
            git push origin main || {
              echo "‚ö†Ô∏è  Push failed, trying to pull and merge..."
              git pull --rebase origin main
              git push origin main
            }
            echo "‚úÖ Changes pushed successfully"
          fi

      - name: Upload logs as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-${{ github.run_number }}
          path: |
            pipeline_output.log
            /content/forex-alpha-models/logs/*.log
          retention-days: 7
