name: Smart Forex Brain Pipeline (Auto-Detect Mode)

on:
  workflow_dispatch:
  push:
    paths:
      - 'colab_trigger.txt'
    branches:
      - main

jobs:
  run-forex-brain:
    runs-on: ubuntu-latest
    timeout-minutes: 55

    env:
      FOREX_PAT: ${{ secrets.FOREX_PAT }}
      BROWSERLESS_TOKEN: ${{ secrets.BROWSERLESS_TOKEN }}
      ALPHA_VANTAGE_KEY: ${{ secrets.ALPHA_VANTAGE_KEY }}
      GIT_USER_NAME: "Forex AI Bot"
      GIT_USER_EMAIL: "nakatonabira3@gmail.com"
      GITHUB_USERNAME: "rahim-dotAI"
      GITHUB_REPO: "forex-ai-models"
      PYTHONIOENCODING: utf-8
      PYTHONUNBUFFERED: 1
      GITHUB_ACTIONS: "true"
      SINGLE_RUN_MODE: "true"

    steps:
      - name: Pre-cleanup
        run: |
          sudo rm -rf /content 2>/dev/null || true
          rm -f .gitmodules 2>/dev/null || true

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0
          lfs: false
          submodules: false
          clean: true

      - name: Verify secrets
        run: |
          MISSING_SECRETS=()
          [ -z "${FOREX_PAT}" ] && MISSING_SECRETS+=("FOREX_PAT")
          [ -z "${BROWSERLESS_TOKEN}" ] && MISSING_SECRETS+=("BROWSERLESS_TOKEN")
          [ -z "${ALPHA_VANTAGE_KEY}" ] && MISSING_SECRETS+=("ALPHA_VANTAGE_KEY")
          if [ ${#MISSING_SECRETS[@]} -gt 0 ]; then
            echo "Missing secrets: ${MISSING_SECRETS[*]}"
          else
            echo "All secrets configured"
          fi

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install --no-cache-dir \
            mplfinance firebase-admin dropbox requests beautifulsoup4 \
            pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm \
            joblib matplotlib alpha_vantage tqdm scikit-learn river \
            jupyter nbconvert

      - name: Create directory structure
        run: |
          sudo mkdir -p /content/forex-alpha-models/{csvs,pickles,logs,merged_data_pickles,temp_pickles,forex-ai-models}
          sudo chown -R $USER:$USER /content
          sudo chmod -R 755 /content
          mkdir -p $GITHUB_WORKSPACE/outputs/{csvs,pickles,logs,merged_data_pickles,temp_pickles}

      - name: Configure Git
        run: |
          git config --global user.name "${GIT_USER_NAME}"
          git config --global user.email "${GIT_USER_EMAIL}"
          git config --global advice.detachedHead false
          git config --global credential.helper store
          echo "https://${GITHUB_USERNAME}:${FOREX_PAT}@github.com" > ~/.git-credentials

      - name: Clone forex-ai-models repository
        run: |
          cd /content/forex-alpha-models
          if [ ! -d "forex-ai-models/.git" ]; then
            echo "üîÑ Cloning forex-ai-models repository..."
            git clone -b main https://${GITHUB_USERNAME}:${FOREX_PAT}@github.com/${GITHUB_USERNAME}/${GITHUB_REPO}.git forex-ai-models
          else
            echo "üì• Pulling latest changes..."
            cd forex-ai-models
            git pull origin main
          fi
          cd forex-ai-models
          git config user.name "${GIT_USER_NAME}"
          git config user.email "${GIT_USER_EMAIL}"
          
          echo ""
          echo "üìÇ Repository contents:"
          ls -la

      - name: Check dependencies and decide execution mode
        id: check_deps
        run: |
          echo "=========================================="
          echo "CHECKING DEPENDENCIES FOR MODE SELECTION"
          echo "=========================================="
          
          REPO_PATH="/content/forex-alpha-models/forex-ai-models"
          REQUIRED_FILES_MISSING=0
          
          echo ""
          echo "üìä Checking for required files in $REPO_PATH..."
          
          # Check for CSV data files
          echo ""
          echo "1Ô∏è‚É£ Checking CSV files..."
          CSV_COUNT=$(find "$REPO_PATH" -maxdepth 1 -name "*.csv" 2>/dev/null | wc -l)
          if [ $CSV_COUNT -gt 0 ]; then
            echo "   ‚úÖ Found $CSV_COUNT CSV files"
            ls -lh "$REPO_PATH"/*.csv 2>/dev/null | head -5
          else
            echo "   ‚ùå No CSV files found"
            REQUIRED_FILES_MISSING=1
          fi
          
          # Check for pickle files
          echo ""
          echo "2Ô∏è‚É£ Checking pickle files..."
          PKL_COUNT=$(find "$REPO_PATH" -maxdepth 1 -name "*_2244.pkl" 2>/dev/null | wc -l)
          if [ $PKL_COUNT -gt 0 ]; then
            echo "   ‚úÖ Found $PKL_COUNT merged pickle files"
            ls -lh "$REPO_PATH"/*_2244.pkl 2>/dev/null | head -5
          else
            echo "   ‚ùå No merged pickle files found"
            REQUIRED_FILES_MISSING=1
          fi
          
          # Check for database files
          echo ""
          echo "3Ô∏è‚É£ Checking database files..."
          if [ -f "$REPO_PATH/infinite_memory.db" ] || [ -f "$REPO_PATH/ml_persistent_memory.db" ]; then
            echo "   ‚úÖ Database files exist"
            ls -lh "$REPO_PATH"/*.db 2>/dev/null
          else
            echo "   ‚ö†Ô∏è  No database files (will be created on first run)"
          fi
          
          echo ""
          echo "=========================================="
          
          if [ $REQUIRED_FILES_MISSING -eq 1 ]; then
            echo "mode=full" >> $GITHUB_OUTPUT
            echo "üî¥ DECISION: Running FULL pipeline"
            echo "   Reason: Required data files missing from repository"
          else
            echo "mode=ultra" >> $GITHUB_OUTPUT
            echo "üü¢ DECISION: Running ULTRA-PERSISTENT mode"
            echo "   Reason: All required files present in repository"
          fi
          
          echo "=========================================="

      - name: Copy data files to working directories
        if: steps.check_deps.outputs.mode == 'ultra'
        run: |
          echo "üì¶ Copying data from repository to working directories..."
          
          REPO_PATH="/content/forex-alpha-models/forex-ai-models"
          
          # Copy CSV files
          if ls "$REPO_PATH"/*.csv 1> /dev/null 2>&1; then
            cp "$REPO_PATH"/*.csv /content/forex-alpha-models/csvs/
            echo "‚úÖ Copied CSV files"
          fi
          
          # Copy pickle files
          if ls "$REPO_PATH"/*_2244.pkl 1> /dev/null 2>&1; then
            cp "$REPO_PATH"/*_2244.pkl /content/forex-alpha-models/merged_data_pickles/
            echo "‚úÖ Copied merged pickle files"
          fi
          
          # Copy other pickle files
          if ls "$REPO_PATH"/*.pkl 1> /dev/null 2>&1; then
            cp "$REPO_PATH"/*.pkl /content/forex-alpha-models/pickles/ 2>/dev/null || true
            echo "‚úÖ Copied other pickle files"
          fi
          
          # Copy database files
          if ls "$REPO_PATH"/*.db 1> /dev/null 2>&1; then
            cp "$REPO_PATH"/*.db /content/forex-alpha-models/forex-ai-models/
            echo "‚úÖ Copied database files"
          fi
          
          echo ""
          echo "üìä Working directory status:"
          echo "CSVs: $(ls -1 /content/forex-alpha-models/csvs/*.csv 2>/dev/null | wc -l) files"
          echo "Merged pickles: $(ls -1 /content/forex-alpha-models/merged_data_pickles/*.pkl 2>/dev/null | wc -l) files"
          echo "Other pickles: $(ls -1 /content/forex-alpha-models/pickles/*.pkl 2>/dev/null | wc -l) files"

      - name: Convert notebook to Python
        run: |
          jupyter nbconvert --to python "AI_Forex_Brain_2.ipynb" --output ai_forex_brain_2
          sed -i '/get_ipython()/d' ai_forex_brain_2.py
          sed -i '/^get_ipython/d' ai_forex_brain_2.py
          sed -i 's/\.mkdir(exist_ok=True)/.mkdir(parents=True, exist_ok=True)/g' ai_forex_brain_2.py

      - name: Apply path fixes
        run: |
          cat > path_fixes.sed << 'EOF'
          s|ROOT_DIR = Path("/content")|ROOT_DIR = Path("/content/forex-alpha-models")|g
          s|ROOT_PATH = ROOT_DIR / "forex-alpha-models"|ROOT_PATH = Path("/content/forex-alpha-models")|g
          s|BASE_DIR = Path(".")|BASE_DIR = Path("/content/forex-alpha-models")|g
          s|REPO_FOLDER = .* / "forex-ai-models"|REPO_FOLDER = Path("/content/forex-alpha-models/forex-ai-models")|g
          s|PICKLE_FOLDER = .* / "pickles"|PICKLE_FOLDER = Path("/content/forex-alpha-models/pickles")|g
          s|FINAL_PICKLE_FOLDER = .* / "merged_data_pickles"|FINAL_PICKLE_FOLDER = Path("/content/forex-alpha-models/merged_data_pickles")|g
          s|CSV_FOLDER = .* / "csvs"|CSV_FOLDER = Path("/content/forex-alpha-models/csvs")|g
          s|LOG_FOLDER = .* / "logs"|LOG_FOLDER = Path("/content/forex-alpha-models/logs")|g
          EOF
          sed -i -f path_fixes.sed ai_forex_brain_2.py

      - name: Extract THREE Ultra-Persistent Cells
        if: steps.check_deps.outputs.mode == 'ultra'
        run: |
          cat > extract_three_cells.py << 'PYTHON_EOF'
          import re
          
          with open("ai_forex_brain_2.py", "r", encoding="utf-8") as f:
              content = f.read()
          
          # Define the THREE cell markers in order
          markers = [
              {
                  'name': 'Ultra-Persistent Pipeline (VERSION 3.6)',
                  'pattern': r'# ={60,}\n# VERSION 3\.6 ‚Äì Unified Loader \+ Merge Pickles'
              },
              {
                  'name': 'Unified Pipeline (FX CSV Combine)',
                  'pattern': r'# ={60,}\n# FX CSV Combine \+ Incremental Indicators Pipeline'
              },
              {
                  'name': 'Hybrid Pipeline v7.5',
                  'pattern': r'#!/usr/bin/env python3\n"""\nUltimate Hybrid Forex Pipeline v7\.5'
              }
          ]
          
          print("=" * 80)
          print("EXTRACTING THREE ULTRA-PERSISTENT CELLS")
          print("=" * 80)
          
          cell_positions = []
          
          # Find all three cells
          for i, marker_info in enumerate(markers, 1):
              match = re.search(marker_info['pattern'], content)
              if match:
                  pos = match.start()
                  cell_positions.append({
                      'index': i,
                      'name': marker_info['name'],
                      'position': pos
                  })
                  print(f"\n‚úÖ Cell {i} Found: {marker_info['name']}")
                  print(f"   Position: {pos:,}")
                  # Show first line of content
                  first_line = content[pos:pos+100].split('\n')[0]
                  print(f"   First line: {first_line[:70]}...")
              else:
                  print(f"\n‚ùå Cell {i} NOT FOUND: {marker_info['name']}")
          
          if len(cell_positions) < 3:
              print("\n" + "=" * 80)
              print("‚ö†Ô∏è  WARNING: Not all 3 cells found!")
              print(f"   Found: {len(cell_positions)}/3 cells")
              print("=" * 80)
              
              # Fallback: extract from first found cell to end
              if cell_positions:
                  start_pos = min(c['position'] for c in cell_positions)
                  print(f"\nüîÑ Fallback: Extracting from position {start_pos:,} to END")
                  extracted_code = content[start_pos:]
              else:
                  print("\n‚ùå No cells found! Using full notebook")
                  extracted_code = content
          else:
              # Extract from FIRST cell to END of file
              start_pos = cell_positions[0]['position']
              extracted_code = content[start_pos:]
              
              print("\n" + "=" * 80)
              print("‚úÖ EXTRACTION SUCCESSFUL")
              print("=" * 80)
              print(f"\nExtracting from Cell 1 ({cell_positions[0]['name']}) to END")
              print(f"Start position: {start_pos:,}")
              print(f"Extracted length: {len(extracted_code):,} characters")
              print(f"Total lines: {len(extracted_code.split(chr(10))):,}")
          
          # Write extracted code
          with open("ai_forex_brain_2_ultra.py", "w", encoding="utf-8") as f:
              f.write(extracted_code)
          
          # Verify extraction
          lines = extracted_code.split('\n')
          print("\n" + "=" * 80)
          print("EXTRACTION SUMMARY")
          print("=" * 80)
          print(f"Total lines: {len(lines):,}")
          print(f"\nFirst 5 lines:")
          for i, line in enumerate(lines[:5], 1):
              print(f"  {i:3d}: {line[:75]}")
          print(f"\nLast 5 lines:")
          for i, line in enumerate(lines[-5:], len(lines)-4):
              print(f"  {i:3d}: {line[:75]}")
          
          # Count cells included
          cell_markers_found = sum([
              1 for m in markers 
              if re.search(m['pattern'], extracted_code)
          ])
          print(f"\nüìã Cells included in extraction: {cell_markers_found}/3")
          
          print("\n‚úÖ Ultra-persistent script ready: ai_forex_brain_2_ultra.py")
          print("=" * 80)
          PYTHON_EOF
          
          python extract_three_cells.py

      - name: Run Pipeline (Full Mode)
        if: steps.check_deps.outputs.mode == 'full'
        run: |
          echo "=========================================="
          echo "üî¥ RUNNING FULL PIPELINE MODE"
          echo "=========================================="
          echo "This will execute ALL cells in the notebook:"
          echo "  1. Data fetching from APIs"
          echo "  2. Indicator calculation"
          echo "  3. Pickle creation"
          echo "  4. Model training"
          echo "  5. Signal generation"
          echo "=========================================="
          python ai_forex_brain_2.py 2>&1 | tee pipeline_output.log
          EXIT_CODE=${PIPESTATUS[0]}
          if [ $EXIT_CODE -ne 0 ]; then
            echo "‚ùå Pipeline failed with exit code $EXIT_CODE"
            tail -n 50 pipeline_output.log
            exit $EXIT_CODE
          fi
        timeout-minutes: 50

      - name: Run Pipeline (Ultra-Persistent Mode - 3 Cells)
        if: steps.check_deps.outputs.mode == 'ultra'
        run: |
          echo "=========================================="
          echo "üü¢ RUNNING ULTRA-PERSISTENT MODE (3 CELLS)"
          echo "=========================================="
          echo "Executing in order:"
          echo "  1. VERSION 3.6 - Ultra-Persistent Pipeline"
          echo "  2. FX CSV Combine + Incremental Indicators"
          echo "  3. Ultimate Hybrid Forex Pipeline v7.5"
          echo "=========================================="
          python ai_forex_brain_2_ultra.py 2>&1 | tee pipeline_output.log
          EXIT_CODE=${PIPESTATUS[0]}
          if [ $EXIT_CODE -ne 0 ]; then
            echo "‚ùå Pipeline failed with exit code $EXIT_CODE"
            tail -n 100 pipeline_output.log
            exit $EXIT_CODE
          fi
        timeout-minutes: 50

      - name: Copy outputs back to repository
        run: |
          echo "üì¶ Copying outputs back to repository..."
          
          # Copy from working directories to repo
          cp -r /content/forex-alpha-models/csvs/*.csv \
            /content/forex-alpha-models/forex-ai-models/ 2>/dev/null || true
          
          cp -r /content/forex-alpha-models/merged_data_pickles/*_2244.pkl \
            /content/forex-alpha-models/forex-ai-models/ 2>/dev/null || true
          
          cp -r /content/forex-alpha-models/pickles/*.pkl \
            /content/forex-alpha-models/forex-ai-models/ 2>/dev/null || true
          
          cp -r /content/forex-alpha-models/forex-ai-models/*.db \
            /content/forex-alpha-models/forex-ai-models/ 2>/dev/null || true
          
          cp -r /content/forex-alpha-models/forex-ai-models/*.json \
            /content/forex-alpha-models/forex-ai-models/ 2>/dev/null || true
          
          echo "‚úÖ Outputs copied"

      - name: Commit and push changes
        run: |
          cd /content/forex-alpha-models/forex-ai-models
          
          git add .
          
          if git diff --cached --quiet; then
            echo "‚ÑπÔ∏è  No changes to commit"
          else
            git commit -m "ü§ñ Auto-update: Ultra-Persistent Pipeline - $(date +'%Y-%m-%d %H:%M:%S UTC')"
            git push origin main || {
              echo "‚ö†Ô∏è  Push failed, trying to pull and merge..."
              git pull --rebase origin main
              git push origin main
            }
            echo "‚úÖ Changes pushed successfully"
          fi

      - name: Upload logs as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-${{ github.run_number }}
          path: |
            pipeline_output.log
            /content/forex-alpha-models/logs/*.log
          retention-days: 7
