{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "SAVE_DIR = \"forex_ai_outputs\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "os.chdir(SAVE_DIR)\n",
        "print(\"Working directory:\", os.getcwd())\n",
        "\n"
      ],
      "metadata": {
        "id": "rTpnXB4JwTyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8Oonb6gP_ho"
      },
      "outputs": [],
      "source": [
        "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Fully Automatic Fresh-Run GitHub Workflow in Colab / GitHub Actions\n",
        "# -------------------------------\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "# -------------------------------\n",
        "# 0Ô∏è‚É£ User Config\n",
        "# -------------------------------\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "REPO_FOLDER = GITHUB_REPO  # Local folder\n",
        "GITHUB_PAT = \"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\"  # Working PAT\n",
        "GIT_USER_EMAIL = \"nakatonabira3@gmail.com\"\n",
        "\n",
        "# -------------------------------\n",
        "# 1Ô∏è‚É£ Install Git and Git LFS (Safe across environments)\n",
        "# -------------------------------\n",
        "print(\"‚öôÔ∏è Checking Git and Git LFS...\")\n",
        "\n",
        "def safe_run(cmd):\n",
        "    \"\"\"Run shell command safely with clear logging.\"\"\"\n",
        "    try:\n",
        "        subprocess.run(cmd, shell=True, check=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è Skipped or failed: {cmd}\\n   Reason: {e}\")\n",
        "\n",
        "USE_SUDO = shutil.which(\"sudo\") is not None\n",
        "\n",
        "if shutil.which(\"git\") is None:\n",
        "    cmd = \"apt-get update -qq && apt-get install -y git\"\n",
        "    if USE_SUDO:\n",
        "        cmd = \"sudo \" + cmd\n",
        "    safe_run(cmd)\n",
        "else:\n",
        "    print(\"‚úÖ Git already installed.\")\n",
        "\n",
        "if shutil.which(\"git-lfs\") is None:\n",
        "    cmd = \"apt-get install -y git-lfs\"\n",
        "    if USE_SUDO:\n",
        "        cmd = \"sudo \" + cmd\n",
        "    safe_run(cmd)\n",
        "else:\n",
        "    print(\"‚úÖ Git LFS already installed.\")\n",
        "\n",
        "safe_run(\"git lfs install\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2Ô∏è‚É£ Configure secure Git credentials\n",
        "# -------------------------------\n",
        "print(\"üîê Configuring secure GitHub credentials...\")\n",
        "safe_run(\"git config --global credential.helper store\")\n",
        "cred_path = os.path.expanduser(\"~/.git-credentials\")\n",
        "with open(cred_path, \"w\") as f:\n",
        "    f.write(f\"https://{GITHUB_USERNAME}:{GITHUB_PAT}@github.com\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3Ô∏è‚É£ Remove local repo (fresh run)\n",
        "# -------------------------------\n",
        "if os.path.exists(REPO_FOLDER):\n",
        "    print(f\"üóëÔ∏è Removing existing local repo '{REPO_FOLDER}' for a fresh run...\")\n",
        "    shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "# -------------------------------\n",
        "# 4Ô∏è‚É£ Configure Git identity\n",
        "# -------------------------------\n",
        "print(\"üîß Configuring Git identity...\")\n",
        "safe_run(f'git config --global user.name \"{GITHUB_USERNAME}\"')\n",
        "safe_run(f'git config --global user.email \"{GIT_USER_EMAIL}\"')\n",
        "\n",
        "# -------------------------------\n",
        "# 5Ô∏è‚É£ Clone repo fresh\n",
        "# -------------------------------\n",
        "print(f\"üì• Cloning repo '{REPO_FOLDER}' from GitHub securely...\")\n",
        "safe_run(f\"git clone https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\")\n",
        "os.chdir(REPO_FOLDER)\n",
        "\n",
        "# -------------------------------\n",
        "# 6Ô∏è‚É£ Track CSV/PKL files with Git LFS\n",
        "# -------------------------------\n",
        "print(\"üìå Tracking CSV/PKL files with Git LFS...\")\n",
        "safe_run(\"git lfs track '*.csv'\")\n",
        "safe_run(\"git lfs track '*.pkl'\")\n",
        "safe_run(\"git add .gitattributes\")\n",
        "safe_run('git commit -m \"Track CSV/PKL files with Git LFS\" || echo \"No .gitattributes changes\"')\n",
        "\n",
        "# -------------------------------\n",
        "# 7Ô∏è‚É£ Stage, commit, and push changes (Safe)\n",
        "# -------------------------------\n",
        "print(\"üìÇ Staging all new/modified files...\")\n",
        "safe_run(\"git add -A\")\n",
        "safe_run('git commit -m \"Auto-update: new or modified files\" || echo \"No new changes to commit\"')\n",
        "\n",
        "# Avoid accidental repo wipes\n",
        "file_count = sum(len(files) for _, _, files in os.walk('.'))\n",
        "if file_count < 10:\n",
        "    print(\"‚ö†Ô∏è Too few files detected ‚Äî possible clone failure. Skipping push.\")\n",
        "else:\n",
        "    print(\"üöÄ Pushing changes to GitHub...\")\n",
        "    safe_run(\"git push origin main\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8Ô∏è‚É£ List LFS-tracked files\n",
        "# -------------------------------\n",
        "print(\"üìã LFS-tracked files:\")\n",
        "safe_run(\"git lfs ls-files\")\n",
        "\n",
        "print(\"‚úÖ Fresh-run GitHub repo workflow complete!\")\n"
      ],
      "metadata": {
        "id": "XG3vVI7iiFQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JS9qXRF_JXJO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your keys (only for this session)\n",
        "os.environ['ALPHA_VANTAGE_KEY'] = '1W58NPZXOG5SLHZ6'\n",
        "os.environ['BROWSERLESS_TOKEN'] = '2St0qUktyKsA0Bsb5b510553885cae26942e44c26c0f19c3d'\n",
        "\n",
        "# Test if they work\n",
        "print(\"Alpha Vantage Key:\", os.environ.get('ALPHA_VANTAGE_KEY'))\n",
        "print(\"Browserless Token:\", os.environ.get('BROWSERLESS_TOKEN'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VkW5aGq5yFf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------\n",
        "SAVE_FOLDER = \"/content/forex-ai-models\"  # Cloned GitHub repo folder\n",
        "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "GIT_NAME = \"Abdul Rahim\"\n",
        "GIT_EMAIL = \"nakatonabira3@gmail.com\"\n",
        "\n",
        "# -----------------------------\n",
        "# GitHub Auth & Repo Info\n",
        "# -----------------------------\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "GITHUB_TOKEN = \"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\"  # Classic PAT\n",
        "BRANCH = \"main\"\n",
        "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# -----------------------------\n",
        "# Setup Git identity (once per session)\n",
        "# -----------------------------\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Helper functions\n",
        "# -----------------------------\n",
        "def fetch_alpha_vantage_fx(pair, outputsize='compact'):\n",
        "    \"\"\"Fetch FX data from Alpha Vantage.\"\"\"\n",
        "    base_url = 'https://www.alphavantage.co/query'\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "    params = {\n",
        "        'function': 'FX_DAILY',\n",
        "        'from_symbol': from_currency,\n",
        "        'to_symbol': to_currency,\n",
        "        'outputsize': outputsize,\n",
        "        'datatype': 'json',\n",
        "        'apikey': os.environ['ALPHA_VANTAGE_KEY']\n",
        "    }\n",
        "    response = requests.get(base_url, params=params, timeout=30)\n",
        "    data = response.json()\n",
        "    if 'Time Series FX (Daily)' not in data:\n",
        "        print(f\"Failed to fetch {pair}: {data}\")\n",
        "        return pd.DataFrame()\n",
        "    ts = data['Time Series FX (Daily)']\n",
        "    df = pd.DataFrame(ts).T\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df.sort_index(inplace=True)\n",
        "    return df.rename(columns={\n",
        "        '1. open': 'open',\n",
        "        '2. high': 'high',\n",
        "        '3. low': 'low',\n",
        "        '4. close': 'close'\n",
        "    }).astype(float)\n",
        "\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    \"\"\"Compute MD5 hash of a file in chunks (faster, memory efficient).\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def ensure_repo_cloned(repo_url, repo_folder, branch=\"main\"):\n",
        "    \"\"\"Clone repo if missing or pull latest if exists.\"\"\"\n",
        "    if not os.path.exists(os.path.join(repo_folder, \".git\")):\n",
        "        if os.path.exists(repo_folder):\n",
        "            subprocess.run([\"rm\", \"-rf\", repo_folder], check=True)\n",
        "        print(\"üì• Cloning repo...\")\n",
        "        try:\n",
        "            subprocess.run([\"git\", \"clone\", \"-b\", branch, repo_url, repo_folder], check=True)\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"‚ö†Ô∏è Branch '{branch}' not found. Cloning default branch...\")\n",
        "            subprocess.run([\"git\", \"clone\", repo_url, repo_folder], check=True)\n",
        "    else:\n",
        "        print(\"üîÑ Repo exists, pulling latest changes...\")\n",
        "        os.chdir(repo_folder)\n",
        "        subprocess.run([\"git\", \"fetch\", \"origin\"], check=True)\n",
        "        subprocess.run([\"git\", \"checkout\", branch], check=False)\n",
        "        subprocess.run([\"git\", \"pull\", \"origin\", branch], check=False)\n",
        "        os.chdir(\"..\")\n",
        "\n",
        "def ensure_repo_initialized(repo_folder):\n",
        "    \"\"\"Handle empty repo / initial commit and set main branch.\"\"\"\n",
        "    ensure_repo_cloned(REPO_URL, repo_folder, BRANCH)\n",
        "    os.chdir(repo_folder)\n",
        "    result = subprocess.run([\"git\", \"status\"], capture_output=True, text=True)\n",
        "    if \"nothing to commit\" in result.stdout:\n",
        "        return  # Already initialized\n",
        "    subprocess.run([\"git\", \"commit\", \"--allow-empty\", \"-m\", \"Initial commit\"], check=False)\n",
        "    subprocess.run([\"git\", \"branch\", \"-M\", \"main\"], check=False)\n",
        "    subprocess.run([\"git\", \"push\", \"-u\", \"origin\", \"main\"], check=False)\n",
        "    os.chdir(\"..\")\n",
        "\n",
        "# -----------------------------\n",
        "# Initialize repo (first time only)\n",
        "# -----------------------------\n",
        "ensure_repo_initialized(SAVE_FOLDER)\n",
        "\n",
        "# -----------------------------\n",
        "# List of currency pairs\n",
        "# -----------------------------\n",
        "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "# -----------------------------\n",
        "# Fetch, merge, save, commit & push\n",
        "# -----------------------------\n",
        "for pair in pairs:\n",
        "    filename = pair.replace(\"/\", \"_\") + \".csv\"\n",
        "    filepath = os.path.join(SAVE_FOLDER, filename)\n",
        "\n",
        "    # Load existing data (only if file exists)\n",
        "    if os.path.exists(filepath):\n",
        "        existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "        print(f\"Loaded {pair}: {existing_df.shape[0]} rows\")\n",
        "    else:\n",
        "        existing_df = pd.DataFrame()\n",
        "        print(f\"No existing data for {pair}\")\n",
        "\n",
        "    old_hash = file_hash(filepath)\n",
        "\n",
        "    # Fetch new data\n",
        "    new_df = fetch_alpha_vantage_fx(pair)\n",
        "    if new_df.empty:\n",
        "        print(f\"No new data fetched for {pair}\")\n",
        "        continue\n",
        "\n",
        "    # Merge efficiently (drop duplicates directly)\n",
        "    if not existing_df.empty:\n",
        "        combined_df = pd.concat([existing_df, new_df])\n",
        "        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
        "    else:\n",
        "        combined_df = new_df\n",
        "\n",
        "    combined_df.sort_index(inplace=True)\n",
        "    combined_df.to_csv(filepath)\n",
        "\n",
        "    new_hash = file_hash(filepath)\n",
        "\n",
        "    # Commit & push only if changed\n",
        "    if old_hash != new_hash:\n",
        "        print(f\"üîÑ Updating {pair} on GitHub...\")\n",
        "        os.chdir(SAVE_FOLDER)\n",
        "        subprocess.run([\"git\", \"add\", filename], check=False)\n",
        "        subprocess.run([\"git\", \"commit\", \"-m\", f\"Update {pair} historical FX data\"], check=False)\n",
        "        subprocess.run(\n",
        "            f\"git push https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git {BRANCH}\",\n",
        "            shell=True\n",
        "        )\n",
        "        os.chdir(\"..\")\n",
        "    else:\n",
        "        print(f\"No changes for {pair}, skipping push.\")\n",
        "\n",
        "print(\"‚úÖ All pairs processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI-GSB3t6YYf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------\n",
        "SAVE_FOLDER = \"/content/forex-ai-models\"  # Cloned GitHub repo folder\n",
        "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "GIT_NAME = \"Abdul Rahim\"\n",
        "GIT_EMAIL = \"nakatonabira3@gmail.com\"\n",
        "\n",
        "# -----------------------------\n",
        "# GitHub Auth & Repo Info\n",
        "# -----------------------------\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "GITHUB_TOKEN = \"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\"  # Classic PAT\n",
        "BRANCH = \"main\"\n",
        "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# -----------------------------\n",
        "# Setup Git identity (once per session)\n",
        "# -----------------------------\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Helper function: file hash\n",
        "# -----------------------------\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    if not os.path.exists(filepath):\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "# -----------------------------\n",
        "# Ensure repo exists, pull latest, setup LFS\n",
        "# -----------------------------\n",
        "def ensure_repo():\n",
        "    if not os.path.exists(os.path.join(SAVE_FOLDER, \".git\")):\n",
        "        if os.path.exists(SAVE_FOLDER):\n",
        "            subprocess.run([\"rm\", \"-rf\", SAVE_FOLDER], check=True)\n",
        "        print(\"üì• Cloning repo...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, SAVE_FOLDER], check=True)\n",
        "    else:\n",
        "        print(\"üîÑ Repo exists, pulling latest changes...\")\n",
        "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"fetch\", \"origin\"], check=True)\n",
        "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"checkout\", BRANCH], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"pull\", \"origin\", BRANCH], check=False)\n",
        "\n",
        "    # Install LFS and track CSV/PKL\n",
        "    print(\"‚öôÔ∏è Installing Git LFS...\")\n",
        "    subprocess.run(\"apt-get update && apt-get install git-lfs -y\", shell=True)\n",
        "    subprocess.run([\"git\", \"lfs\", \"install\"], check=False)\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"lfs\", \"track\", \"*.csv\"], check=False)\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"lfs\", \"track\", \"*.pkl\"], check=False)\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"add\", \".gitattributes\"], check=False)\n",
        "    subprocess.run(\n",
        "        f'git -C {SAVE_FOLDER} commit -m \"Track CSV/PKL files with Git LFS\" || echo \"No changes to commit for .gitattributes\"',\n",
        "        shell=True\n",
        "    )\n",
        "\n",
        "ensure_repo()\n",
        "\n",
        "# -----------------------------\n",
        "# FX pairs and timeframes\n",
        "# -----------------------------\n",
        "fx_pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "timeframes = {\n",
        "    \"1m_7d\": (\"1m\", \"7d\"),\n",
        "    \"5m_1mo\": (\"5m\", \"1mo\"),\n",
        "    \"15m_60d\": (\"15m\", \"60d\"),\n",
        "    \"1h_2y\": (\"1h\", \"2y\"),\n",
        "    \"1d_5y\": (\"1d\", \"5y\")\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Worker function for each pair/timeframe\n",
        "# -----------------------------\n",
        "def process_pair_tf(pair, tf_name, interval, period):\n",
        "    symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
        "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
        "    filepath = os.path.join(SAVE_FOLDER, filename)\n",
        "\n",
        "    # Load existing\n",
        "    if os.path.exists(filepath) and os.path.getsize(filepath) > 0:\n",
        "        existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "        print(f\"Loaded existing {pair} {tf_name}, {existing_df.shape[0]} rows\")\n",
        "    else:\n",
        "        existing_df = pd.DataFrame()\n",
        "        print(f\"No existing data for {pair} {tf_name}\")\n",
        "\n",
        "    old_hash = file_hash(filepath)\n",
        "\n",
        "    # Fetch new data\n",
        "    try:\n",
        "        df = yf.download(\n",
        "            symbol,\n",
        "            period=period,\n",
        "            interval=interval,\n",
        "            progress=False,\n",
        "            auto_adjust=False,\n",
        "            threads=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Failed to fetch {pair} {tf_name}: {e}\", None\n",
        "\n",
        "    if df.empty:\n",
        "        return f\"‚ö†Ô∏è Skipped {pair} {tf_name}: No data\", None\n",
        "\n",
        "    # Standardize\n",
        "    df = df[['Open', 'High', 'Low', 'Close']]\n",
        "    df.columns = ['open', 'high', 'low', 'close']\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    # Merge with existing\n",
        "    if not existing_df.empty:\n",
        "        combined_df = pd.concat([existing_df, df])\n",
        "        combined_df = combined_df[~combined_df.index.duplicated(keep=\"last\")]\n",
        "    else:\n",
        "        combined_df = df\n",
        "\n",
        "    combined_df.sort_index(inplace=True)\n",
        "    combined_df.to_csv(filepath)\n",
        "\n",
        "    new_hash = file_hash(filepath)\n",
        "\n",
        "    # Only mark for commit if changed\n",
        "    if old_hash != new_hash:\n",
        "        return f\"üìå Updated {pair} {tf_name}\", filename\n",
        "    else:\n",
        "        return f\"‚úÖ No changes for {pair} {tf_name}\", None\n",
        "\n",
        "# -----------------------------\n",
        "# Run all downloads in parallel\n",
        "# -----------------------------\n",
        "changed_files = []\n",
        "tasks = []\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    for pair in fx_pairs:\n",
        "        for tf_name, (interval, period) in timeframes.items():\n",
        "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period))\n",
        "\n",
        "for future in as_completed(tasks):\n",
        "    msg, filename = future.result()\n",
        "    print(msg)\n",
        "    if filename:\n",
        "        changed_files.append(filename)\n",
        "\n",
        "# -----------------------------\n",
        "# Commit & push once if needed\n",
        "# -----------------------------\n",
        "if changed_files:\n",
        "    print(f\"üöÄ Committing {len(changed_files)} updated files in one push...\")\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"add\"] + changed_files, check=False)\n",
        "    subprocess.run(\n",
        "        [\"git\", \"-C\", SAVE_FOLDER, \"commit\", \"-m\", \"Update multiple FX files\"],\n",
        "        check=False\n",
        "    )\n",
        "    subprocess.run(\n",
        "        f\"git -C {SAVE_FOLDER} push https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git {BRANCH}\",\n",
        "        shell=True\n",
        "    )\n",
        "else:\n",
        "    print(\"‚úÖ No changes detected, nothing to push.\")\n",
        "\n",
        "print(\"üéØ All yfinance pairs & timeframes processed (parallel, single push).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po616iyd7jAy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------\n",
        "SAVE_FOLDER = \"/content/forex-ai-models\"\n",
        "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "GIT_NAME = \"Abdul Rahim\"\n",
        "GIT_EMAIL = \"nakatonabira3@gmail.com\"\n",
        "\n",
        "# Setup Git identity\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "\n",
        "# -----------------------------\n",
        "# GitHub repo info\n",
        "# -----------------------------\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "GITHUB_TOKEN = \"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\"  # classic PAT\n",
        "BRANCH = \"main\"\n",
        "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# -----------------------------\n",
        "# Ensure repo exists, pull latest, track LFS\n",
        "# -----------------------------\n",
        "def ensure_repo():\n",
        "    if not os.path.exists(os.path.join(SAVE_FOLDER, \".git\")):\n",
        "        if os.path.exists(SAVE_FOLDER):\n",
        "            subprocess.run([\"rm\", \"-rf\", SAVE_FOLDER], check=True)\n",
        "        print(\"üì• Cloning repo...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, SAVE_FOLDER], check=True)\n",
        "    else:\n",
        "        print(\"üîÑ Repo exists, pulling latest changes...\")\n",
        "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"fetch\", \"origin\"], check=True)\n",
        "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"checkout\", BRANCH], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"pull\", \"origin\", BRANCH], check=False)\n",
        "\n",
        "    # Track combined CSVs with Git LFS\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"lfs\", \"track\", \"*_combined.csv\"], check=False)\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"add\", \".gitattributes\"], check=False)\n",
        "    subprocess.run(\n",
        "        f'git -C {SAVE_FOLDER} commit -m \"Track combined CSVs with Git LFS\" || echo \"No changes for .gitattributes\"',\n",
        "        shell=True\n",
        "    )\n",
        "\n",
        "ensure_repo()\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: file hash\n",
        "# -----------------------------\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    if not os.path.exists(filepath):\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: combine AV daily and YF higher-frequency data\n",
        "# -----------------------------\n",
        "def combine_fx_data(av_df, yf_df):\n",
        "    if av_df is None or av_df.empty:\n",
        "        return yf_df\n",
        "    if yf_df is None or yf_df.empty:\n",
        "        return av_df\n",
        "\n",
        "    if av_df.index.tz is not None:\n",
        "        av_df.index = av_df.index.tz_convert(None)\n",
        "    if yf_df.index.tz is not None:\n",
        "        yf_df.index = yf_df.index.tz_convert(None)\n",
        "\n",
        "    combined_df = pd.merge(yf_df, av_df[['open','high','low','close']],\n",
        "                           left_index=True, right_index=True,\n",
        "                           how='outer', suffixes=('','_av'))\n",
        "\n",
        "    for col in ['open','high','low','close']:\n",
        "        combined_df[col] = combined_df[col].fillna(combined_df[f'{col}_av'])\n",
        "    combined_df.drop(columns=[f'{col}_av' for col in ['open','high','low','close']], errors='ignore', inplace=True)\n",
        "    combined_df.sort_index(inplace=True)\n",
        "    combined_df.dropna(subset=['open','high','low','close'], inplace=True)\n",
        "    for col in ['open','high','low','close']:\n",
        "        combined_df[col] = combined_df[col].astype(float)\n",
        "    return combined_df\n",
        "\n",
        "# -----------------------------\n",
        "# FX pairs and timeframes\n",
        "# -----------------------------\n",
        "pairs = [\"EUR/USD\",\"GBP/USD\",\"USD/JPY\",\"AUD/USD\"]\n",
        "timeframes = [\"1m_7d\", \"5m_1mo\", \"15m_60d\", \"1h_2y\", \"1d_5y\"]\n",
        "\n",
        "# -----------------------------\n",
        "# Worker function for one pair/timeframe\n",
        "# -----------------------------\n",
        "def process_pair_tf(pair, tf_name):\n",
        "    av_file = os.path.join(SAVE_FOLDER, pair.replace(\"/\",\"_\")+\"_daily.csv\")\n",
        "    av_df = pd.read_csv(av_file, index_col=0, parse_dates=True) if os.path.exists(av_file) else pd.DataFrame()\n",
        "\n",
        "    yf_file = os.path.join(SAVE_FOLDER, f\"{pair.replace('/','_')}_{tf_name}.csv\")\n",
        "    yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
        "\n",
        "    combined_df = combine_fx_data(av_df, yf_df)\n",
        "    if combined_df.empty:\n",
        "        return f\"No data to combine for {pair} {tf_name}\", None\n",
        "\n",
        "    combined_file = os.path.join(SAVE_FOLDER, f\"{pair.replace('/','_')}_{tf_name}_combined.csv\")\n",
        "    old_hash = file_hash(combined_file)\n",
        "    combined_df.to_csv(combined_file)\n",
        "    new_hash = file_hash(combined_file)\n",
        "\n",
        "    if old_hash != new_hash:\n",
        "        return f\"Changes detected for {pair} {tf_name}, saving combined CSV\", combined_file\n",
        "    else:\n",
        "        return f\"No changes for {pair} {tf_name}, skipping save\", None\n",
        "\n",
        "# -----------------------------\n",
        "# Run all combinations in parallel\n",
        "# -----------------------------\n",
        "changed_files = []\n",
        "tasks = []\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    for pair in pairs:\n",
        "        for tf_name in timeframes:\n",
        "            tasks.append(executor.submit(process_pair_tf, pair, tf_name))\n",
        "\n",
        "for future in as_completed(tasks):\n",
        "    msg, filename = future.result()\n",
        "    print(msg)\n",
        "    if filename:\n",
        "        changed_files.append(filename)\n",
        "\n",
        "# -----------------------------\n",
        "# Commit & push once if any files changed\n",
        "# -----------------------------\n",
        "if changed_files:\n",
        "    print(f\"üöÄ Committing {len(changed_files)} combined files in one push...\")\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"add\"] + changed_files, check=False)\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"commit\", \"-m\", \"Update combined FX data\"], check=False)\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"push\", \"origin\", \"main\"], check=False)\n",
        "else:\n",
        "    print(\"‚úÖ No combined files changed, nothing to push.\")\n",
        "\n",
        "print(\"‚úÖ All FX pairs processed and combined (parallel, single push).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z96OOkCQSzof"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import re\n",
        "\n",
        "def fetch_live_rate(pair):\n",
        "    \"\"\"\n",
        "    Fetch live FX rate from X-Rates using Browserless.\n",
        "    \"\"\"\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "    browserless_token = os.environ.get('BROWSERLESS_TOKEN')\n",
        "    if not browserless_token:\n",
        "        raise ValueError(\"Set BROWSERLESS_TOKEN in your environment variables\")\n",
        "\n",
        "    url = f\"https://production-sfo.browserless.io/content?token={browserless_token}\"\n",
        "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
        "\n",
        "    try:\n",
        "        res = requests.post(url, json=payload)\n",
        "        # Regex to extract the FX value\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
        "        return float(match.group(1).replace(',', '')) if match else 0\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch {pair}: {e}\")\n",
        "        return 0\n",
        "\n",
        "# --- Fetch live prices for all pairs ---\n",
        "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "live_prices = {pair: fetch_live_rate(pair) for pair in pairs}\n",
        "\n",
        "for pair, price in live_prices.items():\n",
        "    print(f\"{pair}: {price}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn3FtCL-9kXV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import subprocess\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------\n",
        "SAVE_FOLDER = \"/content/forex-ai-models\"\n",
        "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "GIT_NAME = \"Abdul Rahim\"\n",
        "GIT_EMAIL = \"nakatonabira3@gmail.com\"\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "\n",
        "combined_save_path = os.path.join(SAVE_FOLDER, \"combined_with_indicators\")\n",
        "Path(combined_save_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "# -----------------------------\n",
        "# Helper Functions\n",
        "# -----------------------------\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    if not os.path.exists(filepath):\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def combine_fx_data(av_df, yf_df):\n",
        "    \"\"\"Merge AlphaVantage + Yahoo Finance data\"\"\"\n",
        "    if av_df is None or av_df.empty:\n",
        "        return yf_df\n",
        "    if yf_df is None or yf_df.empty:\n",
        "        return av_df\n",
        "    if av_df.index.tz is not None:\n",
        "        av_df.index = av_df.index.tz_convert(None)\n",
        "    if yf_df.index.tz is not None:\n",
        "        yf_df.index = yf_df.index.tz_convert(None)\n",
        "    combined_df = pd.merge(yf_df, av_df[['open','high','low','close']],\n",
        "                           left_index=True, right_index=True,\n",
        "                           how='outer', suffixes=('','_av'))\n",
        "    for col in ['open','high','low','close']:\n",
        "        combined_df[col] = combined_df[col].fillna(combined_df[f'{col}_av'])\n",
        "    combined_df.drop(columns=[f'{col}_av' for col in ['open','high','low','close']], inplace=True, errors='ignore')\n",
        "    combined_df.sort_index(inplace=True)\n",
        "    combined_df.dropna(subset=['open','high','low','close'], inplace=True)\n",
        "    for col in ['open','high','low','close']:\n",
        "        combined_df[col] = combined_df[col].astype(float)\n",
        "    return combined_df\n",
        "\n",
        "def add_all_indicators(df):\n",
        "    \"\"\"Add trend, momentum, volatility, and support/resistance indicators\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "\n",
        "    # Trend indicators\n",
        "    trend_indicators = {\n",
        "        'SMA_10': lambda df: ta.trend.sma_indicator(df['close'], window=10),\n",
        "        'SMA_50': lambda df: ta.trend.sma_indicator(df['close'], window=50),\n",
        "        'SMA_200': lambda df: ta.trend.sma_indicator(df['close'], window=200),\n",
        "        'EMA_10': lambda df: ta.trend.ema_indicator(df['close'], window=10),\n",
        "        'EMA_50': lambda df: ta.trend.ema_indicator(df['close'], window=50),\n",
        "        'EMA_200': lambda df: ta.trend.ema_indicator(df['close'], window=200),\n",
        "        'MACD': lambda df: ta.trend.macd(df['close']),\n",
        "        'MACD_signal': lambda df: ta.trend.macd_signal(df['close']),\n",
        "        'ADX': lambda df: ta.trend.adx(df['high'], df['low'], df['close'], window=14)\n",
        "    }\n",
        "\n",
        "    # Momentum indicators\n",
        "    momentum_indicators = {\n",
        "        'RSI_14': lambda df: ta.momentum.rsi(df['close'], window=14),\n",
        "        'StochRSI': lambda df: ta.momentum.stochrsi(df['close'], window=14),\n",
        "        'CCI': lambda df: ta.trend.cci(df['high'], df['low'], df['close'], window=20),\n",
        "        'ROC': lambda df: ta.momentum.roc(df['close'], window=12),\n",
        "        'Williams_%R': lambda df: WilliamsRIndicator(df['high'], df['low'], df['close'], lbp=14).williams_r()\n",
        "    }\n",
        "\n",
        "    # Volatility indicators\n",
        "    volatility_indicators = {\n",
        "        'Bollinger_High': lambda df: ta.volatility.bollinger_hband(df['close'], window=20, window_dev=2),\n",
        "        'Bollinger_Low': lambda df: ta.volatility.bollinger_lband(df['close'], window=20, window_dev=2),\n",
        "        'ATR': lambda df: ta.volatility.average_true_range(df['high'], df['low'], df['close'], window=14),\n",
        "        'STDDEV_20': lambda df: df['close'].rolling(window=20).std(),\n",
        "    }\n",
        "\n",
        "    # Volume indicators (optional)\n",
        "    volume_indicators = {}\n",
        "    if 'volume' in df.columns:\n",
        "        volume_indicators = {\n",
        "            'OBV': lambda df: ta.volume.on_balance_volume(df['close'], df['volume']),\n",
        "            'MFI': lambda df: ta.volume.money_flow_index(df['high'], df['low'], df['close'], df['volume'], window=14)\n",
        "        }\n",
        "\n",
        "    # Combine all indicators\n",
        "    all_indicators = {**trend_indicators, **momentum_indicators, **volatility_indicators, **volume_indicators}\n",
        "    for name, func in all_indicators.items():\n",
        "        try:\n",
        "            df[name] = func(df)\n",
        "        except:\n",
        "            df[name] = np.nan\n",
        "\n",
        "    # Crossovers\n",
        "    df['EMA_10_cross_EMA_50'] = (df['EMA_10'] > df['EMA_50']).astype(int)\n",
        "    df['EMA_50_cross_EMA_200'] = (df['EMA_50'] > df['EMA_200']).astype(int)\n",
        "    df['SMA_10_cross_SMA_50'] = (df['SMA_10'] > df['SMA_50']).astype(int)\n",
        "    df['SMA_50_cross_SMA_200'] = (df['SMA_50'] > df['SMA_200']).astype(int)\n",
        "\n",
        "    # Support/Resistance\n",
        "    df['recent_high'] = df['high'].rolling(20).max()\n",
        "    df['recent_low'] = df['low'].rolling(20).min()\n",
        "    df['pivot_point'] = (df['high'] + df['low'] + df['close'])/3\n",
        "    df['support_1'] = 2*df['pivot_point'] - df['high']\n",
        "    df['resistance_1'] = 2*df['pivot_point'] - df['low']\n",
        "\n",
        "    # Fill NaNs and normalize\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df.ffill(inplace=True)\n",
        "    df.bfill(inplace=True)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        scaler = MinMaxScaler()\n",
        "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # Long/Short scores for GA\n",
        "    df['long_score'] = df['EMA_10_cross_EMA_50'] + df['EMA_50_cross_EMA_200'] + df['SMA_10_cross_SMA_50']*0.5 + df['SMA_50_cross_SMA_200']*0.5 + df['ADX'] + df['RSI_14']\n",
        "    df['short_score'] = (1 - df['EMA_10_cross_EMA_50']) + (1 - df['EMA_50_cross_EMA_200']) + (1 - df['SMA_10_cross_SMA_50'])*0.5 + (1 - df['SMA_50_cross_SMA_200'])*0.5 + (1 - df['ADX']) + (1 - df['RSI_14'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# Worker function\n",
        "# -----------------------------\n",
        "def process_pair_file(pair, tf_file):\n",
        "    av_file = os.path.join(SAVE_FOLDER, f\"{pair.replace('/','_')}_daily.csv\")\n",
        "    try:\n",
        "        av_df = pd.read_csv(av_file, index_col=0, parse_dates=True)\n",
        "    except Exception:\n",
        "        av_df = pd.DataFrame()\n",
        "    tf_name = tf_file.replace(\".csv\",\"\").replace(f\"{pair.replace('/','_')}_\",\"\")\n",
        "    try:\n",
        "        yf_df = pd.read_csv(os.path.join(SAVE_FOLDER, tf_file), index_col=0, parse_dates=True)\n",
        "    except Exception:\n",
        "        print(f\"‚ö†Ô∏è Failed to read {tf_file}\")\n",
        "        return None, f\"{pair} {tf_name} (skipped)\"\n",
        "    combined_df = combine_fx_data(av_df, yf_df)\n",
        "    combined_df = add_all_indicators(combined_df)\n",
        "\n",
        "    save_file = os.path.join(combined_save_path, f\"{pair.replace('/','_')}_{tf_name}_combined.pkl\")\n",
        "    old_hash = file_hash(save_file)\n",
        "    combined_df.to_pickle(save_file, protocol=4)\n",
        "    new_hash = file_hash(save_file)\n",
        "    return save_file if old_hash != new_hash else None, f\"{pair} {tf_name}\"\n",
        "\n",
        "# -----------------------------\n",
        "# Run in parallel\n",
        "# -----------------------------\n",
        "changed_files = []\n",
        "tasks = []\n",
        "\n",
        "# Pre-filter files to process for better thread management\n",
        "files_to_process = []\n",
        "for pair in pairs:\n",
        "    for tf_file in os.listdir(SAVE_FOLDER):\n",
        "        if not tf_file.startswith(pair.replace('/','_')):\n",
        "            continue\n",
        "        # Exclude daily & already combined files\n",
        "        if tf_file.endswith(\"daily.csv\") or tf_file.endswith(\"_combined.csv\") or tf_file.endswith(\"_combined.pkl\"):\n",
        "            continue\n",
        "        files_to_process.append((pair, tf_file))\n",
        "\n",
        "# Use optimal number of workers (minimum 1)\n",
        "max_workers = max(1, min(8, len(files_to_process), (os.cpu_count() or 4) * 2))\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "    for pair, tf_file in files_to_process:\n",
        "        tasks.append(executor.submit(process_pair_file, pair, tf_file))\n",
        "\n",
        "    for future in as_completed(tasks):\n",
        "        changed_file, msg = future.result()\n",
        "        print(msg)\n",
        "        if changed_file:\n",
        "            changed_files.append(changed_file)\n",
        "\n",
        "# -----------------------------\n",
        "# Commit & push once\n",
        "# -----------------------------\n",
        "if changed_files:\n",
        "    print(f\"üöÄ Committing {len(changed_files)} files in one push...\")\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"add\"] + changed_files, check=False)\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"commit\", \"-m\", \"Update combined data + indicators\"], check=False)\n",
        "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"push\", \"origin\", \"main\"], check=False)\n",
        "else:\n",
        "    print(\"‚úÖ No changes detected, nothing to push.\")\n",
        "\n",
        "print(\"‚úÖ All FX pairs processed, combined, indicators added, and saved (parallel, single push).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLrjBiQ7e9zb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import datetime as dt\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "TIMEFRAMES = {\n",
        "    \"1m_7d\": (\"1m\", \"7d\"),\n",
        "    \"5m_1mo\": (\"5m\", \"1mo\"),\n",
        "    \"15m_60d\": (\"15m\", \"60d\"),\n",
        "    \"1h_2y\": (\"1h\", \"2y\"),\n",
        "    \"1d_5y\": (\"1d\", \"5y\"),\n",
        "}\n",
        "INJECT_CANDLES = 5\n",
        "MODEL_DIR = Path(\"models\")\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "BROKER_JSON = \"broker_signals.json\"\n",
        "BROKER_LOG = \"broker_signals_log.csv\"\n",
        "\n",
        "# -----------------------------\n",
        "# Delete old models to prevent feature mismatch\n",
        "# -----------------------------\n",
        "for f in MODEL_DIR.glob(\"*.pkl\"):\n",
        "    f.unlink()\n",
        "\n",
        "# -----------------------------\n",
        "# LIVE PRICE FETCH\n",
        "# -----------------------------\n",
        "def fetch_live_rate(pair):\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "    browserless_token = os.environ.get('BROWSERLESS_TOKEN')\n",
        "    if not browserless_token:\n",
        "        return 0\n",
        "    url = f\"https://production-sfo.browserless.io/content?token={browserless_token}\"\n",
        "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
        "    try:\n",
        "        res = requests.post(url, json=payload)\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
        "        return float(match.group(1).replace(',', '')) if match else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "live_prices = {pair: fetch_live_rate(pair) for pair in PAIRS}\n",
        "\n",
        "# -----------------------------\n",
        "# DATA & INDICATORS\n",
        "# -----------------------------\n",
        "def fetch_data(symbol, interval, period):\n",
        "    df = yf.download(symbol.replace('/', '') + \"=X\", interval=interval, period=period,\n",
        "                     progress=False, auto_adjust=True)\n",
        "    df.dropna(inplace=True)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [col[0] for col in df.columns]\n",
        "    return df\n",
        "\n",
        "def inject_live_price(df, live_price, n_candles=INJECT_CANDLES):\n",
        "    df_copy = df.copy()\n",
        "    n_inject = min(n_candles, len(df_copy))\n",
        "    for col in [\"Open\",\"High\",\"Low\",\"Close\"]:\n",
        "        if col in df_copy.columns:\n",
        "            df_copy.iloc[-n_inject:, df_copy.columns.get_loc(col)] = live_price\n",
        "    return df_copy\n",
        "\n",
        "def add_all_indicators(df):\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "\n",
        "    # Classic indicators\n",
        "    df[\"SMA_10\"] = ta.trend.sma_indicator(df[\"Close\"], 10)\n",
        "    df[\"SMA_50\"] = ta.trend.sma_indicator(df[\"Close\"], 50)\n",
        "    df[\"EMA_10\"] = ta.trend.ema_indicator(df[\"Close\"], 10)\n",
        "    df[\"EMA_50\"] = ta.trend.ema_indicator(df[\"Close\"], 50)\n",
        "    df[\"EMA_200\"] = ta.trend.ema_indicator(df[\"Close\"], 200)\n",
        "    df[\"MACD\"] = ta.trend.macd(df[\"Close\"])\n",
        "    df[\"MACD_signal\"] = ta.trend.macd_signal(df[\"Close\"])\n",
        "    df[\"ADX\"] = ta.trend.adx(df[\"High\"], df[\"Low\"], df[\"Close\"], window=14)\n",
        "    df[\"WilliamsR\"] = WilliamsRIndicator(df[\"High\"], df[\"Low\"], df[\"Close\"], lbp=14).williams_r()\n",
        "    df[\"Bollinger_High\"] = ta.volatility.bollinger_hband(df[\"Close\"], window=20)\n",
        "    df[\"Bollinger_Low\"] = ta.volatility.bollinger_lband(df[\"Close\"], window=20)\n",
        "    df[\"ATR\"] = ta.volatility.average_true_range(df[\"High\"], df[\"Low\"], df[\"Close\"], window=14)\n",
        "\n",
        "    # ML features\n",
        "    close = df[\"Close\"].values\n",
        "    df[\"return\"] = np.concatenate([[0], (close[1:] - close[:-1])/close[:-1]])\n",
        "    df[\"ma_fast\"] = pd.Series(close).rolling(5).mean()\n",
        "    df[\"ma_slow\"] = pd.Series(close).rolling(20).mean()\n",
        "    delta = np.diff(close, prepend=close[0])\n",
        "    gain = np.where(delta>0, delta, 0)\n",
        "    loss = np.where(delta<0, -delta, 0)\n",
        "    avg_gain = pd.Series(gain).rolling(14).mean()\n",
        "    avg_loss = pd.Series(loss).rolling(14).mean()\n",
        "    rs = avg_gain / avg_loss.replace(0, 1e-9)\n",
        "    df[\"rsi\"] = 100 - (100/(1+rs))\n",
        "\n",
        "    # Fill NaNs and scale\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df.fillna(0, inplace=True)\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        scaler = MinMaxScaler()\n",
        "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "    return df\n",
        "\n",
        "def generate_features(df):\n",
        "    df_features = add_all_indicators(df)\n",
        "    feat = df_features.select_dtypes(include=[np.number])\n",
        "    return feat\n",
        "\n",
        "def make_labels(df):\n",
        "    future = df[\"Close\"].shift(-1)\n",
        "    signal = np.where(future>df[\"Close\"], 1, np.where(future<df[\"Close\"], -1, 0))\n",
        "    return signal[:-1]\n",
        "\n",
        "# -----------------------------\n",
        "# MODEL TRAIN/LOAD\n",
        "# -----------------------------\n",
        "def train_or_load_model(features, labels, model_path):\n",
        "    if features.empty or len(labels)==0:\n",
        "        return None, []\n",
        "    trained_features = list(features.columns)\n",
        "    model = SGDClassifier(loss=\"log_loss\", max_iter=1000, tol=1e-3)\n",
        "    model.fit(features, labels)\n",
        "    joblib.dump({\"model\": model, \"features\": trained_features}, model_path)\n",
        "    return model, trained_features\n",
        "\n",
        "def hybrid_signal(model, features, trained_features):\n",
        "    if model is None or features.empty:\n",
        "        return pd.Series([0]*len(features), index=features.index)\n",
        "    for col in trained_features:\n",
        "        if col not in features.columns:\n",
        "            features[col] = 0\n",
        "    features_aligned = features[trained_features].copy()\n",
        "    features_aligned.fillna(0, inplace=True)\n",
        "    preds = model.predict(features_aligned)\n",
        "    return pd.Series(preds, index=features.index)\n",
        "\n",
        "# -----------------------------\n",
        "# PROCESS ONE PAIR/TIMEFRAME\n",
        "# -----------------------------\n",
        "def process_pair_tf(pair, tf_name, interval, period, live_price):\n",
        "    df = fetch_data(pair, interval, period)\n",
        "    if len(df)<2:\n",
        "        return None, None, None\n",
        "    df_live = inject_live_price(df, live_price)\n",
        "    features = generate_features(df_live)\n",
        "    labels = make_labels(df_live)\n",
        "    features = features.iloc[:len(labels)]\n",
        "    model_path = MODEL_DIR / f\"{pair.replace('/','_')}_{tf_name}.pkl\"\n",
        "    model, trained_features = train_or_load_model(features, labels, model_path)\n",
        "    signals = hybrid_signal(model, features, trained_features)\n",
        "    df_live = df_live.iloc[:len(signals)].copy()\n",
        "    df_live[\"hybrid_signal\"] = signals.values\n",
        "    latest_signal = int(df_live[\"hybrid_signal\"].iloc[-1])\n",
        "    long_count = int((df_live[\"hybrid_signal\"]==1).sum())\n",
        "    short_count = int((df_live[\"hybrid_signal\"]==-1).sum())\n",
        "    hold_count = int((df_live[\"hybrid_signal\"]==0).sum())\n",
        "    result = {\n",
        "        \"pair\": pair,\n",
        "        \"timeframe\": tf_name,\n",
        "        \"long\": long_count,\n",
        "        \"short\": short_count,\n",
        "        \"hold\": hold_count,\n",
        "        \"latest_signal\": latest_signal,\n",
        "        \"live_price\": live_price\n",
        "    }\n",
        "    return result, df, df_live\n",
        "\n",
        "# -----------------------------\n",
        "# RUN HYBRID PIPELINE\n",
        "# -----------------------------\n",
        "def run_hybrid():\n",
        "    broker_output = {\"timestamp\": dt.datetime.now(dt.timezone.utc).isoformat(), \"pairs\": {}}\n",
        "    log_rows = []\n",
        "    tasks = []\n",
        "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
        "        for pair in PAIRS:\n",
        "            live_price = live_prices[pair]\n",
        "            broker_output[\"pairs\"][pair] = {}\n",
        "            for tf_name, (interval, period) in TIMEFRAMES.items():\n",
        "                tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period, live_price))\n",
        "        for future in as_completed(tasks):\n",
        "            result, df, df_live = future.result()\n",
        "            if result is None:\n",
        "                continue\n",
        "            pair, tf_name = result[\"pair\"], result[\"timeframe\"]\n",
        "            broker_output[\"pairs\"][pair][tf_name] = {\n",
        "                \"long\": result[\"long\"],\n",
        "                \"short\": result[\"short\"],\n",
        "                \"hold\": result[\"hold\"],\n",
        "                \"latest_signal\": result[\"latest_signal\"],\n",
        "                \"live_price\": float(result[\"live_price\"])\n",
        "            }\n",
        "            log_rows.append({\n",
        "                \"timestamp\": dt.datetime.now(dt.timezone.utc),\n",
        "                \"pair\": pair,\n",
        "                \"timeframe\": tf_name,\n",
        "                **{k: result[k] for k in [\"long\",\"short\",\"hold\",\"latest_signal\"]},\n",
        "                \"live_price\": result[\"live_price\"]\n",
        "            })\n",
        "    # Save JSON\n",
        "    with open(BROKER_JSON, \"w\") as f:\n",
        "        json.dump(broker_output, f, indent=2)\n",
        "    # Append CSV log\n",
        "    log_df = pd.DataFrame(log_rows)\n",
        "    if not os.path.exists(BROKER_LOG):\n",
        "        log_df.to_csv(BROKER_LOG, index=False)\n",
        "    else:\n",
        "        log_df.to_csv(BROKER_LOG, mode=\"a\", header=False, index=False)\n",
        "    print(f\"üíæ Broker JSON saved: {BROKER_JSON}\")\n",
        "    print(f\"üìë Signals logged to CSV: {BROKER_LOG}\")\n",
        "    return broker_output\n",
        "\n",
        "# -----------------------------\n",
        "# RUN SCRIPT\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    signals = run_hybrid()\n",
        "    print(\"\\nüìä Latest Signals JSON:\\n\", json.dumps(signals, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyFRcSiOmP4I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "CSV_FOLDER = \"/content/forex-ai-models\"  # folder where fetched CSVs are\n",
        "SAVE_FOLDER = \"./combined_data\"          # folder for backtest-ready .pkl files\n",
        "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "pairs = [\"EUR/USD\",\"GBP/USD\",\"USD/JPY\",\"AUD/USD\"]\n",
        "\n",
        "# -----------------------------\n",
        "# Helper function: temporary hybrid signal\n",
        "# -----------------------------\n",
        "def generate_temp_signal(df, fast=5, slow=20):\n",
        "    if len(df) < slow:\n",
        "        return pd.Series([0]*len(df), index=df.index)\n",
        "    fast_ma = df['close'].rolling(fast).mean()\n",
        "    slow_ma = df['close'].rolling(slow).mean()\n",
        "    signal = (fast_ma - slow_ma).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
        "    return signal.fillna(0)\n",
        "\n",
        "# -----------------------------\n",
        "# Process all CSVs\n",
        "# -----------------------------\n",
        "for pair in pairs:\n",
        "    csv_files = list(Path(CSV_FOLDER).glob(f\"{pair.replace('/','_')}_*_combined.csv\"))\n",
        "\n",
        "    if not csv_files:\n",
        "        print(f\"‚ö†Ô∏è No CSV files found for {pair}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
        "\n",
        "            # Check required OHLC columns\n",
        "            for col in ['open','high','low','close']:\n",
        "                if col not in df.columns:\n",
        "                    print(f\"‚ö†Ô∏è {csv_file} missing column {col}, skipping.\")\n",
        "                    continue\n",
        "\n",
        "            # Generate temporary hybrid_signal based on MA\n",
        "            df['hybrid_signal'] = generate_temp_signal(df)\n",
        "\n",
        "            # Compute ATR if missing\n",
        "            if 'atr' not in df.columns:\n",
        "                high = df['high'].values\n",
        "                low = df['low'].values\n",
        "                close = df['close'].values\n",
        "                tr = pd.Series(\n",
        "                    [max(h-l, abs(h-close[i-1]), abs(l-close[i-1])) if i>0 else h-l\n",
        "                     for i,(h,l) in enumerate(zip(high,low))]\n",
        "                )\n",
        "                df['atr'] = tr.rolling(14).mean().fillna(1e-5).clip(lower=1e-5)\n",
        "\n",
        "            # Save as pickle\n",
        "            pkl_file = Path(SAVE_FOLDER) / f\"{csv_file.stem}.pkl\"\n",
        "            df.to_pickle(pkl_file)\n",
        "            print(f\"‚úÖ Saved {pkl_file} with temporary hybrid_signal\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to process {csv_file}: {e}\")\n",
        "\n",
        "print(\"üéØ All CSVs processed, hybrid_signal generated, and converted to .pkl for backtest.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFBLL-2I6h0u"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Hybrid Vectorized Backtest + GA + Momentum-aware Live Browserless Signals + Email\n",
        "- Multi-timeframe calculations preserved\n",
        "- GA loads previous population and continues from last generation\n",
        "- Live signals with SL, TP, and 1-100 scoring\n",
        "- High-confidence trade flags included\n",
        "- Signals sent via Gmail (hardcoded App password)\n",
        "- GA evaluation fully parallelized\n",
        "\"\"\"\n",
        "\n",
        "import os, json, pickle, random, re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "SAVE_FOLDER = \"/content/forex-ai-models/combined_with_indicators\"\n",
        "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
        "BEST_CHROM_FILE = \"/content/forex-ai-models/best_chromosome.pkl\"\n",
        "TRADE_MEMORY_FILE = \"/content/forex-ai-models/trade_memory.pkl\"\n",
        "POPULATION_FILE = \"/content/forex-ai-models/population.pkl\"\n",
        "GEN_COUNT_FILE = \"/content/forex-ai-models/generation_count.pkl\"\n",
        "SIGNALS_JSON_PATH = \"/content/forex-ai-models/broker_signals.json\"\n",
        "\n",
        "pairs = ['EUR/USD', 'GBP/USD', 'USD/JPY', 'AUD/USD']\n",
        "\n",
        "ATR_PERIOD = 14\n",
        "MIN_ATR = 1e-5\n",
        "BASE_CAPITAL = 100\n",
        "MAX_POSITION_FRACTION = 0.1\n",
        "\n",
        "POPULATION_SIZE = 12\n",
        "GENERATIONS = 10\n",
        "MUTATION_RATE = 0.2\n",
        "EARLY_STOPPING = 5\n",
        "TOURNAMENT_SIZE = 3\n",
        "EPS = 1e-8\n",
        "\n",
        "# -----------------------------\n",
        "# Gmail App password\n",
        "# -----------------------------\n",
        "GMAIL_USER = \"nakatonabira3@gmail.com\"\n",
        "GMAIL_APP_PASSWORD = \"gmwohahtltmcewug\"  # <-- your App password\n",
        "\n",
        "# -----------------------------\n",
        "# Browserless fetch\n",
        "# -----------------------------\n",
        "def fetch_live_rate(pair: str, timeout: int = 8) -> float:\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "    token = os.environ.get('BROWSERLESS_TOKEN')\n",
        "    if not token: return 0.0\n",
        "    url = f\"https://production-sfo.browserless.io/content?token={token}\"\n",
        "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
        "    try:\n",
        "        res = requests.post(url, json=payload, timeout=timeout)\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
        "        return float(match.group(1).replace(',', '')) if match else 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è fetch_live_rate error for {pair}: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# -----------------------------\n",
        "# Data prep\n",
        "# -----------------------------\n",
        "def make_index_tz_naive(df):\n",
        "    if isinstance(df.index, pd.DatetimeIndex) and df.index.tz is not None:\n",
        "        return df.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def seed_hybrid_signal_if_needed(df):\n",
        "    if 'hybrid_signal' not in df.columns: df['hybrid_signal'] = 0.0\n",
        "    if df['hybrid_signal'].abs().sum() == 0:\n",
        "        fast = df['close'].rolling(10, min_periods=1).mean()\n",
        "        slow = df['close'].rolling(50, min_periods=1).mean()\n",
        "        df['hybrid_signal'] = (fast - slow).fillna(0)\n",
        "    df['hybrid_signal'] = df['hybrid_signal'].fillna(0).astype(float)\n",
        "    return df\n",
        "\n",
        "def ensure_atr(df):\n",
        "    if 'atr' in df.columns and not df['atr'].isnull().all():\n",
        "        df['atr'] = df['atr'].fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "        return df\n",
        "    high, low, close = df['high'].values, df['low'].values, df['close'].values\n",
        "    tr = np.maximum.reduce([\n",
        "        high - low,\n",
        "        np.abs(high - np.roll(close, 1)),\n",
        "        np.abs(low - np.roll(close, 1))\n",
        "    ])\n",
        "    tr[0] = (high[0]-low[0]) if len(tr)>0 else MIN_ATR\n",
        "    df['atr'] = pd.Series(tr, index=df.index).rolling(ATR_PERIOD, min_periods=1).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "    return df\n",
        "\n",
        "def load_combined_data(folder):\n",
        "    combined_data = {}\n",
        "    precomputed = {}\n",
        "    for pair in pairs:\n",
        "        combined_data[pair] = {}\n",
        "        prefix = pair.replace('/','_')\n",
        "        for p in Path(folder).glob(f\"{prefix}_*_combined.pkl\"):\n",
        "            tf_name = p.name.replace(f\"{prefix}_\",\"\").replace(\"_combined.pkl\",\"\")\n",
        "            try:\n",
        "                df = pd.read_pickle(p)\n",
        "                df = make_index_tz_naive(df)\n",
        "                if not all(c in df.columns for c in ['open','high','low','close']): continue\n",
        "                df = seed_hybrid_signal_if_needed(df)\n",
        "                df = ensure_atr(df)\n",
        "                combined_data[pair][tf_name] = df\n",
        "            except: pass\n",
        "    return combined_data\n",
        "\n",
        "# -----------------------------\n",
        "# Vectorized Backtest\n",
        "# -----------------------------\n",
        "def run_vector_backtest_vectorized(combined_data, capital, base_risk, atr_sl, atr_tp, conf_mult, tf_weights_per_pair, trade_memory=None):\n",
        "    if trade_memory is None: trade_memory = {pair: [] for pair in combined_data.keys()}\n",
        "    results = {}\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # Precompute signals, prices, atr for all pairs\n",
        "    precomputed = {}\n",
        "    for pair, tfs in combined_data.items():\n",
        "        if not tfs:\n",
        "            results[pair] = {'equity_curve': pd.Series([capital]), 'total_pnl':0,'max_drawdown':0}\n",
        "            continue\n",
        "        all_idxs = sorted(set().union(*[set(df.index) for df in tfs.values()]))\n",
        "        df_all = pd.DataFrame(index=all_idxs)\n",
        "        for tf_name, df in tfs.items():\n",
        "            df_all[f'close_{tf_name}'] = df['close'].reindex(df_all.index).ffill()\n",
        "            df_all[f'signal_{tf_name}'] = df['hybrid_signal'].reindex(df_all.index).ffill().fillna(0.0)\n",
        "            df_all[f'atr_{tf_name}'] = df['atr'].reindex(df_all.index).ffill().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "        df_all['price'] = df_all[[c for c in df_all.columns if c.startswith('close_')]].mean(axis=1).clip(lower=epsilon)\n",
        "        df_all['atr'] = df_all[[c for c in df_all.columns if c.startswith('atr_')]].mean(axis=1).clip(lower=MIN_ATR)\n",
        "        precomputed[pair] = df_all\n",
        "\n",
        "    # Evaluate\n",
        "    for pair, df_all in precomputed.items():\n",
        "        tfs = combined_data.get(pair, {})\n",
        "        if not tfs:\n",
        "            continue\n",
        "        agg_signal = sum([df_all[f'signal_{tf}']*tf_weights_per_pair.get(pair,{}).get(tf,0.0) for tf in tfs.keys()])\n",
        "        mean_abs_signal = np.mean([df_all[f'signal_{tf}'].abs().mean() for tf in tfs.keys()]) if tfs else 0.0\n",
        "        conf_threshold = conf_mult * (mean_abs_signal + EPS)\n",
        "        df_all['agg_signal'] = np.where(np.abs(agg_signal)>=conf_threshold, agg_signal, 0.0)\n",
        "\n",
        "        price, atr, agg_signal = df_all['price'].values, df_all['atr'].values, df_all['agg_signal'].values\n",
        "        n = len(price)\n",
        "        if n<=1:\n",
        "            results[pair] = {'equity_curve': pd.Series([capital]), 'total_pnl':0,'max_drawdown':0}\n",
        "            continue\n",
        "\n",
        "        memory_factor = 1.0\n",
        "        if trade_memory.get(pair):\n",
        "            total_prev_pnl = sum([float(tr.get('pnl',0)) for tr in trade_memory[pair]])\n",
        "            memory_factor = max(0.2, 1.0 + total_prev_pnl / max(1.0, capital*10.0))\n",
        "\n",
        "        size = (capital*base_risk*np.abs(agg_signal))/(atr_sl*(atr/price)+EPS)\n",
        "        size = np.minimum(size*memory_factor, capital*MAX_POSITION_FRACTION)\n",
        "        size = np.nan_to_num(size,nan=0.0,posinf=capital*MAX_POSITION_FRACTION)\n",
        "        direction = np.sign(agg_signal)\n",
        "        pnl = direction*size*(atr_tp*atr/price)\n",
        "        equity = np.zeros(n,dtype=float)\n",
        "        equity[0]=capital\n",
        "        for i in range(1,n): equity[i] = equity[i-1]+float(pnl[i])\n",
        "        trade_memory.setdefault(pair,[]).append({'equity':float(equity[-1]), 'pnl':float(equity[-1]-capital)})\n",
        "        if len(trade_memory[pair])>200: trade_memory[pair] = trade_memory[pair][-200:]\n",
        "        equity_series = pd.Series(equity,index=df_all.index)\n",
        "        results[pair] = {'equity_curve': equity_series,'total_pnl':float(equity[-1]-capital),'max_drawdown':float((equity_series.cummax()-equity_series).max())}\n",
        "\n",
        "    total_pnl = sum([r['total_pnl'] for r in results.values()])\n",
        "    max_dd = max([r['max_drawdown'] for r in results.values()] or [0.0])\n",
        "    score = total_pnl/(1.0+max_dd) if (1.0+max_dd)!=0 else total_pnl\n",
        "    return score, results, trade_memory\n",
        "\n",
        "# -----------------------------\n",
        "# GA functions\n",
        "# -----------------------------\n",
        "def build_tf_names(combined_data): return {pair: sorted(list(combined_data[pair].keys())) for pair in pairs}\n",
        "\n",
        "def create_chromosome(tf_names_map):\n",
        "    chrom = [random.uniform(1.0,2.0), random.uniform(2.0,4.0), random.uniform(0.005,0.02), random.uniform(0.3,0.7)]\n",
        "    for pair in pairs:\n",
        "        n=max(1,len(tf_names_map.get(pair,[])))\n",
        "        w=np.random.dirichlet(np.ones(n)).tolist()\n",
        "        chrom.extend(w)\n",
        "    return chrom\n",
        "\n",
        "def decode_chromosome(chrom,tf_names_map):\n",
        "    atr_sl, atr_tp, base_risk, conf = chrom[:4]\n",
        "    tf_weights_per_pair={}\n",
        "    idx=4\n",
        "    for pair in pairs:\n",
        "        n=max(1,len(tf_names_map.get(pair,[])))\n",
        "        w=np.array(chrom[idx:idx+n],dtype=float)\n",
        "        if w.sum()<=0: w=np.ones_like(w)/float(len(w))\n",
        "        else: w=w/(w.sum()+EPS)\n",
        "        tf_list=tf_names_map.get(pair,[])\n",
        "        tf_weights_per_pair[pair]={tf: float(weight) for tf,weight in zip(tf_list,w)} if tf_list else {}\n",
        "        idx+=n\n",
        "    return atr_sl, atr_tp, base_risk, conf, tf_weights_per_pair\n",
        "\n",
        "def tournament_selection(scored_population,k=TOURNAMENT_SIZE):\n",
        "    selected=random.sample(scored_population,k)\n",
        "    selected.sort(reverse=True,key=lambda x:x[0])\n",
        "    return selected[0][1]\n",
        "\n",
        "# -----------------------------\n",
        "# GA runner (parallelized)\n",
        "# -----------------------------\n",
        "def run_ga_vectorized_parallel(combined_data, generations=GENERATIONS, population_size=POPULATION_SIZE, mutation_rate=MUTATION_RATE):\n",
        "    tf_names_map=build_tf_names(combined_data)\n",
        "    if os.path.exists(POPULATION_FILE):\n",
        "        try: population=pickle.load(open(POPULATION_FILE,'rb'))\n",
        "        except: population=[create_chromosome(tf_names_map) for _ in range(population_size)]\n",
        "    else: population=[create_chromosome(tf_names_map) for _ in range(population_size)]\n",
        "    trade_memory={}\n",
        "    if os.path.exists(TRADE_MEMORY_FILE):\n",
        "        try: trade_memory=pickle.load(open(TRADE_MEMORY_FILE,'rb'))\n",
        "        except: trade_memory={}\n",
        "    last_gen=0\n",
        "    if os.path.exists(GEN_COUNT_FILE):\n",
        "        try: last_gen=pickle.load(open(GEN_COUNT_FILE,'rb'))\n",
        "        except: last_gen=0\n",
        "\n",
        "    best_score_ever=-np.inf\n",
        "    best_chrom_ever=None\n",
        "    early_stop_counter=0\n",
        "\n",
        "    def evaluate_chrom(chrom):\n",
        "        atr_sl, atr_tp, base_risk, conf, tf_weights_per_pair = decode_chromosome(chrom, tf_names_map)\n",
        "        score, _, _ = run_vector_backtest_vectorized(combined_data, BASE_CAPITAL, base_risk, atr_sl, atr_tp, conf, tf_weights_per_pair, trade_memory)\n",
        "        return score, chrom\n",
        "\n",
        "    for gen in range(last_gen+1,last_gen+1+generations):\n",
        "        scored_population = Parallel(n_jobs=-1)(delayed(evaluate_chrom)(c) for c in population)\n",
        "        scored_population.sort(reverse=True,key=lambda x:x[0])\n",
        "        best_score,best_chrom=scored_population[0]\n",
        "        if best_score<best_score_ever:\n",
        "            best_score=best_score_ever\n",
        "            best_chrom=best_chrom_ever\n",
        "\n",
        "        print(f\"=== Generation {gen} === Best Score: {best_score:.2f}\")\n",
        "        if best_score>best_score_ever:\n",
        "            best_score_ever=best_score\n",
        "            best_chrom_ever=best_chrom\n",
        "            early_stop_counter=0\n",
        "        else:\n",
        "            early_stop_counter+=1\n",
        "            if early_stop_counter>=EARLY_STOPPING:\n",
        "                print(\"‚ö†Ô∏è Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        # Generate next population\n",
        "        next_population=[best_chrom]\n",
        "        while len(next_population)<population_size:\n",
        "            parent1=tournament_selection(scored_population)\n",
        "            parent2=tournament_selection(scored_population)\n",
        "            child=[(p1+p2)/2.0 for p1,p2 in zip(parent1,parent2)]\n",
        "            child=[c*random.uniform(0.95,1.05) if random.random()<mutation_rate else c for c in child]\n",
        "            next_population.append(child)\n",
        "        population=next_population\n",
        "\n",
        "        # Save state\n",
        "        pickle.dump(population,open(POPULATION_FILE,'wb'))\n",
        "        pickle.dump(trade_memory,open(TRADE_MEMORY_FILE,'wb'))\n",
        "        pickle.dump(best_chrom_ever,open(BEST_CHROM_FILE,'wb'))\n",
        "        pickle.dump(gen,open(GEN_COUNT_FILE,'wb'))\n",
        "\n",
        "    print(\"‚úÖ GA complete. Best chromosome saved.\")\n",
        "    return best_chrom_ever, trade_memory\n",
        "\n",
        "# -----------------------------\n",
        "# Live signals with high-confidence flag\n",
        "# -----------------------------\n",
        "def generate_live_signals_with_sl_tp(best_chrom, combined_data):\n",
        "    tf_names_map = build_tf_names(combined_data)\n",
        "    atr_sl, atr_tp, base_risk, conf, tf_weights_per_pair = decode_chromosome(best_chrom, tf_names_map)\n",
        "\n",
        "    live_signals = {}\n",
        "    prev_signals = {}\n",
        "\n",
        "    if os.path.exists(SIGNALS_JSON_PATH):\n",
        "        try:\n",
        "            prev_data = json.load(open(SIGNALS_JSON_PATH, 'r'))\n",
        "            prev_signals = {pair: data.get('strength', 0.0) for pair, data in prev_data.get(\"pairs\", {}).items()}\n",
        "        except:\n",
        "            prev_signals = {}\n",
        "\n",
        "    for pair in pairs:\n",
        "        tfs = combined_data.get(pair, {})\n",
        "\n",
        "        price = fetch_live_rate(pair)\n",
        "        if price <= 0:\n",
        "            price = np.mean([df['close'].iloc[-1] for df in tfs.values()]) if tfs else 1.0\n",
        "\n",
        "        signal_strength = sum([\n",
        "            tf_weights_per_pair.get(pair, {}).get(tf, 0.0) * tfs[tf]['hybrid_signal'].iloc[-1]\n",
        "            for tf in tf_names_map.get(pair, [])\n",
        "        ])\n",
        "\n",
        "        prev_strength = prev_signals.get(pair, 0.0)\n",
        "        if np.sign(signal_strength) != np.sign(prev_strength):\n",
        "            signal_strength = 0.7 * prev_strength + 0.3 * signal_strength\n",
        "\n",
        "        direction = \"BUY\" if signal_strength > 0 else \"SELL\" if signal_strength < 0 else \"HOLD\"\n",
        "\n",
        "        recent_atr = np.mean([tfs[tf]['atr'].iloc[-1] for tf in tf_names_map.get(pair, [])]) if tfs else 1.0\n",
        "\n",
        "        score_100 = min(max(int(100*(abs(signal_strength)/(recent_atr+EPS))**0.5), 1), 100)\n",
        "\n",
        "        sl_multiplier = 0.5\n",
        "        tp_multiplier = 1.0\n",
        "        SL = price - atr_sl * recent_atr * sl_multiplier if direction == \"BUY\" else price + atr_sl * recent_atr * sl_multiplier\n",
        "        TP = price + atr_tp * recent_atr * tp_multiplier if direction == \"BUY\" else price - atr_tp * recent_atr * tp_multiplier\n",
        "\n",
        "        # High-confidence flag\n",
        "        high_confidence = score_100 >= 80\n",
        "\n",
        "        live_signals[pair] = {\n",
        "            \"direction\": direction,\n",
        "            \"strength\": float(signal_strength),\n",
        "            \"score_1_100\": score_100,\n",
        "            \"last_price\": float(price),\n",
        "            \"SL\": float(SL),\n",
        "            \"TP\": float(TP),\n",
        "            \"high_confidence\": high_confidence\n",
        "        }\n",
        "\n",
        "    with open(SIGNALS_JSON_PATH, 'w') as f:\n",
        "        json.dump({\"timestamp\": pd.Timestamp.now().isoformat(), \"pairs\": live_signals}, f, indent=2)\n",
        "\n",
        "    print(f\"üì° Live signals with SL/TP generated and saved to {SIGNALS_JSON_PATH}\")\n",
        "    return live_signals\n",
        "\n",
        "# -----------------------------\n",
        "# Email builder with high-confidence visual flag\n",
        "# -----------------------------\n",
        "def send_forex_email(signals, recipient=\"nakatonabira3@gmail.com\"):\n",
        "    def format_price(price, pair=\"\"):\n",
        "        decimals = 3 if \"JPY\" in pair else 4\n",
        "        return f\"{price:.{decimals}f}\" if price else \"-\"\n",
        "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
        "    table_rows = \"\"\n",
        "    flags_map = {\"USD\":\"üá∫üá∏\",\"EUR\":\"üá™üá∫\",\"GBP\":\"üá¨üáß\",\"JPY\":\"üáØüáµ\",\"AUD\":\"üá¶üá∫\"}\n",
        "    for pair, data in signals.items():\n",
        "        f1,f2 = pair.split(\"/\")\n",
        "        flag_str = f\"{flags_map.get(f1,'')} {flags_map.get(f2,'')}\"\n",
        "        high_conf = \"üî•\" if data.get(\"high_confidence\") else \"\"\n",
        "        table_rows += f\"\"\"\n",
        "        <tr>\n",
        "          <td>{flag_str} {pair}</td>\n",
        "          <td>{format_price(data['last_price'], pair)}</td>\n",
        "          <td>{data['direction']}</td>\n",
        "          <td>{data['score_1_100']} {high_conf}</td>\n",
        "          <td>SL:{format_price(data['SL'], pair)} | TP:{format_price(data['TP'], pair)}</td>\n",
        "        </tr>\n",
        "        \"\"\"\n",
        "    html = f\"\"\"\n",
        "    <html>\n",
        "      <body>\n",
        "        <h2>Forex Signals - {today}</h2>\n",
        "        <table border=\"1\" style=\"border-collapse:collapse;text-align:center;\">\n",
        "          <tr><th>Instrument</th><th>Price</th><th>Signal</th><th>Score</th><th>SL/TP</th></tr>\n",
        "          {table_rows}\n",
        "        </table>\n",
        "      </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    msg = MIMEMultipart(\"alternative\")\n",
        "    msg['From'] = f\"Forex Bot <{GMAIL_USER}>\"\n",
        "    msg['To'] = recipient\n",
        "    msg['Subject'] = f\"Forex Signals - {today}\"\n",
        "    msg.attach(MIMEText(html, \"html\"))\n",
        "    try:\n",
        "        with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\n",
        "            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)\n",
        "            server.sendmail(GMAIL_USER, recipient, msg.as_string())\n",
        "        print(f\"üìß Email sent to {recipient}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Email send failed: {e}\")\n",
        "\n",
        "# -----------------------------\n",
        "# MAIN\n",
        "# -----------------------------\n",
        "if __name__==\"__main__\":\n",
        "    print(\"Loading combined data...\")\n",
        "    combined_data = load_combined_data(SAVE_FOLDER)\n",
        "\n",
        "    print(\"üéØ Running GA + vectorized backtest (parallelized)...\")\n",
        "    best_chrom, trade_memory = run_ga_vectorized_parallel(combined_data)\n",
        "\n",
        "    print(\"üì° Generating live signals with SL/TP, 1-100 score, and high-confidence flag...\")\n",
        "    signals = generate_live_signals_with_sl_tp(best_chrom, combined_data)\n",
        "\n",
        "    print(json.dumps(signals, indent=2))\n",
        "\n",
        "    print(\"üì® Sending email with signals...\")\n",
        "    send_forex_email(signals, recipient=\"nakatonabira3@gmail.com\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}