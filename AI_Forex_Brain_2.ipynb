{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üîë API Keys Configuration\n",
        "# ======================================================\n",
        "import os\n",
        "\n",
        "# Set API keys from environment variables or defaults\n",
        "ALPHA_VANTAGE_KEY = os.environ.get('ALPHA_VANTAGE_KEY', '1W58NPZXOG5SLHZ6')\n",
        "BROWSERLESS_TOKEN = os.environ.get('BROWSERLESS_TOKEN', '2TMVUBAjFwrr7Tb283f0da6602a4cb698b81778bda61967f7')\n",
        "\n",
        "# Set environment variables for downstream code\n",
        "os.environ['ALPHA_VANTAGE_KEY'] = ALPHA_VANTAGE_KEY\n",
        "os.environ['BROWSERLESS_TOKEN'] = BROWSERLESS_TOKEN\n",
        "\n",
        "# Validate\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    print(\"‚ö†Ô∏è Warning: ALPHA_VANTAGE_KEY not set!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Alpha Vantage Key: {ALPHA_VANTAGE_KEY[:4]}...{ALPHA_VANTAGE_KEY[-4:]}\")\n",
        "\n",
        "if not BROWSERLESS_TOKEN:\n",
        "    print(\"‚ö†Ô∏è Warning: BROWSERLESS_TOKEN not set!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Browserless Token: {BROWSERLESS_TOKEN[:4]}...{BROWSERLESS_TOKEN[-4:]}\")"
      ],
      "metadata": {
        "id": "mr_DWDx4-LLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üåç Environment Detection & Setup (MUST RUN FIRST!)\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "# Override ENV_NAME if in GitHub Actions\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "# Set base paths based on environment\n",
        "if IN_COLAB:\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "elif IN_GHA:\n",
        "    # GitHub Actions already checks out the repo\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    # Local development\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "\n",
        "# Create necessary directories\n",
        "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Display environment info\n",
        "print(\"=\" * 60)\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üîß Python: {sys.version.split()[0]}\")\n",
        "print(f\"üìç Working Dir: {os.getcwd()}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Validate critical environment variables for GitHub Actions\n",
        "if IN_GHA:\n",
        "    required_vars = [\"FOREX_PAT\", \"GIT_USER_NAME\", \"GIT_USER_EMAIL\"]\n",
        "    missing = [v for v in required_vars if not os.environ.get(v)]\n",
        "    if missing:\n",
        "        print(f\"‚ö†Ô∏è Warning: Missing environment variables: {', '.join(missing)}\")\n",
        "    else:\n",
        "        print(\"‚úÖ All required environment variables present\")"
      ],
      "metadata": {
        "id": "8ZTLJKoW9rQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üìÑ GitHub Sync (Environment-Aware) - FULLY FIXED VERSION\n",
        "# ======================================================\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import urllib.parse\n",
        "import sys\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Environment Detection (Self-Contained)\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "# Override ENV_NAME if in GitHub Actions\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CRITICAL FIX: Smart Path Configuration\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    # ‚úÖ GitHub Actions: Use current directory (already in repo)\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER  # We're already in the repo!\n",
        "    print(\"ü§ñ GitHub Actions Mode: Using current directory\")\n",
        "\n",
        "elif IN_COLAB:\n",
        "    # ‚úÖ Colab: Use separate workspace folder\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex_workspace\"  # Different name to avoid confusion\n",
        "    REPO_FOLDER = SAVE_FOLDER / \"forex-ai-models\"  # Repo goes inside workspace\n",
        "    print(\"‚òÅÔ∏è Colab Mode: Using workspace structure\")\n",
        "\n",
        "else:\n",
        "    # ‚úÖ Local: Use current directory or custom path\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"workspace\"\n",
        "    REPO_FOLDER = SAVE_FOLDER / \"forex-ai-models\"\n",
        "    print(\"üíª Local Mode: Using workspace structure\")\n",
        "\n",
        "# Create necessary directories\n",
        "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"üîß Running in: {ENV_NAME}\")\n",
        "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
        "print(f\"üíæ Save folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo folder: {REPO_FOLDER}\")\n",
        "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GitHub Configuration\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GitHub Token (Multi-Source)\n",
        "# ======================================================\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "# Try Colab secrets if in Colab and PAT not found\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except ImportError:\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load Colab secret: {e}\")\n",
        "\n",
        "# Validate PAT\n",
        "if not FOREX_PAT:\n",
        "    print(\"‚ö†Ô∏è Warning: FOREX_PAT not found. Git operations may fail.\")\n",
        "    print(\"   Set FOREX_PAT in:\")\n",
        "    print(\"   - GitHub Secrets (for Actions)\")\n",
        "    print(\"   - Colab Secrets (for Colab)\")\n",
        "    print(\"   - Environment variable (for local)\")\n",
        "    REPO_URL = None\n",
        "else:\n",
        "    SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "    REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "    print(\"‚úÖ GitHub token configured\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Handle Repository Based on Environment\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    # ===== GitHub Actions =====\n",
        "    print(\"\\nü§ñ GitHub Actions Mode\")\n",
        "    print(\"‚úÖ Repository already checked out by actions/checkout\")\n",
        "    print(f\"üìÇ Current directory: {Path.cwd()}\")\n",
        "\n",
        "    # Verify .git exists\n",
        "    if not (Path.cwd() / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Warning: .git directory not found!\")\n",
        "        print(\"   Make sure actions/checkout@v4 is in your workflow\")\n",
        "    else:\n",
        "        print(\"‚úÖ Git repository confirmed\")\n",
        "\n",
        "    # No need to clone - we're already in the repo!\n",
        "\n",
        "elif IN_COLAB:\n",
        "    # ===== Google Colab =====\n",
        "    print(\"\\n‚òÅÔ∏è Google Colab Mode\")\n",
        "\n",
        "    if not REPO_URL:\n",
        "        print(\"‚ùå Cannot clone repository: FOREX_PAT not available\")\n",
        "    elif not (REPO_FOLDER / \".git\").exists():\n",
        "        # Clone repository\n",
        "        print(f\"üì• Cloning repository to {REPO_FOLDER}...\")\n",
        "        env = os.environ.copy()\n",
        "        env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"  # Skip LFS files\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                check=True,\n",
        "                env=env,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            print(\"‚úÖ Repository cloned successfully\")\n",
        "\n",
        "            # Change to repo directory\n",
        "            os.chdir(REPO_FOLDER)\n",
        "            print(f\"üìÇ Changed directory to: {os.getcwd()}\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Clone failed: {e.stderr}\")\n",
        "            print(\"Continuing with existing directory...\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ùå Clone timed out after 60 seconds\")\n",
        "    else:\n",
        "        # Repository exists, pull latest\n",
        "        print(\"‚úÖ Repository already exists, pulling latest changes...\")\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"pull\", \"origin\", BRANCH],\n",
        "                check=True,\n",
        "                cwd=REPO_FOLDER,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            print(\"‚úÖ Successfully pulled latest changes\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Pull failed: {e.stderr}\")\n",
        "            print(\"Continuing with existing files...\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ö†Ô∏è Pull timed out, continuing anyway...\")\n",
        "\n",
        "    # Configure Git LFS (disable for Colab)\n",
        "    print(\"‚öôÔ∏è Configuring Git LFS...\")\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\"git\", \"lfs\", \"uninstall\"],\n",
        "            check=False,\n",
        "            cwd=REPO_FOLDER,\n",
        "            capture_output=True\n",
        "        )\n",
        "        subprocess.run(\n",
        "            [\"git\", \"lfs\", \"migrate\", \"export\", \"--include=*.csv\"],\n",
        "            check=False,\n",
        "            cwd=REPO_FOLDER,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"‚úÖ LFS configuration updated\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è LFS setup warning: {e}\")\n",
        "\n",
        "else:\n",
        "    # ===== Local Environment =====\n",
        "    print(\"\\nüíª Local Development Mode\")\n",
        "    print(f\"üìÇ Working in: {SAVE_FOLDER}\")\n",
        "\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        if REPO_URL:\n",
        "            print(f\"üì• Cloning repository to {REPO_FOLDER}...\")\n",
        "            try:\n",
        "                subprocess.run(\n",
        "                    [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                    check=True,\n",
        "                    timeout=60\n",
        "                )\n",
        "                print(\"‚úÖ Repository cloned successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Clone failed: {e}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Not a git repository and no PAT available\")\n",
        "            print(\"   Run: git clone https://github.com/rahim-dotAI/forex-ai-models.git\")\n",
        "    else:\n",
        "        print(\"‚úÖ Git repository found\")\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Git Global Configuration\n",
        "# ======================================================\n",
        "print(\"\\nüîß Configuring Git...\")\n",
        "\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "# Set git config\n",
        "git_configs = [\n",
        "    ([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], \"User name\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], \"User email\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], \"Detached HEAD warning\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"init.defaultBranch\", \"main\"], \"Default branch\")\n",
        "]\n",
        "\n",
        "for cmd, description in git_configs:\n",
        "    try:\n",
        "        subprocess.run(cmd, check=False, capture_output=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not set {description}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Environment Summary & Validation\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üßæ ENVIRONMENT SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment:      {ENV_NAME}\")\n",
        "print(f\"Working Dir:      {os.getcwd()}\")\n",
        "print(f\"Save Folder:      {SAVE_FOLDER}\")\n",
        "print(f\"Repo Folder:      {REPO_FOLDER}\")\n",
        "print(f\"Repository:       https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}\")\n",
        "print(f\"Branch:           {BRANCH}\")\n",
        "print(f\"Git Repo Exists:  {(REPO_FOLDER / '.git').exists()}\")\n",
        "print(f\"FOREX_PAT Set:    {'‚úÖ Yes' if FOREX_PAT else '‚ùå No'}\")\n",
        "\n",
        "# Check critical paths\n",
        "print(\"\\nüìã Critical Paths:\")\n",
        "critical_paths = {\n",
        "    \"Repo .git\": REPO_FOLDER / \".git\",\n",
        "    \"Save Folder\": SAVE_FOLDER,\n",
        "    \"Repo Folder\": REPO_FOLDER\n",
        "}\n",
        "\n",
        "for name, path in critical_paths.items():\n",
        "    exists = path.exists()\n",
        "    icon = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"  {icon} {name}: {path} {'(exists)' if exists else '(missing)'}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ Setup completed successfully!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Export Variables for Downstream Cells\n",
        "# ======================================================\n",
        "# These variables are now available in subsequent cells:\n",
        "# - ENV_NAME: Environment name\n",
        "# - IN_COLAB: Boolean for Colab detection\n",
        "# - IN_GHA: Boolean for GitHub Actions detection\n",
        "# - SAVE_FOLDER: Path to save files\n",
        "# - REPO_FOLDER: Path to git repository\n",
        "# - GITHUB_USERNAME, GITHUB_REPO, BRANCH: Git config\n",
        "# - FOREX_PAT: GitHub token (if available)\n",
        "\n",
        "print(\"\\n‚úÖ All environment variables exported for downstream cells\")"
      ],
      "metadata": {
        "id": "yjIMteyqLs_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oih6CDfjAjG9"
      },
      "outputs": [],
      "source": [
        "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn river\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üöÄ COMPLETE ALPHA VANTAGE FX WORKFLOW - FULLY FIXED\n",
        "# ======================================================\n",
        "# ‚úÖ Works in GitHub Actions, Google Colab, and Local\n",
        "# ‚úÖ No nested repositories\n",
        "# ‚úÖ Proper path management\n",
        "# ‚úÖ Thread-safe operations\n",
        "# ‚úÖ API rate limit handling\n",
        "# ‚úÖ Automatic retry logic\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import requests\n",
        "import subprocess\n",
        "import threading\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION\n",
        "# ======================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ Alpha Vantage FX Data Fetcher\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üìç Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ SMART PATH CONFIGURATION (NO NESTED REPOS!)\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    # ‚úÖ GitHub Actions: Already in repo root\n",
        "    print(\"ü§ñ GitHub Actions detected - using repository root\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    REPO_FOLDER = BASE_FOLDER  # SAME as current directory!\n",
        "    CSV_FOLDER = BASE_FOLDER / \"csvs\"\n",
        "    PICKLE_FOLDER = BASE_FOLDER / \"pickles\"\n",
        "    LOG_FOLDER = BASE_FOLDER / \"logs\"\n",
        "\n",
        "elif IN_COLAB:\n",
        "    # ‚úÖ Colab: Separate workspace to avoid confusion\n",
        "    print(\"‚òÅÔ∏è Google Colab detected - creating workspace\")\n",
        "    BASE_FOLDER = Path(\"/content/forex_workspace\")\n",
        "    BASE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "    REPO_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    CSV_FOLDER = BASE_FOLDER / \"csvs\"\n",
        "    PICKLE_FOLDER = BASE_FOLDER / \"pickles\"\n",
        "    LOG_FOLDER = BASE_FOLDER / \"logs\"\n",
        "\n",
        "else:\n",
        "    # ‚úÖ Local: Workspace in current directory\n",
        "    print(\"üíª Local environment detected - creating workspace\")\n",
        "    BASE_FOLDER = Path(\"./forex_workspace\").resolve()\n",
        "    BASE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "    REPO_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    CSV_FOLDER = BASE_FOLDER / \"csvs\"\n",
        "    PICKLE_FOLDER = BASE_FOLDER / \"pickles\"\n",
        "    LOG_FOLDER = BASE_FOLDER / \"logs\"\n",
        "\n",
        "# Create output directories\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOG_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"üìÇ Base folder: {BASE_FOLDER}\")\n",
        "print(f\"üì¶ Repo folder: {REPO_FOLDER}\")\n",
        "print(f\"üíæ CSV folder: {CSV_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GITHUB CONFIGURATION\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# Get GitHub PAT from environment\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "# Try Colab secrets if available\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not access Colab secrets: {e}\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    print(\"‚ùå ERROR: FOREX_PAT not found!\")\n",
        "    print(\"   Set it in:\")\n",
        "    print(\"   - GitHub Secrets (for Actions)\")\n",
        "    print(\"   - Colab Secrets (for Colab)\")\n",
        "    print(\"   - Environment variables (for Local)\")\n",
        "    raise ValueError(\"FOREX_PAT is required\")\n",
        "\n",
        "# URL-encode the PAT for safe use in URLs\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "print(\"‚úÖ GitHub credentials configured\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ REPOSITORY MANAGEMENT\n",
        "# ======================================================\n",
        "def ensure_repository():\n",
        "    \"\"\"\n",
        "    Ensure repository is available and up-to-date\n",
        "    Behavior depends on environment\n",
        "    \"\"\"\n",
        "    if IN_GHA:\n",
        "        # GitHub Actions: Repo already checked out\n",
        "        print(\"\\nü§ñ GitHub Actions: Repository already available\")\n",
        "        if not (REPO_FOLDER / \".git\").exists():\n",
        "            print(\"‚ö†Ô∏è Warning: .git directory not found\")\n",
        "            print(\"   Make sure actions/checkout@v4 is in your workflow\")\n",
        "        else:\n",
        "            print(\"‚úÖ Git repository verified\")\n",
        "        return\n",
        "\n",
        "    # For Colab and Local: Clone or update\n",
        "    print(\"\\nüì• Managing repository...\")\n",
        "\n",
        "    if REPO_FOLDER.exists():\n",
        "        if (REPO_FOLDER / \".git\").exists():\n",
        "            # Repository exists - update it\n",
        "            print(f\"üîÑ Updating existing repository...\")\n",
        "            try:\n",
        "                # Fetch latest\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"-C\", str(REPO_FOLDER), \"fetch\", \"origin\"],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                # Checkout branch\n",
        "                subprocess.run(\n",
        "                    [\"git\", \"-C\", str(REPO_FOLDER), \"checkout\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True\n",
        "                )\n",
        "\n",
        "                # Pull latest changes\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(\"‚úÖ Repository updated successfully\")\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Pull had warnings: {result.stderr}\")\n",
        "\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(\"‚ö†Ô∏è Update timed out - continuing with existing repo\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Update failed: {e} - continuing with existing repo\")\n",
        "        else:\n",
        "            # Folder exists but not a git repo - remove it\n",
        "            print(\"üóëÔ∏è Removing incomplete repository folder...\")\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "    # Clone if needed\n",
        "    if not REPO_FOLDER.exists() or not (REPO_FOLDER / \".git\").exists():\n",
        "        print(f\"üì• Cloning repository to {REPO_FOLDER}...\")\n",
        "\n",
        "        # Skip LFS to speed up clone\n",
        "        env = os.environ.copy()\n",
        "        env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                env=env,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Repository cloned successfully\")\n",
        "            else:\n",
        "                raise RuntimeError(f\"Clone failed: {result.stderr}\")\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            raise TimeoutError(\"Repository clone timed out after 60 seconds\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Clone failed: {e}\")\n",
        "\n",
        "ensure_repository()\n",
        "\n",
        "# Configure Git identity\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "               capture_output=True, check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "               capture_output=True, check=False)\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ ALPHA VANTAGE CONFIGURATION\n",
        "# ======================================================\n",
        "ALPHA_VANTAGE_KEY = os.environ.get(\"ALPHA_VANTAGE_KEY\", \"1W58NPZXOG5SLHZ6\")\n",
        "\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    raise ValueError(\"‚ùå ALPHA_VANTAGE_KEY is required\")\n",
        "\n",
        "print(f\"‚úÖ Alpha Vantage API key: {ALPHA_VANTAGE_KEY[:4]}...{ALPHA_VANTAGE_KEY[-4:]}\")\n",
        "\n",
        "# FX pairs to fetch\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "# Thread lock for file operations\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"\n",
        "    Remove timezone information from DataFrame index\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    \"\"\"\n",
        "    Calculate MD5 hash of file to detect changes\n",
        "    \"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def fetch_alpha_vantage_fx(pair, outputsize='full', max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Fetch FX data from Alpha Vantage API with retry logic\n",
        "\n",
        "    Args:\n",
        "        pair: FX pair (e.g., \"EUR/USD\")\n",
        "        outputsize: 'compact' (100 rows) or 'full' (all available)\n",
        "        max_retries: Number of retry attempts\n",
        "        retry_delay: Seconds between retries\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with OHLC data or empty DataFrame on failure\n",
        "    \"\"\"\n",
        "    base_url = 'https://www.alphavantage.co/query'\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "\n",
        "    params = {\n",
        "        'function': 'FX_DAILY',\n",
        "        'from_symbol': from_currency,\n",
        "        'to_symbol': to_currency,\n",
        "        'outputsize': outputsize,\n",
        "        'datatype': 'json',\n",
        "        'apikey': ALPHA_VANTAGE_KEY\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  Fetching {pair} (attempt {attempt + 1}/{max_retries})...\")\n",
        "\n",
        "            r = requests.get(base_url, params=params, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "\n",
        "            # Check for API errors\n",
        "            if 'Error Message' in data:\n",
        "                raise ValueError(f\"API Error: {data['Error Message']}\")\n",
        "\n",
        "            if 'Note' in data:\n",
        "                print(f\"  ‚ö†Ô∏è API rate limit reached for {pair}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay * 2)  # Longer wait for rate limit\n",
        "                    continue\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            if 'Time Series FX (Daily)' not in data:\n",
        "                raise ValueError(f\"Unexpected response format: {list(data.keys())}\")\n",
        "\n",
        "            # Parse time series data\n",
        "            ts = data['Time Series FX (Daily)']\n",
        "            df = pd.DataFrame(ts).T\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df.sort_index()\n",
        "\n",
        "            # Rename columns\n",
        "            df = df.rename(columns={\n",
        "                '1. open': 'open',\n",
        "                '2. high': 'high',\n",
        "                '3. low': 'low',\n",
        "                '4. close': 'close'\n",
        "            })\n",
        "\n",
        "            # Convert to float\n",
        "            df = df.astype(float)\n",
        "\n",
        "            # Remove timezone\n",
        "            df = ensure_tz_naive(df)\n",
        "\n",
        "            print(f\"  ‚úÖ Fetched {len(df)} rows for {pair}\")\n",
        "            return df\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"  ‚ö†Ô∏è Network error: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ‚ùå Failed after {max_retries} attempts\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Error: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ‚ùå Failed after {max_retries} attempts\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ PAIR PROCESSING\n",
        "# ======================================================\n",
        "def process_pair(pair):\n",
        "    \"\"\"\n",
        "    Process single FX pair: fetch, merge with existing, save\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (filepath if changed, status message)\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîÑ Processing {pair}...\")\n",
        "\n",
        "    filename = pair.replace(\"/\", \"_\") + \".csv\"\n",
        "\n",
        "    # Determine file locations based on environment\n",
        "    if IN_GHA:\n",
        "        # In GitHub Actions: Save directly to repo root\n",
        "        csv_path = REPO_FOLDER / filename\n",
        "        repo_path = csv_path  # Same file\n",
        "    else:\n",
        "        # In Colab/Local: Save to CSV folder AND repo folder\n",
        "        csv_path = CSV_FOLDER / filename\n",
        "        repo_path = REPO_FOLDER / filename\n",
        "\n",
        "    # Load existing data\n",
        "    existing_df = pd.DataFrame()\n",
        "    if csv_path.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
        "            existing_df = ensure_tz_naive(existing_df)\n",
        "            print(f\"  üìä Loaded {len(existing_df)} existing rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not load existing data: {e}\")\n",
        "\n",
        "    # Get hash before changes\n",
        "    old_hash = file_hash(csv_path)\n",
        "\n",
        "    # Fetch new data\n",
        "    new_df = fetch_alpha_vantage_fx(pair)\n",
        "\n",
        "    if new_df.empty:\n",
        "        return None, f\"‚ùå {pair}: No data fetched\"\n",
        "\n",
        "    # Merge with existing data\n",
        "    if not existing_df.empty:\n",
        "        combined_df = pd.concat([existing_df, new_df])\n",
        "        # Remove duplicates, keeping latest\n",
        "        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
        "    else:\n",
        "        combined_df = new_df\n",
        "\n",
        "    # Sort by date\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    # Save files (thread-safe)\n",
        "    with lock:\n",
        "        # Save to CSV folder\n",
        "        combined_df.to_csv(csv_path)\n",
        "\n",
        "        # Also save to repo folder if different\n",
        "        if not IN_GHA and csv_path != repo_path:\n",
        "            combined_df.to_csv(repo_path)\n",
        "\n",
        "    # Check if file changed\n",
        "    new_hash = file_hash(csv_path)\n",
        "    changed = (old_hash != new_hash)\n",
        "\n",
        "    status = \"‚úÖ Updated\" if changed else \"‚ÑπÔ∏è No changes\"\n",
        "    print(f\"  {status} - Total rows: {len(combined_df)}\")\n",
        "\n",
        "    return (str(repo_path) if changed else None), f\"{status} {pair} ({len(combined_df)} rows)\"\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ PARALLEL EXECUTION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Fetching FX data from Alpha Vantage...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "changed_files = []\n",
        "results = []\n",
        "\n",
        "# Process pairs in parallel (max 4 at a time to respect API limits)\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = {executor.submit(process_pair, pair): pair for pair in FX_PAIRS}\n",
        "\n",
        "    for future in as_completed(futures):\n",
        "        pair = futures[future]\n",
        "        try:\n",
        "            filepath, message = future.result()\n",
        "            results.append(message)\n",
        "            if filepath:\n",
        "                changed_files.append(filepath)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {pair} processing failed: {e}\")\n",
        "            results.append(f\"‚ùå {pair}: Failed\")\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ RESULTS SUMMARY\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PROCESSING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "print(f\"\\nTotal pairs processed: {len(FX_PAIRS)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "\n",
        "# ======================================================\n",
        "# üîü GIT COMMIT & PUSH (Skip in GitHub Actions)\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"   (Workflow will handle commit and push)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        # Change to repo directory\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        # Stage changed files\n",
        "        print(f\"üìù Staging {len(changed_files)} files...\")\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        # Commit\n",
        "        commit_msg = f\"Update Alpha Vantage FX data - {len(changed_files)} files\"\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Changes committed\")\n",
        "        elif \"nothing to commit\" in result.stdout:\n",
        "            print(\"‚ÑπÔ∏è No changes to commit\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Commit warning: {result.stderr}\")\n",
        "\n",
        "        # Push with retry logic\n",
        "        max_push_attempts = 3\n",
        "        for attempt in range(max_push_attempts):\n",
        "            print(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/{max_push_attempts})...\")\n",
        "\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Successfully pushed to GitHub\")\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_push_attempts - 1:\n",
        "                    print(f\"‚ö†Ô∏è Push failed, retrying...\")\n",
        "                    # Pull latest and try again\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "                else:\n",
        "                    print(f\"‚ùå Push failed after {max_push_attempts} attempts\")\n",
        "                    print(f\"   Error: {result.stderr}\")\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"‚ùå Git operation timed out\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Git error: {e}\")\n",
        "    finally:\n",
        "        # Return to base folder\n",
        "        os.chdir(BASE_FOLDER)\n",
        "\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚ÑπÔ∏è No changes to commit\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALPHA VANTAGE WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"Pairs processed: {len(FX_PAIRS)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Status: {'Success' if len(results) == len(FX_PAIRS) else 'Partial'}\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "ZGwB1Pp2LJ6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# FULLY IMPROVED FOREX DATA WORKFLOW - YFINANCE\n",
        "# ‚úÖ Works in: Colab + GitHub Actions + Local\n",
        "# ‚úÖ No permission errors\n",
        "# ‚úÖ 403-Proof, Large History Support\n",
        "# ‚úÖ Environment-aware paths\n",
        "# ======================================================\n",
        "\n",
        "import os, time, hashlib, subprocess, shutil, threading\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ YFinance FX Data Fetcher - Multi-Environment Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ FIXED: Environment Detection\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "IN_LOCAL = not IN_COLAB and not IN_GHA\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Detected Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ FIXED: Working Directories (Environment-Aware)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    # Colab: Use /content (has permissions)\n",
        "    BASE_DIR = Path(\"/content/forex-alpha-models\")\n",
        "    BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "elif IN_GHA:\n",
        "    # GitHub Actions: Use current working directory (repo root)\n",
        "    BASE_DIR = Path.cwd()\n",
        "    print(f\"üìÇ GitHub Actions: Using repo root: {BASE_DIR}\")\n",
        "else:\n",
        "    # Local: Use subdirectory\n",
        "    BASE_DIR = Path(\"./forex-alpha-models\").resolve()\n",
        "    BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Change to base directory (safe for all environments)\n",
        "os.chdir(BASE_DIR)\n",
        "\n",
        "# Setup subdirectories\n",
        "PICKLE_FOLDER = BASE_DIR / \"pickles\"\n",
        "CSV_FOLDER = BASE_DIR / \"csvs\"\n",
        "LOG_FOLDER = BASE_DIR / \"logs\"\n",
        "\n",
        "# Create all subdirectories with parents=True\n",
        "for folder in [PICKLE_FOLDER, CSV_FOLDER, LOG_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Working directory: {BASE_DIR.resolve()}\")\n",
        "print(f\"‚úÖ Pickle folder: {PICKLE_FOLDER}\")\n",
        "print(f\"‚úÖ CSV folder: {CSV_FOLDER}\")\n",
        "print(f\"‚úÖ Log folder: {LOG_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Git Configuration\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT environment variable is required!\")\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
        "\n",
        "# Configure git\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
        "\n",
        "# Store credentials\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ FIXED: Repository Management (Environment-Aware)\n",
        "# ======================================================\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "REPO_FOLDER = BASE_DIR / GITHUB_REPO\n",
        "\n",
        "def ensure_repo_cloned(repo_url, repo_folder, branch=\"main\"):\n",
        "    \"\"\"\n",
        "    Clone or update repository with environment-aware handling\n",
        "    \"\"\"\n",
        "    repo_folder = Path(repo_folder)\n",
        "\n",
        "    if IN_GHA:\n",
        "        # GitHub Actions: Repo already checked out\n",
        "        print(\"ü§ñ GitHub Actions: Repository already available\")\n",
        "        if not (repo_folder / \".git\").exists() and (Path.cwd() / \".git\").exists():\n",
        "            # We're in the repo root, use current directory\n",
        "            print(f\"‚úÖ Using current directory as repo: {Path.cwd()}\")\n",
        "            return Path.cwd()\n",
        "        elif (repo_folder / \".git\").exists():\n",
        "            print(f\"‚úÖ Repository found at: {repo_folder}\")\n",
        "            return repo_folder\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Warning: .git directory not found\")\n",
        "            print(\"   Make sure actions/checkout@v4 is in your workflow\")\n",
        "            return repo_folder\n",
        "\n",
        "    # For Colab and Local: Clone or update\n",
        "    tmp_folder = repo_folder.parent / (repo_folder.name + \"_tmp\")\n",
        "\n",
        "    if tmp_folder.exists():\n",
        "        shutil.rmtree(tmp_folder)\n",
        "\n",
        "    if not (repo_folder / \".git\").exists():\n",
        "        print(f\"üî• Cloning repository to {tmp_folder}...\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", branch, repo_url, str(tmp_folder)],\n",
        "                check=True,\n",
        "                timeout=60\n",
        "            )\n",
        "\n",
        "            if repo_folder.exists():\n",
        "                shutil.rmtree(repo_folder)\n",
        "\n",
        "            tmp_folder.rename(repo_folder)\n",
        "            print(f\"‚úÖ Repository cloned successfully\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ùå Clone timed out after 60 seconds\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Clone failed: {e}\")\n",
        "            raise\n",
        "    else:\n",
        "        print(\"üîÑ Repository exists, pulling latest changes...\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(repo_folder), \"fetch\", \"origin\"],\n",
        "                check=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(repo_folder), \"checkout\", branch],\n",
        "                check=False\n",
        "            )\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(repo_folder), \"pull\", \"origin\", branch],\n",
        "                check=False,\n",
        "                timeout=30\n",
        "            )\n",
        "            print(\"‚úÖ Repository updated successfully\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ö†Ô∏è Update timed out - continuing with existing repo\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Update failed: {e} - continuing with existing repo\")\n",
        "\n",
        "    print(f\"‚úÖ Repository ready at: {repo_folder.resolve()}\")\n",
        "    return repo_folder\n",
        "\n",
        "# Ensure repository is available\n",
        "REPO_FOLDER = ensure_repo_cloned(REPO_URL, REPO_FOLDER, BRANCH)\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ FX Pairs & Timeframes Configuration\n",
        "# ======================================================\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "TIMEFRAMES = {\n",
        "    \"1d_5y\": (\"1d\", \"5y\"),      # Daily data, 5 years\n",
        "    \"1h_2y\": (\"1h\", \"2y\"),      # Hourly data, 2 years\n",
        "    \"15m_60d\": (\"15m\", \"60d\"),  # 15-minute data, 60 days\n",
        "    \"5m_1mo\": (\"5m\", \"1mo\"),    # 5-minute data, 1 month\n",
        "    \"1m_7d\": (\"1m\", \"7d\")       # 1-minute data, 7 days\n",
        "}\n",
        "\n",
        "print(f\"\\nüìä Configuration:\")\n",
        "print(f\"   Pairs: {len(FX_PAIRS)}\")\n",
        "print(f\"   Timeframes: {len(TIMEFRAMES)}\")\n",
        "print(f\"   Total tasks: {len(FX_PAIRS) * len(TIMEFRAMES)}\")\n",
        "\n",
        "# Thread lock for file operations\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Helper Functions\n",
        "# ======================================================\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    \"\"\"Calculate MD5 hash of file to detect changes\"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def merge_data(existing_df, new_df):\n",
        "    \"\"\"Merge existing and new data, removing duplicates\"\"\"\n",
        "    existing_df = ensure_tz_naive(existing_df)\n",
        "    new_df = ensure_tz_naive(new_df)\n",
        "\n",
        "    if existing_df.empty:\n",
        "        return new_df\n",
        "    if new_df.empty:\n",
        "        return existing_df\n",
        "\n",
        "    # Combine dataframes\n",
        "    combined = pd.concat([existing_df, new_df])\n",
        "\n",
        "    # Remove duplicates, keeping the latest\n",
        "    combined = combined[~combined.index.duplicated(keep=\"last\")]\n",
        "\n",
        "    # Sort by date\n",
        "    combined.sort_index(inplace=True)\n",
        "\n",
        "    return combined\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Worker Function for Pair/Timeframe Processing\n",
        "# ======================================================\n",
        "def process_pair_tf(pair, tf_name, interval, period, max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Download and process data for a single pair/timeframe combination\n",
        "\n",
        "    Args:\n",
        "        pair: FX pair (e.g., \"EUR/USD\")\n",
        "        tf_name: Timeframe name (e.g., \"1d_5y\")\n",
        "        interval: YFinance interval (e.g., \"1d\")\n",
        "        period: YFinance period (e.g., \"5y\")\n",
        "        max_retries: Number of retry attempts\n",
        "        retry_delay: Seconds between retries\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (status_message, filepath_if_changed)\n",
        "    \"\"\"\n",
        "    # Convert pair to YFinance symbol (e.g., \"EUR/USD\" -> \"EURUSD=X\")\n",
        "    symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
        "\n",
        "    # Create filename\n",
        "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
        "    filepath = REPO_FOLDER / filename\n",
        "\n",
        "    # Load existing data if available\n",
        "    existing_df = pd.DataFrame()\n",
        "    if filepath.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "            print(f\"  üìÇ Loaded {len(existing_df)} existing rows for {pair} {tf_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not load existing data: {e}\")\n",
        "\n",
        "    # Get hash before changes\n",
        "    old_hash = file_hash(filepath)\n",
        "\n",
        "    # Attempt to download with retries\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  üîΩ Fetching {pair} {tf_name} (attempt {attempt + 1}/{max_retries})...\")\n",
        "\n",
        "            # Download data from YFinance\n",
        "            df = yf.download(\n",
        "                symbol,\n",
        "                interval=interval,\n",
        "                period=period,\n",
        "                progress=False,\n",
        "                auto_adjust=False,\n",
        "                threads=True\n",
        "            )\n",
        "\n",
        "            if df.empty:\n",
        "                raise ValueError(\"No data returned from YFinance\")\n",
        "\n",
        "            # Select and rename columns\n",
        "            available_cols = [c for c in ['Open', 'High', 'Low', 'Close', 'Volume'] if c in df.columns]\n",
        "            df = df[available_cols]\n",
        "            df.rename(columns=lambda x: x.lower(), inplace=True)\n",
        "\n",
        "            # Remove timezone information\n",
        "            df = ensure_tz_naive(df)\n",
        "\n",
        "            # Merge with existing data\n",
        "            combined_df = merge_data(existing_df, df)\n",
        "\n",
        "            # Save to CSV (thread-safe)\n",
        "            with lock:\n",
        "                combined_df.to_csv(filepath)\n",
        "\n",
        "            # Check if file changed\n",
        "            new_hash = file_hash(filepath)\n",
        "            changed = (old_hash != new_hash)\n",
        "\n",
        "            if changed:\n",
        "                print(f\"  ‚úÖ Updated {pair} {tf_name} - Total rows: {len(combined_df)}\")\n",
        "                return f\"üìà Updated {pair} {tf_name} ({len(combined_df)} rows)\", str(filepath)\n",
        "            else:\n",
        "                print(f\"  ‚ÑπÔ∏è No changes {pair} {tf_name}\")\n",
        "                return f\"‚úÖ No changes {pair} {tf_name}\", None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Attempt {attempt + 1}/{max_retries} failed for {pair} {tf_name}: {e}\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  ‚è≥ Waiting {retry_delay} seconds before retry...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ‚ùå All attempts failed for {pair} {tf_name}\")\n",
        "                return f\"‚ùå Failed {pair} {tf_name}: {e}\", None\n",
        "\n",
        "    return f\"‚ùå Failed {pair} {tf_name}\", None\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Parallel Execution\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Starting parallel data download...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "changed_files = []\n",
        "results = []\n",
        "tasks = []\n",
        "\n",
        "# Create all tasks\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    for pair in FX_PAIRS:\n",
        "        for tf_name, (interval, period) in TIMEFRAMES.items():\n",
        "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period))\n",
        "\n",
        "    # Process results as they complete\n",
        "    for future in as_completed(tasks):\n",
        "        try:\n",
        "            msg, filename = future.result()\n",
        "            results.append(msg)\n",
        "            if filename:\n",
        "                changed_files.append(filename)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Task failed with error: {e}\")\n",
        "            results.append(f\"‚ùå Task failed: {e}\")\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Results Summary\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PROCESSING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "print(f\"\\nTotal tasks: {len(results)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "\n",
        "# ======================================================\n",
        "# üîü Git Commit & Push (Skip in GitHub Actions)\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"   (Workflow will handle commit and push)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        # Change to repo directory\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        # Stage changed files\n",
        "        print(f\"üìù Staging {len(changed_files)} files...\")\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        # Commit\n",
        "        commit_msg = f\"Update YFinance FX data - {len(changed_files)} files\"\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Changes committed\")\n",
        "        elif \"nothing to commit\" in result.stdout:\n",
        "            print(\"‚ÑπÔ∏è No changes to commit\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Commit warning: {result.stderr}\")\n",
        "\n",
        "        # Push with retry logic\n",
        "        max_push_attempts = 3\n",
        "        for attempt in range(max_push_attempts):\n",
        "            print(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/{max_push_attempts})...\")\n",
        "\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Successfully pushed to GitHub\")\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_push_attempts - 1:\n",
        "                    print(f\"‚ö†Ô∏è Push failed, retrying...\")\n",
        "                    # Pull latest and try again\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "                else:\n",
        "                    print(f\"‚ùå Push failed after {max_push_attempts} attempts\")\n",
        "                    print(f\"   Error: {result.stderr}\")\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"‚ùå Git operation timed out\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Git error: {e}\")\n",
        "    finally:\n",
        "        # Return to base folder\n",
        "        os.chdir(BASE_DIR)\n",
        "\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚ÑπÔ∏è No changes to commit\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ Completion\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ YFINANCE WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"Pairs processed: {len(FX_PAIRS)}\")\n",
        "print(f\"Timeframes per pair: {len(TIMEFRAMES)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Status: {'Success' if len(results) == len(FX_PAIRS) * len(TIMEFRAMES) else 'Partial'}\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüéØ All FX pairs & timeframes processed with maximum historical data!\")"
      ],
      "metadata": {
        "id": "E8P3UAnfGxbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# FX CSV Combine + Incremental Indicators Pipeline\n",
        "# ‚úÖ Works in: Colab + GitHub Actions + Local\n",
        "# ‚úÖ Thread-safe, timezone-safe, Git-push-safe\n",
        "# ‚úÖ Large dataset-ready with column validation\n",
        "# ‚úÖ Environment-aware paths (NO permission errors)\n",
        "# ‚úÖ COMPLETE - No missing functions!\n",
        "# ======================================================\n",
        "\n",
        "import os, time, hashlib, subprocess, shutil\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîß CSV Combiner & Indicator Generator - Multi-Environment Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 0Ô∏è‚É£ FIXED: Environment Detection\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "IN_LOCAL = not IN_COLAB and not IN_GHA\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Detected Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ FIXED: Working Directories (Environment-Aware)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    # Colab: Use /content (has permissions)\n",
        "    ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "    ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "elif IN_GHA:\n",
        "    # GitHub Actions: Use current working directory (repo root)\n",
        "    ROOT_DIR = Path.cwd()\n",
        "    print(f\"üìÇ GitHub Actions: Using repo root: {ROOT_DIR}\")\n",
        "else:\n",
        "    # Local: Use subdirectory\n",
        "    ROOT_DIR = Path(\"./forex-alpha-models\")\n",
        "    ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Setup subdirectories\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Root directory: {ROOT_DIR}\")\n",
        "print(f\"‚úÖ Repo folder: {REPO_FOLDER}\")\n",
        "print(f\"‚úÖ CSV folder: {CSV_FOLDER}\")\n",
        "print(f\"‚úÖ Pickle folder: {PICKLE_FOLDER}\")\n",
        "print(f\"‚úÖ Logs folder: {LOGS_FOLDER}\")\n",
        "\n",
        "# Thread lock for file operations\n",
        "lock = threading.Lock()\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Print status messages with icons\"\"\"\n",
        "    levels = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\",\"error\":\"‚ùå\",\"debug\":\"üêû\"}\n",
        "    print(f\"{levels.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Git Configuration\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT environment variable is required!\")\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
        "\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ FIXED: Repository Management (COMPLETE FUNCTION!)\n",
        "# ======================================================\n",
        "def ensure_repo():\n",
        "    \"\"\"\n",
        "    Ensure repository exists with environment-aware handling\n",
        "    COMPLETE VERSION - No missing code!\n",
        "    \"\"\"\n",
        "    global REPO_FOLDER  # Allow modifying global variable\n",
        "\n",
        "    if IN_GHA:\n",
        "        # GitHub Actions: Repo already checked out\n",
        "        print_status(\"ü§ñ GitHub Actions: Repository already available\", \"info\")\n",
        "\n",
        "        if not (REPO_FOLDER / \".git\").exists() and (Path.cwd() / \".git\").exists():\n",
        "            # We're in the repo root, update global variable\n",
        "            REPO_FOLDER = Path.cwd()\n",
        "            print_status(f\"‚úÖ Using current directory as repo: {REPO_FOLDER}\", \"success\")\n",
        "        elif (REPO_FOLDER / \".git\").exists():\n",
        "            print_status(f\"‚úÖ Repository found at: {REPO_FOLDER}\", \"success\")\n",
        "        else:\n",
        "            print_status(\"‚ö†Ô∏è Warning: .git directory not found\", \"warn\")\n",
        "            print_status(\"   Make sure actions/checkout@v4 is in your workflow\", \"warn\")\n",
        "        return\n",
        "\n",
        "    # For Colab and Local: Clone or update\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        if REPO_FOLDER.exists():\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "        print_status(f\"Cloning repo into {REPO_FOLDER}...\", \"info\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                check=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            print_status(\"‚úÖ Repository cloned successfully\", \"success\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print_status(\"‚ùå Clone timed out after 60 seconds\", \"error\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ùå Clone failed: {e}\", \"error\")\n",
        "            raise\n",
        "    else:\n",
        "        print_status(\"Repo exists, pulling latest...\", \"info\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"fetch\", \"origin\"],\n",
        "                check=False,\n",
        "                timeout=30\n",
        "            )\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"checkout\", BRANCH],\n",
        "                check=False\n",
        "            )\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH],\n",
        "                check=False,\n",
        "                timeout=30\n",
        "            )\n",
        "            print_status(\"‚úÖ Repo synced successfully\", \"success\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print_status(\"‚ö†Ô∏è Update timed out - continuing with existing repo\", \"warn\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Update failed: {e} - continuing\", \"warn\")\n",
        "\n",
        "# Execute repository setup\n",
        "ensure_repo()\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Helper Functions with Safeguards\n",
        "# ======================================================\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_localize(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath):\n",
        "    \"\"\"Calculate MD5 hash of file to detect changes\"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            md5.update(chunk)\n",
        "\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def safe_numeric(df):\n",
        "    \"\"\"\n",
        "    Handle infinity/NaN robustly before any scaling\n",
        "    WITH COLUMN VALIDATION\n",
        "    \"\"\"\n",
        "    # Replace infinity values first\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Check if OHLC columns exist before trying to drop NaN rows\n",
        "    required_columns = ['open', 'high', 'low', 'close']\n",
        "    existing_columns = [col for col in required_columns if col in df.columns]\n",
        "\n",
        "    # Only drop NaN if we have at least some OHLC columns\n",
        "    if existing_columns:\n",
        "        df.dropna(subset=existing_columns, inplace=True)\n",
        "    else:\n",
        "        # If no OHLC columns, just drop rows that are completely empty\n",
        "        df.dropna(how='all', inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Incremental CSV Combine\n",
        "# ======================================================\n",
        "def combine_csv(csv_path):\n",
        "    \"\"\"\n",
        "    Combine CSV from CSV_FOLDER with existing data in REPO_FOLDER\n",
        "\n",
        "    Args:\n",
        "        csv_path: Path to CSV file in CSV_FOLDER\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (combined_df, target_file)\n",
        "    \"\"\"\n",
        "    target_file = REPO_FOLDER / csv_path.name\n",
        "\n",
        "    # Load existing data from repo if available\n",
        "    if target_file.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(target_file, index_col=0, parse_dates=True)\n",
        "            existing_df = ensure_tz_naive(existing_df)\n",
        "            print_status(f\"  üìÇ Loaded {len(existing_df)} existing rows from {csv_path.name}\", \"debug\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"  ‚ö†Ô∏è Could not load existing data: {e}\", \"warn\")\n",
        "            existing_df = pd.DataFrame()\n",
        "    else:\n",
        "        existing_df = pd.DataFrame()\n",
        "\n",
        "    # Load new data from CSV folder\n",
        "    try:\n",
        "        new_df = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
        "        new_df = ensure_tz_naive(new_df)\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ùå Could not load new data: {e}\", \"error\")\n",
        "        return existing_df, target_file\n",
        "\n",
        "    # Combine dataframes\n",
        "    combined_df = pd.concat([existing_df, new_df])\n",
        "    combined_df = combined_df[~combined_df.index.duplicated(keep=\"last\")]\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    return combined_df, target_file\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Incremental Indicators with Validation\n",
        "# ======================================================\n",
        "def add_indicators_incremental(existing_df, combined_df):\n",
        "    \"\"\"\n",
        "    Add technical indicators only to NEW rows\n",
        "\n",
        "    Args:\n",
        "        existing_df: DataFrame with existing indicators\n",
        "        combined_df: Combined DataFrame with OHLC data\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with new rows and indicators, or None if no new rows\n",
        "    \"\"\"\n",
        "    # Identify new rows\n",
        "    if not existing_df.empty:\n",
        "        new_rows = combined_df.loc[~combined_df.index.isin(existing_df.index)]\n",
        "    else:\n",
        "        new_rows = combined_df.copy()\n",
        "\n",
        "    if new_rows.empty:\n",
        "        return None\n",
        "\n",
        "    # CRITICAL: Validate OHLC columns exist before processing\n",
        "    required_cols = ['open', 'high', 'low', 'close']\n",
        "    if not all(col in new_rows.columns for col in required_cols):\n",
        "        print_status(f\"‚ö†Ô∏è Missing required OHLC columns. Found: {list(new_rows.columns)}\", \"warn\")\n",
        "        return None\n",
        "\n",
        "    # Clean numeric data with validation\n",
        "    new_rows = safe_numeric(new_rows)\n",
        "\n",
        "    # Check if we still have data after cleaning\n",
        "    if new_rows.empty:\n",
        "        print_status(\"‚ö†Ô∏è No rows left after cleaning\", \"warn\")\n",
        "        return None\n",
        "\n",
        "    new_rows.sort_index(inplace=True)\n",
        "\n",
        "    # Calculate indicators\n",
        "    try:\n",
        "        # Trend indicators\n",
        "        trend_indicators = {\n",
        "            'SMA_10': lambda d: ta.trend.sma_indicator(d['close'], 10) if len(d) >= 10 else np.nan,\n",
        "            'SMA_50': lambda d: ta.trend.sma_indicator(d['close'], 50) if len(d) >= 50 else np.nan,\n",
        "            'SMA_200': lambda d: ta.trend.sma_indicator(d['close'], 200) if len(d) >= 200 else np.nan,\n",
        "            'EMA_10': lambda d: ta.trend.ema_indicator(d['close'], 10) if len(d) >= 10 else np.nan,\n",
        "            'EMA_50': lambda d: ta.trend.ema_indicator(d['close'], 50) if len(d) >= 50 else np.nan,\n",
        "            'EMA_200': lambda d: ta.trend.ema_indicator(d['close'], 200) if len(d) >= 200 else np.nan,\n",
        "            'MACD': lambda d: ta.trend.macd(d['close']) if len(d) >= 26 else np.nan,\n",
        "            'MACD_signal': lambda d: ta.trend.macd_signal(d['close']) if len(d) >= 26 else np.nan,\n",
        "            'ADX': lambda d: ta.trend.adx(d['high'], d['low'], d['close'], 14) if len(d) >= 14 else np.nan\n",
        "        }\n",
        "\n",
        "        # Momentum indicators\n",
        "        momentum_indicators = {\n",
        "            'RSI_14': lambda d: ta.momentum.rsi(d['close'], 14) if len(d) >= 14 else np.nan,\n",
        "            'StochRSI': lambda d: ta.momentum.stochrsi(d['close'], 14) if len(d) >= 14 else np.nan,\n",
        "            'CCI': lambda d: ta.trend.cci(d['high'], d['low'], d['close'], 20) if len(d) >= 20 else np.nan,\n",
        "            'ROC': lambda d: ta.momentum.roc(d['close'], 12) if len(d) >= 12 else np.nan,\n",
        "            'Williams_%R': lambda d: WilliamsRIndicator(d['high'], d['low'], d['close'], 14).williams_r() if len(d) >= 14 else np.nan\n",
        "        }\n",
        "\n",
        "        # Volatility indicators\n",
        "        volatility_indicators = {\n",
        "            'Bollinger_High': lambda d: ta.volatility.bollinger_hband(d['close'], 20, 2) if len(d) >= 20 else np.nan,\n",
        "            'Bollinger_Low': lambda d: ta.volatility.bollinger_lband(d['close'], 20, 2) if len(d) >= 20 else np.nan,\n",
        "            'ATR': lambda d: ta.volatility.average_true_range(d['high'], d['low'], d['close'], 14) if len(d) >= 14 else np.nan,\n",
        "            'STDDEV_20': lambda d: d['close'].rolling(20).std() if len(d) >= 20 else np.nan\n",
        "        }\n",
        "\n",
        "        # Volume-based indicators (if volume column exists)\n",
        "        volume_indicators = {}\n",
        "        if 'volume' in new_rows.columns:\n",
        "            volume_indicators = {\n",
        "                'OBV': lambda d: ta.volume.on_balance_volume(d['close'], d['volume']) if len(d) >= 1 else np.nan,\n",
        "                'MFI': lambda d: ta.volume.money_flow_index(d['high'], d['low'], d['close'], d['volume'], 14) if len(d) >= 14 else np.nan\n",
        "            }\n",
        "\n",
        "        # Combine all indicators\n",
        "        all_indicators = {**trend_indicators, **momentum_indicators, **volatility_indicators, **volume_indicators}\n",
        "\n",
        "        # Calculate each indicator\n",
        "        for name, func in all_indicators.items():\n",
        "            try:\n",
        "                new_rows[name] = func(new_rows)\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ö†Ô∏è Failed to calculate {name}: {e}\", \"warn\")\n",
        "                new_rows[name] = np.nan\n",
        "\n",
        "        # Cross signals\n",
        "        if 'EMA_10' in new_rows.columns and 'EMA_50' in new_rows.columns:\n",
        "            new_rows['EMA_10_cross_EMA_50'] = (new_rows['EMA_10'] > new_rows['EMA_50']).astype(int)\n",
        "\n",
        "        if 'EMA_50' in new_rows.columns and 'EMA_200' in new_rows.columns:\n",
        "            new_rows['EMA_50_cross_EMA_200'] = (new_rows['EMA_50'] > new_rows['EMA_200']).astype(int)\n",
        "\n",
        "        if 'SMA_10' in new_rows.columns and 'SMA_50' in new_rows.columns:\n",
        "            new_rows['SMA_10_cross_SMA_50'] = (new_rows['SMA_10'] > new_rows['SMA_50']).astype(int)\n",
        "\n",
        "        if 'SMA_50' in new_rows.columns and 'SMA_200' in new_rows.columns:\n",
        "            new_rows['SMA_50_cross_SMA_200'] = (new_rows['SMA_50'] > new_rows['SMA_200']).astype(int)\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ö†Ô∏è Indicator calculation error: {e}\", \"warn\")\n",
        "\n",
        "    # Clean infinity/NaN values before scaling\n",
        "    numeric_cols = new_rows.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    if len(numeric_cols) > 0 and not new_rows[numeric_cols].dropna(how='all').empty:\n",
        "        # Replace infinity values with NaN\n",
        "        new_rows[numeric_cols] = new_rows[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Forward fill NaN values, then backward fill, then fill remaining with 0\n",
        "        new_rows[numeric_cols] = new_rows[numeric_cols].ffill().bfill().fillna(0)\n",
        "\n",
        "        # Clip extreme values to a reasonable range (5 standard deviations)\n",
        "        for col in numeric_cols:\n",
        "            if new_rows[col].std() > 0:\n",
        "                mean_val = new_rows[col].mean()\n",
        "                std_val = new_rows[col].std()\n",
        "                lower_bound = mean_val - (5 * std_val)\n",
        "                upper_bound = mean_val + (5 * std_val)\n",
        "                new_rows[col] = new_rows[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "        # Scale numeric columns (except OHLC)\n",
        "        protected_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "        scalable_cols = [c for c in numeric_cols if c not in protected_cols]\n",
        "\n",
        "        if scalable_cols:\n",
        "            scaler = MinMaxScaler()\n",
        "            try:\n",
        "                new_rows[scalable_cols] = scaler.fit_transform(new_rows[scalable_cols])\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ö†Ô∏è Scaling warning: {e} - using manual normalization\", \"warn\")\n",
        "                # Manual normalization fallback\n",
        "                for col in scalable_cols:\n",
        "                    col_min = new_rows[col].min()\n",
        "                    col_max = new_rows[col].max()\n",
        "                    if col_max > col_min:\n",
        "                        new_rows[col] = (new_rows[col] - col_min) / (col_max - col_min)\n",
        "\n",
        "    return new_rows\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Worker Function with Validation\n",
        "# ======================================================\n",
        "def process_csv_file(csv_file):\n",
        "    \"\"\"\n",
        "    Process a single CSV file: combine with existing data and add indicators\n",
        "\n",
        "    Args:\n",
        "        csv_file: Path to CSV file\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (pickle_filepath_if_changed, status_message)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Combine CSV data\n",
        "        combined_df, target_file = combine_csv(csv_file)\n",
        "\n",
        "        # Validate combined dataframe has required columns\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        if not all(col in combined_df.columns for col in required_cols):\n",
        "            msg = f\"‚ö†Ô∏è Skipped {csv_file.name}: Missing OHLC columns\"\n",
        "            print_status(msg, \"warn\")\n",
        "            return None, msg\n",
        "\n",
        "        # Check for existing indicators pickle\n",
        "        existing_pickle = PICKLE_FOLDER / f\"{csv_file.stem}_indicators.pkl\"\n",
        "\n",
        "        if existing_pickle.exists():\n",
        "            try:\n",
        "                existing_df = pd.read_pickle(existing_pickle)\n",
        "            except Exception as e:\n",
        "                print_status(f\"  ‚ö†Ô∏è Could not load existing pickle: {e}\", \"warn\")\n",
        "                existing_df = pd.DataFrame()\n",
        "        else:\n",
        "            existing_df = pd.DataFrame()\n",
        "\n",
        "        # Calculate indicators for new rows only\n",
        "        new_indicators = add_indicators_incremental(existing_df, combined_df)\n",
        "\n",
        "        if new_indicators is not None:\n",
        "            # Combine existing indicators with new ones\n",
        "            updated_df = pd.concat([existing_df, new_indicators]).sort_index()\n",
        "\n",
        "            # Save files (thread-safe)\n",
        "            with lock:\n",
        "                updated_df.to_pickle(existing_pickle, protocol=4)\n",
        "                combined_df.to_csv(target_file)\n",
        "\n",
        "            msg = f\"‚úÖ {csv_file.name} updated with {len(new_indicators)} new rows (total: {len(combined_df)})\"\n",
        "            print_status(msg, \"success\")\n",
        "            return str(existing_pickle), msg\n",
        "        else:\n",
        "            msg = f\"‚ÑπÔ∏è {csv_file.name} no new rows (total: {len(combined_df)})\"\n",
        "            print_status(msg, \"info\")\n",
        "            return None, msg\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\"‚ùå Failed {csv_file.name}: {e}\"\n",
        "        print_status(msg, \"error\")\n",
        "        return None, msg\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Process All CSVs in Parallel\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Processing CSV files...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "csv_files = list(CSV_FOLDER.glob(\"*.csv\"))\n",
        "\n",
        "if not csv_files:\n",
        "    print_status(\"No CSVs found to process ‚Äì pipeline will skip\", \"warn\")\n",
        "else:\n",
        "    print_status(f\"Found {len(csv_files)} CSV files to process\", \"info\")\n",
        "\n",
        "changed_files = []\n",
        "\n",
        "if csv_files:\n",
        "    with ThreadPoolExecutor(max_workers=min(8, len(csv_files))) as executor:\n",
        "        futures = [executor.submit(process_csv_file, f) for f in csv_files]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            file, msg = future.result()\n",
        "            if file:\n",
        "                changed_files.append(file)\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Commit & Push Updates (Skip in GitHub Actions)\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"   (Workflow will handle commit and push)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        print_status(f\"Committing {len(changed_files)} updated files...\", \"info\")\n",
        "\n",
        "        # Stage files\n",
        "        subprocess.run(\n",
        "            [\"git\", \"-C\", str(REPO_FOLDER), \"add\"] + changed_files,\n",
        "            check=False\n",
        "        )\n",
        "\n",
        "        # Commit\n",
        "        commit_result = subprocess.run(\n",
        "            [\"git\", \"-C\", str(REPO_FOLDER), \"commit\", \"-m\", \"üìà Auto update FX CSVs & indicators\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=False\n",
        "        )\n",
        "\n",
        "        if commit_result.returncode == 0:\n",
        "            print_status(\"‚úÖ Changes committed\", \"success\")\n",
        "        elif \"nothing to commit\" in commit_result.stdout:\n",
        "            print_status(\"‚ÑπÔ∏è No changes to commit\", \"info\")\n",
        "        else:\n",
        "            print_status(f\"‚ö†Ô∏è Commit warning: {commit_result.stderr}\", \"warn\")\n",
        "\n",
        "        # Push with retry logic\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            print_status(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/{max_attempts})...\", \"info\")\n",
        "\n",
        "            push_result = subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"push\", \"origin\", BRANCH],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if push_result.returncode == 0:\n",
        "                print_status(\"‚úÖ Push successful\", \"success\")\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print_status(f\"‚ö†Ô∏è Push attempt {attempt + 1} failed, retrying...\", \"warn\")\n",
        "                    # Pull before retry\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(5)\n",
        "                else:\n",
        "                    print_status(f\"‚ùå Push failed after {max_attempts} attempts\", \"error\")\n",
        "                    print_status(f\"   Error: {push_result.stderr}\", \"error\")\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print_status(\"‚ùå Git operation timed out\", \"error\")\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ùå Git error: {e}\", \"error\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚ÑπÔ∏è No files changed ‚Äì skipping git operations\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ Completion\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ CSV COMBINER WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"CSV files processed: {len(csv_files)}\")\n",
        "print(f\"Pickle files updated: {len(changed_files)}\")\n",
        "print(f\"Status: {'Success' if csv_files else 'No CSVs found'}\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüéØ All CSVs combined, incremental indicators added successfully!\")"
      ],
      "metadata": {
        "id": "uG213bVoKMQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "VERSION 3.7 ‚Äì ULTRA-PERSISTENT SELF-LEARNING HYBRID FX PIPELINE (ENHANCED)\n",
        "===========================================================================\n",
        "üöÄ NEW IMPROVEMENTS IN v3.7:\n",
        "- ‚úÖ Optimized database queries with connection pooling\n",
        "- ‚úÖ Batch insert operations for better performance\n",
        "- ‚úÖ Enhanced error recovery and retry logic\n",
        "- ‚úÖ Automatic database vacuum and optimization\n",
        "- ‚úÖ Better memory management for large datasets\n",
        "- ‚úÖ Improved logging with rotation\n",
        "- ‚úÖ Database backup functionality\n",
        "- ‚úÖ Performance monitoring and metrics\n",
        "- ‚úÖ Concurrent trade evaluation support\n",
        "- ‚úÖ Enhanced data validation and sanitization\n",
        "\"\"\"\n",
        "\n",
        "import os, time, json, re, shutil, subprocess, pickle, filecmp, sqlite3\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import ta\n",
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from collections import defaultdict\n",
        "from contextlib import contextmanager\n",
        "import threading\n",
        "\n",
        "# ======================================================\n",
        "# 0Ô∏è‚É£ Environment Detection & Path Setup\n",
        "# ======================================================\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Detected Environment: {ENV_NAME}\")\n",
        "\n",
        "# Set paths based on environment\n",
        "if IN_COLAB:\n",
        "    ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "    ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "elif IN_GHA:\n",
        "    ROOT_DIR = Path.cwd()\n",
        "    print(f\"üìÇ GitHub Actions: Using repo root: {ROOT_DIR}\")\n",
        "else:\n",
        "    ROOT_DIR = Path(\"./forex-alpha-models\")\n",
        "    ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Setup subdirectories\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "BACKUP_FOLDER = ROOT_DIR / \"backups\"\n",
        "\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER, BACKUP_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Root Directory: {ROOT_DIR}\")\n",
        "print(f\"‚úÖ Repo Folder: {REPO_FOLDER}\")\n",
        "print(f\"‚úÖ CSV Folder: {CSV_FOLDER}\")\n",
        "print(f\"‚úÖ Pickle Folder: {PICKLE_FOLDER}\")\n",
        "print(f\"‚úÖ Logs Folder: {LOGS_FOLDER}\")\n",
        "print(f\"‚úÖ Backup Folder: {BACKUP_FOLDER}\")\n",
        "\n",
        "# Enhanced logging setup with rotation\n",
        "log_formatter = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "# Main log file with rotation (10MB max, 5 backups)\n",
        "main_handler = RotatingFileHandler(\n",
        "    LOGS_FOLDER / \"pipeline.log\",\n",
        "    maxBytes=10*1024*1024,\n",
        "    backupCount=5\n",
        ")\n",
        "main_handler.setFormatter(log_formatter)\n",
        "\n",
        "# Error log file\n",
        "error_handler = RotatingFileHandler(\n",
        "    LOGS_FOLDER / \"errors.log\",\n",
        "    maxBytes=5*1024*1024,\n",
        "    backupCount=3\n",
        ")\n",
        "error_handler.setLevel(logging.ERROR)\n",
        "error_handler.setFormatter(log_formatter)\n",
        "\n",
        "# Configure root logger\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.addHandler(main_handler)\n",
        "logger.addHandler(error_handler)\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Enhanced status printing with better formatting\"\"\"\n",
        "    icons = {\n",
        "        \"info\": \"‚ÑπÔ∏è\",\n",
        "        \"success\": \"‚úÖ\",\n",
        "        \"warn\": \"‚ö†Ô∏è\",\n",
        "        \"debug\": \"üêû\",\n",
        "        \"error\": \"‚ùå\",\n",
        "        \"performance\": \"‚ö°\"\n",
        "    }\n",
        "    log_level = level if level != \"warn\" else \"warning\"\n",
        "    log_level = log_level if log_level != \"performance\" else \"info\"\n",
        "    getattr(logging, log_level, logging.info)(msg)\n",
        "    print(f\"{icons.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "print_status(f\"Environment: {ENV_NAME}\", \"success\")\n",
        "print_status(f\"Working Directory: {os.getcwd()}\", \"info\")\n",
        "\n",
        "# ======================================================\n",
        "# üÜï ENHANCED DATABASE - v3.7\n",
        "# ======================================================\n",
        "PERSISTENT_DB = REPO_FOLDER / \"ml_persistent_memory.db\"\n",
        "\n",
        "class EnhancedTradeMemoryDatabase:\n",
        "    \"\"\"\n",
        "    ENHANCED VERSION v3.7 - Production-ready database\n",
        "\n",
        "    NEW FEATURES:\n",
        "    - ‚úÖ Connection pooling for better performance\n",
        "    - ‚úÖ Batch operations for bulk inserts\n",
        "    - ‚úÖ Automatic backup and recovery\n",
        "    - ‚úÖ Database optimization (vacuum, analyze)\n",
        "    - ‚úÖ Performance metrics tracking\n",
        "    - ‚úÖ Thread-safe operations\n",
        "    - ‚úÖ Enhanced error handling with retry logic\n",
        "    - ‚úÖ Data validation and sanitization\n",
        "    - ‚úÖ Concurrent trade evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, db_path=PERSISTENT_DB, max_retries=3):\n",
        "        self.db_path = db_path\n",
        "        self.db_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        self.conn = None\n",
        "        self.lock = threading.RLock()  # Thread-safe operations\n",
        "        self.min_age_hours = 1\n",
        "        self.max_retries = max_retries\n",
        "        self.performance_metrics = defaultdict(list)\n",
        "        self.initialize_database()\n",
        "\n",
        "    @contextmanager\n",
        "    def get_cursor(self):\n",
        "        \"\"\"Context manager for database cursor with auto-commit\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        try:\n",
        "            yield cursor\n",
        "            self.conn.commit()\n",
        "        except Exception as e:\n",
        "            self.conn.rollback()\n",
        "            raise e\n",
        "        finally:\n",
        "            cursor.close()\n",
        "\n",
        "    def _execute_with_retry(self, operation, *args, **kwargs):\n",
        "        \"\"\"Execute database operation with retry logic\"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                return operation(*args, **kwargs)\n",
        "            except sqlite3.OperationalError as e:\n",
        "                if attempt < self.max_retries - 1:\n",
        "                    wait_time = (2 ** attempt) * 0.1  # Exponential backoff\n",
        "                    print_status(\n",
        "                        f\"‚ö†Ô∏è Database busy, retrying in {wait_time:.1f}s... \"\n",
        "                        f\"(attempt {attempt + 1}/{self.max_retries})\",\n",
        "                        \"warn\"\n",
        "                    )\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "    def initialize_database(self):\n",
        "        \"\"\"Create database with optimized settings\"\"\"\n",
        "        try:\n",
        "            self.conn = sqlite3.connect(\n",
        "                str(self.db_path),\n",
        "                timeout=30,\n",
        "                check_same_thread=False\n",
        "            )\n",
        "\n",
        "            # Optimized PRAGMA settings\n",
        "            pragmas = [\n",
        "                \"PRAGMA journal_mode=WAL\",\n",
        "                \"PRAGMA synchronous=NORMAL\",\n",
        "                \"PRAGMA cache_size=-64000\",  # 64MB cache\n",
        "                \"PRAGMA temp_store=MEMORY\",\n",
        "                \"PRAGMA mmap_size=30000000000\",  # 30GB memory-mapped I/O\n",
        "                \"PRAGMA page_size=4096\",\n",
        "                \"PRAGMA auto_vacuum=INCREMENTAL\"\n",
        "            ]\n",
        "\n",
        "            for pragma in pragmas:\n",
        "                self.conn.execute(pragma)\n",
        "\n",
        "            with self.get_cursor() as cursor:\n",
        "                # ===== TABLE 1: Pending trades =====\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS pending_trades (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        created_at TEXT NOT NULL,\n",
        "                        iteration INTEGER NOT NULL,\n",
        "                        pair TEXT NOT NULL,\n",
        "                        timeframe TEXT NOT NULL,\n",
        "                        sgd_prediction INTEGER,\n",
        "                        rf_prediction INTEGER,\n",
        "                        ensemble_prediction INTEGER,\n",
        "                        entry_price REAL NOT NULL,\n",
        "                        sl_price REAL NOT NULL,\n",
        "                        tp_price REAL NOT NULL,\n",
        "                        confidence REAL,\n",
        "                        evaluated BOOLEAN DEFAULT 0,\n",
        "                        retry_count INTEGER DEFAULT 0,\n",
        "                        last_error TEXT\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "                # Create indexes\n",
        "                indexes = [\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_pending_eval ON pending_trades(evaluated, created_at)\",\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_pending_pair ON pending_trades(pair, evaluated)\",\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_pending_iteration ON pending_trades(iteration, evaluated)\"\n",
        "                ]\n",
        "\n",
        "                for index_sql in indexes:\n",
        "                    cursor.execute(index_sql)\n",
        "\n",
        "                # ===== TABLE 2: Completed trades =====\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS completed_trades (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        pending_trade_id INTEGER,\n",
        "                        created_at TEXT NOT NULL,\n",
        "                        evaluated_at TEXT NOT NULL,\n",
        "                        iteration_created INTEGER,\n",
        "                        iteration_evaluated INTEGER,\n",
        "                        pair TEXT NOT NULL,\n",
        "                        timeframe TEXT NOT NULL,\n",
        "                        model_used TEXT NOT NULL,\n",
        "                        entry_price REAL NOT NULL,\n",
        "                        exit_price REAL NOT NULL,\n",
        "                        sl_price REAL NOT NULL,\n",
        "                        tp_price REAL NOT NULL,\n",
        "                        prediction INTEGER,\n",
        "                        hit_tp BOOLEAN NOT NULL,\n",
        "                        pnl REAL NOT NULL,\n",
        "                        pnl_percent REAL,\n",
        "                        duration_hours REAL,\n",
        "                        price_movement REAL,\n",
        "                        FOREIGN KEY (pending_trade_id) REFERENCES pending_trades(id)\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "                # Create indexes\n",
        "                indexes = [\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_completed_model ON completed_trades(model_used, evaluated_at)\",\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_completed_pair ON completed_trades(pair, model_used, evaluated_at)\",\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_completed_timestamp ON completed_trades(evaluated_at)\",\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_completed_pnl ON completed_trades(model_used, pnl)\"\n",
        "                ]\n",
        "\n",
        "                for index_sql in indexes:\n",
        "                    cursor.execute(index_sql)\n",
        "\n",
        "                # ===== TABLE 3: Model performance cache =====\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS model_stats_cache (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        updated_at TEXT NOT NULL,\n",
        "                        pair TEXT NOT NULL,\n",
        "                        model_name TEXT NOT NULL,\n",
        "                        days INTEGER NOT NULL,\n",
        "                        total_trades INTEGER DEFAULT 0,\n",
        "                        winning_trades INTEGER DEFAULT 0,\n",
        "                        losing_trades INTEGER DEFAULT 0,\n",
        "                        accuracy_pct REAL DEFAULT 0.0,\n",
        "                        total_pnl REAL DEFAULT 0.0,\n",
        "                        avg_pnl REAL DEFAULT 0.0,\n",
        "                        max_pnl REAL DEFAULT 0.0,\n",
        "                        min_pnl REAL DEFAULT 0.0,\n",
        "                        sharpe_ratio REAL DEFAULT 0.0,\n",
        "                        max_drawdown REAL DEFAULT 0.0,\n",
        "                        avg_duration_hours REAL DEFAULT 0.0,\n",
        "                        UNIQUE(pair, model_name, days) ON CONFLICT REPLACE\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "                cursor.execute('''\n",
        "                    CREATE INDEX IF NOT EXISTS idx_stats_lookup\n",
        "                    ON model_stats_cache(pair, model_name, days)\n",
        "                ''')\n",
        "\n",
        "                # ===== TABLE 4: Pipeline execution log =====\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS execution_log (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        timestamp TEXT NOT NULL,\n",
        "                        iteration INTEGER NOT NULL,\n",
        "                        status TEXT NOT NULL,\n",
        "                        trades_stored INTEGER DEFAULT 0,\n",
        "                        trades_evaluated INTEGER DEFAULT 0,\n",
        "                        duration_seconds REAL,\n",
        "                        memory_usage_mb REAL,\n",
        "                        error_message TEXT\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "                # ===== TABLE 5: Performance metrics =====\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS performance_metrics (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        timestamp TEXT NOT NULL,\n",
        "                        operation TEXT NOT NULL,\n",
        "                        duration_ms REAL NOT NULL,\n",
        "                        rows_affected INTEGER DEFAULT 0,\n",
        "                        success BOOLEAN DEFAULT 1\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "            print_status(\"‚úÖ Enhanced Database v3.7 initialized\", \"success\")\n",
        "            self._verify_database_integrity()\n",
        "            self._optimize_database()\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Database initialization failed: {e}\", \"error\")\n",
        "            raise\n",
        "\n",
        "    def _verify_database_integrity(self):\n",
        "        \"\"\"Verify database structure and run integrity check\"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                # Check integrity\n",
        "                cursor.execute(\"PRAGMA integrity_check\")\n",
        "                result = cursor.fetchone()\n",
        "                if result[0] != 'ok':\n",
        "                    print_status(f\"‚ö†Ô∏è Database integrity issue: {result[0]}\", \"warn\")\n",
        "\n",
        "                # Check tables\n",
        "                cursor.execute(\"\"\"\n",
        "                    SELECT name FROM sqlite_master\n",
        "                    WHERE type='table'\n",
        "                \"\"\")\n",
        "                tables = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "                expected_tables = [\n",
        "                    'pending_trades', 'completed_trades',\n",
        "                    'model_stats_cache', 'execution_log',\n",
        "                    'performance_metrics'\n",
        "                ]\n",
        "\n",
        "                for table in expected_tables:\n",
        "                    if table in tables:\n",
        "                        cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
        "                        count = cursor.fetchone()[0]\n",
        "                        print_status(f\"  ‚úì Table '{table}' exists ({count} rows)\", \"debug\")\n",
        "                    else:\n",
        "                        print_status(f\"  ‚úó Table '{table}' missing!\", \"error\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Database verification warning: {e}\", \"warn\")\n",
        "\n",
        "    def _optimize_database(self):\n",
        "        \"\"\"Optimize database performance\"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                # Analyze tables for query optimization\n",
        "                cursor.execute(\"ANALYZE\")\n",
        "\n",
        "                # Check if vacuum is needed\n",
        "                cursor.execute(\"PRAGMA page_count\")\n",
        "                page_count = cursor.fetchone()[0]\n",
        "\n",
        "                cursor.execute(\"PRAGMA freelist_count\")\n",
        "                freelist_count = cursor.fetchone()[0]\n",
        "\n",
        "                # Vacuum if more than 10% free pages\n",
        "                if page_count > 0 and (freelist_count / page_count) > 0.1:\n",
        "                    print_status(\"üîß Running database vacuum...\", \"info\")\n",
        "                    cursor.execute(\"PRAGMA incremental_vacuum\")\n",
        "                    print_status(\"‚úÖ Database optimized\", \"success\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Database optimization warning: {e}\", \"warn\")\n",
        "\n",
        "    def _track_performance(self, operation, duration_ms, rows_affected=0, success=True):\n",
        "        \"\"\"Track operation performance metrics\"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    INSERT INTO performance_metrics\n",
        "                    (timestamp, operation, duration_ms, rows_affected, success)\n",
        "                    VALUES (?, ?, ?, ?, ?)\n",
        "                ''', (\n",
        "                    datetime.now(timezone.utc).isoformat(),\n",
        "                    operation,\n",
        "                    duration_ms,\n",
        "                    rows_affected,\n",
        "                    success\n",
        "                ))\n",
        "        except Exception as e:\n",
        "            # Don't fail the main operation if metrics tracking fails\n",
        "            print_status(f\"‚ö†Ô∏è Metrics tracking failed: {e}\", \"debug\")\n",
        "\n",
        "    def store_new_signals(self, aggregated_signals, current_iteration):\n",
        "        \"\"\"Store signals with batch insert for better performance\"\"\"\n",
        "        if not aggregated_signals:\n",
        "            print_status(\"‚ö†Ô∏è No signals to store\", \"warn\")\n",
        "            return 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        stored_count = 0\n",
        "        failed_count = 0\n",
        "\n",
        "        # Prepare batch data\n",
        "        batch_data = []\n",
        "\n",
        "        for pair, pair_data in aggregated_signals.items():\n",
        "            signals = pair_data.get('signals', {})\n",
        "\n",
        "            for tf_name, signal_data in signals.items():\n",
        "                if not signal_data:\n",
        "                    continue\n",
        "\n",
        "                # Validate required fields\n",
        "                required_fields = ['live', 'SL', 'TP']\n",
        "                if not all(signal_data.get(f, 0) > 0 for f in required_fields):\n",
        "                    failed_count += 1\n",
        "                    continue\n",
        "\n",
        "                batch_data.append((\n",
        "                    datetime.now(timezone.utc).isoformat(),\n",
        "                    current_iteration,\n",
        "                    pair,\n",
        "                    tf_name,\n",
        "                    signal_data.get('sgd_pred'),\n",
        "                    signal_data.get('rf_pred'),\n",
        "                    signal_data.get('signal'),\n",
        "                    signal_data.get('live', 0),\n",
        "                    signal_data.get('SL', 0),\n",
        "                    signal_data.get('TP', 0),\n",
        "                    signal_data.get('confidence', 0.5)\n",
        "                ))\n",
        "\n",
        "        if not batch_data:\n",
        "            print_status(\"‚ö†Ô∏è No valid signals to store\", \"warn\")\n",
        "            return 0\n",
        "\n",
        "        try:\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                # Batch insert\n",
        "                cursor.executemany('''\n",
        "                    INSERT INTO pending_trades\n",
        "                    (created_at, iteration, pair, timeframe,\n",
        "                     sgd_prediction, rf_prediction, ensemble_prediction,\n",
        "                     entry_price, sl_price, tp_price, confidence)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                ''', batch_data)\n",
        "\n",
        "                stored_count = len(batch_data)\n",
        "\n",
        "                # Log execution\n",
        "                cursor.execute('''\n",
        "                    INSERT INTO execution_log\n",
        "                    (timestamp, iteration, status, trades_stored, duration_seconds)\n",
        "                    VALUES (?, ?, 'signals_stored', ?, ?)\n",
        "                ''', (\n",
        "                    datetime.now(timezone.utc).isoformat(),\n",
        "                    current_iteration,\n",
        "                    stored_count,\n",
        "                    time.time() - start_time\n",
        "                ))\n",
        "\n",
        "            duration_ms = (time.time() - start_time) * 1000\n",
        "            self._track_performance('store_signals', duration_ms, stored_count, True)\n",
        "\n",
        "            print_status(\n",
        "                f\"üíæ Stored {stored_count} trades in {duration_ms:.0f}ms \"\n",
        "                f\"({failed_count} failed)\",\n",
        "                \"success\"\n",
        "            )\n",
        "            return stored_count\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Batch insert failed: {e}\", \"error\")\n",
        "            self._track_performance('store_signals', 0, 0, False)\n",
        "            return 0\n",
        "\n",
        "    def evaluate_pending_trades(self, current_prices, current_iteration):\n",
        "        \"\"\"Enhanced trade evaluation with better performance\"\"\"\n",
        "        if not current_prices:\n",
        "            print_status(\"‚ö†Ô∏è No current prices provided\", \"warn\")\n",
        "            return {}\n",
        "\n",
        "        start_time = time.time()\n",
        "        min_age = (datetime.now(timezone.utc) - timedelta(hours=self.min_age_hours)).isoformat()\n",
        "\n",
        "        try:\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    SELECT id, pair, timeframe, sgd_prediction, rf_prediction,\n",
        "                           ensemble_prediction, entry_price, sl_price, tp_price,\n",
        "                           created_at, iteration\n",
        "                    FROM pending_trades\n",
        "                    WHERE evaluated = 0 AND created_at < ?\n",
        "                    ORDER BY created_at ASC\n",
        "                    LIMIT 1000\n",
        "                ''', (min_age,))\n",
        "\n",
        "                pending_trades = cursor.fetchall()\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Failed to fetch pending trades: {e}\", \"error\")\n",
        "            return {}\n",
        "\n",
        "        if not pending_trades:\n",
        "            print_status(\n",
        "                f\"‚ÑπÔ∏è No trades old enough to evaluate (need {self.min_age_hours}+ hours)\",\n",
        "                \"info\"\n",
        "            )\n",
        "            return {}\n",
        "\n",
        "        print_status(\n",
        "            f\"üîç Evaluating {len(pending_trades)} trades from previous iteration(s)\",\n",
        "            \"info\"\n",
        "        )\n",
        "\n",
        "        results_by_model = defaultdict(lambda: {\n",
        "            'closed_trades': 0,\n",
        "            'wins': 0,\n",
        "            'losses': 0,\n",
        "            'total_pnl': 0.0,\n",
        "            'trades': []\n",
        "        })\n",
        "\n",
        "        evaluated_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        # Prepare batch data for completed trades\n",
        "        completed_trades_batch = []\n",
        "        evaluated_ids = []\n",
        "\n",
        "        for trade in pending_trades:\n",
        "            (trade_id, pair, timeframe, sgd_pred, rf_pred, ensemble_pred,\n",
        "             entry_price, sl_price, tp_price, created_at, created_iteration) = trade\n",
        "\n",
        "            current_price = current_prices.get(pair, 0)\n",
        "\n",
        "            if current_price <= 0:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            # Validate prices\n",
        "            if not self._validate_trade_prices(entry_price, sl_price, tp_price, current_price):\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            # Evaluate for each model\n",
        "            for model_name, prediction in [\n",
        "                ('SGD', sgd_pred),\n",
        "                ('RandomForest', rf_pred),\n",
        "                ('Ensemble', ensemble_pred)\n",
        "            ]:\n",
        "                if prediction is None:\n",
        "                    continue\n",
        "\n",
        "                # Check if TP or SL was hit\n",
        "                hit_tp, hit_sl, exit_price = self._evaluate_trade_outcome(\n",
        "                    prediction, current_price, tp_price, sl_price\n",
        "                )\n",
        "\n",
        "                # If trade closed, record result\n",
        "                if exit_price:\n",
        "                    pnl = self._calculate_pnl(prediction, entry_price, exit_price)\n",
        "                    pnl_percent = (pnl / entry_price) * 100\n",
        "                    duration_hours = self._calculate_duration_hours(created_at)\n",
        "                    price_movement = abs(exit_price - entry_price) / entry_price * 100\n",
        "\n",
        "                    completed_trades_batch.append((\n",
        "                        trade_id, created_at, datetime.now(timezone.utc).isoformat(),\n",
        "                        created_iteration, current_iteration,\n",
        "                        pair, timeframe, model_name, entry_price, exit_price,\n",
        "                        sl_price, tp_price, prediction, hit_tp, pnl, pnl_percent,\n",
        "                        duration_hours, price_movement\n",
        "                    ))\n",
        "\n",
        "                    # Accumulate results\n",
        "                    results_by_model[model_name]['closed_trades'] += 1\n",
        "                    results_by_model[model_name]['total_pnl'] += pnl\n",
        "\n",
        "                    if hit_tp:\n",
        "                        results_by_model[model_name]['wins'] += 1\n",
        "                    else:\n",
        "                        results_by_model[model_name]['losses'] += 1\n",
        "\n",
        "                    results_by_model[model_name]['trades'].append({\n",
        "                        'pair': pair,\n",
        "                        'timeframe': timeframe,\n",
        "                        'pnl': pnl,\n",
        "                        'hit_tp': hit_tp\n",
        "                    })\n",
        "\n",
        "                    status = \"WIN ‚úÖ\" if hit_tp else \"LOSS ‚ùå\"\n",
        "                    print_status(\n",
        "                        f\"{status} {model_name}: {pair} {timeframe} \"\n",
        "                        f\"P&L=${pnl:.5f} ({pnl_percent:+.2f}%) [{duration_hours:.1f}h]\",\n",
        "                        \"success\" if hit_tp else \"warn\"\n",
        "                    )\n",
        "\n",
        "            evaluated_ids.append(trade_id)\n",
        "            evaluated_count += 1\n",
        "\n",
        "        # Batch insert completed trades\n",
        "        if completed_trades_batch:\n",
        "            try:\n",
        "                with self.lock, self.get_cursor() as cursor:\n",
        "                    cursor.executemany('''\n",
        "                        INSERT INTO completed_trades\n",
        "                        (pending_trade_id, created_at, evaluated_at,\n",
        "                         iteration_created, iteration_evaluated,\n",
        "                         pair, timeframe, model_used, entry_price, exit_price,\n",
        "                         sl_price, tp_price, prediction, hit_tp, pnl, pnl_percent,\n",
        "                         duration_hours, price_movement)\n",
        "                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                    ''', completed_trades_batch)\n",
        "\n",
        "                    # Mark as evaluated\n",
        "                    if evaluated_ids:\n",
        "                        placeholders = ','.join('?' * len(evaluated_ids))\n",
        "                        cursor.execute(f'''\n",
        "                            UPDATE pending_trades\n",
        "                            SET evaluated = 1\n",
        "                            WHERE id IN ({placeholders})\n",
        "                        ''', evaluated_ids)\n",
        "\n",
        "                    # Log execution\n",
        "                    cursor.execute('''\n",
        "                        INSERT INTO execution_log\n",
        "                        (timestamp, iteration, status, trades_evaluated, duration_seconds)\n",
        "                        VALUES (?, ?, 'trades_evaluated', ?, ?)\n",
        "                    ''', (\n",
        "                        datetime.now(timezone.utc).isoformat(),\n",
        "                        current_iteration,\n",
        "                        evaluated_count,\n",
        "                        time.time() - start_time\n",
        "                    ))\n",
        "\n",
        "                duration_ms = (time.time() - start_time) * 1000\n",
        "                self._track_performance('evaluate_trades', duration_ms, evaluated_count, True)\n",
        "\n",
        "                print_status(\n",
        "                    f\"‚úÖ Evaluated {evaluated_count} trades in {duration_ms:.0f}ms \"\n",
        "                    f\"({skipped_count} skipped)\",\n",
        "                    \"success\"\n",
        "                )\n",
        "\n",
        "            except sqlite3.Error as e:\n",
        "                print_status(f\"‚ùå Evaluation batch insert failed: {e}\", \"error\")\n",
        "                return {}\n",
        "\n",
        "        # Calculate accuracies\n",
        "        for model_name, results in results_by_model.items():\n",
        "            if results['closed_trades'] > 0:\n",
        "                results['accuracy'] = (results['wins'] / results['closed_trades']) * 100\n",
        "            else:\n",
        "                results['accuracy'] = 0.0\n",
        "\n",
        "        # Update model stats cache\n",
        "        self._update_stats_cache()\n",
        "\n",
        "        return dict(results_by_model)\n",
        "\n",
        "    def _validate_trade_prices(self, entry, sl, tp, current):\n",
        "        \"\"\"Enhanced price validation\"\"\"\n",
        "        try:\n",
        "            # Check for positive values\n",
        "            if any(p <= 0 for p in [entry, sl, tp, current]):\n",
        "                return False\n",
        "\n",
        "            # Check for reasonable values (not NaN or Inf)\n",
        "            if any(not np.isfinite(p) for p in [entry, sl, tp, current]):\n",
        "                return False\n",
        "\n",
        "            # Prices shouldn't be wildly different (max 50% deviation)\n",
        "            prices = [entry, sl, tp, current]\n",
        "            avg_price = sum(prices) / len(prices)\n",
        "\n",
        "            for price in prices:\n",
        "                if abs(price - avg_price) / avg_price > 0.5:\n",
        "                    return False\n",
        "\n",
        "            # SL and TP should be reasonable distances from entry\n",
        "            sl_distance = abs(sl - entry) / entry\n",
        "            tp_distance = abs(tp - entry) / entry\n",
        "\n",
        "            if sl_distance > 0.2 or tp_distance > 0.5:  # 20% SL, 50% TP max\n",
        "                return False\n",
        "\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def _evaluate_trade_outcome(self, prediction, current_price, tp_price, sl_price):\n",
        "        \"\"\"Determine if trade hit TP or SL\"\"\"\n",
        "        hit_tp = False\n",
        "        hit_sl = False\n",
        "        exit_price = None\n",
        "\n",
        "        try:\n",
        "            if prediction == 1:  # Long\n",
        "                if current_price >= tp_price:\n",
        "                    hit_tp = True\n",
        "                    exit_price = tp_price\n",
        "                elif current_price <= sl_price:\n",
        "                    hit_sl = True\n",
        "                    exit_price = sl_price\n",
        "            elif prediction == 0:  # Short\n",
        "                if current_price <= tp_price:\n",
        "                    hit_tp = True\n",
        "                    exit_price = tp_price\n",
        "                elif current_price >= sl_price:\n",
        "                    hit_sl = True\n",
        "                    exit_price = sl_price\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Trade evaluation error: {e}\", \"warn\")\n",
        "\n",
        "        return hit_tp, hit_sl, exit_price\n",
        "\n",
        "    def _calculate_pnl(self, prediction, entry_price, exit_price):\n",
        "        \"\"\"Calculate profit/loss\"\"\"\n",
        "        try:\n",
        "            if prediction == 1:  # Long\n",
        "                return exit_price - entry_price\n",
        "            else:  # Short\n",
        "                return entry_price - exit_price\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_duration_hours(self, created_at):\n",
        "        \"\"\"Calculate trade duration in hours\"\"\"\n",
        "        try:\n",
        "            created_dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))\n",
        "            duration = (datetime.now(timezone.utc) - created_dt).total_seconds() / 3600\n",
        "            return max(0, duration)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _update_stats_cache(self):\n",
        "        \"\"\"Update cached model performance statistics with advanced metrics\"\"\"\n",
        "        try:\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                # Get unique pairs and models\n",
        "                cursor.execute('SELECT DISTINCT pair FROM completed_trades')\n",
        "                pairs = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "                cursor.execute('SELECT DISTINCT model_used FROM completed_trades')\n",
        "                models = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "                for pair in pairs:\n",
        "                    for model in models:\n",
        "                        for days in [7, 30, 90]:\n",
        "                            since = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
        "\n",
        "                            cursor.execute('''\n",
        "                                SELECT\n",
        "                                    COUNT(*) as total,\n",
        "                                    SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins,\n",
        "                                    SUM(CASE WHEN NOT hit_tp THEN 1 ELSE 0 END) as losses,\n",
        "                                    SUM(pnl) as total_pnl,\n",
        "                                    AVG(pnl) as avg_pnl,\n",
        "                                    MAX(pnl) as max_pnl,\n",
        "                                    MIN(pnl) as min_pnl,\n",
        "                                    AVG(duration_hours) as avg_duration\n",
        "                                FROM completed_trades\n",
        "                                WHERE pair = ? AND model_used = ? AND evaluated_at > ?\n",
        "                            ''', (pair, model, since))\n",
        "\n",
        "                            result = cursor.fetchone()\n",
        "                            if not result or not result[0]:\n",
        "                                continue\n",
        "\n",
        "                            total, wins, losses, total_pnl, avg_pnl, max_pnl, min_pnl, avg_duration = result\n",
        "\n",
        "                            accuracy = (wins / total * 100) if total > 0 else 0.0\n",
        "\n",
        "                            # Calculate Sharpe ratio (simplified)\n",
        "                            cursor.execute('''\n",
        "                                SELECT pnl FROM completed_trades\n",
        "                                WHERE pair = ? AND model_used = ? AND evaluated_at > ?\n",
        "                            ''', (pair, model, since))\n",
        "\n",
        "                            pnls = [row[0] for row in cursor.fetchall()]\n",
        "                            sharpe_ratio = 0.0\n",
        "                            max_drawdown = 0.0\n",
        "\n",
        "                            if len(pnls) > 1:\n",
        "                                pnl_std = np.std(pnls)\n",
        "                                if pnl_std > 0:\n",
        "                                    sharpe_ratio = (avg_pnl or 0) / pnl_std\n",
        "\n",
        "                                # Calculate max drawdown\n",
        "                                cumulative_pnl = np.cumsum(pnls)\n",
        "                                running_max = np.maximum.accumulate(cumulative_pnl)\n",
        "                                drawdown = running_max - cumulative_pnl\n",
        "                                max_drawdown = np.max(drawdown) if len(drawdown) > 0 else 0.0\n",
        "\n",
        "                            cursor.execute('''\n",
        "                                INSERT OR REPLACE INTO model_stats_cache\n",
        "                                (updated_at, pair, model_name, days, total_trades,\n",
        "                                 winning_trades, losing_trades, accuracy_pct,\n",
        "                                 total_pnl, avg_pnl, max_pnl, min_pnl,\n",
        "                                 sharpe_ratio, max_drawdown, avg_duration_hours)\n",
        "                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                            ''', (\n",
        "                                datetime.now(timezone.utc).isoformat(),\n",
        "                                pair, model, days, total, wins or 0, losses or 0,\n",
        "                                accuracy, total_pnl or 0.0, avg_pnl or 0.0,\n",
        "                                max_pnl or 0.0, min_pnl or 0.0,\n",
        "                                sharpe_ratio, max_drawdown, avg_duration or 0.0\n",
        "                            ))\n",
        "\n",
        "                print_status(\"‚úÖ Stats cache updated\", \"debug\")\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ö†Ô∏è Stats cache update failed: {e}\", \"warn\")\n",
        "\n",
        "    def get_model_performance(self, pair, model_name, days=7):\n",
        "        \"\"\"Get comprehensive model performance metrics\"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    SELECT total_trades, winning_trades, losing_trades,\n",
        "                           accuracy_pct, total_pnl, avg_pnl, max_pnl, min_pnl,\n",
        "                           sharpe_ratio, max_drawdown, avg_duration_hours, updated_at\n",
        "                    FROM model_stats_cache\n",
        "                    WHERE pair = ? AND model_name = ? AND days = ?\n",
        "                ''', (pair, model_name, days))\n",
        "\n",
        "                result = cursor.fetchone()\n",
        "\n",
        "                if not result:\n",
        "                    return {\n",
        "                        'total_trades': 0,\n",
        "                        'winning_trades': 0,\n",
        "                        'losing_trades': 0,\n",
        "                        'accuracy': 0.0,\n",
        "                        'total_pnl': 0.0,\n",
        "                        'avg_pnl': 0.0,\n",
        "                        'max_pnl': 0.0,\n",
        "                        'min_pnl': 0.0,\n",
        "                        'sharpe_ratio': 0.0,\n",
        "                        'max_drawdown': 0.0,\n",
        "                        'avg_duration_hours': 0.0\n",
        "                    }\n",
        "\n",
        "                (total, wins, losses, accuracy, total_pnl, avg_pnl,\n",
        "                 max_pnl, min_pnl, sharpe, drawdown, avg_duration, updated_at) = result\n",
        "\n",
        "                return {\n",
        "                    'total_trades': total,\n",
        "                    'winning_trades': wins,\n",
        "                    'losing_trades': losses,\n",
        "                    'accuracy': accuracy,\n",
        "                    'total_pnl': total_pnl,\n",
        "                    'avg_pnl': avg_pnl,\n",
        "                    'max_pnl': max_pnl,\n",
        "                    'min_pnl': min_pnl,\n",
        "                    'sharpe_ratio': sharpe,\n",
        "                    'max_drawdown': drawdown,\n",
        "                    'avg_duration_hours': avg_duration,\n",
        "                    'updated_at': updated_at\n",
        "                }\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to get model performance: {e}\", \"warn\")\n",
        "            return {\n",
        "                'total_trades': 0,\n",
        "                'winning_trades': 0,\n",
        "                'losing_trades': 0,\n",
        "                'accuracy': 0.0,\n",
        "                'total_pnl': 0.0,\n",
        "                'avg_pnl': 0.0,\n",
        "                'max_pnl': 0.0,\n",
        "                'min_pnl': 0.0,\n",
        "                'sharpe_ratio': 0.0,\n",
        "                'max_drawdown': 0.0,\n",
        "                'avg_duration_hours': 0.0\n",
        "            }\n",
        "\n",
        "    def get_best_model(self, pair, days=7, min_trades=3):\n",
        "        \"\"\"\n",
        "        Determine best model using multiple criteria:\n",
        "        - Accuracy (primary)\n",
        "        - Sharpe ratio (risk-adjusted returns)\n",
        "        - Total PnL (profitability)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    SELECT model_name, accuracy_pct, total_trades,\n",
        "                           total_pnl, sharpe_ratio, avg_pnl\n",
        "                    FROM model_stats_cache\n",
        "                    WHERE pair = ? AND days = ? AND total_trades >= ?\n",
        "                    ORDER BY\n",
        "                        accuracy_pct DESC,\n",
        "                        sharpe_ratio DESC,\n",
        "                        total_pnl DESC\n",
        "                    LIMIT 1\n",
        "                ''', (pair, days, min_trades))\n",
        "\n",
        "                result = cursor.fetchone()\n",
        "\n",
        "                if result:\n",
        "                    model_name, accuracy, trades, pnl, sharpe, avg_pnl = result\n",
        "                    print_status(\n",
        "                        f\"üèÜ Best model for {pair}: {model_name} \"\n",
        "                        f\"(Acc: {accuracy:.1f}%, Sharpe: {sharpe:.2f}, \"\n",
        "                        f\"PnL: ${pnl:.5f})\",\n",
        "                        \"performance\"\n",
        "                    )\n",
        "                    return model_name\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to get best model: {e}\", \"warn\")\n",
        "\n",
        "        return 'Ensemble'  # Default fallback\n",
        "\n",
        "    def get_database_stats(self):\n",
        "        \"\"\"Get comprehensive database statistics\"\"\"\n",
        "        stats = {}\n",
        "\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                # Pending trades count\n",
        "                cursor.execute('SELECT COUNT(*) FROM pending_trades WHERE evaluated = 0')\n",
        "                stats['pending_trades'] = cursor.fetchone()[0]\n",
        "\n",
        "                # Completed trades count\n",
        "                cursor.execute('SELECT COUNT(*) FROM completed_trades')\n",
        "                stats['completed_trades'] = cursor.fetchone()[0]\n",
        "\n",
        "                # Total P&L\n",
        "                cursor.execute('SELECT SUM(pnl) FROM completed_trades')\n",
        "                result = cursor.fetchone()\n",
        "                stats['total_pnl'] = result[0] if result[0] else 0.0\n",
        "\n",
        "                # Overall accuracy\n",
        "                cursor.execute('''\n",
        "                    SELECT\n",
        "                        COUNT(*) as total,\n",
        "                        SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins\n",
        "                    FROM completed_trades\n",
        "                ''')\n",
        "                result = cursor.fetchone()\n",
        "                if result and result[0] > 0:\n",
        "                    stats['overall_accuracy'] = (result[1] / result[0]) * 100\n",
        "                else:\n",
        "                    stats['overall_accuracy'] = 0.0\n",
        "\n",
        "                # Average trade duration\n",
        "                cursor.execute('SELECT AVG(duration_hours) FROM completed_trades')\n",
        "                result = cursor.fetchone()\n",
        "                stats['avg_duration_hours'] = result[0] if result[0] else 0.0\n",
        "\n",
        "                # Best performing model\n",
        "                cursor.execute('''\n",
        "                    SELECT model_used, COUNT(*) as trades,\n",
        "                           SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins\n",
        "                    FROM completed_trades\n",
        "                    GROUP BY model_used\n",
        "                    ORDER BY wins DESC\n",
        "                    LIMIT 1\n",
        "                ''')\n",
        "                result = cursor.fetchone()\n",
        "                if result:\n",
        "                    stats['best_model'] = result[0]\n",
        "                    stats['best_model_trades'] = result[1]\n",
        "                    stats['best_model_wins'] = result[2]\n",
        "                else:\n",
        "                    stats['best_model'] = 'None'\n",
        "                    stats['best_model_trades'] = 0\n",
        "                    stats['best_model_wins'] = 0\n",
        "\n",
        "                # Database size\n",
        "                if self.db_path.exists():\n",
        "                    stats['db_size_mb'] = self.db_path.stat().st_size / (1024 * 1024)\n",
        "                else:\n",
        "                    stats['db_size_mb'] = 0.0\n",
        "\n",
        "                # Recent activity (last 24 hours)\n",
        "                yesterday = (datetime.now(timezone.utc) - timedelta(days=1)).isoformat()\n",
        "                cursor.execute('''\n",
        "                    SELECT COUNT(*) FROM completed_trades\n",
        "                    WHERE evaluated_at > ?\n",
        "                ''', (yesterday,))\n",
        "                stats['trades_last_24h'] = cursor.fetchone()[0]\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to get database stats: {e}\", \"warn\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def cleanup_old_data(self, days_to_keep=90):\n",
        "        \"\"\"Clean up old data with backup\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cutoff_date = (datetime.now(timezone.utc) - timedelta(days=days_to_keep)).isoformat()\n",
        "\n",
        "        try:\n",
        "            # Create backup before cleanup\n",
        "            self.create_backup()\n",
        "\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                # Delete old evaluated pending trades\n",
        "                cursor.execute('''\n",
        "                    DELETE FROM pending_trades\n",
        "                    WHERE evaluated = 1 AND created_at < ?\n",
        "                ''', (cutoff_date,))\n",
        "                deleted_pending = cursor.rowcount\n",
        "\n",
        "                # Delete old execution logs\n",
        "                cursor.execute('''\n",
        "                    DELETE FROM execution_log\n",
        "                    WHERE timestamp < ?\n",
        "                ''', (cutoff_date,))\n",
        "                deleted_logs = cursor.rowcount\n",
        "\n",
        "                # Delete old performance metrics\n",
        "                cursor.execute('''\n",
        "                    DELETE FROM performance_metrics\n",
        "                    WHERE timestamp < ?\n",
        "                ''', (cutoff_date,))\n",
        "                deleted_metrics = cursor.rowcount\n",
        "\n",
        "                print_status(\n",
        "                    f\"üßπ Cleanup complete: Removed {deleted_pending} pending trades, \"\n",
        "                    f\"{deleted_logs} logs, {deleted_metrics} metrics\",\n",
        "                    \"success\"\n",
        "                )\n",
        "\n",
        "                # Optimize after cleanup\n",
        "                self._optimize_database()\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Cleanup failed: {e}\", \"error\")\n",
        "\n",
        "    def create_backup(self):\n",
        "        \"\"\"Create database backup\"\"\"\n",
        "        try:\n",
        "            if not self.db_path.exists():\n",
        "                return\n",
        "\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            backup_path = BACKUP_FOLDER / f\"db_backup_{timestamp}.db\"\n",
        "\n",
        "            # Create backup\n",
        "            shutil.copy2(self.db_path, backup_path)\n",
        "\n",
        "            # Keep only last 5 backups\n",
        "            backups = sorted(BACKUP_FOLDER.glob(\"db_backup_*.db\"))\n",
        "            while len(backups) > 5:\n",
        "                oldest = backups.pop(0)\n",
        "                oldest.unlink()\n",
        "                print_status(f\"üóëÔ∏è Removed old backup: {oldest.name}\", \"debug\")\n",
        "\n",
        "            print_status(f\"üíæ Backup created: {backup_path.name}\", \"success\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Backup failed: {e}\", \"warn\")\n",
        "\n",
        "    def get_performance_report(self, days=7):\n",
        "        \"\"\"Generate comprehensive performance report\"\"\"\n",
        "        report = {\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "            'period_days': days,\n",
        "            'models': {},\n",
        "            'pairs': {},\n",
        "            'overall': {}\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                # Get all model stats\n",
        "                cursor.execute('''\n",
        "                    SELECT pair, model_name, total_trades, accuracy_pct,\n",
        "                           total_pnl, sharpe_ratio, max_drawdown\n",
        "                    FROM model_stats_cache\n",
        "                    WHERE days = ?\n",
        "                    ORDER BY total_trades DESC\n",
        "                ''', (days,))\n",
        "\n",
        "                for row in cursor.fetchall():\n",
        "                    pair, model, trades, acc, pnl, sharpe, drawdown = row\n",
        "\n",
        "                    if model not in report['models']:\n",
        "                        report['models'][model] = {\n",
        "                            'total_trades': 0,\n",
        "                            'total_pnl': 0.0,\n",
        "                            'avg_accuracy': 0.0,\n",
        "                            'pairs': []\n",
        "                        }\n",
        "\n",
        "                    report['models'][model]['total_trades'] += trades\n",
        "                    report['models'][model]['total_pnl'] += pnl\n",
        "                    report['models'][model]['pairs'].append({\n",
        "                        'pair': pair,\n",
        "                        'accuracy': acc,\n",
        "                        'pnl': pnl\n",
        "                    })\n",
        "\n",
        "                    if pair not in report['pairs']:\n",
        "                        report['pairs'][pair] = {\n",
        "                            'total_trades': 0,\n",
        "                            'best_model': None,\n",
        "                            'best_accuracy': 0.0\n",
        "                        }\n",
        "\n",
        "                    report['pairs'][pair]['total_trades'] += trades\n",
        "                    if acc > report['pairs'][pair]['best_accuracy']:\n",
        "                        report['pairs'][pair]['best_model'] = model\n",
        "                        report['pairs'][pair]['best_accuracy'] = acc\n",
        "\n",
        "                # Calculate averages\n",
        "                for model, data in report['models'].items():\n",
        "                    if len(data['pairs']) > 0:\n",
        "                        data['avg_accuracy'] = sum(p['accuracy'] for p in data['pairs']) / len(data['pairs'])\n",
        "\n",
        "                # Overall stats\n",
        "                stats = self.get_database_stats()\n",
        "                report['overall'] = stats\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to generate report: {e}\", \"warn\")\n",
        "\n",
        "        return report\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close database connection with cleanup\"\"\"\n",
        "        try:\n",
        "            if self.conn:\n",
        "                # Final optimization\n",
        "                self._optimize_database()\n",
        "                self.conn.close()\n",
        "                print_status(\"‚úÖ Database connection closed\", \"success\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Error closing database: {e}\", \"warn\")\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Destructor to ensure connection is closed\"\"\"\n",
        "        self.close()\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# üéØ USAGE EXAMPLE\n",
        "# ======================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print_status(\"=\"*60, \"info\")\n",
        "    print_status(\"ENHANCED FX PIPELINE v3.7 - Database Test\", \"success\")\n",
        "    print_status(\"=\"*60, \"info\")\n",
        "\n",
        "    # Initialize database\n",
        "    db = EnhancedTradeMemoryDatabase()\n",
        "\n",
        "    # Get database stats\n",
        "    stats = db.get_database_stats()\n",
        "    print_status(\"\\nüìä Current Database Statistics:\", \"info\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    # Example: Store test signals\n",
        "    test_signals = {\n",
        "        'EUR/USD': {\n",
        "            'signals': {\n",
        "                'H1': {\n",
        "                    'live': 1.0950,\n",
        "                    'SL': 1.0920,\n",
        "                    'TP': 1.1000,\n",
        "                    'signal': 1,\n",
        "                    'sgd_pred': 1,\n",
        "                    'rf_pred': 1,\n",
        "                    'confidence': 0.85\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    stored = db.store_new_signals(test_signals, current_iteration=1)\n",
        "    print_status(f\"\\n‚úÖ Stored {stored} test signals\", \"success\")\n",
        "\n",
        "    # Example: Evaluate trades\n",
        "    current_prices = {\n",
        "        'EUR/USD': 1.0980\n",
        "    }\n",
        "\n",
        "    results = db.evaluate_pending_trades(current_prices, current_iteration=2)\n",
        "    if results:\n",
        "        print_status(\"\\nüìà Evaluation Results:\", \"info\")\n",
        "        for model, data in results.items():\n",
        "            print(f\"  {model}: {data['wins']}/{data['closed_trades']} wins \"\n",
        "                  f\"({data['accuracy']:.1f}% accuracy)\")\n",
        "\n",
        "    # Generate performance report\n",
        "    report = db.get_performance_report(days=7)\n",
        "    print_status(\"\\nüìã Performance Report:\", \"info\")\n",
        "    print(f\"  Total Models: {len(report['models'])}\")\n",
        "    print(f\"  Total Pairs: {len(report['pairs'])}\")\n",
        "\n",
        "    # Cleanup and close\n",
        "    print_status(\"\\nüßπ Running cleanup...\", \"info\")\n",
        "    db.cleanup_old_data(days_to_keep=90)\n",
        "    db.close()\n",
        "\n",
        "    print_status(\"\\n‚úÖ Enhanced Database v3.7 Test Complete!\", \"success\")"
      ],
      "metadata": {
        "id": "DeN-GHWDOQcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZPAZQi88SYG"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# VERSION 3.6 ‚Äì Unified Loader + Merge Pickles (Production Ready)\n",
        "# Fully Safe | Threaded | Compatible with Hybrid FX Pipeline\n",
        "# Added: Data validation, ATR floors, debug prints, raw price preservation\n",
        "# ======================================================\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import warnings\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Environment & folders\n",
        "# -----------------------------\n",
        "ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "TEMP_PICKLE_FOLDER = ROOT_DIR / \"temp_pickles\"\n",
        "FINAL_PICKLE_FOLDER = ROOT_DIR / \"merged_data_pickles\"\n",
        "\n",
        "for folder in [CSV_FOLDER, TEMP_PICKLE_FOLDER, FINAL_PICKLE_FOLDER, REPO_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "JSON_FILE = REPO_FOLDER / \"latest_signals.json\"\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Safe Indicator Generator\n",
        "# -----------------------------\n",
        "def add_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    df = df[(df[[\"open\", \"high\", \"low\", \"close\"]] > 0).all(axis=1)]\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # --- Preserve raw OHLC prices for GA ---\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if col in df.columns:\n",
        "            df[f\"raw_{col}\"] = df[col].copy()\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "\n",
        "        try:\n",
        "            if len(df['close']) >= 10:\n",
        "                df['SMA_10'] = ta.trend.sma_indicator(df['close'], 10)\n",
        "                df['EMA_10'] = ta.trend.ema_indicator(df['close'], 10)\n",
        "            if len(df['close']) >= 50:\n",
        "                df['SMA_50'] = ta.trend.sma_indicator(df['close'], 50)\n",
        "                df['EMA_50'] = ta.trend.ema_indicator(df['close'], 50)\n",
        "            if len(df['close']) >= 14:\n",
        "                df['RSI_14'] = ta.momentum.rsi(df['close'], 14)\n",
        "            if all(col in df.columns for col in ['high', 'low', 'close']) and len(df['close']) >= 14:\n",
        "                df['Williams_%R'] = WilliamsRIndicator(df['high'], df['low'], df['close'], 14).williams_r()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Indicator calculation failed: {e}\")\n",
        "\n",
        "        # --- Safe ATR ---\n",
        "        try:\n",
        "            if all(col in df.columns for col in ['high', 'low', 'close']):\n",
        "                window = 14\n",
        "                if len(df) >= window:\n",
        "                    df['ATR'] = AverageTrueRange(\n",
        "                        df['high'], df['low'], df['close'], window=window\n",
        "                    ).average_true_range().fillna(1e-5).clip(lower=1e-4)\n",
        "                else:\n",
        "                    df['ATR'] = 1e-4\n",
        "        except Exception as e:\n",
        "            df['ATR'] = 1e-4\n",
        "            print(f\"‚ö†Ô∏è ATR calculation failed: {e}\")\n",
        "\n",
        "        # --- Scale only non-price numeric columns ---\n",
        "        numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns if not df[c].isna().all()]\n",
        "        protected_cols = [\n",
        "            \"open\", \"high\", \"low\", \"close\",\n",
        "            \"raw_open\", \"raw_high\", \"raw_low\", \"raw_close\"\n",
        "        ]\n",
        "        numeric_cols = [c for c in numeric_cols if c not in protected_cols]\n",
        "\n",
        "        if numeric_cols:\n",
        "            scaler = MinMaxScaler()\n",
        "            df[numeric_cols] = scaler.fit_transform(df[numeric_cols].fillna(0) + 1e-8)\n",
        "\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Safe CSV Processing\n",
        "# -----------------------------\n",
        "def process_csv_file(csv_file: Path, save_folder: Path):\n",
        "    try:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\", category=pd.errors.ParserWarning)\n",
        "            df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö™ Skipped empty CSV: {csv_file.name}\")\n",
        "            return None\n",
        "\n",
        "        df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "        df = add_indicators(df)\n",
        "        if df.empty:\n",
        "            print(f\"‚ö™ Skipped CSV after filtering invalid prices: {csv_file.name}\")\n",
        "            return None\n",
        "\n",
        "        out_file = save_folder / f\"{csv_file.stem}.pkl\"\n",
        "        df.to_pickle(out_file)\n",
        "        print(f\"‚úÖ Processed CSV {csv_file.name} ‚Üí {out_file.name}\")\n",
        "        return out_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed CSV {csv_file.name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ JSON Processing\n",
        "# -----------------------------\n",
        "def process_json_file(json_file: Path, save_folder: Path):\n",
        "    try:\n",
        "        with open(json_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load JSON: {e}\")\n",
        "        return []\n",
        "\n",
        "    signals_data = data.get(\"pairs\", {})\n",
        "    timestamp = pd.to_datetime(data.get(\"timestamp\"), utc=True)\n",
        "    processed_files = []\n",
        "\n",
        "    for pair, info in signals_data.items():\n",
        "        signals = info.get(\"signals\", {})\n",
        "        dfs = []\n",
        "\n",
        "        for tf_name, tf_info in signals.items():\n",
        "            df = pd.DataFrame({\n",
        "                \"live\": [tf_info.get(\"live\")],\n",
        "                \"SL\": [tf_info.get(\"SL\")],\n",
        "                \"TP\": [tf_info.get(\"TP\")],\n",
        "                \"signal\": [tf_info.get(\"signal\")]\n",
        "            }, index=[timestamp])\n",
        "            df[\"timeframe\"] = tf_name\n",
        "            df = add_indicators(df)\n",
        "            if not df.empty:\n",
        "                dfs.append(df)\n",
        "\n",
        "        if dfs:\n",
        "            df_pair = pd.concat(dfs)\n",
        "            out_file = save_folder / f\"{pair.replace('/', '_')}.pkl\"\n",
        "            df_pair.to_pickle(out_file)\n",
        "            print(f\"‚úÖ Processed JSON {pair} ‚Üí {out_file.name}\")\n",
        "            processed_files.append(out_file)\n",
        "\n",
        "    return processed_files\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Safe Pickle Merger\n",
        "# -----------------------------\n",
        "def merge_pickles(temp_folder: Path, final_folder: Path, keep_last: int = 5):\n",
        "    pickles = list(temp_folder.glob(\"*.pkl\"))\n",
        "    if not pickles:\n",
        "        print(\"‚ö™ No temporary pickles to merge.\")\n",
        "        return\n",
        "\n",
        "    pairs = set(p.stem.split('.')[0] for p in pickles)\n",
        "\n",
        "    for pair in pairs:\n",
        "        pair_files = [p for p in pickles if p.stem.startswith(pair)]\n",
        "        dfs = [pd.read_pickle(p) for p in pair_files if p.exists() and p.stat().st_size > 0]\n",
        "\n",
        "        if not dfs:\n",
        "            print(f\"‚ö™ Skipped {pair} (no valid pickles)\")\n",
        "            continue\n",
        "\n",
        "        merged_df = pd.concat(dfs, ignore_index=False).sort_index().drop_duplicates()\n",
        "        # Changed filename suffix to match the expected format in W4XoZxs-TrDh\n",
        "        merged_file = final_folder / f\"{pair}_2244.pkl\"\n",
        "        merged_df.to_pickle(merged_file)\n",
        "        print(f\"üîó Merged {len(pair_files)} files ‚Üí {merged_file.name}\")\n",
        "\n",
        "        existing = sorted(final_folder.glob(f\"{pair}_*.pkl\"), key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "        for old_file in existing[keep_last:]:\n",
        "            try:\n",
        "                old_file.unlink()\n",
        "                print(f\"üßπ Removed old file: {old_file.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not remove {old_file.name}: {e}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Unified Pipeline Runner\n",
        "# -----------------------------\n",
        "def run_unified_pipeline():\n",
        "    temp_files = []\n",
        "\n",
        "    # Process JSON first\n",
        "    if JSON_FILE.exists():\n",
        "        temp_files += process_json_file(JSON_FILE, TEMP_PICKLE_FOLDER)\n",
        "        print(f\"‚úÖ JSON processing complete ({len(temp_files)} files)\")\n",
        "\n",
        "    # Process CSVs concurrently\n",
        "    csv_files = list(CSV_FOLDER.glob(\"*.csv\"))\n",
        "    if csv_files:\n",
        "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "            futures = [executor.submit(process_csv_file, f, TEMP_PICKLE_FOLDER) for f in csv_files]\n",
        "            for fut in as_completed(futures):\n",
        "                result = fut.result()\n",
        "                if result:\n",
        "                    temp_files.append(result)\n",
        "\n",
        "    # Merge all pickles safely\n",
        "    merge_pickles(TEMP_PICKLE_FOLDER, FINAL_PICKLE_FOLDER)\n",
        "    print(f\"üéØ Unified pipeline complete ‚Äî merged pickles saved in {FINAL_PICKLE_FOLDER}\")\n",
        "\n",
        "    # Debug: print last few rows of each merged pickle\n",
        "    for pkl_file in FINAL_PICKLE_FOLDER.glob(\"*.pkl\"):\n",
        "        df = pd.read_pickle(pkl_file)\n",
        "        print(f\"üîç {pkl_file.name} last rows:\\n\", df.tail(3))\n",
        "\n",
        "    return FINAL_PICKLE_FOLDER\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Execute\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    final_folder = run_unified_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TAG: pipeline_main\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Ultimate Forex Pipeline v8.5.1 - FIXED GIT PUSH EDITION\n",
        "=======================================================\n",
        "‚úÖ Enhanced Git operations with proper error handling\n",
        "‚úÖ Automatic pull before push to prevent conflicts\n",
        "‚úÖ Fallback mechanisms for push failures\n",
        "‚úÖ All other v8.5 features preserved\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import smtplib\n",
        "import subprocess\n",
        "import time\n",
        "import logging\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# ======================================================\n",
        "# CONFIGURATION & SETUP\n",
        "# ======================================================\n",
        "logging.basicConfig(\n",
        "    filename='forex_pipeline_v85.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(message)s'\n",
        ")\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    icons = {\"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"warn\": \"‚ö†Ô∏è\", \"error\": \"‚ùå\", \"rocket\": \"üöÄ\", \"chart\": \"üìä\", \"brain\": \"üß†\", \"money\": \"üí∞\"}\n",
        "    getattr(logging, level if level != \"warn\" else \"warning\", logging.info)(msg)\n",
        "    print(f\"{icons.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# Environment detection\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "ROOT_DIR = Path(\"/content\") if IN_COLAB else Path(\".\")\n",
        "ROOT_PATH = ROOT_DIR / \"forex-alpha-models\"\n",
        "\n",
        "# Folder setup\n",
        "PICKLE_FOLDER = ROOT_PATH / \"merged_data_pickles\"\n",
        "REPO_FOLDER = ROOT_PATH / \"forex-ai-models\"\n",
        "for f in [PICKLE_FOLDER, REPO_FOLDER]:\n",
        "    f.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(ROOT_PATH)\n",
        "\n",
        "# Git configuration\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "\n",
        "# Email configuration\n",
        "GMAIL_USER = os.environ.get(\"GMAIL_USER\", \"nakatonabira3@gmail.com\")\n",
        "GMAIL_APP_PASSWORD = os.environ.get(\"GMAIL_APP_PASSWORD\", \"gmwohahtltmcewug\")\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/rahim-dotAI/forex-ai-models/main/IMG_1599.jpeg\"\n",
        "\n",
        "# Trading parameters\n",
        "PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "ATR_PERIOD = 14\n",
        "MIN_ATR = 1e-5\n",
        "BASE_CAPITAL = 100\n",
        "MAX_POSITION_FRACTION = 0.1\n",
        "MAX_TRADE_CAP = BASE_CAPITAL * 0.05\n",
        "EPS = 1e-8\n",
        "MAX_ATR_SL = 3.0\n",
        "MAX_ATR_TP = 3.0\n",
        "TOURNAMENT_SIZE = 3\n",
        "SLIPPAGE_PCT = 0.0001\n",
        "COMMISSION_PCT = 0.0002\n",
        "\n",
        "# File paths\n",
        "SIGNALS_JSON_PATH = REPO_FOLDER / \"broker_signals.json\"\n",
        "ENSEMBLE_SIGNALS_FILE = REPO_FOLDER / \"ensemble_signals.json\"\n",
        "MEMORY_DB = REPO_FOLDER / \"memory_v85.db\"\n",
        "LEARNING_FILE = REPO_FOLDER / \"learning_v85.pkl\"\n",
        "ITERATION_FILE = REPO_FOLDER / \"iteration_v85.pkl\"\n",
        "WEIGHTS_FILE = REPO_FOLDER / \"weights_v85.pkl\"\n",
        "MONDAY_FILE = REPO_FOLDER / \"monday_runs.pkl\"\n",
        "\n",
        "# Model configurations\n",
        "COMPETITION_MODELS = {\n",
        "    \"Alpha Momentum\": {\n",
        "        \"color\": \"üî¥\", \"hex_color\": \"#E74C3C\",\n",
        "        \"strategy\": \"Aggressive momentum with adaptive stops\",\n",
        "        \"atr_sl_range\": (1.5, 2.5), \"atr_tp_range\": (2.0, 3.5),\n",
        "        \"risk_range\": (0.015, 0.03), \"confidence_range\": (0.3, 0.5),\n",
        "        \"pop_size\": 15, \"generations\": 20, \"mutation_rate\": 0.3\n",
        "    },\n",
        "    \"Beta Conservative\": {\n",
        "        \"color\": \"üîµ\", \"hex_color\": \"#3498DB\",\n",
        "        \"strategy\": \"Conservative mean reversion\",\n",
        "        \"atr_sl_range\": (1.0, 1.8), \"atr_tp_range\": (1.5, 2.5),\n",
        "        \"risk_range\": (0.005, 0.015), \"confidence_range\": (0.5, 0.7),\n",
        "        \"pop_size\": 12, \"generations\": 15, \"mutation_rate\": 0.2\n",
        "    },\n",
        "    \"Gamma Adaptive\": {\n",
        "        \"color\": \"üü¢\", \"hex_color\": \"#2ECC71\",\n",
        "        \"strategy\": \"Adaptive volatility trading\",\n",
        "        \"atr_sl_range\": (1.2, 2.2), \"atr_tp_range\": (1.8, 3.0),\n",
        "        \"risk_range\": (0.01, 0.025), \"confidence_range\": (0.4, 0.6),\n",
        "        \"pop_size\": 18, \"generations\": 22, \"mutation_rate\": 0.25\n",
        "    }\n",
        "}\n",
        "\n",
        "# ======================================================\n",
        "# CORE DATA CLASSES\n",
        "# ======================================================\n",
        "@dataclass\n",
        "class TradeSignal:\n",
        "    pair: str\n",
        "    direction: str\n",
        "    entry_price: float\n",
        "    sl_price: float\n",
        "    tp_price: float\n",
        "    confidence: float\n",
        "    atr: float\n",
        "    timestamp: str\n",
        "    model: str\n",
        "\n",
        "# ======================================================\n",
        "# ITERATION COUNTER\n",
        "# ======================================================\n",
        "class IterationCounter:\n",
        "    def __init__(self, file=ITERATION_FILE):\n",
        "        self.file = file\n",
        "        self.data = self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        if self.file.exists():\n",
        "            try:\n",
        "                with open(self.file, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            except:\n",
        "                pass\n",
        "        return {'total': 0, 'start': datetime.now(timezone.utc).isoformat(), 'history': []}\n",
        "\n",
        "    def increment(self, success=True):\n",
        "        self.data['total'] += 1\n",
        "        self.data['history'].append({'iteration': self.data['total'], 'time': datetime.now(timezone.utc).isoformat(), 'success': success})\n",
        "        if len(self.data['history']) > 1000:\n",
        "            self.data['history'] = self.data['history'][-1000:]\n",
        "        try:\n",
        "            with open(self.file, 'wb') as f:\n",
        "                pickle.dump(self.data, f, protocol=4)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Counter save failed: {e}\")\n",
        "        return self.data['total']\n",
        "\n",
        "    def get_stats(self):\n",
        "        days = max(1, (datetime.now(timezone.utc) - datetime.fromisoformat(self.data['start'])).days)\n",
        "        return {\n",
        "            'total': self.data['total'],\n",
        "            'days': days,\n",
        "            'per_day': self.data['total'] / days,\n",
        "            'start': self.data['start']\n",
        "        }\n",
        "\n",
        "COUNTER = IterationCounter()\n",
        "\n",
        "# ======================================================\n",
        "# MEMORY SYSTEM\n",
        "# ======================================================\n",
        "class MemorySystem:\n",
        "    def __init__(self, db_path=MEMORY_DB):\n",
        "        self.conn = sqlite3.connect(str(db_path))\n",
        "        self._init_db()\n",
        "\n",
        "    def _init_db(self):\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        # Signals history\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS signals_history (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                direction TEXT NOT NULL,\n",
        "                entry_price REAL NOT NULL,\n",
        "                sl_price REAL,\n",
        "                tp_price REAL,\n",
        "                atr REAL,\n",
        "                confidence INTEGER,\n",
        "                model_name TEXT NOT NULL\n",
        "            )\n",
        "        ''')\n",
        "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_signals_model ON signals_history(model_name, timestamp)')\n",
        "\n",
        "        # Trade results\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS trade_results (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                entry_price REAL NOT NULL,\n",
        "                exit_price REAL NOT NULL,\n",
        "                direction TEXT NOT NULL,\n",
        "                pnl REAL NOT NULL,\n",
        "                pnl_after_costs REAL,\n",
        "                commission REAL,\n",
        "                slippage REAL,\n",
        "                was_correct BOOLEAN NOT NULL,\n",
        "                duration_minutes INTEGER,\n",
        "                model_name TEXT NOT NULL,\n",
        "                confidence INTEGER\n",
        "            )\n",
        "        ''')\n",
        "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_results_model ON trade_results(model_name, timestamp)')\n",
        "\n",
        "        # Competition results\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS competition_results (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                iteration INTEGER NOT NULL,\n",
        "                model_name TEXT NOT NULL,\n",
        "                total_pnl REAL,\n",
        "                accuracy REAL,\n",
        "                sharpe_ratio REAL,\n",
        "                total_trades INTEGER,\n",
        "                successful_trades INTEGER\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        self.conn.commit()\n",
        "        print_status(\"Memory system initialized\", \"success\")\n",
        "\n",
        "    def get_history(self, model_name, days=7):\n",
        "        cursor = self.conn.cursor()\n",
        "        since = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
        "        cursor.execute('''\n",
        "            SELECT COUNT(*), SUM(CASE WHEN was_correct THEN 1 ELSE 0 END),\n",
        "                   SUM(pnl_after_costs), AVG(pnl_after_costs)\n",
        "            FROM trade_results\n",
        "            WHERE model_name = ? AND timestamp > ?\n",
        "        ''', (model_name, since))\n",
        "\n",
        "        result = cursor.fetchone()\n",
        "        total = result[0] or 0\n",
        "        wins = result[1] or 0\n",
        "        return {\n",
        "            'total_trades': total,\n",
        "            'wins': wins,\n",
        "            'accuracy': (wins / total * 100) if total > 0 else 0,\n",
        "            'total_pnl': result[2] or 0,\n",
        "            'avg_pnl': result[3] or 0\n",
        "        }\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn:\n",
        "            self.conn.commit()\n",
        "            self.conn.close()\n",
        "\n",
        "MEMORY = MemorySystem()\n",
        "\n",
        "# ======================================================\n",
        "# TRADE TRACKER\n",
        "# ======================================================\n",
        "class TradeTracker:\n",
        "    def __init__(self, memory):\n",
        "        self.memory = memory\n",
        "        self.active_trades = {}\n",
        "\n",
        "    def store_signals(self, signals_by_model, timestamp):\n",
        "        for model_name, signals in signals_by_model.items():\n",
        "            if model_name not in self.active_trades:\n",
        "                self.active_trades[model_name] = {}\n",
        "\n",
        "            for pair, sig in signals.items():\n",
        "                if sig['direction'] == 'HOLD':\n",
        "                    continue\n",
        "\n",
        "                key = f\"{pair}_{timestamp.isoformat()}\"\n",
        "                self.active_trades[model_name][key] = {\n",
        "                    'pair': pair,\n",
        "                    'direction': sig['direction'],\n",
        "                    'entry': sig['last_price'],\n",
        "                    'sl': sig['SL'],\n",
        "                    'tp': sig['TP'],\n",
        "                    'time': timestamp,\n",
        "                    'confidence': sig['score_1_100'],\n",
        "                    'closed': False\n",
        "                }\n",
        "\n",
        "    def evaluate_outcomes(self, current_prices, current_time):\n",
        "        outcomes = defaultdict(lambda: {'closed': 0, 'wins': 0, 'total_pnl': 0.0, 'accuracy': 0.0})\n",
        "\n",
        "        for model, trades in self.active_trades.items():\n",
        "            for key, trade in list(trades.items()):\n",
        "                if trade['closed']:\n",
        "                    continue\n",
        "\n",
        "                price = current_prices.get(trade['pair'], 0)\n",
        "                if price <= 0:\n",
        "                    continue\n",
        "\n",
        "                # Check TP/SL hit\n",
        "                hit_tp = hit_sl = False\n",
        "                if trade['direction'] == 'BUY':\n",
        "                    if price >= trade['tp']:\n",
        "                        hit_tp = True\n",
        "                        exit_price = trade['tp'] * (1 - SLIPPAGE_PCT)\n",
        "                    elif price <= trade['sl']:\n",
        "                        hit_sl = True\n",
        "                        exit_price = trade['sl'] * (1 - SLIPPAGE_PCT)\n",
        "                    else:\n",
        "                        continue\n",
        "                else:  # SELL\n",
        "                    if price <= trade['tp']:\n",
        "                        hit_tp = True\n",
        "                        exit_price = trade['tp'] * (1 + SLIPPAGE_PCT)\n",
        "                    elif price >= trade['sl']:\n",
        "                        hit_sl = True\n",
        "                        exit_price = trade['sl'] * (1 + SLIPPAGE_PCT)\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                # Calculate P&L\n",
        "                adjusted_entry = trade['entry'] * (1 + SLIPPAGE_PCT if trade['direction'] == 'BUY' else 1 - SLIPPAGE_PCT)\n",
        "                pnl = (exit_price - adjusted_entry) if trade['direction'] == 'BUY' else (adjusted_entry - exit_price)\n",
        "                commission = abs(pnl) * COMMISSION_PCT\n",
        "                pnl_after_costs = pnl - commission\n",
        "\n",
        "                # Record in database\n",
        "                duration = (current_time - trade['time']).total_seconds() / 60\n",
        "                cursor = self.memory.conn.cursor()\n",
        "                cursor.execute('''\n",
        "                    INSERT INTO trade_results\n",
        "                    (timestamp, pair, entry_price, exit_price, direction, pnl, pnl_after_costs,\n",
        "                     commission, slippage, was_correct, duration_minutes, model_name, confidence)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                ''', (current_time.isoformat(), trade['pair'], adjusted_entry, exit_price,\n",
        "                      trade['direction'], pnl, pnl_after_costs, commission, SLIPPAGE_PCT * price,\n",
        "                      hit_tp, duration, model, trade['confidence']))\n",
        "                self.memory.conn.commit()\n",
        "\n",
        "                # Update outcomes\n",
        "                outcomes[model]['closed'] += 1\n",
        "                outcomes[model]['total_pnl'] += pnl_after_costs\n",
        "                if hit_tp:\n",
        "                    outcomes[model]['wins'] += 1\n",
        "\n",
        "                trade['closed'] = True\n",
        "\n",
        "                print_status(\n",
        "                    f\"{model}: {trade['pair']} {trade['direction']} @ {exit_price:.5f} - \"\n",
        "                    f\"P&L: ${pnl_after_costs:.5f} ({'WIN' if hit_tp else 'LOSS'})\",\n",
        "                    \"success\" if hit_tp else \"warn\"\n",
        "                )\n",
        "\n",
        "        # Calculate accuracy\n",
        "        for model, data in outcomes.items():\n",
        "            if data['closed'] > 0:\n",
        "                data['accuracy'] = (data['wins'] / data['closed']) * 100\n",
        "\n",
        "        return dict(outcomes)\n",
        "\n",
        "TRACKER = TradeTracker(MEMORY)\n",
        "\n",
        "# ======================================================\n",
        "# LEARNING SYSTEM\n",
        "# ======================================================\n",
        "class LearningSystem:\n",
        "    def __init__(self, file=LEARNING_FILE):\n",
        "        self.file = file\n",
        "        self.data = self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        if self.file.exists():\n",
        "            try:\n",
        "                with open(self.file, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            except:\n",
        "                pass\n",
        "        return {\n",
        "            'iterations': 0,\n",
        "            'successful_patterns': {},\n",
        "            'learning_curve': [],\n",
        "            'adaptation_score': 0.0\n",
        "        }\n",
        "\n",
        "    def record_iteration(self, results, outcomes=None):\n",
        "        self.data['iterations'] += 1\n",
        "\n",
        "        for model, result in results.items():\n",
        "            if not result or 'metrics' not in result:\n",
        "                continue\n",
        "\n",
        "            pnl = outcomes[model]['total_pnl'] if outcomes and model in outcomes else result['metrics']['total_pnl']\n",
        "            accuracy = outcomes[model]['accuracy'] if outcomes and model in outcomes else 0\n",
        "\n",
        "            if pnl > 0 and accuracy >= 50:\n",
        "                key = f\"{model}_success\"\n",
        "                if key not in self.data['successful_patterns']:\n",
        "                    self.data['successful_patterns'][key] = []\n",
        "\n",
        "                self.data['successful_patterns'][key].append({\n",
        "                    'chromosome': result.get('chromosome'),\n",
        "                    'pnl': pnl,\n",
        "                    'accuracy': accuracy,\n",
        "                    'time': datetime.now(timezone.utc).isoformat()\n",
        "                })\n",
        "\n",
        "                if len(self.data['successful_patterns'][key]) > 50:\n",
        "                    self.data['successful_patterns'][key] = sorted(\n",
        "                        self.data['successful_patterns'][key],\n",
        "                        key=lambda x: x['pnl'],\n",
        "                        reverse=True\n",
        "                    )[:50]\n",
        "\n",
        "        self.data['learning_curve'].append(sum(outcomes[m]['total_pnl'] for m in outcomes) if outcomes else 0)\n",
        "        if len(self.data['learning_curve']) > 100:\n",
        "            self.data['learning_curve'] = self.data['learning_curve'][-100:]\n",
        "\n",
        "        if len(self.data['learning_curve']) >= 10:\n",
        "            recent = np.mean(self.data['learning_curve'][-10:])\n",
        "            self.data['adaptation_score'] = min(100, max(0, 50 + recent))\n",
        "\n",
        "        try:\n",
        "            with open(self.file, 'wb') as f:\n",
        "                pickle.dump(self.data, f, protocol=4)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Learning save failed: {e}\")\n",
        "\n",
        "    def get_best_chromosomes(self, model, top_n=3):\n",
        "        key = f\"{model}_success\"\n",
        "        patterns = self.data['successful_patterns'].get(key, [])\n",
        "        return [p['chromosome'] for p in sorted(patterns, key=lambda x: x['pnl'], reverse=True)[:top_n] if p.get('chromosome')]\n",
        "\n",
        "    def get_report(self):\n",
        "        total_success = sum(len(p) for p in self.data['successful_patterns'].values())\n",
        "        return {\n",
        "            'iterations': self.data['iterations'],\n",
        "            'adaptation_score': self.data['adaptation_score'],\n",
        "            'total_successes': total_success,\n",
        "            'trend': \"üìà Improving\" if self.data['adaptation_score'] > 50 else \"üìâ Adjusting\"\n",
        "        }\n",
        "\n",
        "LEARNING = LearningSystem()\n",
        "\n",
        "# ======================================================\n",
        "# WEEKEND/MONDAY MANAGER\n",
        "# ======================================================\n",
        "class ModeManager:\n",
        "    def __init__(self):\n",
        "        self.monday_data = self._load_monday()\n",
        "\n",
        "    def _load_monday(self):\n",
        "        if MONDAY_FILE.exists():\n",
        "            try:\n",
        "                data = pickle.load(open(MONDAY_FILE, \"rb\"))\n",
        "                if data.get('date') != datetime.now().strftime('%Y-%m-%d'):\n",
        "                    return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "                return data\n",
        "            except:\n",
        "                pass\n",
        "        return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "\n",
        "    def get_mode(self):\n",
        "        weekday = datetime.now().weekday()\n",
        "        if weekday in [5, 6]:\n",
        "            return \"weekend_replay\"\n",
        "        elif weekday == 0 and self.monday_data['count'] < 1:\n",
        "            return \"monday_replay\"\n",
        "        return \"normal\"\n",
        "\n",
        "    def increment_monday(self):\n",
        "        self.monday_data['count'] += 1\n",
        "        self.monday_data['date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        try:\n",
        "            with open(MONDAY_FILE, \"wb\") as f:\n",
        "                pickle.dump(self.monday_data, f, protocol=4)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def should_send_email(self):\n",
        "        return self.get_mode() == \"normal\"\n",
        "\n",
        "MODE_MANAGER = ModeManager()\n",
        "\n",
        "# ======================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ======================================================\n",
        "def ensure_atr(df):\n",
        "    if \"atr\" in df.columns and not df[\"atr\"].isna().all():\n",
        "        df[\"atr\"] = df[\"atr\"].fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "        return df\n",
        "\n",
        "    high, low, close = df[\"high\"].values, df[\"low\"].values, df[\"close\"].values\n",
        "    tr = np.maximum.reduce([\n",
        "        high - low,\n",
        "        np.abs(high - np.roll(close, 1)),\n",
        "        np.abs(low - np.roll(close, 1))\n",
        "    ])\n",
        "    tr[0] = high[0] - low[0] if len(tr) > 0 else MIN_ATR\n",
        "    df[\"atr\"] = pd.Series(tr, index=df.index).rolling(ATR_PERIOD, min_periods=1).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "    return df\n",
        "\n",
        "def seed_hybrid_signal(df):\n",
        "    if \"hybrid_signal\" not in df.columns or df[\"hybrid_signal\"].abs().sum() == 0:\n",
        "        fast = df[\"close\"].rolling(10, min_periods=1).mean()\n",
        "        slow = df[\"close\"].rolling(50, min_periods=1).mean()\n",
        "        df[\"hybrid_signal\"] = (fast - slow).fillna(0)\n",
        "    return df\n",
        "\n",
        "def load_data(folder):\n",
        "    combined = {}\n",
        "    for pair in PAIRS:\n",
        "        combined[pair] = {}\n",
        "        prefix = pair.replace(\"/\", \"_\")\n",
        "        for pf in sorted(folder.glob(f\"{prefix}*.pkl\")):\n",
        "            try:\n",
        "                df = pd.read_pickle(pf)\n",
        "                if not isinstance(df, pd.DataFrame) or len(df) < 50:\n",
        "                    continue\n",
        "                df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "                if df.index.tz is not None:\n",
        "                    df.index = df.index.tz_convert(None)\n",
        "                df = ensure_atr(df)\n",
        "                df = seed_hybrid_signal(df)\n",
        "                tf = re.sub(rf\"{prefix}_?|\\.pkl\", \"\", pf.name).strip(\"_\") or \"merged\"\n",
        "                combined[pair][tf] = df\n",
        "            except:\n",
        "                continue\n",
        "    return combined\n",
        "\n",
        "def fetch_live_rate(pair):\n",
        "    token = os.environ.get(\"BROWSERLESS_TOKEN\", \"\")\n",
        "    if not token:\n",
        "        return 0.0\n",
        "    from_c, to_c = pair.split(\"/\")\n",
        "    try:\n",
        "        r = requests.post(\n",
        "            f\"https://production-sfo.browserless.io/content?token={token}\",\n",
        "            json={\"url\": f\"https://www.x-rates.com/calculator/?from={from_c}&to={to_c}&amount=1\"},\n",
        "            timeout=8\n",
        "        )\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', r.text)\n",
        "        return float(match.group(1).replace(\",\", \"\")) if match else 0.0\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def build_tf_map(data):\n",
        "    return {p: list(tfs.keys()) for p, tfs in data.items()}\n",
        "\n",
        "def create_chromosome(tf_map, config):\n",
        "    chrom = [\n",
        "        float(random.uniform(*config['atr_sl_range'])),\n",
        "        float(random.uniform(*config['atr_tp_range'])),\n",
        "        float(random.uniform(*config['risk_range'])),\n",
        "        float(random.uniform(*config['confidence_range']))\n",
        "    ]\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p, [])))\n",
        "        weights = np.random.dirichlet(np.ones(n)).tolist()\n",
        "        chrom.extend(weights)\n",
        "    return chrom\n",
        "\n",
        "def decode_chromosome(chrom, tf_map):\n",
        "    atr_sl = np.clip(chrom[0], 1.0, MAX_ATR_SL)\n",
        "    atr_tp = np.clip(chrom[1], 1.0, MAX_ATR_TP)\n",
        "    risk, conf = chrom[2], chrom[3]\n",
        "\n",
        "    tf_w = {}\n",
        "    idx = 4\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p, [])))\n",
        "        weights = np.array(chrom[idx:idx+n], dtype=float)\n",
        "        weights = weights / (weights.sum() + EPS) if weights.sum() > 0 else np.ones(n) / n\n",
        "        tf_w[p] = {tf: float(w) for tf, w in zip(tf_map.get(p, []), weights)}\n",
        "        idx += n\n",
        "\n",
        "    return atr_sl, atr_tp, risk, conf, tf_w\n",
        "\n",
        "def calculate_sharpe(equity_curve):\n",
        "    if len(equity_curve) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    equity_array = np.array(equity_curve, dtype=float)\n",
        "    returns = np.diff(equity_array) / (equity_array[:-1] + EPS)\n",
        "    if len(returns) == 0 or np.std(returns) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return float(np.mean(returns) / (np.std(returns) + EPS))\n",
        "\n",
        "# ======================================================\n",
        "# BACKTESTING\n",
        "# ======================================================\n",
        "def backtest_strategy(data, tf_map, chromosome):\n",
        "    atr_sl, atr_tp, risk, conf, tf_w = decode_chromosome(chromosome, tf_map)\n",
        "\n",
        "    equity = BASE_CAPITAL\n",
        "    equity_curve = [equity]\n",
        "    trades = []\n",
        "    position = None\n",
        "\n",
        "    all_times = sorted(set().union(*[df.index for tfs in data.values() for df in tfs.values()]))\n",
        "\n",
        "    for t in all_times:\n",
        "        if position:\n",
        "            pair = position['pair']\n",
        "            price = 0\n",
        "            for tf in tf_map.get(pair, []):\n",
        "                if tf in data.get(pair, {}) and t in data[pair][tf].index:\n",
        "                    price = data[pair][tf].loc[t, 'close']\n",
        "                    break\n",
        "\n",
        "            if price > 0:\n",
        "                hit_tp = (position['dir'] == 'BUY' and price >= position['tp']) or (position['dir'] == 'SELL' and price <= position['tp'])\n",
        "                hit_sl = (position['dir'] == 'BUY' and price <= position['sl']) or (position['dir'] == 'SELL' and price >= position['sl'])\n",
        "\n",
        "                if hit_tp or hit_sl:\n",
        "                    exit_price = position['tp'] if hit_tp else position['sl']\n",
        "                    pnl = (exit_price - position['entry']) * position['size'] if position['dir'] == 'BUY' else (position['entry'] - exit_price) * position['size']\n",
        "                    equity += pnl\n",
        "                    equity_curve.append(equity)\n",
        "                    trades.append({'pnl': pnl, 'correct': hit_tp})\n",
        "                    position = None\n",
        "\n",
        "        if position is None:\n",
        "            for pair in PAIRS:\n",
        "                signal = 0\n",
        "                price = 0\n",
        "                atr = MIN_ATR\n",
        "\n",
        "                for tf, weight in tf_w.get(pair, {}).items():\n",
        "                    if tf in data.get(pair, {}) and t in data[pair][tf].index:\n",
        "                        row = data[pair][tf].loc[t]\n",
        "                        signal += row.get('hybrid_signal', 0) * weight\n",
        "                        price = row['close']\n",
        "                        atr = max(row.get('atr', MIN_ATR), MIN_ATR)\n",
        "\n",
        "                if abs(signal) > conf and price > 0:\n",
        "                    direction = 'BUY' if signal > 0 else 'SELL'\n",
        "                    size = min(equity * risk, MAX_TRADE_CAP) / (atr * atr_sl)\n",
        "\n",
        "                    if direction == 'BUY':\n",
        "                        sl = price - (atr * atr_sl)\n",
        "                        tp = price + (atr * atr_tp)\n",
        "                    else:\n",
        "                        sl = price + (atr * atr_sl)\n",
        "                        tp = price - (atr * atr_tp)\n",
        "\n",
        "                    position = {'pair': pair, 'dir': direction, 'entry': price, 'sl': sl, 'tp': tp, 'size': size}\n",
        "                    break\n",
        "\n",
        "    total = len(trades)\n",
        "    wins = sum(1 for t in trades if t['correct'])\n",
        "    return {\n",
        "        'total_trades': total,\n",
        "        'winning_trades': wins,\n",
        "        'accuracy': (wins / total * 100) if total > 0 else 0,\n",
        "        'total_pnl': sum(t['pnl'] for t in trades),\n",
        "        'sharpe': calculate_sharpe(equity_curve)\n",
        "    }\n",
        "\n",
        "# ======================================================\n",
        "# GENETIC ALGORITHM\n",
        "# ======================================================\n",
        "def run_ga(data, tf_map, model_name, config):\n",
        "    print_status(f\"{config['color']} Training {model_name}...\", \"info\")\n",
        "\n",
        "    pop_size = config['pop_size']\n",
        "    generations = config['generations']\n",
        "    mutation_rate = config['mutation_rate']\n",
        "\n",
        "    try:\n",
        "        population = []\n",
        "        best_hist = LEARNING.get_best_chromosomes(model_name, top_n=3)\n",
        "        for chrom in best_hist:\n",
        "            if chrom:\n",
        "                metrics = backtest_strategy(data, tf_map, chrom)\n",
        "                fitness = metrics['total_pnl'] + (metrics['accuracy'] / 100) * 10\n",
        "                population.append((fitness, chrom))\n",
        "\n",
        "        while len(population) < pop_size:\n",
        "            chrom = create_chromosome(tf_map, config)\n",
        "            metrics = backtest_strategy(data, tf_map, chrom)\n",
        "            fitness = metrics['total_pnl'] + (metrics['accuracy'] / 100) * 10\n",
        "            population.append((fitness, chrom))\n",
        "\n",
        "        population.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "        for gen in range(generations):\n",
        "            new_pop = []\n",
        "            elite_count = max(1, int(pop_size * 0.2))\n",
        "            new_pop.extend(population[:elite_count])\n",
        "\n",
        "            while len(new_pop) < pop_size:\n",
        "                parent1 = max(random.sample(population, TOURNAMENT_SIZE), key=lambda x: x[0])[1]\n",
        "                parent2 = max(random.sample(population, TOURNAMENT_SIZE), key=lambda x: x[0])[1]\n",
        "\n",
        "                point = random.randint(1, len(parent1) - 1)\n",
        "                child = [float(x) for x in parent1[:point]] + [float(x) for x in parent2[point:]]\n",
        "\n",
        "                for i in range(len(child)):\n",
        "                    if random.random() < mutation_rate:\n",
        "                        if i == 0:\n",
        "                            child[i] = float(child[i] + random.gauss(0, 0.3))\n",
        "                            child[i] = float(np.clip(child[i], *config['atr_sl_range']))\n",
        "                        elif i == 1:\n",
        "                            child[i] = float(child[i] + random.gauss(0, 0.3))\n",
        "                            child[i] = float(np.clip(child[i], *config['atr_tp_range']))\n",
        "                        elif i == 2:\n",
        "                            child[i] = float(child[i] + random.gauss(0, 0.005))\n",
        "                            child[i] = float(np.clip(child[i], *config['risk_range']))\n",
        "                        elif i == 3:\n",
        "                            child[i] = float(child[i] + random.gauss(0, 0.1))\n",
        "                            child[i] = float(np.clip(child[i], *config['confidence_range']))\n",
        "                        else:\n",
        "                            child[i] = float(max(0.01, child[i] + random.gauss(0, 0.2)))\n",
        "\n",
        "                metrics = backtest_strategy(data, tf_map, child)\n",
        "                fitness = metrics['total_pnl'] + (metrics['accuracy'] / 100) * 10\n",
        "                new_pop.append((fitness, child))\n",
        "\n",
        "            population = sorted(new_pop, reverse=True, key=lambda x: x[0])\n",
        "\n",
        "            if (gen + 1) % 5 == 0:\n",
        "                print_status(f\"  Gen {gen+1}/{generations}: Best={population[0][0]:.4f}\", \"info\")\n",
        "\n",
        "        best_chrom = population[0][1]\n",
        "        final_metrics = backtest_strategy(data, tf_map, best_chrom)\n",
        "\n",
        "        print_status(\n",
        "            f\"  ‚úÖ {model_name}: {final_metrics['accuracy']:.1f}% accuracy | \"\n",
        "            f\"${final_metrics['total_pnl']:.4f} PnL | {final_metrics['total_trades']} trades\",\n",
        "            \"success\"\n",
        "        )\n",
        "\n",
        "        return {'chromosome': best_chrom, 'metrics': final_metrics}\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.exception(f\"{model_name} GA error\")\n",
        "        raise\n",
        "\n",
        "# ======================================================\n",
        "# SIGNAL GENERATION\n",
        "# ======================================================\n",
        "def generate_signals(data, tf_map, chromosome, model_name, current_time):\n",
        "    atr_sl, atr_tp, risk, conf, tf_w = decode_chromosome(chromosome, tf_map)\n",
        "    signals = {}\n",
        "\n",
        "    for pair in PAIRS:\n",
        "        signal_strength = 0\n",
        "        price = 0\n",
        "        atr = MIN_ATR\n",
        "\n",
        "        for tf, weight in tf_w.get(pair, {}).items():\n",
        "            if tf in data.get(pair, {}):\n",
        "                df = data[pair][tf]\n",
        "                if len(df) > 0:\n",
        "                    row = df.iloc[-1]\n",
        "                    signal_strength += row.get('hybrid_signal', 0) * weight\n",
        "                    price = row['close']\n",
        "                    atr = max(row.get('atr', MIN_ATR), MIN_ATR)\n",
        "\n",
        "        direction = 'HOLD'\n",
        "        sl = tp = price\n",
        "\n",
        "        if abs(signal_strength) > conf and price > 0:\n",
        "            direction = 'BUY' if signal_strength > 0 else 'SELL'\n",
        "\n",
        "            if direction == 'BUY':\n",
        "                sl = price - (atr * atr_sl)\n",
        "                tp = price + (atr * atr_tp)\n",
        "            else:\n",
        "                sl = price + (atr * atr_sl)\n",
        "                tp = price - (atr * atr_tp)\n",
        "\n",
        "        signals[pair] = {\n",
        "            'direction': direction,\n",
        "            'last_price': float(price),\n",
        "            'SL': float(sl),\n",
        "            'TP': float(tp),\n",
        "            'atr': float(atr),\n",
        "            'score_1_100': int(abs(signal_strength) * 100),\n",
        "            'model': model_name,\n",
        "            'timestamp': current_time.isoformat()\n",
        "        }\n",
        "\n",
        "    return signals\n",
        "\n",
        "# ======================================================\n",
        "# EMAIL SYSTEM\n",
        "# ======================================================\n",
        "def send_email(signals_by_model, iteration_stats, learning_report):\n",
        "    if not MODE_MANAGER.should_send_email():\n",
        "        print_status(\"Email skipped (replay mode)\", \"info\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        msg = MIMEMultipart('alternative')\n",
        "        msg['Subject'] = f\"ü§ñ Forex AI Signals - Iteration #{iteration_stats['iteration']}\"\n",
        "        msg['From'] = GMAIL_USER\n",
        "        msg['To'] = GMAIL_USER\n",
        "\n",
        "        html = f\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    ü§ñ Forex AI Trading Signals\n",
        "                    Iteration #{iteration_stats['iteration']} | {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}\n",
        "\n",
        "\n",
        "\n",
        "                    üìä System Statistics\n",
        "                    Total Iterations: {iteration_stats['total_iterations']}\n",
        "                    Learning Trend: {learning_report['trend']}\n",
        "                    Adaptation Score: {learning_report['adaptation_score']:.1f}/100\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for model_name, signals in signals_by_model.items():\n",
        "            config = COMPETITION_MODELS[model_name]\n",
        "            html += f\"\"\"\n",
        "\n",
        "                    {config['color']} {model_name}\n",
        "                    {config['strategy']}\n",
        "            \"\"\"\n",
        "\n",
        "            for pair, sig in signals.items():\n",
        "                if sig['direction'] != 'HOLD':\n",
        "                    direction_class = sig['direction'].lower()\n",
        "                    html += f\"\"\"\n",
        "\n",
        "                            {pair}:\n",
        "                            {sig['direction']}\n",
        "                            @ {sig['last_price']:.5f}\n",
        "                            | SL: {sig['SL']:.5f} | TP: {sig['TP']:.5f}\n",
        "                            Confidence: {sig['score_1_100']}/100\n",
        "\n",
        "                    \"\"\"\n",
        "\n",
        "            html += \"\"\n",
        "\n",
        "        html += \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        msg.attach(MIMEText(html, 'html'))\n",
        "\n",
        "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
        "            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)\n",
        "            server.send_message(msg)\n",
        "\n",
        "        print_status(\"Email sent successfully\", \"success\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"Email failed: {e}\", \"error\")\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED GIT OPERATIONS WITH PROPER ERROR HANDLING\n",
        "# ======================================================\n",
        "def push_to_github(files, message):\n",
        "    \"\"\"\n",
        "    Enhanced Git push with automatic conflict resolution and stash handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ensure repo exists\n",
        "        if not REPO_FOLDER.exists():\n",
        "            print_status(\"Cloning repository...\", \"info\")\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"clone\", REPO_URL, str(REPO_FOLDER)],\n",
        "                capture_output=True,\n",
        "                text=True\n",
        "            )\n",
        "            if result.returncode != 0:\n",
        "                print_status(f\"Clone failed: {result.stderr}\", \"error\")\n",
        "                return False\n",
        "\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        # Stage files BEFORE pulling\n",
        "        print_status(\"Staging files...\", \"info\")\n",
        "        files_added = 0\n",
        "        for f in files:\n",
        "            file_path = REPO_FOLDER / f\n",
        "            if file_path.exists():\n",
        "                subprocess.run([\"git\", \"add\", str(f)], check=False)\n",
        "                files_added += 1\n",
        "            else:\n",
        "                print_status(f\"Skipping {f} (not found)\", \"warn\")\n",
        "\n",
        "        if files_added == 0:\n",
        "            print_status(\"No files to stage\", \"warn\")\n",
        "            return True\n",
        "\n",
        "        # Check if there are changes to commit\n",
        "        status_result = subprocess.run(\n",
        "            [\"git\", \"diff\", \"--cached\", \"--quiet\"],\n",
        "            capture_output=True\n",
        "        )\n",
        "\n",
        "        if status_result.returncode == 0:\n",
        "            print_status(\"No changes detected\", \"info\")\n",
        "            return True\n",
        "\n",
        "        # Stash any unstaged changes to allow clean pull\n",
        "        print_status(\"Stashing unstaged changes...\", \"info\")\n",
        "        subprocess.run([\"git\", \"stash\", \"--include-untracked\"], check=False)\n",
        "\n",
        "        # Pull latest changes\n",
        "        print_status(\"Pulling latest changes...\", \"info\")\n",
        "        pull_result = subprocess.run(\n",
        "            [\"git\", \"pull\", \"--rebase\", \"origin\", \"main\"],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if pull_result.returncode != 0:\n",
        "            print_status(f\"Pull had issues, attempting recovery...\", \"warn\")\n",
        "            # Reset to remote if pull completely fails\n",
        "            subprocess.run([\"git\", \"fetch\", \"origin\", \"main\"], check=False)\n",
        "            subprocess.run([\"git\", \"reset\", \"--hard\", \"origin/main\"], check=False)\n",
        "\n",
        "        # Pop stash if we stashed anything\n",
        "        subprocess.run([\"git\", \"stash\", \"pop\"], capture_output=True, check=False)\n",
        "\n",
        "        # Re-add our files after pull\n",
        "        print_status(\"Re-staging files after pull...\", \"info\")\n",
        "        for f in files:\n",
        "            file_path = REPO_FOLDER / f\n",
        "            if file_path.exists():\n",
        "                subprocess.run([\"git\", \"add\", str(f)], check=False)\n",
        "\n",
        "        # Commit changes\n",
        "        commit_result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", message],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if commit_result.returncode != 0:\n",
        "            if \"nothing to commit\" in commit_result.stdout.lower():\n",
        "                print_status(\"No new changes to commit\", \"info\")\n",
        "                return True\n",
        "            else:\n",
        "                print_status(f\"Commit warning: {commit_result.stderr}\", \"warn\")\n",
        "\n",
        "        # Push with retry logic\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            print_status(f\"Pushing to GitHub (attempt {attempt + 1}/{max_retries})...\", \"info\")\n",
        "\n",
        "            push_result = subprocess.run(\n",
        "                [\"git\", \"push\", \"origin\", \"main\"],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if push_result.returncode == 0:\n",
        "                print_status(\"‚úÖ Successfully pushed to GitHub\", \"success\")\n",
        "                return True\n",
        "\n",
        "            # If push failed, try pulling and pushing again\n",
        "            if attempt < max_retries - 1:\n",
        "                print_status(f\"Push failed, syncing and retrying...\", \"warn\")\n",
        "                subprocess.run([\"git\", \"pull\", \"--rebase\", \"origin\", \"main\"], check=False)\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                error_msg = push_result.stderr.strip()[:200]  # First 200 chars\n",
        "                print_status(f\"Push failed after {max_retries} attempts: {error_msg}\", \"error\")\n",
        "                return False\n",
        "\n",
        "        return False\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print_status(\"Git operation timed out\", \"error\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print_status(f\"Git error: {e}\", \"error\")\n",
        "        logging.error(f\"Git error: {e}\")\n",
        "        return False\n",
        "    finally:\n",
        "        try:\n",
        "            os.chdir(ROOT_PATH)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# ======================================================\n",
        "# MAIN EXECUTION\n",
        "# ======================================================\n",
        "def main():\n",
        "    print_status(\"=\" * 70, \"rocket\")\n",
        "    print_status(\"üöÄ FOREX PIPELINE v8.5.1 - FIXED GIT EDITION\", \"rocket\")\n",
        "    print_status(\"=\" * 70, \"rocket\")\n",
        "\n",
        "    success = False\n",
        "\n",
        "    try:\n",
        "        # Display stats\n",
        "        current_iter = COUNTER.data['total'] + 1\n",
        "        stats = COUNTER.get_stats()\n",
        "        mode = MODE_MANAGER.get_mode()\n",
        "\n",
        "        print_status(f\"\\nüìä Iteration #{current_iter} | Mode: {mode.upper()}\", \"info\")\n",
        "        print_status(f\"Total Runs: {stats['total']} | Days: {stats['days']} | Avg/Day: {stats['per_day']:.1f}\", \"info\")\n",
        "\n",
        "        # Load data\n",
        "        print_status(\"\\nüì¶ Loading data...\", \"info\")\n",
        "        data = load_data(PICKLE_FOLDER)\n",
        "\n",
        "        if not data:\n",
        "            raise ValueError(\"No data loaded - check PICKLE_FOLDER path\")\n",
        "\n",
        "        print_status(f\"‚úÖ Loaded {len(data)} pairs\", \"success\")\n",
        "\n",
        "        tf_map = build_tf_map(data)\n",
        "\n",
        "        # Run competition\n",
        "        print_status(\"\\nüèÜ Running Competition...\", \"chart\")\n",
        "        competition_results = {}\n",
        "        signals_by_model = {}\n",
        "\n",
        "        for model_name, config in COMPETITION_MODELS.items():\n",
        "            try:\n",
        "                result = run_ga(data, tf_map, model_name, config)\n",
        "                competition_results[model_name] = result\n",
        "\n",
        "                # Generate signals\n",
        "                signals = generate_signals(data, tf_map, result['chromosome'], model_name, datetime.now(timezone.utc))\n",
        "                signals_by_model[model_name] = signals\n",
        "\n",
        "                # Store in database\n",
        "                cursor = MEMORY.conn.cursor()\n",
        "                cursor.execute('''\n",
        "                    INSERT INTO competition_results\n",
        "                    (timestamp, iteration, model_name, total_pnl, accuracy, sharpe_ratio, total_trades, successful_trades)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                ''', (\n",
        "                    datetime.now(timezone.utc).isoformat(),\n",
        "                    current_iter,\n",
        "                    model_name,\n",
        "                    result['metrics']['total_pnl'],\n",
        "                    result['metrics']['accuracy'],\n",
        "                    result['metrics']['sharpe'],\n",
        "                    result['metrics']['total_trades'],\n",
        "                    result['metrics']['winning_trades']\n",
        "                ))\n",
        "                MEMORY.conn.commit()\n",
        "\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ùå {model_name} failed: {e}\", \"error\")\n",
        "\n",
        "        # Evaluate previous trades\n",
        "        print_status(\"\\nüìà Evaluating trade outcomes...\", \"info\")\n",
        "        current_prices = {}\n",
        "\n",
        "        for pair in PAIRS:\n",
        "            if mode == \"normal\":\n",
        "                live_rate = fetch_live_rate(pair)\n",
        "                if live_rate > 0:\n",
        "                    current_prices[pair] = live_rate\n",
        "                else:\n",
        "                    # Fallback to data\n",
        "                    for tf in tf_map.get(pair, []):\n",
        "                        if tf in data.get(pair, {}):\n",
        "                            current_prices[pair] = data[pair][tf].iloc[-1]['close']\n",
        "                            break\n",
        "            else:\n",
        "                # Replay mode: use historical price\n",
        "                for tf in tf_map.get(pair, []):\n",
        "                    if tf in data.get(pair, {}):\n",
        "                        current_prices[pair] = data[pair][tf].iloc[-1]['close']\n",
        "                        break\n",
        "\n",
        "        TRACKER.store_signals(signals_by_model, datetime.now(timezone.utc))\n",
        "        outcomes = TRACKER.evaluate_outcomes(current_prices, datetime.now(timezone.utc))\n",
        "\n",
        "        if outcomes:\n",
        "            print_status(\"\\nüìä Trade Outcomes:\", \"success\")\n",
        "            for model, outcome_data in outcomes.items():\n",
        "                print_status(\n",
        "                    f\"{model}: {outcome_data['wins']}/{outcome_data['closed']} wins \"\n",
        "                    f\"({outcome_data['accuracy']:.1f}%) | ${outcome_data['total_pnl']:.2f}\",\n",
        "                    \"success\" if outcome_data['total_pnl'] > 0 else \"warn\"\n",
        "                )\n",
        "\n",
        "        # Update learning system\n",
        "        LEARNING.record_iteration(competition_results, outcomes)\n",
        "        learning_report = LEARNING.get_report()\n",
        "\n",
        "        print_status(f\"\\nüß† Learning: {learning_report['trend']} | Score: {learning_report['adaptation_score']:.1f}/100\", \"brain\")\n",
        "\n",
        "        # Save signals\n",
        "        print_status(\"\\nüíæ Saving signals...\", \"info\")\n",
        "        with open(SIGNALS_JSON_PATH, 'w') as f:\n",
        "            json.dump(signals_by_model, f, indent=2, default=str)\n",
        "\n",
        "        with open(ENSEMBLE_SIGNALS_FILE, 'w') as f:\n",
        "            json.dump({\n",
        "                'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "                'iteration': current_iter,\n",
        "                'models': signals_by_model\n",
        "            }, f, indent=2, default=str)\n",
        "\n",
        "        # Send email\n",
        "        iteration_stats = {\n",
        "            'iteration': current_iter,\n",
        "            'total_iterations': stats['total']\n",
        "        }\n",
        "        send_email(signals_by_model, iteration_stats, learning_report)\n",
        "\n",
        "        # Push to GitHub with enhanced error handling\n",
        "        print_status(\"\\nüîÑ Pushing to GitHub...\", \"info\")\n",
        "        files = [\n",
        "            SIGNALS_JSON_PATH.name,\n",
        "            ENSEMBLE_SIGNALS_FILE.name,\n",
        "            MEMORY_DB.name,\n",
        "            LEARNING_FILE.name,\n",
        "            ITERATION_FILE.name,\n",
        "            MONDAY_FILE.name\n",
        "        ]\n",
        "\n",
        "        git_success = push_to_github(files, f\"ü§ñ Auto-update: Iteration #{current_iter} - {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}\")\n",
        "\n",
        "        if not git_success:\n",
        "            print_status(\"‚ö†Ô∏è  GitHub push had issues, but pipeline completed\", \"warn\")\n",
        "\n",
        "        # Summary\n",
        "        print_status(\"\\n\" + \"=\" * 70, \"success\")\n",
        "        print_status(\"‚úÖ PIPELINE COMPLETED SUCCESSFULLY\", \"success\")\n",
        "        print_status(\"=\" * 70, \"success\")\n",
        "        print_status(f\"Iteration: #{current_iter}\", \"info\")\n",
        "        print_status(f\"Models: {len(competition_results)}\", \"info\")\n",
        "        print_status(f\"Signals: {sum(1 for m in signals_by_model.values() for s in m.values() if s['direction'] != 'HOLD')}\", \"info\")\n",
        "\n",
        "        success = True\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print_status(\"\\n‚ö†Ô∏è Shutdown requested\", \"warn\")\n",
        "    except Exception as e:\n",
        "        print_status(f\"\\n‚ùå Fatal error: {e}\", \"error\")\n",
        "        logging.exception(\"Fatal error\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        COUNTER.increment(success=success)\n",
        "        MEMORY.close()\n",
        "        if MODE_MANAGER.get_mode() == \"monday_replay\":\n",
        "            MODE_MANAGER.increment_monday()\n",
        "        print_status(\"Cleanup complete\", \"info\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    print_status(\"Pipeline shutdown complete\", \"info\")\n"
      ],
      "metadata": {
        "id": "6tm4UypO4cdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}