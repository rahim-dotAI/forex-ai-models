{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr_DWDx4-LLJ"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üîë API Keys Configuration\n",
        "# ======================================================\n",
        "import os\n",
        "\n",
        "# Set API keys from environment variables or defaults\n",
        "ALPHA_VANTAGE_KEY = os.environ.get('ALPHA_VANTAGE_KEY', '1W58NPZXOG5SLHZ6')\n",
        "BROWSERLESS_TOKEN = os.environ.get('BROWSERLESS_TOKEN', '2TMVUBAjFwrr7Tb283f0da6602a4cb698b81778bda61967f7')\n",
        "\n",
        "# Set environment variables for downstream code\n",
        "os.environ['ALPHA_VANTAGE_KEY'] = ALPHA_VANTAGE_KEY\n",
        "os.environ['BROWSERLESS_TOKEN'] = BROWSERLESS_TOKEN\n",
        "\n",
        "# Validate\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    print(\"‚ö†Ô∏è Warning: ALPHA_VANTAGE_KEY not set!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Alpha Vantage Key: {ALPHA_VANTAGE_KEY[:4]}...{ALPHA_VANTAGE_KEY[-4:]}\")\n",
        "\n",
        "if not BROWSERLESS_TOKEN:\n",
        "    print(\"‚ö†Ô∏è Warning: BROWSERLESS_TOKEN not set!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Browserless Token: {BROWSERLESS_TOKEN[:4]}...{BROWSERLESS_TOKEN[-4:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H52F2WkfvWOc"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üåç Environment Detection & Setup (MUST RUN FIRST!)\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "# Override ENV_NAME if in GitHub Actions\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "# Set base paths based on environment\n",
        "if IN_COLAB:\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "elif IN_GHA:\n",
        "    # GitHub Actions already checks out the repo\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    # Local development\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "\n",
        "# Create necessary directories with organized structure\n",
        "DIRECTORIES = {\n",
        "    \"data_raw\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "}\n",
        "\n",
        "# Create all directories\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Display environment info\n",
        "print(\"=\" * 60)\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üîß Python: {sys.version.split()[0]}\")\n",
        "print(f\"üìç Working Dir: {os.getcwd()}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Validate critical environment variables for GitHub Actions\n",
        "if IN_GHA:\n",
        "    required_vars = [\"FOREX_PAT\", \"GIT_USER_NAME\", \"GIT_USER_EMAIL\"]\n",
        "    missing = [v for v in required_vars if not os.environ.get(v)]\n",
        "    if missing:\n",
        "        print(f\"‚ö†Ô∏è  Warning: Missing environment variables: {', '.join(missing)}\")\n",
        "        sys.exit(1)  # Fail fast in CI if critical vars missing\n",
        "    else:\n",
        "        print(\"‚úÖ All required environment variables present\")\n",
        "\n",
        "# Export commonly used paths as globals\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw\"]\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "DB_PATH = DIRECTORIES[\"database\"] / \"memory_v85.db\"\n",
        "LOG_PATH = DIRECTORIES[\"logs\"] / \"pipeline.log\"\n",
        "OUTPUT_PATH = DIRECTORIES[\"outputs\"] / \"signals.json\"\n",
        "\n",
        "print(f\"\\nüìÅ Key Paths:\")\n",
        "print(f\"   CSV: {CSV_FOLDER}\")\n",
        "print(f\"   Pickles: {PICKLE_FOLDER}\")\n",
        "print(f\"   Database: {DB_PATH}\")\n",
        "print(f\"   Logs: {LOG_PATH}\")\n",
        "print(f\"   Signals: {OUTPUT_PATH}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMHCk7ldwo3p"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üìÑ GitHub Sync (Environment-Aware) - ALIGNED VERSION\n",
        "# ======================================================\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import urllib.parse\n",
        "import sys\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Environment Detection (MUST MATCH YOUR FIRST CELL!)\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "# Override ENV_NAME if in GitHub Actions\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CRITICAL FIX: Use SAME paths as environment detection\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    # ‚úÖ MATCHES YOUR ENVIRONMENT DETECTION\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"  # Same as env detection!\n",
        "    REPO_FOLDER = SAVE_FOLDER  # Repo IS the save folder\n",
        "    print(\"‚òÅÔ∏è Colab Mode: Cloning directly to /content/forex-ai-models\")\n",
        "\n",
        "elif IN_GHA:\n",
        "    # ‚úÖ GitHub Actions: Use current directory (already in repo)\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER  # We're already in the repo!\n",
        "    print(\"ü§ñ GitHub Actions Mode: Using current directory\")\n",
        "\n",
        "else:\n",
        "    # ‚úÖ Local: Use current directory\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "    print(\"üíª Local Mode: Using current directory\")\n",
        "\n",
        "# Create necessary directories WITH your organized structure\n",
        "DIRECTORIES = {\n",
        "    \"data_raw\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"üîß Running in: {ENV_NAME}\")\n",
        "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
        "print(f\"üíæ Save folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo folder: {REPO_FOLDER}\")\n",
        "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GitHub Configuration\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GitHub Token (Multi-Source)\n",
        "# ======================================================\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "# Try Colab secrets if in Colab and PAT not found\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except ImportError:\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load Colab secret: {e}\")\n",
        "\n",
        "# Validate PAT\n",
        "if not FOREX_PAT:\n",
        "    print(\"‚ö†Ô∏è Warning: FOREX_PAT not found. Git operations may fail.\")\n",
        "    print(\"   Set FOREX_PAT in:\")\n",
        "    print(\"   - GitHub Secrets (for Actions)\")\n",
        "    print(\"   - Colab Secrets (for Colab)\")\n",
        "    print(\"   - Environment variable (for local)\")\n",
        "    REPO_URL = None\n",
        "else:\n",
        "    SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "    REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "    print(\"‚úÖ GitHub token configured\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Handle Repository Based on Environment\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    # ===== GitHub Actions =====\n",
        "    print(\"\\nü§ñ GitHub Actions Mode\")\n",
        "    print(\"‚úÖ Repository already checked out by actions/checkout\")\n",
        "    print(f\"üìÇ Current directory: {Path.cwd()}\")\n",
        "\n",
        "    # Verify .git exists\n",
        "    if not (Path.cwd() / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Warning: .git directory not found!\")\n",
        "        print(\"   Make sure actions/checkout@v4 is in your workflow\")\n",
        "    else:\n",
        "        print(\"‚úÖ Git repository confirmed\")\n",
        "\n",
        "elif IN_COLAB:\n",
        "    # ===== Google Colab =====\n",
        "    print(\"\\n‚òÅÔ∏è Google Colab Mode\")\n",
        "\n",
        "    if not REPO_URL:\n",
        "        print(\"‚ùå Cannot clone repository: FOREX_PAT not available\")\n",
        "    elif not (REPO_FOLDER / \".git\").exists():\n",
        "        # Check if directory exists but isn't a git repo\n",
        "        if REPO_FOLDER.exists():\n",
        "            print(f\"‚ö†Ô∏è Directory exists but is not a git repo. Removing...\")\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "            print(\"‚úÖ Cleaned up non-git directory\")\n",
        "\n",
        "        # Clone repository\n",
        "        print(f\"üì• Cloning repository to {REPO_FOLDER}...\")\n",
        "        env = os.environ.copy()\n",
        "        env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"  # Skip LFS files\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                check=True,\n",
        "                env=env,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            print(\"‚úÖ Repository cloned successfully\")\n",
        "\n",
        "            # Change to repo directory\n",
        "            os.chdir(REPO_FOLDER)\n",
        "            print(f\"üìÇ Changed directory to: {os.getcwd()}\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Clone failed: {e.stderr}\")\n",
        "            print(\"Creating directory structure manually...\")\n",
        "            REPO_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ùå Clone timed out after 60 seconds\")\n",
        "            REPO_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "    else:\n",
        "        # Repository exists, pull latest\n",
        "        print(\"‚úÖ Repository already exists, pulling latest changes...\")\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"pull\", \"origin\", BRANCH],\n",
        "                check=True,\n",
        "                cwd=REPO_FOLDER,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            print(\"‚úÖ Successfully pulled latest changes\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Pull failed: {e.stderr}\")\n",
        "            print(\"Continuing with existing files...\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ö†Ô∏è Pull timed out, continuing anyway...\")\n",
        "\n",
        "    # Configure Git LFS (disable for Colab)\n",
        "    print(\"‚öôÔ∏è Configuring Git LFS...\")\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\"git\", \"lfs\", \"uninstall\"],\n",
        "            check=False,\n",
        "            cwd=REPO_FOLDER,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"‚úÖ LFS disabled for Colab\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è LFS setup warning: {e}\")\n",
        "\n",
        "else:\n",
        "    # ===== Local Environment =====\n",
        "    print(\"\\nüíª Local Development Mode\")\n",
        "    print(f\"üìÇ Working in: {SAVE_FOLDER}\")\n",
        "\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Not a git repository\")\n",
        "        print(\"   Run: git clone https://github.com/rahim-dotAI/forex-ai-models.git\")\n",
        "    else:\n",
        "        print(\"‚úÖ Git repository found\")\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Create Organized Directory Structure\n",
        "# ======================================================\n",
        "print(\"\\nüìÅ Creating organized directory structure...\")\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"   ‚úÖ {dir_name}: {dir_path}\")\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Git Global Configuration\n",
        "# ======================================================\n",
        "print(\"\\nüîß Configuring Git...\")\n",
        "\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "# Set git config\n",
        "git_configs = [\n",
        "    ([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], \"User name\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], \"User email\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], \"Detached HEAD warning\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"init.defaultBranch\", \"main\"], \"Default branch\")\n",
        "]\n",
        "\n",
        "for cmd, description in git_configs:\n",
        "    try:\n",
        "        subprocess.run(cmd, check=False, capture_output=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not set {description}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Export Path Constants (MATCH YOUR ENVIRONMENT DETECTION!)\n",
        "# ======================================================\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw\"]\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "DB_PATH = DIRECTORIES[\"database\"] / \"memory_v85.db\"\n",
        "LOG_PATH = DIRECTORIES[\"logs\"] / \"pipeline.log\"\n",
        "OUTPUT_PATH = DIRECTORIES[\"outputs\"] / \"signals.json\"\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Environment Summary & Validation\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üßæ ENVIRONMENT SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment:      {ENV_NAME}\")\n",
        "print(f\"Working Dir:      {os.getcwd()}\")\n",
        "print(f\"Save Folder:      {SAVE_FOLDER}\")\n",
        "print(f\"Repo Folder:      {REPO_FOLDER}\")\n",
        "print(f\"Repository:       https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}\")\n",
        "print(f\"Branch:           {BRANCH}\")\n",
        "print(f\"Git Repo Exists:  {(REPO_FOLDER / '.git').exists()}\")\n",
        "print(f\"FOREX_PAT Set:    {'‚úÖ Yes' if FOREX_PAT else '‚ùå No'}\")\n",
        "\n",
        "# Check critical paths\n",
        "print(\"\\nüìã Critical Paths:\")\n",
        "print(f\"   CSV Folder:    {CSV_FOLDER}\")\n",
        "print(f\"   Pickle Folder: {PICKLE_FOLDER}\")\n",
        "print(f\"   Database:      {DB_PATH}\")\n",
        "print(f\"   Logs:          {LOG_PATH}\")\n",
        "print(f\"   Signals:       {OUTPUT_PATH}\")\n",
        "\n",
        "print(\"\\nüìÇ Directory Status:\")\n",
        "critical_paths = {\n",
        "    \"Repo .git\": REPO_FOLDER / \".git\",\n",
        "    \"Data Raw\": CSV_FOLDER,\n",
        "    \"Data Processed\": PICKLE_FOLDER,\n",
        "    \"Database\": DIRECTORIES[\"database\"],\n",
        "    \"Logs\": DIRECTORIES[\"logs\"],\n",
        "    \"Outputs\": DIRECTORIES[\"outputs\"]\n",
        "}\n",
        "\n",
        "for name, path in critical_paths.items():\n",
        "    exists = path.exists()\n",
        "    icon = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"  {icon} {name}: {path}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ Setup completed successfully!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# üîü Export Variables for Downstream Cells\n",
        "# ======================================================\n",
        "# These variables are now available in subsequent cells:\n",
        "# - ENV_NAME: Environment name\n",
        "# - IN_COLAB: Boolean for Colab detection\n",
        "# - IN_GHA: Boolean for GitHub Actions detection\n",
        "# - SAVE_FOLDER: Path to save files (same as REPO_FOLDER in Colab)\n",
        "# - REPO_FOLDER: Path to git repository\n",
        "# - CSV_FOLDER, PICKLE_FOLDER, DB_PATH, LOG_PATH, OUTPUT_PATH: Organized paths\n",
        "# - GITHUB_USERNAME, GITHUB_REPO, BRANCH: Git config\n",
        "# - FOREX_PAT: GitHub token (if available)\n",
        "\n",
        "print(\"\\n‚úÖ All environment variables exported for downstream cells\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oih6CDfjAjG9"
      },
      "outputs": [],
      "source": [
        "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn river\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVMes9cDyXky"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ALPHA VANTAGE FX DATA FETCHER - OPTIMIZED FOR DAILY USE\n",
        "=======================================================\n",
        "‚úÖ Designed to run ONCE per day (not every 2 hours)\n",
        "‚úÖ Reduces API usage from 48/day to 4/day\n",
        "‚úÖ Environment variable SKIP_ALPHA_VANTAGE support\n",
        "‚úÖ Data quality validation before saving\n",
        "‚úÖ Works in GitHub Actions, Google Colab, and Local\n",
        "‚úÖ Thread-safe operations with retry logic\n",
        "‚úÖ Clear naming: pair_daily_av.csv (av = Alpha Vantage)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import hashlib\n",
        "import requests\n",
        "import subprocess\n",
        "import threading\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# üÜï SKIP CHECK - Exit early if not needed\n",
        "# ======================================================\n",
        "SKIP_ALPHA_VANTAGE = os.environ.get(\"SKIP_ALPHA_VANTAGE\", \"false\").lower() == \"true\"\n",
        "\n",
        "if SKIP_ALPHA_VANTAGE:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"‚è≠Ô∏è  ALPHA VANTAGE SKIPPED (runs separately at midnight)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"‚ÑπÔ∏è  Alpha Vantage daily data doesn't change hourly\")\n",
        "    print(\"‚ÑπÔ∏è  Using existing data from last midnight run\")\n",
        "    print(\"=\" * 70)\n",
        "    sys.exit(0)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION\n",
        "# ======================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ Alpha Vantage FX Data Fetcher - Daily Optimized v2.0\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üìç Environment: {ENV_NAME}\")\n",
        "print(f\"‚è∞ Current Time: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
        "print(f\"üîÑ Fetch Mode: Daily (saves API calls)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ PATH CONFIGURATION\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "# Directory structure\n",
        "DIRECTORIES = {\n",
        "    \"data_raw_alpha\": SAVE_FOLDER / \"data\" / \"raw\" / \"alpha_vantage\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "    \"quarantine\": SAVE_FOLDER / \"data\" / \"quarantine\" / \"alpha_vantage\",\n",
        "}\n",
        "\n",
        "for dir_path in DIRECTORIES.values():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw_alpha\"]\n",
        "QUARANTINE_FOLDER = DIRECTORIES[\"quarantine\"]\n",
        "LOG_FOLDER = DIRECTORIES[\"logs\"]\n",
        "\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üìä Alpha Vantage CSV: {CSV_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ DATA QUALITY VALIDATOR\n",
        "# ======================================================\n",
        "class DataQualityValidator:\n",
        "    \"\"\"Validate data quality before saving\"\"\"\n",
        "\n",
        "    MIN_ROWS = 50\n",
        "    MIN_PRICE_CV = 0.01  # 0.01% minimum variation\n",
        "    MIN_UNIQUE_RATIO = 0.01  # 1% unique prices\n",
        "    MIN_TRUE_RANGE = 1e-10\n",
        "    MIN_QUALITY_SCORE = 40.0\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df, pair):\n",
        "        \"\"\"\n",
        "        Validate DataFrame quality\n",
        "        Returns: (is_valid, quality_score, metrics, issues)\n",
        "        \"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return False, 0.0, {}, [\"Empty DataFrame\"]\n",
        "\n",
        "        issues = []\n",
        "        metrics = {}\n",
        "\n",
        "        metrics['row_count'] = len(df)\n",
        "        if len(df) < DataQualityValidator.MIN_ROWS:\n",
        "            issues.append(f\"Too few rows: {len(df)}\")\n",
        "\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            issues.append(f\"Missing columns: {missing_cols}\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        ohlc_data = df[required_cols].dropna()\n",
        "        if len(ohlc_data) == 0:\n",
        "            issues.append(\"No valid OHLC data\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        metrics['valid_rows'] = len(ohlc_data)\n",
        "        metrics['valid_ratio'] = len(ohlc_data) / len(df)\n",
        "\n",
        "        close_prices = ohlc_data['close']\n",
        "        metrics['price_mean'] = float(close_prices.mean())\n",
        "        metrics['price_std'] = float(close_prices.std())\n",
        "        metrics['price_cv'] = (metrics['price_std'] / metrics['price_mean']) * 100 if metrics['price_mean'] > 0 else 0.0\n",
        "\n",
        "        metrics['unique_prices'] = close_prices.nunique()\n",
        "        metrics['unique_ratio'] = metrics['unique_prices'] / len(close_prices)\n",
        "\n",
        "        high = ohlc_data['high'].values\n",
        "        low = ohlc_data['low'].values\n",
        "        close = ohlc_data['close'].values\n",
        "\n",
        "        tr = np.maximum.reduce([\n",
        "            high - low,\n",
        "            np.abs(high - np.roll(close, 1)),\n",
        "            np.abs(low - np.roll(close, 1))\n",
        "        ])\n",
        "        tr[0] = high[0] - low[0]\n",
        "\n",
        "        metrics['true_range_median'] = float(np.median(tr))\n",
        "        metrics['true_range_mean'] = float(np.mean(tr))\n",
        "\n",
        "        # Quality score (0-100)\n",
        "        quality_score = 0.0\n",
        "        quality_score += metrics['valid_ratio'] * 30\n",
        "\n",
        "        if metrics['price_cv'] >= 1.0:\n",
        "            quality_score += 30\n",
        "        elif metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV:\n",
        "            quality_score += (metrics['price_cv'] / 1.0) * 30\n",
        "\n",
        "        quality_score += min(metrics['unique_ratio'] * 20, 20)\n",
        "\n",
        "        if metrics['true_range_median'] >= 1e-5:\n",
        "            quality_score += 20\n",
        "        elif metrics['true_range_median'] >= DataQualityValidator.MIN_TRUE_RANGE:\n",
        "            quality_score += (metrics['true_range_median'] / 1e-5) * 20\n",
        "\n",
        "        metrics['quality_score'] = quality_score\n",
        "        is_valid = (quality_score >= DataQualityValidator.MIN_QUALITY_SCORE)\n",
        "\n",
        "        return is_valid, quality_score, metrics, issues\n",
        "\n",
        "validator = DataQualityValidator()\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GITHUB CONFIGURATION\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "if FOREX_PAT:\n",
        "    print(\"‚úÖ GitHub credentials configured\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Warning: FOREX_PAT not found\")\n",
        "\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "               capture_output=True, check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "               capture_output=True, check=False)\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ ALPHA VANTAGE CONFIGURATION\n",
        "# ======================================================\n",
        "ALPHA_VANTAGE_KEY = os.environ.get(\"ALPHA_VANTAGE_KEY\")\n",
        "\n",
        "if not ALPHA_VANTAGE_KEY and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        ALPHA_VANTAGE_KEY = userdata.get(\"ALPHA_VANTAGE_KEY\")\n",
        "        if ALPHA_VANTAGE_KEY:\n",
        "            os.environ[\"ALPHA_VANTAGE_KEY\"] = ALPHA_VANTAGE_KEY\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    raise ValueError(\"‚ùå ALPHA_VANTAGE_KEY is required\")\n",
        "\n",
        "print(f\"‚úÖ Alpha Vantage API key: {ALPHA_VANTAGE_KEY[:4]}...{ALPHA_VANTAGE_KEY[-4:]}\")\n",
        "\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "print(f\"üìä Fetching {len(FX_PAIRS)} pairs: {', '.join(FX_PAIRS)}\")\n",
        "print(f\"üí° Daily API usage: {len(FX_PAIRS)} requests/day (16% of 25 limit)\")\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    \"\"\"Calculate MD5 hash of file to detect changes\"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def fetch_alpha_vantage_fx(pair, outputsize='full', max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Fetch FX data from Alpha Vantage API with retry logic\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with OHLC data or empty DataFrame on failure\n",
        "    \"\"\"\n",
        "    base_url = 'https://www.alphavantage.co/query'\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "\n",
        "    params = {\n",
        "        'function': 'FX_DAILY',\n",
        "        'from_symbol': from_currency,\n",
        "        'to_symbol': to_currency,\n",
        "        'outputsize': outputsize,\n",
        "        'datatype': 'json',\n",
        "        'apikey': ALPHA_VANTAGE_KEY\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  üîΩ Fetching {pair} (attempt {attempt + 1}/{max_retries})...\")\n",
        "\n",
        "            r = requests.get(base_url, params=params, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "\n",
        "            if 'Error Message' in data:\n",
        "                raise ValueError(f\"API Error: {data['Error Message']}\")\n",
        "\n",
        "            if 'Note' in data:\n",
        "                print(f\"  ‚ö†Ô∏è API rate limit reached for {pair}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay * 2)\n",
        "                    continue\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            if 'Time Series FX (Daily)' not in data:\n",
        "                raise ValueError(f\"Unexpected response format: {list(data.keys())}\")\n",
        "\n",
        "            ts = data['Time Series FX (Daily)']\n",
        "            df = pd.DataFrame(ts).T\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df.sort_index()\n",
        "\n",
        "            df = df.rename(columns={\n",
        "                '1. open': 'open',\n",
        "                '2. high': 'high',\n",
        "                '3. low': 'low',\n",
        "                '4. close': 'close'\n",
        "            })\n",
        "\n",
        "            df = df.astype(float)\n",
        "            df = ensure_tz_naive(df)\n",
        "\n",
        "            print(f\"  ‚úÖ Fetched {len(df)} rows for {pair}\")\n",
        "            return df\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"  ‚ö†Ô∏è Network error: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Error: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ PAIR PROCESSING WITH QUALITY VALIDATION\n",
        "# ======================================================\n",
        "def process_pair(pair):\n",
        "    \"\"\"\n",
        "    Process single FX pair: fetch, validate quality, merge, save\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (filepath if changed, status message, quality_score)\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîÑ Processing {pair}...\")\n",
        "\n",
        "    filename = pair.replace(\"/\", \"_\") + \"_daily_av.csv\"\n",
        "    file_path = CSV_FOLDER / filename\n",
        "\n",
        "    # Load existing data\n",
        "    existing_df = pd.DataFrame()\n",
        "    if file_path.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
        "            existing_df = ensure_tz_naive(existing_df)\n",
        "            print(f\"  üìä Loaded {len(existing_df)} existing rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not load existing data: {e}\")\n",
        "\n",
        "    old_hash = file_hash(file_path)\n",
        "\n",
        "    # Fetch new data\n",
        "    new_df = fetch_alpha_vantage_fx(pair)\n",
        "\n",
        "    if new_df.empty:\n",
        "        return None, f\"‚ùå {pair}: No data fetched\", 0.0\n",
        "\n",
        "    # Merge with existing data\n",
        "    if not existing_df.empty:\n",
        "        combined_df = pd.concat([existing_df, new_df])\n",
        "        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
        "    else:\n",
        "        combined_df = new_df\n",
        "\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    # Validate quality\n",
        "    is_valid, quality_score, metrics, issues = validator.validate_dataframe(\n",
        "        combined_df, pair\n",
        "    )\n",
        "\n",
        "    print(f\"  üìä Quality score: {quality_score:.1f}/100\")\n",
        "\n",
        "    if not is_valid:\n",
        "        print(f\"  ‚ö†Ô∏è Quality issues: {'; '.join(issues[:2])}\")\n",
        "        print(f\"     CV: {metrics.get('price_cv', 0):.4f}%, Unique: {metrics.get('unique_ratio', 0):.1%}\")\n",
        "\n",
        "        if quality_score < DataQualityValidator.MIN_QUALITY_SCORE:\n",
        "            print(f\"  ‚ùå Data quality too low - quarantining\")\n",
        "\n",
        "            quarantine_file = QUARANTINE_FOLDER / f\"{filename}.bad\"\n",
        "            with lock:\n",
        "                combined_df.to_csv(quarantine_file)\n",
        "\n",
        "                report_file = QUARANTINE_FOLDER / f\"{filename}.quality.txt\"\n",
        "                with open(report_file, 'w') as f:\n",
        "                    f.write(f\"Quality Report for {pair} (Alpha Vantage)\\n\")\n",
        "                    f.write(f\"{'='*50}\\n\")\n",
        "                    f.write(f\"Quality Score: {quality_score:.1f}/100\\n\")\n",
        "                    f.write(f\"Issues: {'; '.join(issues)}\\n\")\n",
        "                    f.write(f\"\\nMetrics:\\n\")\n",
        "                    for k, v in metrics.items():\n",
        "                        f.write(f\"  {k}: {v}\\n\")\n",
        "\n",
        "            return None, f\"‚ùå {pair}: Quality too low ({quality_score:.1f}/100)\", quality_score\n",
        "\n",
        "    # Save the file\n",
        "    with lock:\n",
        "        combined_df.to_csv(file_path)\n",
        "\n",
        "    new_hash = file_hash(file_path)\n",
        "    changed = (old_hash != new_hash)\n",
        "\n",
        "    status = \"‚úÖ Updated\" if changed else \"‚ÑπÔ∏è No changes\"\n",
        "    print(f\"  {status} - {len(combined_df)} rows, quality: {quality_score:.1f}/100\")\n",
        "\n",
        "    return (str(file_path) if changed else None), f\"{status} {pair} ({len(combined_df)} rows, Q:{quality_score:.0f})\", quality_score\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ EXECUTION WITH RATE LIMITING\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Fetching FX data with quality validation...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "changed_files = []\n",
        "results = []\n",
        "quality_scores = {}\n",
        "\n",
        "# Sequential processing with delays to respect rate limits\n",
        "for pair in FX_PAIRS:\n",
        "    try:\n",
        "        filepath, message, quality = process_pair(pair)\n",
        "        results.append(message)\n",
        "        if filepath:\n",
        "            changed_files.append(filepath)\n",
        "            quality_scores[filepath] = quality\n",
        "\n",
        "        # Rate limiting: Wait 15 seconds between requests\n",
        "        if pair != FX_PAIRS[-1]:  # Don't wait after last pair\n",
        "            print(f\"\\n‚è≥ Waiting 15 seconds (rate limiting)...\")\n",
        "            time.sleep(15)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {pair} processing failed: {e}\")\n",
        "        results.append(f\"‚ùå {pair}: Failed\")\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ RESULTS SUMMARY\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PROCESSING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "print(f\"\\nTotal pairs processed: {len(FX_PAIRS)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"API calls made: {len(FX_PAIRS)}\")\n",
        "\n",
        "if quality_scores:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä QUALITY REPORT\")\n",
        "    print(\"=\" * 70)\n",
        "    avg_quality = sum(quality_scores.values()) / len(quality_scores)\n",
        "    print(f\"Average quality score: {avg_quality:.1f}/100\")\n",
        "\n",
        "    print(f\"\\nFiles by quality:\")\n",
        "    for fname, score in sorted(quality_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {'‚úÖ' if score >= 60 else '‚ö†Ô∏è'} {Path(fname).name}: {score:.1f}/100\")\n",
        "\n",
        "quarantined = list(QUARANTINE_FOLDER.glob(\"*.bad\"))\n",
        "if quarantined:\n",
        "    print(f\"\\n‚ö†Ô∏è  QUARANTINED FILES: {len(quarantined)}\")\n",
        "    for qfile in quarantined:\n",
        "        print(f\"  ‚ùå {qfile.stem}\")\n",
        "\n",
        "# ======================================================\n",
        "# üîü GIT COMMIT & PUSH\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Handled by workflow\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files and FOREX_PAT:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        commit_msg = f\"üìä Alpha Vantage daily update - {len(changed_files)} files\"\n",
        "        if quality_scores:\n",
        "            commit_msg += f\" (Avg Q:{avg_quality:.0f})\"\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Changes committed\")\n",
        "\n",
        "            SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "            REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "            for attempt in range(3):\n",
        "                print(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/3)...\")\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"push\", REPO_URL, BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(\"‚úÖ Successfully pushed to GitHub\")\n",
        "                    break\n",
        "                elif attempt < 2:\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", REPO_URL, BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Git error: {e}\")\n",
        "    finally:\n",
        "        os.chdir(SAVE_FOLDER)\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è No changes to commit\")\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALPHA VANTAGE WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Quality validated: ‚úÖ\")\n",
        "if quality_scores:\n",
        "    print(f\"Average quality: {avg_quality:.1f}/100\")\n",
        "print(f\"API calls: {len(FX_PAIRS)}/25 daily limit\")\n",
        "print(f\"Status: {'‚úÖ Success' if len(results) == len(FX_PAIRS) else '‚ö†Ô∏è Partial'}\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüí° Optimization Summary:\")\n",
        "print(\"   ‚Ä¢ Runs once daily at midnight\")\n",
        "print(\"   ‚Ä¢ Uses 4 API calls/day (16% of limit)\")\n",
        "print(\"   ‚Ä¢ Saves 44 calls/day compared to hourly fetching\")\n",
        "print(\"   ‚Ä¢ Daily OHLC data doesn't change intraday\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBW31rh39aMb"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "YFINANCE FX DATA FETCHER - CLEAN STRUCTURE EDITION\n",
        "===================================================\n",
        "‚úÖ Aligned with clean repo structure (data/raw/yfinance)\n",
        "‚úÖ Relaxed quality thresholds for more data acceptance\n",
        "‚úÖ Automatic OHLC logic fixing\n",
        "‚úÖ Enhanced fallback options\n",
        "‚úÖ Smart data cleaning before validation\n",
        "‚úÖ Better symbol format handling\n",
        "‚úÖ Multi-environment support (Colab, GHA, Local)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import subprocess\n",
        "import shutil\n",
        "import threading\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ YFinance FX Data Fetcher - Clean Structure Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION (MATCHES YOUR SETUP!)\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ UNIFIED PATH CONFIGURATION (MATCHES CLEAN STRUCTURE!)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    print(\"‚òÅÔ∏è Google Colab detected - using clean structure\")\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"  # ‚úÖ MATCHES!\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    print(\"ü§ñ GitHub Actions detected - using repository root\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    print(\"üíª Local environment detected - using clean structure\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "# ‚úÖ CREATE ORGANIZED DIRECTORY STRUCTURE\n",
        "DIRECTORIES = {\n",
        "    \"data_raw_yfinance\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "    \"quarantine\": SAVE_FOLDER / \"data\" / \"quarantine\" / \"yfinance\",\n",
        "}\n",
        "\n",
        "# Create all directories\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Export key paths\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw_yfinance\"]  # ‚úÖ YFinance CSVs here\n",
        "QUARANTINE_FOLDER = DIRECTORIES[\"quarantine\"]\n",
        "LOG_FOLDER = DIRECTORIES[\"logs\"]\n",
        "\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo Folder: {REPO_FOLDER}\")\n",
        "print(f\"üìä YFinance CSV: {CSV_FOLDER}\")\n",
        "print(f\"üóëÔ∏è Quarantine: {QUARANTINE_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GIT CONFIGURATION\n",
        "# ======================================================\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "# Try Colab secrets if in Colab and PAT not found\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not access Colab secrets: {e}\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT is required!\")\n",
        "\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# Configure git\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "               capture_output=True, check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "               capture_output=True, check=False)\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ REPOSITORY MANAGEMENT (SIMPLIFIED)\n",
        "# ======================================================\n",
        "def ensure_repository():\n",
        "    \"\"\"Ensure repository is available and up-to-date\"\"\"\n",
        "    if IN_GHA:\n",
        "        print(\"\\nü§ñ GitHub Actions: Repository already available\")\n",
        "        if not (REPO_FOLDER / \".git\").exists():\n",
        "            print(\"‚ö†Ô∏è Warning: .git directory not found\")\n",
        "        else:\n",
        "            print(\"‚úÖ Git repository verified\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüì• Managing repository...\")\n",
        "\n",
        "    if REPO_FOLDER.exists() and not (REPO_FOLDER / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Directory exists but is not a git repository\")\n",
        "        return\n",
        "\n",
        "    if (REPO_FOLDER / \".git\").exists():\n",
        "        print(f\"üîÑ Pulling latest changes...\")\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Repository updated successfully\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Pull had issues, continuing anyway\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Update failed: {e} - continuing with existing repo\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Repository not found. This script expects the repo to be set up first.\")\n",
        "        print(\"   Please run the GitHub Sync script first!\")\n",
        "\n",
        "ensure_repository()\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ RATE LIMITER\n",
        "# ======================================================\n",
        "class RateLimiter:\n",
        "    \"\"\"Rate limiter for API calls\"\"\"\n",
        "    def __init__(self, requests_per_minute=10, requests_per_hour=350):\n",
        "        self.rpm = requests_per_minute\n",
        "        self.rph = requests_per_hour\n",
        "        self.request_times = []\n",
        "        self.hourly_request_times = []\n",
        "        self.lock = threading.Lock()\n",
        "        self.total_requests = 0\n",
        "\n",
        "    def wait_if_needed(self):\n",
        "        with self.lock:\n",
        "            now = time.time()\n",
        "            self.request_times = [t for t in self.request_times if now - t < 60]\n",
        "            self.hourly_request_times = [t for t in self.hourly_request_times if now - t < 3600]\n",
        "\n",
        "            if len(self.request_times) >= self.rpm:\n",
        "                wait_time = 60 - (now - self.request_times[0])\n",
        "                if wait_time > 0:\n",
        "                    time.sleep(wait_time + 1)\n",
        "                    self.request_times = []\n",
        "\n",
        "            if len(self.hourly_request_times) >= self.rph:\n",
        "                wait_time = 3600 - (now - self.hourly_request_times[0])\n",
        "                if wait_time > 0:\n",
        "                    time.sleep(wait_time + 1)\n",
        "                    self.hourly_request_times = []\n",
        "\n",
        "            self.request_times.append(now)\n",
        "            self.hourly_request_times.append(now)\n",
        "            self.total_requests += 1\n",
        "            time.sleep(1.0 + (hash(str(now)) % 20) / 10)\n",
        "\n",
        "    def get_stats(self):\n",
        "        with self.lock:\n",
        "            return {'total_requests': self.total_requests}\n",
        "\n",
        "rate_limiter = RateLimiter()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ DATA CLEANING & VALIDATION\n",
        "# ======================================================\n",
        "def fix_ohlc_logic(df):\n",
        "    \"\"\"Fix impossible OHLC relationships\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df = df.copy()\n",
        "    required_cols = ['open', 'high', 'low', 'close']\n",
        "\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        return df\n",
        "\n",
        "    # Fix High: should be maximum of OHLC\n",
        "    df['high'] = df[required_cols].max(axis=1)\n",
        "\n",
        "    # Fix Low: should be minimum of OHLC\n",
        "    df['low'] = df[required_cols].min(axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "class DataQualityValidator:\n",
        "    \"\"\"RELAXED validation for more data acceptance\"\"\"\n",
        "\n",
        "    # ‚úÖ RELAXED THRESHOLDS\n",
        "    MIN_ROWS = 5  # Down from 10\n",
        "    MIN_PRICE_CV = 0.01  # Down from 0.1 (1% instead of 10%)\n",
        "    MIN_UNIQUE_RATIO = 0.005  # Down from 0.05 (0.5% instead of 5%)\n",
        "    MIN_TRUE_RANGE = 1e-12  # More lenient\n",
        "    MIN_QUALITY_SCORE = 20.0  # Down from 40.0\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df, pair, tf_name):\n",
        "        \"\"\"Validate with relaxed criteria\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return False, 0.0, {}, [\"Empty DataFrame\"]\n",
        "\n",
        "        issues = []\n",
        "        metrics = {}\n",
        "\n",
        "        metrics['row_count'] = len(df)\n",
        "        if len(df) < DataQualityValidator.MIN_ROWS:\n",
        "            return False, 0.0, metrics, [f\"Too few rows: {len(df)}\"]\n",
        "\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        if not all(col in df.columns for col in required_cols):\n",
        "            return False, 0.0, metrics, [\"Missing OHLC columns\"]\n",
        "\n",
        "        ohlc_data = df[required_cols].dropna()\n",
        "        if len(ohlc_data) == 0:\n",
        "            return False, 0.0, metrics, [\"No valid OHLC data\"]\n",
        "\n",
        "        metrics['valid_rows'] = len(ohlc_data)\n",
        "        metrics['valid_ratio'] = len(ohlc_data) / len(df)\n",
        "\n",
        "        close_prices = ohlc_data['close']\n",
        "        metrics['price_mean'] = float(close_prices.mean())\n",
        "        metrics['price_std'] = float(close_prices.std())\n",
        "        metrics['price_cv'] = (metrics['price_std'] / metrics['price_mean']) * 100 if metrics['price_mean'] > 0 else 0.0\n",
        "\n",
        "        metrics['unique_prices'] = close_prices.nunique()\n",
        "        metrics['unique_ratio'] = metrics['unique_prices'] / len(close_prices)\n",
        "\n",
        "        # Calculate true range\n",
        "        high = ohlc_data['high'].values\n",
        "        low = ohlc_data['low'].values\n",
        "        close = ohlc_data['close'].values\n",
        "\n",
        "        tr = np.maximum.reduce([\n",
        "            high - low,\n",
        "            np.abs(high - np.roll(close, 1)),\n",
        "            np.abs(low - np.roll(close, 1))\n",
        "        ])\n",
        "        tr[0] = high[0] - low[0]\n",
        "\n",
        "        metrics['true_range_median'] = float(np.median(tr))\n",
        "\n",
        "        # Quality score calculation (more lenient)\n",
        "        quality_score = metrics['valid_ratio'] * 30\n",
        "\n",
        "        if metrics['price_cv'] >= 0.5:\n",
        "            quality_score += 40\n",
        "        elif metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV:\n",
        "            quality_score += (metrics['price_cv'] / 0.5) * 40\n",
        "\n",
        "        if metrics['unique_ratio'] >= 0.1:\n",
        "            quality_score += 30\n",
        "        elif metrics['unique_ratio'] >= DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "            quality_score += (metrics['unique_ratio'] / 0.1) * 30\n",
        "\n",
        "        metrics['quality_score'] = quality_score\n",
        "\n",
        "        # Relaxed validation - accept if meets minimum thresholds\n",
        "        is_valid = (\n",
        "            quality_score >= DataQualityValidator.MIN_QUALITY_SCORE and\n",
        "            metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV and\n",
        "            metrics['unique_ratio'] >= DataQualityValidator.MIN_UNIQUE_RATIO\n",
        "        )\n",
        "\n",
        "        if not is_valid:\n",
        "            if metrics['price_cv'] < DataQualityValidator.MIN_PRICE_CV:\n",
        "                issues.append(f\"Low CV: {metrics['price_cv']:.4f}%\")\n",
        "            if metrics['unique_ratio'] < DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "                issues.append(f\"Low unique: {metrics['unique_ratio']:.3%}\")\n",
        "\n",
        "        return is_valid, quality_score, metrics, issues\n",
        "\n",
        "validator = DataQualityValidator()\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ CONFIGURATION\n",
        "# ======================================================\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "# ‚úÖ ENHANCED with more fallback options\n",
        "TIMEFRAMES = {\n",
        "    \"1d_5y\": [\n",
        "        (\"1d\", \"5y\"),\n",
        "        (\"1d\", \"max\"),  # Try max available\n",
        "        (\"1d\", \"3y\"),\n",
        "        (\"1d\", \"2y\"),\n",
        "    ],\n",
        "    \"1h_2y\": [\n",
        "        (\"1h\", \"2y\"),\n",
        "        (\"1h\", \"1y\"),\n",
        "        (\"1h\", \"730d\"),  # Exactly 2 years in days\n",
        "        (\"1h\", \"6mo\")\n",
        "    ],\n",
        "    \"15m_60d\": [\n",
        "        (\"15m\", \"60d\"),\n",
        "        (\"15m\", \"2mo\"),\n",
        "        (\"15m\", \"30d\"),\n",
        "    ],\n",
        "    \"5m_1mo\": [\n",
        "        (\"5m\", \"1mo\"),\n",
        "        (\"5m\", \"30d\"),\n",
        "        (\"5m\", \"14d\"),\n",
        "    ],\n",
        "    \"1m_7d\": [\n",
        "        (\"1m\", \"7d\"),\n",
        "        (\"1m\", \"5d\"),\n",
        "        (\"1m\", \"3d\"),\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"\\nüìä Configuration:\")\n",
        "print(f\"   Pairs: {len(FX_PAIRS)}\")\n",
        "print(f\"   Timeframes: {len(TIMEFRAMES)}\")\n",
        "print(f\"   Total tasks: {len(FX_PAIRS) * len(TIMEFRAMES)}\")\n",
        "print(f\"   Quality threshold: {validator.MIN_QUALITY_SCORE}/100 (RELAXED)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def file_hash(filepath):\n",
        "    \"\"\"Calculate MD5 hash of file\"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def merge_data(existing_df, new_df):\n",
        "    \"\"\"Merge existing and new data, removing duplicates\"\"\"\n",
        "    existing_df = ensure_tz_naive(existing_df)\n",
        "    new_df = ensure_tz_naive(new_df)\n",
        "    if existing_df.empty:\n",
        "        return new_df\n",
        "    if new_df.empty:\n",
        "        return existing_df\n",
        "    combined = pd.concat([existing_df, new_df])\n",
        "    combined = combined[~combined.index.duplicated(keep=\"last\")]\n",
        "    combined.sort_index(inplace=True)\n",
        "    return combined\n",
        "\n",
        "def get_symbol_variants(pair, interval):\n",
        "    \"\"\"Get multiple symbol format variations\"\"\"\n",
        "    base_symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
        "    variants = [base_symbol]\n",
        "\n",
        "    # Additional formats\n",
        "    if interval in [\"1d\", \"1h\"]:\n",
        "        from_curr, to_curr = pair.split(\"/\")\n",
        "        variants.append(f\"{from_curr}{to_curr}=X\")  # No separator\n",
        "        variants.append(f\"{from_curr}=X\")  # Just base currency\n",
        "\n",
        "    return variants\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ WORKER FUNCTION\n",
        "# ======================================================\n",
        "def process_pair_tf(pair, tf_name, interval_period_options, max_retries=3):\n",
        "    \"\"\"\n",
        "    Download YFinance data with OHLC fixing and validation\n",
        "\n",
        "    ‚úÖ Saves to data/raw/yfinance/ with clear naming\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (message, filepath if changed, quality_score)\n",
        "    \"\"\"\n",
        "    # ‚úÖ Save to YFinance folder\n",
        "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
        "    filepath = CSV_FOLDER / filename\n",
        "\n",
        "    existing_df = pd.DataFrame()\n",
        "    if filepath.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "            existing_df = ensure_tz_naive(existing_df)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not load existing data: {e}\")\n",
        "\n",
        "    old_hash = file_hash(filepath)\n",
        "\n",
        "    for option_idx, (interval, period) in enumerate(interval_period_options):\n",
        "        symbol_variants = get_symbol_variants(pair, interval)\n",
        "\n",
        "        for symbol in symbol_variants:\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    rate_limiter.wait_if_needed()\n",
        "\n",
        "                    ticker = yf.Ticker(symbol)\n",
        "                    df = ticker.history(\n",
        "                        period=period,\n",
        "                        interval=interval,\n",
        "                        auto_adjust=False,\n",
        "                        prepost=False,\n",
        "                        actions=False,\n",
        "                        raise_errors=False\n",
        "                    )\n",
        "\n",
        "                    if df.empty:\n",
        "                        raise ValueError(\"Empty data\")\n",
        "\n",
        "                    available_cols = [c for c in ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "                                     if c in df.columns]\n",
        "                    df = df[available_cols]\n",
        "                    df.rename(columns=lambda x: x.lower(), inplace=True)\n",
        "                    df = ensure_tz_naive(df)\n",
        "\n",
        "                    combined_df = merge_data(existing_df, df)\n",
        "\n",
        "                    # ‚úÖ FIX OHLC LOGIC BEFORE VALIDATION\n",
        "                    combined_df = fix_ohlc_logic(combined_df)\n",
        "\n",
        "                    is_valid, quality_score, metrics, issues = validator.validate_dataframe(\n",
        "                        combined_df, pair, tf_name\n",
        "                    )\n",
        "\n",
        "                    if not is_valid:\n",
        "                        if attempt < max_retries - 1:\n",
        "                            time.sleep(3 * (2 ** attempt))\n",
        "                            continue\n",
        "                        elif option_idx < len(interval_period_options) - 1:\n",
        "                            break  # Try next option\n",
        "                        else:\n",
        "                            # Save anyway but mark as low quality\n",
        "                            print(f\"  ‚ö†Ô∏è Low quality ({quality_score:.1f}) but saving: {pair} {tf_name}\")\n",
        "\n",
        "                    # Save the file\n",
        "                    with lock:\n",
        "                        combined_df.to_csv(filepath)\n",
        "\n",
        "                    new_hash = file_hash(filepath)\n",
        "                    changed = (old_hash != new_hash)\n",
        "\n",
        "                    status = \"‚úÖ\" if quality_score >= 50 else \"‚ö†Ô∏è\"\n",
        "                    msg = f\"{status} {pair} {tf_name} - {len(combined_df)} rows, Q:{quality_score:.0f}\"\n",
        "                    print(f\"  {msg}\")\n",
        "                    return msg, str(filepath) if changed else None, quality_score\n",
        "\n",
        "                except Exception as e:\n",
        "                    if attempt < max_retries - 1:\n",
        "                        time.sleep(3 * (2 ** attempt))\n",
        "                    else:\n",
        "                        if option_idx < len(interval_period_options) - 1:\n",
        "                            break  # Try next option\n",
        "\n",
        "    return f\"‚ùå Failed {pair} {tf_name}\", None, 0.0\n",
        "\n",
        "# ======================================================\n",
        "# üîü PARALLEL EXECUTION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Starting YFinance data download...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "changed_files = []\n",
        "results = []\n",
        "quality_scores = {}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    tasks = []\n",
        "    for pair in FX_PAIRS:\n",
        "        for tf_name, options in TIMEFRAMES.items():\n",
        "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, options))\n",
        "\n",
        "    for future in as_completed(tasks):\n",
        "        try:\n",
        "            msg, filename, quality = future.result()\n",
        "            results.append(msg)\n",
        "            if filename:\n",
        "                changed_files.append(filename)\n",
        "                quality_scores[filename] = quality\n",
        "        except Exception as e:\n",
        "            results.append(f\"‚ùå Error: {e}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ SUMMARY\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PROCESSING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "success_count = len([r for r in results if \"‚úÖ\" in r or \"‚ö†Ô∏è\" in r])\n",
        "print(f\"\\nTotal tasks: {len(results)}\")\n",
        "print(f\"Successful: {success_count}/{len(results)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Time: {elapsed_time/60:.1f} min\")\n",
        "\n",
        "if quality_scores:\n",
        "    avg_q = sum(quality_scores.values()) / len(quality_scores)\n",
        "    print(f\"Average quality: {avg_q:.1f}/100\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä QUALITY REPORT\")\n",
        "    print(\"=\" * 70)\n",
        "    for fname, score in sorted(quality_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "        status = \"‚úÖ\" if score >= 50 else \"‚ö†Ô∏è\"\n",
        "        print(f\"  {status} {Path(fname).name}: {score:.1f}/100\")\n",
        "\n",
        "# Check quarantine\n",
        "quarantined = list(QUARANTINE_FOLDER.glob(\"*.bad\"))\n",
        "if quarantined:\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚ö†Ô∏è  QUARANTINED FILES: {len(quarantined)}\")\n",
        "    print(\"=\" * 70)\n",
        "    for qfile in quarantined:\n",
        "        print(f\"  ‚ùå {qfile.stem}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ GIT COMMIT & PUSH\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        commit_msg = f\"Update YFinance data - {len(changed_files)} files\"\n",
        "        if quality_scores:\n",
        "            commit_msg += f\" (Avg Q:{avg_q:.0f})\"\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Changes committed\")\n",
        "\n",
        "            for attempt in range(3):\n",
        "                print(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/3)...\")\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(\"‚úÖ Successfully pushed to GitHub\")\n",
        "                    break\n",
        "                elif attempt < 2:\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  No changes to commit\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Git error: {e}\")\n",
        "    finally:\n",
        "        os.chdir(SAVE_FOLDER)\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è No changes to commit\")\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ YFINANCE WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Quality validated: ‚úÖ\")\n",
        "if quality_scores:\n",
        "    print(f\"Average quality: {avg_q:.1f}/100\")\n",
        "print(f\"Status: {'‚úÖ Success' if success_count == len(results) else '‚ö†Ô∏è Partial'}\")\n",
        "print(f\"Rate limiter: {rate_limiter.get_stats()['total_requests']} requests\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüìÅ Clean File Structure:\")\n",
        "print(f\"   YFinance: {CSV_FOLDER}\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ EUR_USD_1d_5y.csv, EUR_USD_1h_2y.csv, etc.\")\n",
        "print(f\"   Alpha Vantage: {SAVE_FOLDER / 'data' / 'raw' / 'alpha_vantage'}\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ EUR_USD_daily_av.csv\")\n",
        "print(\"\\nüéØ All data sources in organized folders!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2Z_gCxS_g7I"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "FX CSV Combiner + Multi-Type Handler - CLEAN STRUCTURE EDITION\n",
        "==============================================================\n",
        "‚úÖ Aligned with clean repo structure (data/raw/, data/processed/)\n",
        "‚úÖ Combines Alpha Vantage + YFinance data\n",
        "‚úÖ Full-dataset indicator calculation (not incremental)\n",
        "‚úÖ ATR preservation (no clipping or scaling)\n",
        "‚úÖ Quality validation before processing\n",
        "‚úÖ Multi-environment support (Colab, GHA, Local)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import subprocess\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîß CSV Combiner & Multi-Type Handler - Clean Structure Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ UNIFIED PATH CONFIGURATION (MATCHES CLEAN STRUCTURE!)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    print(\"‚òÅÔ∏è Google Colab detected - using clean structure\")\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    print(\"ü§ñ GitHub Actions detected - using repository root\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    print(\"üíª Local environment detected - using clean structure\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "# ‚úÖ CREATE ORGANIZED DIRECTORY STRUCTURE\n",
        "DIRECTORIES = {\n",
        "    \"data_raw_yfinance\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_raw_alpha\": SAVE_FOLDER / \"data\" / \"raw\" / \"alpha_vantage\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "    \"quarantine\": SAVE_FOLDER / \"data\" / \"quarantine\" / \"combiner\",\n",
        "}\n",
        "\n",
        "# Create all directories\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Export key paths\n",
        "YFINANCE_CSV_FOLDER = DIRECTORIES[\"data_raw_yfinance\"]\n",
        "ALPHA_CSV_FOLDER = DIRECTORIES[\"data_raw_alpha\"]\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "QUARANTINE_FOLDER = DIRECTORIES[\"quarantine\"]\n",
        "LOG_FOLDER = DIRECTORIES[\"logs\"]\n",
        "\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo Folder: {REPO_FOLDER}\")\n",
        "print(f\"üìä YFinance CSV: {YFINANCE_CSV_FOLDER}\")\n",
        "print(f\"üìä Alpha CSV: {ALPHA_CSV_FOLDER}\")\n",
        "print(f\"üîß Processed: {PICKLE_FOLDER}\")\n",
        "print(f\"üóëÔ∏è Quarantine: {QUARANTINE_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Print status messages with icons\"\"\"\n",
        "    levels = {\"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"warn\": \"‚ö†Ô∏è\", \"error\": \"‚ùå\", \"debug\": \"üêû\"}\n",
        "    print(f\"{levels.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ DATA QUALITY VALIDATOR\n",
        "# ======================================================\n",
        "class DataQualityValidator:\n",
        "    \"\"\"Validate data quality for OHLC files\"\"\"\n",
        "\n",
        "    MIN_ROWS = 10\n",
        "    MIN_PRICE_CV = 0.01  # 0.01% minimum (relaxed)\n",
        "    MIN_UNIQUE_RATIO = 0.005  # 0.5% unique prices (relaxed)\n",
        "    MIN_TRUE_RANGE = 1e-10\n",
        "    MIN_QUALITY_SCORE = 20.0  # Relaxed from 30\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df, filename):\n",
        "        \"\"\"Validate DataFrame quality\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return False, 0.0, {}, [\"Empty DataFrame\"]\n",
        "\n",
        "        issues = []\n",
        "        metrics = {}\n",
        "\n",
        "        metrics['row_count'] = len(df)\n",
        "        if len(df) < DataQualityValidator.MIN_ROWS:\n",
        "            issues.append(f\"Too few rows: {len(df)}\")\n",
        "\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            issues.append(f\"Missing columns: {missing_cols}\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        ohlc_data = df[required_cols].dropna()\n",
        "        if len(ohlc_data) == 0:\n",
        "            issues.append(\"No valid OHLC data\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        metrics['valid_rows'] = len(ohlc_data)\n",
        "        metrics['valid_ratio'] = len(ohlc_data) / len(df)\n",
        "\n",
        "        close_prices = ohlc_data['close']\n",
        "        metrics['price_mean'] = float(close_prices.mean())\n",
        "        metrics['price_std'] = float(close_prices.std())\n",
        "        metrics['price_cv'] = (metrics['price_std'] / metrics['price_mean'] * 100) if metrics['price_mean'] > 0 else 0.0\n",
        "\n",
        "        metrics['unique_prices'] = close_prices.nunique()\n",
        "        metrics['unique_ratio'] = metrics['unique_prices'] / len(close_prices)\n",
        "\n",
        "        high = ohlc_data['high'].values\n",
        "        low = ohlc_data['low'].values\n",
        "        close = ohlc_data['close'].values\n",
        "\n",
        "        tr = np.maximum.reduce([\n",
        "            high - low,\n",
        "            np.abs(high - np.roll(close, 1)),\n",
        "            np.abs(low - np.roll(close, 1))\n",
        "        ])\n",
        "        tr[0] = high[0] - low[0]\n",
        "\n",
        "        metrics['true_range_median'] = float(np.median(tr))\n",
        "\n",
        "        quality_score = 0.0\n",
        "        quality_score += metrics['valid_ratio'] * 30\n",
        "\n",
        "        if metrics['price_cv'] >= 0.5:\n",
        "            quality_score += 40\n",
        "        elif metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV:\n",
        "            quality_score += (metrics['price_cv'] / 0.5) * 40\n",
        "\n",
        "        if metrics['unique_ratio'] >= 0.1:\n",
        "            quality_score += 30\n",
        "        elif metrics['unique_ratio'] >= DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "            quality_score += (metrics['unique_ratio'] / 0.1) * 30\n",
        "\n",
        "        metrics['quality_score'] = quality_score\n",
        "\n",
        "        is_valid = (\n",
        "            quality_score >= DataQualityValidator.MIN_QUALITY_SCORE and\n",
        "            metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV\n",
        "        )\n",
        "\n",
        "        if not is_valid:\n",
        "            if metrics['price_cv'] < DataQualityValidator.MIN_PRICE_CV:\n",
        "                issues.append(f\"Low CV: {metrics['price_cv']:.4f}%\")\n",
        "            if metrics['unique_ratio'] < DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "                issues.append(f\"Low unique: {metrics['unique_ratio']:.3%}\")\n",
        "\n",
        "        return is_valid, quality_score, metrics, issues\n",
        "\n",
        "validator = DataQualityValidator()\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GIT CONFIGURATION\n",
        "# ======================================================\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not access Colab secrets: {e}\")\n",
        "\n",
        "if FOREX_PAT:\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "                   capture_output=True, check=False)\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "                   capture_output=True, check=False)\n",
        "    print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_localize(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def safe_numeric(df):\n",
        "    \"\"\"Handle infinity/NaN robustly\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    required_columns = ['open', 'high', 'low', 'close']\n",
        "    existing_columns = [col for col in required_columns if col in df_clean.columns]\n",
        "\n",
        "    if existing_columns:\n",
        "        df_clean.dropna(subset=existing_columns, inplace=True)\n",
        "    else:\n",
        "        df_clean.dropna(how='all', inplace=True)\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ CSV DISCOVERY\n",
        "# ======================================================\n",
        "def discover_csv_files():\n",
        "    \"\"\"Discover CSV files from both YFinance and Alpha Vantage folders\"\"\"\n",
        "    csv_files = []\n",
        "\n",
        "    # Search in YFinance folder\n",
        "    yf_files = list(YFINANCE_CSV_FOLDER.glob(\"*.csv\"))\n",
        "    if yf_files:\n",
        "        print_status(f\"üìÇ Found {len(yf_files)} YFinance CSV(s)\", \"debug\")\n",
        "        csv_files.extend(yf_files)\n",
        "\n",
        "    # Search in Alpha Vantage folder\n",
        "    alpha_files = list(ALPHA_CSV_FOLDER.glob(\"*.csv\"))\n",
        "    if alpha_files:\n",
        "        print_status(f\"üìÇ Found {len(alpha_files)} Alpha Vantage CSV(s)\", \"debug\")\n",
        "        csv_files.extend(alpha_files)\n",
        "\n",
        "    return csv_files\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ INDICATOR CALCULATION (FULL DATASET)\n",
        "# ======================================================\n",
        "def add_indicators_full(df):\n",
        "    \"\"\"\n",
        "    ‚úÖ Calculate indicators on FULL dataset (not incremental)\n",
        "    ‚úÖ ATR preserved without clipping or scaling\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    required_cols = ['open', 'high', 'low', 'close']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        return None\n",
        "\n",
        "    df = safe_numeric(df)\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    df = df.copy()\n",
        "    df.sort_index(inplace=True)\n",
        "\n",
        "    # Preserve raw prices\n",
        "    for col in ['open', 'high', 'low', 'close']:\n",
        "        if col in df.columns and f'raw_{col}' not in df.columns:\n",
        "            df[f'raw_{col}'] = df[col].copy()\n",
        "\n",
        "    print_status(f\"  üîß Calculating indicators on {len(df)} rows\", \"debug\")\n",
        "\n",
        "    try:\n",
        "        # Trend indicators\n",
        "        if len(df) >= 10:\n",
        "            df['SMA_10'] = ta.trend.sma_indicator(df['close'], 10)\n",
        "            df['EMA_10'] = ta.trend.ema_indicator(df['close'], 10)\n",
        "\n",
        "        if len(df) >= 20:\n",
        "            df['SMA_20'] = ta.trend.sma_indicator(df['close'], 20)\n",
        "            df['EMA_20'] = ta.trend.ema_indicator(df['close'], 20)\n",
        "\n",
        "        if len(df) >= 50:\n",
        "            df['SMA_50'] = ta.trend.sma_indicator(df['close'], 50)\n",
        "            df['EMA_50'] = ta.trend.ema_indicator(df['close'], 50)\n",
        "\n",
        "        if len(df) >= 200:\n",
        "            df['SMA_200'] = ta.trend.sma_indicator(df['close'], 200)\n",
        "\n",
        "        # MACD\n",
        "        if len(df) >= 26:\n",
        "            macd = ta.trend.MACD(df['close'])\n",
        "            df['MACD'] = macd.macd()\n",
        "            df['MACD_signal'] = macd.macd_signal()\n",
        "            df['MACD_diff'] = macd.macd_diff()\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Trend indicator error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # Momentum indicators\n",
        "        if len(df) >= 14:\n",
        "            df['RSI_14'] = ta.momentum.rsi(df['close'], 14)\n",
        "            df['Williams_%R'] = WilliamsRIndicator(\n",
        "                df['high'], df['low'], df['close'], 14\n",
        "            ).williams_r()\n",
        "            df['Stoch_K'] = ta.momentum.stoch(df['high'], df['low'], df['close'], 14)\n",
        "            df['Stoch_D'] = ta.momentum.stoch_signal(df['high'], df['low'], df['close'], 14)\n",
        "\n",
        "        if len(df) >= 20:\n",
        "            df['CCI_20'] = ta.trend.cci(df['high'], df['low'], df['close'], 20)\n",
        "            df['ROC'] = ta.momentum.roc(df['close'], 12)\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Momentum indicator error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # ‚úÖ CRITICAL: ATR calculation - NO CLIPPING!\n",
        "        if len(df) >= 14:\n",
        "            atr_values = AverageTrueRange(\n",
        "                df['high'], df['low'], df['close'], 14\n",
        "            ).average_true_range()\n",
        "\n",
        "            # Only fill NaN, don't clip\n",
        "            df['ATR'] = atr_values.fillna(1e-10)\n",
        "\n",
        "            atr_median = df['ATR'].median()\n",
        "            if pd.notna(atr_median):\n",
        "                print_status(f\"  üìä ATR median: {atr_median:.8f}\", \"debug\")\n",
        "\n",
        "        # Bollinger Bands\n",
        "        if len(df) >= 20:\n",
        "            bb = ta.volatility.BollingerBands(df['close'], 20, 2)\n",
        "            df['BB_upper'] = bb.bollinger_hband()\n",
        "            df['BB_middle'] = bb.bollinger_mavg()\n",
        "            df['BB_lower'] = bb.bollinger_lband()\n",
        "            df['BB_width'] = bb.bollinger_wband()\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Volatility indicator error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # Derived features\n",
        "        df['price_change'] = df['close'].pct_change()\n",
        "        df['price_change_5'] = df['close'].pct_change(5)\n",
        "        df['high_low_range'] = (df['high'] - df['low']) / df['close']\n",
        "        df['close_open_range'] = (df['close'] - df['open']) / df['open']\n",
        "\n",
        "        if 'volume' in df.columns:\n",
        "            df['vwap'] = (df['close'] * df['volume']).cumsum() / df['volume'].cumsum()\n",
        "\n",
        "        if 'SMA_50' in df.columns:\n",
        "            df['price_vs_sma50'] = (df['close'] - df['SMA_50']) / df['SMA_50']\n",
        "\n",
        "        if 'RSI_14' in df.columns:\n",
        "            df['rsi_momentum'] = df['RSI_14'].diff()\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Derived features error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # ‚úÖ Scale features but PROTECT ATR and raw prices\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        protected_cols = [\n",
        "            'open', 'high', 'low', 'close', 'volume',\n",
        "            'raw_open', 'raw_high', 'raw_low', 'raw_close',\n",
        "            'ATR'  # ‚úÖ PROTECT ATR!\n",
        "        ]\n",
        "\n",
        "        scalable_cols = [c for c in numeric_cols if c not in protected_cols]\n",
        "\n",
        "        if scalable_cols:\n",
        "            df[scalable_cols] = df[scalable_cols].replace([np.inf, -np.inf], np.nan)\n",
        "            cols_with_data = [c for c in scalable_cols if not df[c].isna().all()]\n",
        "\n",
        "            if cols_with_data:\n",
        "                scaler = RobustScaler()\n",
        "                df[cols_with_data] = scaler.fit_transform(\n",
        "                    df[cols_with_data].fillna(0) + 1e-10\n",
        "                )\n",
        "                print_status(f\"  ‚úÖ Scaled {len(cols_with_data)} features (ATR protected)\", \"debug\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Scaling error: {e}\", \"warn\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ MAIN PROCESSING FUNCTION\n",
        "# ======================================================\n",
        "def process_csv_file(csv_file):\n",
        "    \"\"\"Process a single CSV file: validate, combine, add indicators, save\"\"\"\n",
        "    try:\n",
        "        print_status(f\"üìã Processing: {csv_file.name}\", \"info\")\n",
        "\n",
        "        # Load CSV\n",
        "        df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
        "        df = ensure_tz_naive(df)\n",
        "\n",
        "        if df.empty:\n",
        "            msg = f\"‚ö†Ô∏è {csv_file.name}: Empty file\"\n",
        "            print_status(msg, \"warn\")\n",
        "            return None, msg\n",
        "\n",
        "        # ‚úÖ VALIDATE QUALITY\n",
        "        is_valid, quality_score, metrics, issues = validator.validate_dataframe(df, csv_file.name)\n",
        "\n",
        "        print_status(f\"  üìä Quality score: {quality_score:.1f}/100\", \"debug\")\n",
        "\n",
        "        if not is_valid:\n",
        "            print_status(f\"  ‚ö†Ô∏è Quality issues: {'; '.join(issues[:2])}\", \"warn\")\n",
        "\n",
        "            # Quarantine if too low\n",
        "            if quality_score < validator.MIN_QUALITY_SCORE:\n",
        "                print_status(f\"  ‚ùå Quarantining low quality file\", \"error\")\n",
        "\n",
        "                quarantine_file = QUARANTINE_FOLDER / f\"{csv_file.name}.bad\"\n",
        "                with lock:\n",
        "                    df.to_csv(quarantine_file)\n",
        "\n",
        "                    report_file = QUARANTINE_FOLDER / f\"{csv_file.name}.quality.txt\"\n",
        "                    with open(report_file, 'w') as f:\n",
        "                        f.write(f\"Quality Report for {csv_file.name}\\n\")\n",
        "                        f.write(f\"{'='*50}\\n\")\n",
        "                        f.write(f\"Quality Score: {quality_score:.1f}/100\\n\")\n",
        "                        f.write(f\"Issues: {'; '.join(issues)}\\n\")\n",
        "                        f.write(f\"\\nMetrics:\\n\")\n",
        "                        for k, v in metrics.items():\n",
        "                            f.write(f\"  {k}: {v}\\n\")\n",
        "\n",
        "                return None, f\"‚ùå {csv_file.name}: Quarantined (Q:{quality_score:.1f})\"\n",
        "            else:\n",
        "                print_status(f\"  ‚ö†Ô∏è Low quality but acceptable\", \"warn\")\n",
        "\n",
        "        # ‚úÖ ADD INDICATORS (FULL DATASET)\n",
        "        processed_df = add_indicators_full(df)\n",
        "\n",
        "        if processed_df is None:\n",
        "            msg = f\"‚ùå {csv_file.name}: Indicator calculation failed\"\n",
        "            print_status(msg, \"error\")\n",
        "            return None, msg\n",
        "\n",
        "        # ‚úÖ SAVE PROCESSED DATA\n",
        "        pickle_filename = csv_file.stem + \".pkl\"\n",
        "        pickle_path = PICKLE_FOLDER / pickle_filename\n",
        "\n",
        "        with lock:\n",
        "            processed_df.to_pickle(pickle_path, compression='gzip', protocol=4)\n",
        "\n",
        "        atr_median = processed_df['ATR'].median() if 'ATR' in processed_df.columns else 0\n",
        "        msg = f\"‚úÖ {csv_file.name}: {len(processed_df)} rows, Q:{quality_score:.0f}, ATR:{atr_median:.8f}\"\n",
        "        print_status(msg, \"success\")\n",
        "\n",
        "        return str(pickle_path), msg\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\"‚ùå Failed {csv_file.name}: {e}\"\n",
        "        print_status(msg, \"error\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, msg\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ MAIN EXECUTION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Discovering CSV files...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "csv_files = discover_csv_files()\n",
        "\n",
        "if csv_files:\n",
        "    print_status(f\"üìä Total CSV files found: {len(csv_files)}\", \"success\")\n",
        "    for csv_file in csv_files[:5]:\n",
        "        print_status(f\"  ‚Ä¢ {csv_file.name} ({csv_file.stat().st_size / 1024:.1f} KB)\", \"debug\")\n",
        "    if len(csv_files) > 5:\n",
        "        print_status(f\"  ... and {len(csv_files) - 5} more\", \"debug\")\n",
        "else:\n",
        "    print_status(\"‚ö†Ô∏è No CSV files found!\", \"warn\")\n",
        "    print_status(\"   Check that data fetchers have run successfully\", \"warn\")\n",
        "\n",
        "changed_files = []\n",
        "quality_scores = {}\n",
        "\n",
        "# ======================================================\n",
        "# üîü PROCESS FILES\n",
        "# ======================================================\n",
        "if csv_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚öôÔ∏è Processing {len(csv_files)} CSV file(s)...\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=min(8, len(csv_files))) as executor:\n",
        "        futures = [executor.submit(process_csv_file, f) for f in csv_files]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            file, msg = future.result()\n",
        "            if file:\n",
        "                changed_files.append(file)\n",
        "                # Extract quality info\n",
        "                if \"ATR:\" in msg:\n",
        "                    try:\n",
        "                        atr_str = msg.split(\"ATR:\")[1].strip()\n",
        "                        quality_scores[file] = float(atr_str)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ QUALITY REPORT\n",
        "# ======================================================\n",
        "if quality_scores:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä QUALITY REPORT - ATR VALUES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    avg_atr = sum(quality_scores.values()) / len(quality_scores)\n",
        "    print(f\"Average ATR: {avg_atr:.8f}\")\n",
        "    print(f\"\\nATR by file:\")\n",
        "\n",
        "    for filepath, atr in sorted(quality_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "        filename = Path(filepath).stem\n",
        "        status = \"‚úÖ\" if atr > 1e-6 else \"‚ö†Ô∏è\"\n",
        "        print(f\"  {status} {filename}: {atr:.8f}\")\n",
        "\n",
        "    low_atr_files = [f for f, atr in quality_scores.items() if atr < 1e-6]\n",
        "    if low_atr_files:\n",
        "        print(f\"\\n‚ö†Ô∏è  {len(low_atr_files)} file(s) with suspiciously low ATR\")\n",
        "\n",
        "# Check quarantine\n",
        "quarantined = list(QUARANTINE_FOLDER.glob(\"*.bad\"))\n",
        "if quarantined:\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚ö†Ô∏è  QUARANTINED FILES: {len(quarantined)}\")\n",
        "    print(\"=\" * 70)\n",
        "    for qfile in quarantined:\n",
        "        print(f\"  ‚ùå {qfile.stem}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ GIT COMMIT & PUSH\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files and FOREX_PAT:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        commit_msg = f\"Update processed data - {len(changed_files)} files\"\n",
        "        if quality_scores:\n",
        "            commit_msg += f\" (Avg ATR: {avg_atr:.6f})\"\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print_status(\"‚úÖ Changes committed\", \"success\")\n",
        "\n",
        "            for attempt in range(3):\n",
        "                print_status(f\"üì§ Pushing (attempt {attempt + 1}/3)...\", \"info\")\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print_status(\"‚úÖ Push successful\", \"success\")\n",
        "                    break\n",
        "                elif attempt < 2:\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "\n",
        "        elif \"nothing to commit\" in result.stdout.lower():\n",
        "            print_status(\"‚ÑπÔ∏è No changes to commit\", \"info\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ùå Git error: {e}\", \"error\")\n",
        "    finally:\n",
        "        os.chdir(SAVE_FOLDER)\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION SUMMARY\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ CSV COMBINER COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"CSV files found: {len(csv_files)}\")\n",
        "print(f\"Files processed: {len(changed_files)}\")\n",
        "print(f\"Files quarantined: {len(quarantined)}\")\n",
        "\n",
        "if quality_scores:\n",
        "    print(f\"\\nüìà ATR Statistics:\")\n",
        "    print(f\"   Average: {avg_atr:.8f}\")\n",
        "    print(f\"   Files analyzed: {len(quality_scores)}\")\n",
        "\n",
        "print(\"\\nüîß KEY FEATURES:\")\n",
        "print(\"   ‚úÖ Full-dataset indicator calculation\")\n",
        "print(\"   ‚úÖ ATR preserved (no clipping/scaling)\")\n",
        "print(\"   ‚úÖ Quality validation with quarantine\")\n",
        "print(\"   ‚úÖ Clean organized structure\")\n",
        "print(\"   ‚úÖ Thread-safe processing\")\n",
        "\n",
        "print(\"\\nüìÅ Output Locations:\")\n",
        "print(f\"   Processed pickles: {PICKLE_FOLDER}\")\n",
        "print(f\"   Quarantine: {QUARANTINE_FOLDER}\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ULTRA-PERSISTENT SELF-LEARNING FX PIPELINE v6.4.0 - ENHANCED CONTRARIAN\n",
        "========================================================================\n",
        "üéì EXPERIMENTAL: Weekend predictions with advanced contrarian logic\n",
        "\n",
        "NEW IN v6.4.0:\n",
        "‚úÖ CRITICAL FIX: SL/TP swap for contrarian trades\n",
        "‚úÖ Adaptive volatility-based multipliers (1.5x-2.5x)\n",
        "‚úÖ Statistical confidence intervals (Wilson score)\n",
        "‚úÖ Performance-based allocation (dynamic A/B split)\n",
        "‚úÖ Weekend phase tracking (SAT_EARLY, SAT_MID, etc.)\n",
        "‚úÖ Momentum/ADX filter (don't fade strong trends)\n",
        "‚úÖ Real-time experiment metrics export\n",
        "‚úÖ Spread impact tracking\n",
        "\n",
        "CRITICAL FIXES:\n",
        "üîß Contrarian trades now properly swap SL/TP\n",
        "üîß Weekend multiplier adapts to volatility (1.5x-2.5x)\n",
        "üîß Confidence intervals for statistical significance\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import sqlite3\n",
        "import subprocess\n",
        "import pickle\n",
        "import gzip\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üéì Ultra-Persistent FX Pipeline v6.4.0 - ENHANCED CONTRARIAN\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# SIMPLE DATA LOADER\n",
        "# ======================================================\n",
        "\n",
        "class SimpleDataLoader:\n",
        "    \"\"\"Loads data pickles only (not models)\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_data(filepath):\n",
        "        \"\"\"Load data pickle with basic validation\"\"\"\n",
        "        if not filepath.exists():\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                magic = f.read(2)\n",
        "\n",
        "            if magic == b'\\x1f\\x8b':\n",
        "                with gzip.open(filepath, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            else:\n",
        "                with open(filepath, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Cannot load {filepath.name}: {e}\")\n",
        "            return None\n",
        "\n",
        "data_loader = SimpleDataLoader()\n",
        "\n",
        "# ======================================================\n",
        "# ENVIRONMENT DETECTION\n",
        "# ======================================================\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "\n",
        "# Path configuration\n",
        "if IN_COLAB:\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "DIRECTORIES = {\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "    \"learning\": SAVE_FOLDER / \"learning_data\",\n",
        "}\n",
        "\n",
        "for dir_path in DIRECTORIES.values():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "DB_FOLDER = DIRECTORIES[\"database\"]\n",
        "LEARNING_FOLDER = DIRECTORIES[\"learning\"]\n",
        "PERSISTENT_DB = DB_FOLDER / \"memory_v85.db\"\n",
        "\n",
        "# Learning files\n",
        "PREDICTIONS_FILE = LEARNING_FOLDER / \"predictions_history.json\"\n",
        "LEARNING_DB = LEARNING_FOLDER / \"learning_outcomes.json\"\n",
        "AB_TEST_METRICS = LEARNING_FOLDER / \"ab_test_metrics.json\"\n",
        "ALLOCATION_HISTORY = LEARNING_FOLDER / \"allocation_history.json\"\n",
        "\n",
        "print(f\"üìÇ Base: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save: {SAVE_FOLDER}\")\n",
        "print(f\"üìä Data: {PICKLE_FOLDER}\")\n",
        "print(f\"üéì Learning: {LEARNING_FOLDER}\")\n",
        "print(f\"üîó Trade Beacon DB: {LEARNING_DB}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# CLEANUP OLD MODEL FILES\n",
        "# ======================================================\n",
        "\n",
        "def cleanup_old_model_files():\n",
        "    \"\"\"Delete old model pickle files\"\"\"\n",
        "    print(\"\\nüßπ Cleaning up old model files...\")\n",
        "\n",
        "    deleted = 0\n",
        "    patterns = ['*_sgd_model.pkl', '*_rf_model.pkl', '*_model.pkl']\n",
        "\n",
        "    for pattern in patterns:\n",
        "        for model_file in PICKLE_FOLDER.glob(pattern):\n",
        "            try:\n",
        "                model_file.unlink()\n",
        "                deleted += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    corrupted_folder = PICKLE_FOLDER / \"corrupted\"\n",
        "    if corrupted_folder.exists():\n",
        "        try:\n",
        "            import shutil\n",
        "            shutil.rmtree(corrupted_folder)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if deleted > 0:\n",
        "        print(f\"   ‚úì Cleaned up {deleted} old model files\")\n",
        "    else:\n",
        "        print(f\"   ‚úì No old model files found\")\n",
        "\n",
        "cleanup_old_model_files()\n",
        "\n",
        "# ======================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ======================================================\n",
        "\n",
        "def is_weekend(dt=None):\n",
        "    \"\"\"Check if it's weekend (market closed)\"\"\"\n",
        "    if dt is None:\n",
        "        dt = datetime.now(timezone.utc)\n",
        "    return dt.weekday() in [5, 6]\n",
        "\n",
        "def get_weekend_phase(dt=None):\n",
        "    \"\"\"Categorize weekend trading phases\"\"\"\n",
        "    if dt is None:\n",
        "        dt = datetime.now(timezone.utc)\n",
        "\n",
        "    if dt.weekday() == 5:  # Saturday\n",
        "        hour = dt.hour\n",
        "        if hour < 6:\n",
        "            return \"SAT_EARLY\"  # Right after Friday close\n",
        "        elif hour < 18:\n",
        "            return \"SAT_MID\"    # Dead zone\n",
        "        else:\n",
        "            return \"SAT_LATE\"   # Pre-Asian open\n",
        "    elif dt.weekday() == 6:  # Sunday\n",
        "        hour = dt.hour\n",
        "        if hour < 12:\n",
        "            return \"SUN_EARLY\"\n",
        "        elif hour < 21:\n",
        "            return \"SUN_MID\"\n",
        "        else:\n",
        "            return \"SUN_LATE\"   # Asian markets opening\n",
        "\n",
        "    return \"WEEKDAY\"\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Print status with icon\"\"\"\n",
        "    icons = {\n",
        "        \"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"warn\": \"‚ö†Ô∏è\", \"debug\": \"üêû\",\n",
        "        \"error\": \"‚ùå\", \"data\": \"üìä\", \"learning\": \"üéì\", \"trading\": \"üíπ\"\n",
        "    }\n",
        "    icon = icons.get(level, '‚ÑπÔ∏è')\n",
        "    print(f\"{icon} {msg}\")\n",
        "\n",
        "def calculate_confidence_interval(wins, total, confidence=0.95):\n",
        "    \"\"\"Calculate Wilson score confidence interval for win rate\"\"\"\n",
        "    if total == 0:\n",
        "        return 0, 0\n",
        "\n",
        "    p = wins / total\n",
        "    z = 1.96  # 95% confidence\n",
        "\n",
        "    denominator = 1 + z**2 / total\n",
        "    center = (p + z**2 / (2 * total)) / denominator\n",
        "    margin = z * np.sqrt(p * (1 - p) / total + z**2 / (4 * total**2)) / denominator\n",
        "\n",
        "    return max(0, center - margin), min(1, center + margin)\n",
        "\n",
        "# ======================================================\n",
        "# GIT CONFIGURATION\n",
        "# ======================================================\n",
        "\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "if FOREX_PAT:\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "                   capture_output=True, check=False)\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "                   capture_output=True, check=False)\n",
        "    print_status(f\"Git configured: {GIT_USER_NAME}\", \"success\")\n",
        "\n",
        "# ======================================================\n",
        "# ML IMPORTS\n",
        "# ======================================================\n",
        "\n",
        "try:\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    from sklearn.linear_model import SGDClassifier\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    print_status(\"ML libraries loaded\", \"success\")\n",
        "except ImportError as e:\n",
        "    print_status(f\"ML libraries missing: {e}\", \"error\")\n",
        "    raise\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# üéì ENHANCED ADAPTIVE LEARNING SYSTEM (v6.4)\n",
        "# ======================================================\n",
        "\n",
        "class EnhancedContrarianLearningSystem:\n",
        "    \"\"\"\n",
        "    Enhanced learning system with:\n",
        "    - Statistical confidence intervals\n",
        "    - Performance-based allocation\n",
        "    - Weekend phase tracking\n",
        "    - Spread impact analysis\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.predictions_file = PREDICTIONS_FILE\n",
        "        self.learning_db = LEARNING_DB\n",
        "        self.ab_test_metrics = AB_TEST_METRICS\n",
        "        self.allocation_history_file = ALLOCATION_HISTORY\n",
        "\n",
        "        # Load existing data\n",
        "        self.predictions = self._load_predictions()\n",
        "        self.learning_data = self._load_learning_db()\n",
        "        self.allocation_history = self._load_allocation_history()\n",
        "\n",
        "        # Dynamic allocation (starts at 50/50)\n",
        "        self.contrarian_allocation = 0.5\n",
        "        self.contrarian_mode = \"AB_TEST\"  # Options: \"NORMAL\", \"CONTRARIAN\", \"AB_TEST\"\n",
        "\n",
        "        # Update allocation based on history\n",
        "        self._restore_allocation()\n",
        "\n",
        "    def _load_predictions(self):\n",
        "        \"\"\"Load prediction history\"\"\"\n",
        "        if self.predictions_file.exists():\n",
        "            try:\n",
        "                with open(self.predictions_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except:\n",
        "                return []\n",
        "        return []\n",
        "\n",
        "    def _load_learning_db(self):\n",
        "        \"\"\"Load learning outcomes database\"\"\"\n",
        "        if self.learning_db.exists():\n",
        "            try:\n",
        "                with open(self.learning_db, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, list):\n",
        "                        return data\n",
        "                    elif isinstance(data, dict) and 'outcomes' in data:\n",
        "                        return data['outcomes']\n",
        "                    else:\n",
        "                        return []\n",
        "            except:\n",
        "                return []\n",
        "        return []\n",
        "\n",
        "    def _load_allocation_history(self):\n",
        "        \"\"\"Load allocation history\"\"\"\n",
        "        if self.allocation_history_file.exists():\n",
        "            try:\n",
        "                with open(self.allocation_history_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except:\n",
        "                return []\n",
        "        return []\n",
        "\n",
        "    def _restore_allocation(self):\n",
        "        \"\"\"Restore last allocation from history\"\"\"\n",
        "        if self.allocation_history:\n",
        "            self.contrarian_allocation = self.allocation_history[-1].get('allocation', 0.5)\n",
        "\n",
        "    def normalize_features(self, features, target_size=30):\n",
        "        \"\"\"Normalize features to exactly 30 elements\"\"\"\n",
        "        if not features:\n",
        "            return [0.0] * target_size\n",
        "\n",
        "        feat_list = [float(f) for f in features]\n",
        "\n",
        "        if len(feat_list) >= target_size:\n",
        "            return feat_list[:target_size]\n",
        "        else:\n",
        "            return feat_list + [0.0] * (target_size - len(feat_list))\n",
        "\n",
        "    def save_prediction(self, pair, timeframe, prediction, price, sl, tp, features,\n",
        "                       is_weekend, is_contrarian=False, expected_spread=None):\n",
        "        \"\"\"Save a prediction for later evaluation\"\"\"\n",
        "        normalized_features = self.normalize_features(features, target_size=30)\n",
        "\n",
        "        pred_entry = {\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "            'pair': pair,\n",
        "            'timeframe': timeframe,\n",
        "            'prediction': prediction,\n",
        "            'original_prediction': prediction if not is_contrarian else (1 - prediction),\n",
        "            'entry_price': float(price),\n",
        "            'sl': float(sl),\n",
        "            'tp': float(tp),\n",
        "            'features': normalized_features,\n",
        "            'evaluated': False,\n",
        "            'is_weekend': is_weekend,\n",
        "            'weekend_phase': get_weekend_phase(),\n",
        "            'is_contrarian': is_contrarian,\n",
        "            'expected_spread': expected_spread,\n",
        "            'contrarian_allocation': self.contrarian_allocation\n",
        "        }\n",
        "\n",
        "        self.predictions.append(pred_entry)\n",
        "\n",
        "        if len(self.predictions) > 1000:\n",
        "            self.predictions = self.predictions[-1000:]\n",
        "\n",
        "        try:\n",
        "            with open(self.predictions_file, 'w') as f:\n",
        "                json.dump(self.predictions, f, indent=2)\n",
        "        except Exception as e:\n",
        "            print_status(f\"Could not save predictions: {e}\", \"warn\")\n",
        "\n",
        "    def evaluate_predictions(self, current_prices):\n",
        "        \"\"\"Check if old predictions hit TP or SL with adaptive windows\"\"\"\n",
        "        if not current_prices:\n",
        "            print_status(\"No current prices available for evaluation\", \"warn\")\n",
        "            return 0\n",
        "\n",
        "        evaluated_count = 0\n",
        "        now = datetime.now(timezone.utc)\n",
        "\n",
        "        # Adaptive evaluation windows\n",
        "        is_weekend_now = is_weekend(now)\n",
        "\n",
        "        if is_weekend_now:\n",
        "            min_hours = 2.0\n",
        "            max_hours = 72.0\n",
        "            eval_mode = \"WEEKEND (2-12h adaptive)\"\n",
        "        else:\n",
        "            min_hours = 1.0\n",
        "            max_hours = 36.0\n",
        "            eval_mode = \"WEEKDAY (1-6h adaptive)\"\n",
        "\n",
        "        print(f\"‚è∞ Evaluation Mode: {eval_mode}\")\n",
        "        print(f\"üìä Checking {len(self.predictions)} predictions...\")\n",
        "\n",
        "        unevaluated = [p for p in self.predictions if not p.get('evaluated', False)]\n",
        "        print(f\"   Unevaluated predictions: {len(unevaluated)}\")\n",
        "\n",
        "        for idx, pred in enumerate(self.predictions):\n",
        "            if pred.get('evaluated', False):\n",
        "                continue\n",
        "\n",
        "            pair = pred['pair']\n",
        "            if pair not in current_prices:\n",
        "                continue\n",
        "\n",
        "            # Parse timestamp\n",
        "            try:\n",
        "                pred_timestamp = pred['timestamp']\n",
        "                if pred_timestamp.endswith('Z'):\n",
        "                    pred_time = datetime.fromisoformat(pred_timestamp.replace('Z', '+00:00'))\n",
        "                elif '+00:00' in pred_timestamp:\n",
        "                    pred_time = datetime.fromisoformat(pred_timestamp)\n",
        "                else:\n",
        "                    pred_time = datetime.fromisoformat(pred_timestamp).replace(tzinfo=timezone.utc)\n",
        "            except Exception as e:\n",
        "                print_status(f\"Could not parse timestamp: {pred['timestamp']} - {e}\", \"debug\")\n",
        "                continue\n",
        "\n",
        "            hours_elapsed = (now - pred_time).total_seconds() / 3600\n",
        "\n",
        "            if hours_elapsed < min_hours:\n",
        "                continue\n",
        "\n",
        "            current_price = current_prices[pair]\n",
        "            entry = pred['entry_price']\n",
        "            sl = pred['sl']\n",
        "            tp = pred['tp']\n",
        "            prediction = pred['prediction']\n",
        "\n",
        "            # Check if TP or SL was hit\n",
        "            hit_tp = False\n",
        "            hit_sl = False\n",
        "\n",
        "            if prediction == 1:  # BUY\n",
        "                if current_price >= tp:\n",
        "                    hit_tp = True\n",
        "                elif current_price <= sl:\n",
        "                    hit_sl = True\n",
        "            else:  # SELL\n",
        "                if current_price <= tp:\n",
        "                    hit_tp = True\n",
        "                elif current_price >= sl:\n",
        "                    hit_sl = True\n",
        "\n",
        "            # Evaluate if TP/SL hit OR max timeout\n",
        "            if hit_tp or hit_sl or hours_elapsed > max_hours:\n",
        "                # Calculate outcome\n",
        "                if prediction == 1:  # BUY\n",
        "                    pnl = current_price - entry\n",
        "                else:  # SELL\n",
        "                    pnl = entry - current_price\n",
        "\n",
        "                # Spread impact\n",
        "                expected_spread = pred.get('expected_spread', 0)\n",
        "                spread_impact = abs(pnl) < expected_spread if expected_spread else False\n",
        "\n",
        "                # Create outcome\n",
        "                outcome = {\n",
        "                    'pair': pair,\n",
        "                    'timeframe': pred['timeframe'],\n",
        "                    'features': pred['features'],\n",
        "                    'prediction': 'BUY' if prediction == 1 else 'SELL',\n",
        "                    'entry_price': entry,\n",
        "                    'exit_price': float(current_price),\n",
        "                    'tp': tp,\n",
        "                    'sl': sl,\n",
        "                    'pnl': float(pnl),\n",
        "                    'hit_tp': hit_tp,\n",
        "                    'hit_sl': hit_sl,\n",
        "                    'was_correct': pnl > 0,\n",
        "                    'duration_hours': hours_elapsed,\n",
        "                    'timestamp': pred['timestamp'],\n",
        "                    'evaluated_at': now.isoformat(),\n",
        "                    'eval_mode': eval_mode,\n",
        "                    'was_weekend_pred': pred.get('is_weekend', False),\n",
        "                    'weekend_phase': pred.get('weekend_phase', 'UNKNOWN'),\n",
        "                    'is_contrarian': pred.get('is_contrarian', False),\n",
        "                    'min_wait_hours': min_hours,\n",
        "                    'max_wait_hours': max_hours,\n",
        "                    'expected_spread': expected_spread,\n",
        "                    'spread_impact': spread_impact,\n",
        "                    'contrarian_allocation_at_entry': pred.get('contrarian_allocation', 0.5)\n",
        "                }\n",
        "\n",
        "                self.learning_data.append(outcome)\n",
        "                pred['evaluated'] = True\n",
        "                evaluated_count += 1\n",
        "\n",
        "                # Print result\n",
        "                result = \"WIN ‚úÖ\" if pnl > 0 else \"LOSS ‚ùå\"\n",
        "                reason = \"TP HIT\" if hit_tp else (\"SL HIT\" if hit_sl else \"TIMEOUT\")\n",
        "                contrarian_tag = \" [CONTRARIAN]\" if outcome['is_contrarian'] else \" [NORMAL]\"\n",
        "                print_status(f\"{pair} {pred['timeframe']}: {result} ({reason}) | PnL: {pnl:.5f} | {hours_elapsed:.1f}h{contrarian_tag}\", \"learning\")\n",
        "\n",
        "        # Keep only last 5000 outcomes\n",
        "        if len(self.learning_data) > 5000:\n",
        "            self.learning_data = self.learning_data[-5000:]\n",
        "\n",
        "        # Save everything\n",
        "        if evaluated_count > 0:\n",
        "            try:\n",
        "                with open(self.predictions_file, 'w') as f:\n",
        "                    json.dump(self.predictions, f, indent=2)\n",
        "\n",
        "                with open(self.learning_db, 'w') as f:\n",
        "                    json.dump(self.learning_data, f, indent=2)\n",
        "\n",
        "                print_status(f\"‚úÖ Evaluated {evaluated_count} predictions ‚Üí learning_outcomes.json\", \"success\")\n",
        "\n",
        "                # Update allocation after evaluation\n",
        "                self.update_allocation()\n",
        "\n",
        "            except Exception as e:\n",
        "                print_status(f\"Could not save learning data: {e}\", \"warn\")\n",
        "        else:\n",
        "            print_status(f\"No predictions ready for evaluation yet (min wait: {min_hours}h)\", \"info\")\n",
        "\n",
        "        return evaluated_count\n",
        "\n",
        "    def update_allocation(self):\n",
        "        \"\"\"Update contrarian allocation based on performance\"\"\"\n",
        "        stats = self.get_stats(split_by_weekend=True)\n",
        "        normal = stats.get('weekend_normal', {})\n",
        "        contrarian = stats.get('weekend_contrarian', {})\n",
        "\n",
        "        # Need minimum 20 trades in each group\n",
        "        if normal.get('total', 0) >= 20 and contrarian.get('total', 0) >= 20:\n",
        "            normal_wr = normal['win_rate']\n",
        "            contrarian_wr = contrarian['win_rate']\n",
        "\n",
        "            # Check if difference is statistically significant\n",
        "            normal_ci_lower = normal.get('win_rate_ci_lower', normal_wr)\n",
        "            normal_ci_upper = normal.get('win_rate_ci_upper', normal_wr)\n",
        "            contrarian_ci_lower = contrarian.get('win_rate_ci_lower', contrarian_wr)\n",
        "            contrarian_ci_upper = contrarian.get('win_rate_ci_upper', contrarian_wr)\n",
        "\n",
        "            # Gradually shift allocation toward better strategy\n",
        "            old_allocation = self.contrarian_allocation\n",
        "\n",
        "            # If contrarian CI lower bound > normal CI upper bound = clear winner\n",
        "            if contrarian_ci_lower > normal_ci_upper:\n",
        "                self.contrarian_allocation = min(0.9, self.contrarian_allocation + 0.1)\n",
        "                shift_reason = \"Contrarian statistically better\"\n",
        "            elif normal_ci_lower > contrarian_ci_upper:\n",
        "                self.contrarian_allocation = max(0.1, self.contrarian_allocation - 0.1)\n",
        "                shift_reason = \"Normal statistically better\"\n",
        "            elif contrarian_wr > normal_wr + 0.1:  # 10% better without CI overlap\n",
        "                self.contrarian_allocation = min(0.9, self.contrarian_allocation + 0.05)\n",
        "                shift_reason = \"Contrarian trending better\"\n",
        "            elif normal_wr > contrarian_wr + 0.1:\n",
        "                self.contrarian_allocation = max(0.1, self.contrarian_allocation - 0.05)\n",
        "                shift_reason = \"Normal trending better\"\n",
        "            else:\n",
        "                shift_reason = \"No significant difference\"\n",
        "\n",
        "            if old_allocation != self.contrarian_allocation:\n",
        "                self.allocation_history.append({\n",
        "                    'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "                    'allocation': self.contrarian_allocation,\n",
        "                    'normal_wr': normal_wr,\n",
        "                    'contrarian_wr': contrarian_wr,\n",
        "                    'normal_total': normal['total'],\n",
        "                    'contrarian_total': contrarian['total'],\n",
        "                    'reason': shift_reason\n",
        "                })\n",
        "\n",
        "                # Save allocation history\n",
        "                try:\n",
        "                    with open(self.allocation_history_file, 'w') as f:\n",
        "                        json.dump(self.allocation_history, f, indent=2)\n",
        "                    print_status(f\"üìä Allocation updated: {old_allocation:.1%} ‚Üí {self.contrarian_allocation:.1%} ({shift_reason})\", \"success\")\n",
        "                except Exception as e:\n",
        "                    print_status(f\"Could not save allocation history: {e}\", \"warn\")\n",
        "\n",
        "    def get_stats(self, split_by_weekend=True, split_by_phase=False):\n",
        "        \"\"\"Get enhanced learning statistics\"\"\"\n",
        "        if not self.learning_data:\n",
        "            return {}\n",
        "\n",
        "        # Split by weekend/weekday AND contrarian strategy\n",
        "        weekend_normal = []\n",
        "        weekend_contrarian = []\n",
        "        weekday_outcomes = []\n",
        "\n",
        "        # Phase-based splits\n",
        "        phase_outcomes = {}\n",
        "\n",
        "        for o in self.learning_data:\n",
        "            # Infer weekend status\n",
        "            is_weekend_outcome = o.get('was_weekend_pred', False)\n",
        "            if not is_weekend_outcome and 'timestamp' in o:\n",
        "                try:\n",
        "                    ts = o['timestamp']\n",
        "                    if ts.endswith('Z'):\n",
        "                        pred_time = datetime.fromisoformat(ts.replace('Z', '+00:00'))\n",
        "                    elif '+00:00' in ts:\n",
        "                        pred_time = datetime.fromisoformat(ts)\n",
        "                    else:\n",
        "                        pred_time = datetime.fromisoformat(ts).replace(tzinfo=timezone.utc)\n",
        "                    is_weekend_outcome = pred_time.weekday() in [5, 6]\n",
        "                except:\n",
        "                    is_weekend_outcome = False\n",
        "\n",
        "            # Categorize\n",
        "            if is_weekend_outcome:\n",
        "                if o.get('is_contrarian', False):\n",
        "                    weekend_contrarian.append(o)\n",
        "                else:\n",
        "                    weekend_normal.append(o)\n",
        "\n",
        "                # Track by phase\n",
        "                if split_by_phase:\n",
        "                    phase = o.get('weekend_phase', 'UNKNOWN')\n",
        "                    if phase not in phase_outcomes:\n",
        "                        phase_outcomes[phase] = []\n",
        "                    phase_outcomes[phase].append(o)\n",
        "            else:\n",
        "                weekday_outcomes.append(o)\n",
        "\n",
        "        def calc_stats(outcomes):\n",
        "            if not outcomes:\n",
        "                return {\n",
        "                    'total': 0, 'wins': 0, 'losses': 0, 'win_rate': 0.0,\n",
        "                    'win_rate_ci_lower': 0.0, 'win_rate_ci_upper': 0.0,\n",
        "                    'is_significant': False\n",
        "                }\n",
        "\n",
        "            total = len(outcomes)\n",
        "            wins = sum(1 for o in outcomes if o.get('was_correct', False))\n",
        "            win_rate = wins / total if total > 0 else 0\n",
        "\n",
        "            # Calculate confidence interval\n",
        "            ci_lower, ci_upper = calculate_confidence_interval(wins, total)\n",
        "\n",
        "            # Check statistical significance (either clearly above or below 50%)\n",
        "            is_significant = (total >= 30 and (ci_lower > 0.5 or ci_upper < 0.5))\n",
        "\n",
        "            # Average durations\n",
        "            avg_duration = sum(o.get('duration_hours', 0) for o in outcomes) / total if total > 0 else 0\n",
        "\n",
        "            # Spread impact\n",
        "            spread_affected = sum(1 for o in outcomes if o.get('spread_impact', False))\n",
        "\n",
        "            return {\n",
        "                'total': total,\n",
        "                'wins': wins,\n",
        "                'losses': total - wins,\n",
        "                'win_rate': win_rate,\n",
        "                'win_rate_ci_lower': ci_lower,\n",
        "                'win_rate_ci_upper': ci_upper,\n",
        "                'is_significant': is_significant,\n",
        "                'avg_duration_hours': round(avg_duration, 2),\n",
        "                'spread_affected_count': spread_affected\n",
        "            }\n",
        "\n",
        "        all_weekend = weekend_normal + weekend_contrarian\n",
        "\n",
        "        result = {\n",
        "            'overall': calc_stats(self.learning_data),\n",
        "            'weekday': calc_stats(weekday_outcomes),\n",
        "            'weekend_all': calc_stats(all_weekend),\n",
        "            'weekend_normal': calc_stats(weekend_normal),\n",
        "            'weekend_contrarian': calc_stats(weekend_contrarian),\n",
        "            'contrarian_allocation': self.contrarian_allocation,\n",
        "            'last_update': datetime.now(timezone.utc).isoformat()\n",
        "        }\n",
        "\n",
        "        # Add phase-based stats if requested\n",
        "        if split_by_phase and phase_outcomes:\n",
        "            result['weekend_phases'] = {\n",
        "                phase: calc_stats(outcomes)\n",
        "                for phase, outcomes in phase_outcomes.items()\n",
        "            }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def export_experiment_metrics(self):\n",
        "        \"\"\"Export A/B test metrics for dashboard\"\"\"\n",
        "        stats = self.get_stats(split_by_weekend=True)\n",
        "\n",
        "        normal = stats.get('weekend_normal', {})\n",
        "        contrarian = stats.get('weekend_contrarian', {})\n",
        "\n",
        "        # Determine experiment status\n",
        "        if normal.get('total', 0) < 20 or contrarian.get('total', 0) < 20:\n",
        "            status = 'collecting_data'\n",
        "            recommendation = f'Need {max(20 - normal.get(\"total\", 0), 20 - contrarian.get(\"total\", 0))} more trades per group'\n",
        "        elif contrarian.get('is_significant', False) and contrarian.get('win_rate', 0) > 0.5:\n",
        "            status = 'contrarian_winning'\n",
        "            recommendation = 'Increase contrarian allocation to 80-90%'\n",
        "        elif normal.get('is_significant', False) and normal.get('win_rate', 0) > 0.5:\n",
        "            status = 'normal_winning'\n",
        "            recommendation = 'Reduce contrarian allocation to 10-20%'\n",
        "        else:\n",
        "            status = 'inconclusive'\n",
        "            recommendation = 'Continue A/B testing, no clear winner yet'\n",
        "\n",
        "        metrics = {\n",
        "            'experiment': 'weekend_contrarian_v6.4.0',\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "            'status': status,\n",
        "            'recommendation': recommendation,\n",
        "            'allocation': {\n",
        "                'current_contrarian_pct': round(self.contrarian_allocation * 100, 1),\n",
        "                'allocation_history_count': len(self.allocation_history)\n",
        "            },\n",
        "            'data_points': {\n",
        "                'normal': normal.get('total', 0),\n",
        "                'contrarian': contrarian.get('total', 0),\n",
        "                'total_weekend': normal.get('total', 0) + contrarian.get('total', 0)\n",
        "            },\n",
        "            'performance': {\n",
        "                'normal': {\n",
        "                    'win_rate': round(normal.get('win_rate', 0) * 100, 1),\n",
        "                    'ci_lower': round(normal.get('win_rate_ci_lower', 0) * 100, 1),\n",
        "                    'ci_upper': round(normal.get('win_rate_ci_upper', 0) * 100, 1),\n",
        "                    'is_significant': normal.get('is_significant', False)\n",
        "                },\n",
        "                'contrarian': {\n",
        "                    'win_rate': round(contrarian.get('win_rate', 0) * 100, 1),\n",
        "                    'ci_lower': round(contrarian.get('win_rate_ci_lower', 0) * 100, 1),\n",
        "                    'ci_upper': round(contrarian.get('win_rate_ci_upper', 0) * 100, 1),\n",
        "                    'is_significant': contrarian.get('is_significant', False)\n",
        "                },\n",
        "                'improvement_pct': round((contrarian.get('win_rate', 0) - normal.get('win_rate', 0)) * 100, 1)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save metrics\n",
        "        try:\n",
        "            with open(self.ab_test_metrics, 'w') as f:\n",
        "                json.dump(metrics, f, indent=2)\n",
        "            print_status(f\"üìä Experiment metrics exported ‚Üí {self.ab_test_metrics.name}\", \"success\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"Could not save experiment metrics: {e}\", \"warn\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "\n",
        "# Global learning system\n",
        "learning_system = EnhancedContrarianLearningSystem()\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED TRAINING WITH MOMENTUM FILTER\n",
        "# ======================================================\n",
        "\n",
        "def calculate_adaptive_multiplier(df, is_weekend_now):\n",
        "    \"\"\"\n",
        "    Calculate adaptive SL/TP multiplier based on recent volatility\n",
        "    Weekend: 1.5x-2.5x (adaptive to volatility)\n",
        "    Weekday: 2.0x (standard)\n",
        "    \"\"\"\n",
        "    if not is_weekend_now:\n",
        "        return 2.0\n",
        "\n",
        "    if 'ATR' not in df.columns or len(df) < 24:\n",
        "        return 1.5  # Default tight for weekends\n",
        "\n",
        "    # Compare current ATR to recent average\n",
        "    current_atr = df['ATR'].iloc[-1]\n",
        "    recent_atr = df['ATR'].iloc[-24:].mean()\n",
        "\n",
        "    if recent_atr > 0:\n",
        "        volatility_ratio = current_atr / recent_atr\n",
        "    else:\n",
        "        volatility_ratio = 1.0\n",
        "\n",
        "    # Scale: low volatility = 1.5x, high volatility = 2.5x\n",
        "    mult = 1.5 + (volatility_ratio * 1.0)\n",
        "    mult = max(1.5, min(2.5, mult))  # Clamp between 1.5-2.5\n",
        "\n",
        "    return mult\n",
        "\n",
        "def should_apply_contrarian(df, prediction):\n",
        "    \"\"\"\n",
        "    Check if contrarian strategy should be applied\n",
        "    Don't fade strong trends or breakouts\n",
        "    \"\"\"\n",
        "    # Check ADX for trend strength\n",
        "    if 'ADX' in df.columns and len(df) > 0:\n",
        "        adx = df['ADX'].iloc[-1]\n",
        "        if adx > 25:  # Strong trend detected\n",
        "            return False\n",
        "\n",
        "    # Check for Bollinger Band breakout\n",
        "    if all(col in df.columns for col in ['close', 'bb_upper', 'bb_lower']):\n",
        "        price = df['close'].iloc[-1]\n",
        "        bb_upper = df['bb_upper'].iloc[-1]\n",
        "        bb_lower = df['bb_lower'].iloc[-1]\n",
        "\n",
        "        # Breaking out of bands = don't fade\n",
        "        if price > bb_upper or price < bb_lower:\n",
        "            return False\n",
        "\n",
        "    # Check RSI for extreme conditions\n",
        "    if 'RSI' in df.columns:\n",
        "        rsi = df['RSI'].iloc[-1]\n",
        "        # Extreme RSI might be real momentum, not reversal\n",
        "        if rsi > 70 or rsi < 30:\n",
        "            # Check if RSI is diverging (needs at least 5 periods)\n",
        "            if len(df) >= 5:\n",
        "                price_trend = df['close'].iloc[-5:].diff().sum()\n",
        "                rsi_trend = df['RSI'].iloc[-5:].diff().sum()\n",
        "\n",
        "                # If RSI and price agree = strong trend, don't fade\n",
        "                if (price_trend > 0 and rsi_trend > 0) or (price_trend < 0 and rsi_trend < 0):\n",
        "                    return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def train_and_predict_fresh(df, pair_name, timeframe, is_weekend_now):\n",
        "    \"\"\"\n",
        "    Train models from scratch using data\n",
        "    Returns enhanced feature vector with volatility info\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prepare features\n",
        "        exclude_cols = [\n",
        "            'close', 'raw_close', 'raw_open', 'raw_high', 'raw_low',\n",
        "            'open', 'high', 'low', 'volume', 'vwap'\n",
        "        ]\n",
        "\n",
        "        feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "\n",
        "        if not feature_cols or len(df) < 50:\n",
        "            return None, None, 0.5, None, None, None, None\n",
        "\n",
        "        X = df[feature_cols].fillna(0)\n",
        "        y = (df['close'].diff() > 0).astype(int).fillna(0)\n",
        "\n",
        "        # Train SGDClassifier (fast, incremental learning)\n",
        "        sgd = SGDClassifier(\n",
        "            max_iter=1000,\n",
        "            tol=1e-3,\n",
        "            random_state=42,\n",
        "            warm_start=False\n",
        "        )\n",
        "        sgd.fit(X, y)\n",
        "        sgd_pred = int(sgd.predict(X.iloc[[-1]])[0])\n",
        "\n",
        "        # Train RandomForest (limited trees for speed)\n",
        "        rf = RandomForestClassifier(\n",
        "            n_estimators=50,\n",
        "            max_depth=10,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        rf.fit(X, y)\n",
        "        rf_pred = int(rf.predict(X.iloc[[-1]])[0])\n",
        "\n",
        "        # Calculate confidence\n",
        "        confidence = (sgd_pred + rf_pred) / 2.0\n",
        "\n",
        "        # Get feature vector\n",
        "        features = X.iloc[-1].values.tolist()\n",
        "\n",
        "        # Get current price\n",
        "        current_price = df['raw_close'].iloc[-1] if 'raw_close' in df.columns else df['close'].iloc[-1]\n",
        "\n",
        "        # Calculate adaptive multiplier\n",
        "        mult = calculate_adaptive_multiplier(df, is_weekend_now)\n",
        "\n",
        "        # Calculate SL/TP with adaptive multiplier\n",
        "        if 'ATR' in df.columns:\n",
        "            atr = df['ATR'].iloc[-1]\n",
        "            sl_dist = atr * mult\n",
        "            tp_dist = atr * mult\n",
        "        else:\n",
        "            atr_fallback = current_price * 0.01\n",
        "            sl_dist = atr_fallback * mult\n",
        "            tp_dist = atr_fallback * mult\n",
        "\n",
        "        # Calculate SL/TP based on prediction\n",
        "        if sgd_pred == 1:  # Initial BUY signal\n",
        "            sl = max(0, round(current_price - sl_dist, 5))\n",
        "            tp = round(current_price + tp_dist, 5)\n",
        "        else:  # Initial SELL signal\n",
        "            sl = round(current_price + sl_dist, 5)\n",
        "            tp = max(0, round(current_price - tp_dist, 5))\n",
        "\n",
        "        # Estimate spread\n",
        "        expected_spread = atr * 0.1 if 'ATR' in df.columns else current_price * 0.0002\n",
        "\n",
        "        return sgd_pred, rf_pred, confidence, features, sl, tp, expected_spread\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"Training error for {pair_name} {timeframe}: {e}\", \"debug\")\n",
        "        return None, None, 0.5, None, None, None, None\n",
        "\n",
        "# ======================================================\n",
        "# PROCESS PICKLE FILE WITH ENHANCED CONTRARIAN LOGIC\n",
        "# ======================================================\n",
        "\n",
        "def process_pickle_file_contrarian(pickle_path, is_weekend_now):\n",
        "    \"\"\"\n",
        "    Process with enhanced weekend contrarian logic\n",
        "    - Performance-based allocation\n",
        "    - Momentum filtering\n",
        "    - Proper SL/TP swap for contrarian trades\n",
        "    \"\"\"\n",
        "    filename = pickle_path.stem\n",
        "    currencies = ['EUR', 'USD', 'GBP', 'JPY', 'AUD', 'NZD', 'CAD', 'CHF']\n",
        "    pair = None\n",
        "\n",
        "    for curr1 in currencies:\n",
        "        for curr2 in currencies:\n",
        "            if curr1 != curr2 and filename.startswith(f\"{curr1}_{curr2}\"):\n",
        "                pair = f\"{curr1}/{curr2}\"\n",
        "                break\n",
        "        if pair:\n",
        "            break\n",
        "\n",
        "    if not pair:\n",
        "        return None, {}, \"HOLD\"\n",
        "\n",
        "    fname_lower = filename.lower()\n",
        "    if \"1d\" in fname_lower or \"daily\" in fname_lower:\n",
        "        timeframe = \"1d\"\n",
        "    elif \"1h\" in fname_lower:\n",
        "        timeframe = \"1h\"\n",
        "    elif \"15m\" in fname_lower:\n",
        "        timeframe = \"15m\"\n",
        "    elif \"5m\" in fname_lower:\n",
        "        timeframe = \"5m\"\n",
        "    elif \"1m\" in fname_lower:\n",
        "        timeframe = \"1m\"\n",
        "    else:\n",
        "        timeframe = \"unknown\"\n",
        "\n",
        "    try:\n",
        "        df = data_loader.load_data(pickle_path)\n",
        "        if df is None or df.empty:\n",
        "            return pair, {}, \"HOLD\"\n",
        "\n",
        "        current_price = df['raw_close'].iloc[-1] if 'raw_close' in df.columns else df['close'].iloc[-1]\n",
        "\n",
        "        # Train models (with adaptive weekend SL/TP)\n",
        "        sgd_pred, rf_pred, confidence, features, sl, tp, expected_spread = train_and_predict_fresh(\n",
        "            df, pair, timeframe, is_weekend_now\n",
        "        )\n",
        "\n",
        "        if sgd_pred is None:\n",
        "            return pair, {}, \"HOLD\"\n",
        "\n",
        "        # Ensemble prediction\n",
        "        ensemble_pred = 1 if (sgd_pred + rf_pred) >= 1 else 0\n",
        "        original_pred = ensemble_pred\n",
        "\n",
        "        # üî• ENHANCED WEEKEND CONTRARIAN LOGIC\n",
        "        is_contrarian = False\n",
        "        contrarian_filtered = False\n",
        "\n",
        "        if is_weekend_now and learning_system.contrarian_mode == \"AB_TEST\":\n",
        "            # Performance-based allocation (not just 50/50)\n",
        "            pair_hash = hash(pair) % 100\n",
        "            allocation_threshold = int(learning_system.contrarian_allocation * 100)\n",
        "\n",
        "            if pair_hash < allocation_threshold:\n",
        "                # Check if we should apply contrarian (momentum filter)\n",
        "                if should_apply_contrarian(df, ensemble_pred):\n",
        "                    # üî• CRITICAL: Reverse prediction AND swap SL/TP\n",
        "                    ensemble_pred = 1 - ensemble_pred\n",
        "                    sl, tp = tp, sl  # Swap stop-loss and take-profit\n",
        "                    is_contrarian = True\n",
        "                else:\n",
        "                    contrarian_filtered = True\n",
        "\n",
        "        elif is_weekend_now and learning_system.contrarian_mode == \"CONTRARIAN\":\n",
        "            # Always contrarian on weekends (with filter)\n",
        "            if should_apply_contrarian(df, ensemble_pred):\n",
        "                ensemble_pred = 1 - ensemble_pred\n",
        "                sl, tp = tp, sl  # Swap SL/TP\n",
        "                is_contrarian = True\n",
        "            else:\n",
        "                contrarian_filtered = True\n",
        "\n",
        "        # Save prediction with all metadata\n",
        "        if features is not None:\n",
        "            learning_system.save_prediction(\n",
        "                pair=pair,\n",
        "                timeframe=timeframe,\n",
        "                prediction=ensemble_pred,\n",
        "                price=current_price,\n",
        "                sl=sl,\n",
        "                tp=tp,\n",
        "                features=features,\n",
        "                is_weekend=is_weekend_now,\n",
        "                is_contrarian=is_contrarian,\n",
        "                expected_spread=expected_spread\n",
        "            )\n",
        "\n",
        "        signal_data = {\n",
        "            \"signal\": ensemble_pred,\n",
        "            \"sgd_pred\": sgd_pred,\n",
        "            \"rf_pred\": rf_pred,\n",
        "            \"live\": current_price,\n",
        "            \"SL\": sl,\n",
        "            \"TP\": tp,\n",
        "            \"confidence\": confidence,\n",
        "            \"timeframe\": timeframe,\n",
        "            \"is_weekend\": is_weekend_now,\n",
        "            \"is_contrarian\": is_contrarian,\n",
        "            \"contrarian_filtered\": contrarian_filtered,\n",
        "            \"expected_spread\": expected_spread\n",
        "        }\n",
        "\n",
        "        # Print signal with enhanced indicators\n",
        "        action = \"BUY\" if ensemble_pred == 1 else \"SELL\"\n",
        "        strategy_tag = \"\"\n",
        "\n",
        "        if is_contrarian:\n",
        "            strategy_tag = \" [CONTRARIAN]\"\n",
        "        elif contrarian_filtered:\n",
        "            strategy_tag = \" [FILTERED-NORMAL]\"\n",
        "        elif is_weekend_now:\n",
        "            strategy_tag = \" [NORMAL]\"\n",
        "\n",
        "        mult = calculate_adaptive_multiplier(df, is_weekend_now)\n",
        "        mult_tag = f\" [{mult:.1f}x]\" if is_weekend_now else \"\"\n",
        "\n",
        "        print(f\"{'‚úì':2} {pair:8} | {timeframe:3} | {action:4} | Price:{current_price:.5f}{strategy_tag}{mult_tag}\")\n",
        "\n",
        "        return pair, {timeframe: signal_data}, \"LONG\" if ensemble_pred == 1 else \"SHORT\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"Error processing {pickle_path.name}: {e}\", \"error\")\n",
        "        return pair, {}, \"HOLD\"\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# MAIN PIPELINE EXECUTION\n",
        "# ======================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main pipeline with Enhanced Weekend Contrarian Strategy\n",
        "    \"\"\"\n",
        "    print_status(\"Starting Pipeline v6.4.0 (Enhanced Contrarian)\", \"success\")\n",
        "    print()\n",
        "\n",
        "    is_weekend_now = is_weekend()\n",
        "    day_name = datetime.now(timezone.utc).strftime('%A')\n",
        "\n",
        "    if is_weekend_now:\n",
        "        print(\"üèñÔ∏è  WEEKEND MODE: ENHANCED CONTRARIAN STRATEGY\")\n",
        "        print(f\"   ‚Ä¢ Current allocation: {learning_system.contrarian_allocation*100:.0f}% contrarian\")\n",
        "        print(\"   ‚Ä¢ Adaptive SL/TP: 1.5x-2.5x (volatility-based)\")\n",
        "        print(\"   ‚Ä¢ Momentum filter: Don't fade strong trends\")\n",
        "        print(\"   ‚Ä¢ Evaluation windows: 2-12h min, 24-72h max\")\n",
        "        print()\n",
        "        print(\"   üß™ ENHANCEMENTS:\")\n",
        "        print(\"      ‚úÖ SL/TP properly swapped for contrarian trades\")\n",
        "        print(\"      ‚úÖ Performance-based allocation adjustment\")\n",
        "        print(\"      ‚úÖ Statistical confidence intervals\")\n",
        "        print(\"      ‚úÖ Spread impact tracking\")\n",
        "    else:\n",
        "        print(\"üíº WEEKDAY MODE: NORMAL STRATEGY\")\n",
        "        print(\"   ‚Ä¢ Standard prediction logic\")\n",
        "        print(\"   ‚Ä¢ Using 2.0x normal SL/TP\")\n",
        "        print(\"   ‚Ä¢ Evaluation windows: 1-6h min, 12-36h max\")\n",
        "    print()\n",
        "\n",
        "    # STEP 1: Evaluate old predictions\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üéì STEP 1: EVALUATE PREVIOUS PREDICTIONS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    current_prices = {}\n",
        "    for pkl in PICKLE_FOLDER.glob(\"*.pkl\"):\n",
        "        if any(x in pkl.name for x in ['_model', 'indicator_cache', '.bak']):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = data_loader.load_data(pkl)\n",
        "            if df is not None and not df.empty:\n",
        "                parts = pkl.stem.split('_')\n",
        "                if len(parts) >= 2:\n",
        "                    pair = f\"{parts[0]}/{parts[1]}\"\n",
        "                    price = df['raw_close'].iloc[-1] if 'raw_close' in df.columns else df['close'].iloc[-1]\n",
        "                    current_prices[pair] = float(price)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"üìä Current prices loaded for {len(current_prices)} pairs\")\n",
        "    evaluated = learning_system.evaluate_predictions(current_prices)\n",
        "\n",
        "    # Show enhanced stats\n",
        "    stats = learning_system.get_stats(split_by_weekend=True)\n",
        "    if stats and stats.get('overall', {}).get('total', 0) > 0:\n",
        "        print()\n",
        "        print_status(f\"üìà Learning Stats (Enhanced Contrarian):\", \"success\")\n",
        "\n",
        "        overall = stats['overall']\n",
        "        print(f\"\\n   OVERALL:\")\n",
        "        print(f\"      Total: {overall['total']}\")\n",
        "        print(f\"      Wins: {overall['wins']} | Losses: {overall['losses']}\")\n",
        "        print(f\"      Win Rate: {overall['win_rate']*100:.1f}%\")\n",
        "        print(f\"      CI: [{overall['win_rate_ci_lower']*100:.1f}%, {overall['win_rate_ci_upper']*100:.1f}%]\")\n",
        "\n",
        "        weekday = stats.get('weekday', {})\n",
        "        if weekday.get('total', 0) > 0:\n",
        "            print(f\"\\n   WEEKDAY (Normal Strategy):\")\n",
        "            print(f\"      Total: {weekday['total']}\")\n",
        "            print(f\"      Win Rate: {weekday['win_rate']*100:.1f}%\")\n",
        "            print(f\"      CI: [{weekday['win_rate_ci_lower']*100:.1f}%, {weekday['win_rate_ci_upper']*100:.1f}%]\")\n",
        "            print(f\"      Status: {'‚úÖ HEALTHY' if weekday['win_rate'] > 0.4 else '‚ö†Ô∏è NEEDS WORK'}\")\n",
        "\n",
        "        # üÜï Show contrarian performance with confidence intervals\n",
        "        weekend_normal = stats.get('weekend_normal', {})\n",
        "        weekend_contrarian = stats.get('weekend_contrarian', {})\n",
        "\n",
        "        if weekend_normal.get('total', 0) > 0:\n",
        "            print(f\"\\n   WEEKEND - NORMAL STRATEGY:\")\n",
        "            print(f\"      Total: {weekend_normal['total']}\")\n",
        "            print(f\"      Win Rate: {weekend_normal['win_rate']*100:.1f}%\")\n",
        "            print(f\"      CI: [{weekend_normal['win_rate_ci_lower']*100:.1f}%, {weekend_normal['win_rate_ci_upper']*100:.1f}%]\")\n",
        "            print(f\"      Significant: {'‚úÖ Yes' if weekend_normal['is_significant'] else '‚ö†Ô∏è No'}\")\n",
        "\n",
        "        if weekend_contrarian.get('total', 0) > 0:\n",
        "            print(f\"\\n   WEEKEND - CONTRARIAN STRATEGY üß™:\")\n",
        "            print(f\"      Total: {weekend_contrarian['total']}\")\n",
        "            print(f\"      Win Rate: {weekend_contrarian['win_rate']*100:.1f}%\")\n",
        "            print(f\"      CI: [{weekend_contrarian['win_rate_ci_lower']*100:.1f}%, {weekend_contrarian['win_rate_ci_upper']*100:.1f}%]\")\n",
        "            print(f\"      Significant: {'‚úÖ Yes' if weekend_contrarian['is_significant'] else '‚ö†Ô∏è No'}\")\n",
        "\n",
        "            # Compare strategies\n",
        "            if weekend_normal.get('total', 0) > 0:\n",
        "                normal_wr = weekend_normal['win_rate']\n",
        "                contrarian_wr = weekend_contrarian['win_rate']\n",
        "                improvement = (contrarian_wr - normal_wr) * 100\n",
        "\n",
        "                # Check CI overlap\n",
        "                normal_ci_upper = weekend_normal['win_rate_ci_upper']\n",
        "                normal_ci_lower = weekend_normal['win_rate_ci_lower']\n",
        "                contrarian_ci_upper = weekend_contrarian['win_rate_ci_upper']\n",
        "                contrarian_ci_lower = weekend_contrarian['win_rate_ci_lower']\n",
        "\n",
        "                print(f\"\\n   üìä COMPARISON:\")\n",
        "                print(f\"      Difference: {improvement:+.1f}%\")\n",
        "\n",
        "                if contrarian_ci_lower > normal_ci_upper:\n",
        "                    print(f\"      Status: üéâ CONTRARIAN STATISTICALLY BETTER\")\n",
        "                    print(f\"      Recommendation: Increase allocation to 80-90%\")\n",
        "                elif normal_ci_lower > contrarian_ci_upper:\n",
        "                    print(f\"      Status: ‚ùå NORMAL STATISTICALLY BETTER\")\n",
        "                    print(f\"      Recommendation: Reduce allocation to 10-20%\")\n",
        "                elif improvement > 10:\n",
        "                    print(f\"      Status: ‚úÖ CONTRARIAN TRENDING BETTER\")\n",
        "                    print(f\"      Recommendation: Continue testing, increase allocation gradually\")\n",
        "                elif improvement < -10:\n",
        "                    print(f\"      Status: ‚ö†Ô∏è NORMAL TRENDING BETTER\")\n",
        "                    print(f\"      Recommendation: Continue testing, decrease allocation gradually\")\n",
        "                else:\n",
        "                    print(f\"      Status: üî¨ INCONCLUSIVE\")\n",
        "                    print(f\"      Recommendation: Need more data\")\n",
        "\n",
        "        # Show current allocation\n",
        "        print(f\"\\n   ‚öôÔ∏è DYNAMIC ALLOCATION:\")\n",
        "        print(f\"      Current: {stats['contrarian_allocation']*100:.0f}% contrarian / {(1-stats['contrarian_allocation'])*100:.0f}% normal\")\n",
        "        if learning_system.allocation_history:\n",
        "            last_change = learning_system.allocation_history[-1]\n",
        "            print(f\"      Last change: {last_change.get('reason', 'N/A')}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # STEP 2: Generate new predictions with enhanced contrarian logic\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üîÆ STEP 2: GENERATE NEW PREDICTIONS\")\n",
        "    if is_weekend_now:\n",
        "        print(f\"üß™ DYNAMIC ALLOCATION: {learning_system.contrarian_allocation*100:.0f}% Contrarian, {(1-learning_system.contrarian_allocation)*100:.0f}% Normal\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    pickle_files = list(PICKLE_FOLDER.glob(\"*.pkl\"))\n",
        "    pickle_files = [f for f in pickle_files\n",
        "                   if not any(suffix in f.name for suffix in ['_sgd_model', '_rf_model', 'indicator_cache'])]\n",
        "\n",
        "    if not pickle_files:\n",
        "        print_status(\"No data pickles found!\", \"warn\")\n",
        "        return {}\n",
        "\n",
        "    print_status(f\"Found {len(pickle_files)} data files\", \"success\")\n",
        "    print()\n",
        "\n",
        "    signals = {}\n",
        "\n",
        "    for pkl_file in pickle_files:\n",
        "        pair, pair_signals, agg = process_pickle_file_contrarian(pkl_file, is_weekend_now)\n",
        "\n",
        "        if pair and pair_signals:\n",
        "            if pair not in signals:\n",
        "                signals[pair] = {\"signals\": {}, \"aggregated\": \"HOLD\"}\n",
        "\n",
        "            signals[pair][\"signals\"].update(pair_signals)\n",
        "\n",
        "            if agg != \"HOLD\":\n",
        "                signals[pair][\"aggregated\"] = agg\n",
        "\n",
        "    print()\n",
        "    print_status(f\"Generated signals for {len(signals)} pairs\", \"success\")\n",
        "\n",
        "    # Export experiment metrics\n",
        "    if is_weekend_now:\n",
        "        learning_system.export_experiment_metrics()\n",
        "\n",
        "    return signals\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# ENTRY POINT\n",
        "# ======================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        signals = main()\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        print()\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"‚úÖ Pipeline v6.4.0 completed in {elapsed:.2f}s\")\n",
        "        print(\"üß™ ENHANCED CONTRARIAN STRATEGY ACTIVE\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        stats = learning_system.get_stats(split_by_weekend=True)\n",
        "        if stats and stats.get('overall', {}).get('total', 0) > 0:\n",
        "            print(f\"\\nüìä Experiment Summary:\")\n",
        "\n",
        "            weekend_normal = stats.get('weekend_normal', {})\n",
        "            weekend_contrarian = stats.get('weekend_contrarian', {})\n",
        "\n",
        "            if weekend_normal.get('total', 0) >= 10 and weekend_contrarian.get('total', 0) >= 10:\n",
        "                print(f\"\\n   Control Group (Normal):\")\n",
        "                print(f\"      Trades: {weekend_normal['total']}\")\n",
        "                print(f\"      Win Rate: {weekend_normal['win_rate']*100:.1f}%\")\n",
        "                print(f\"      CI: [{weekend_normal['win_rate_ci_lower']*100:.1f}%, {weekend_normal['win_rate_ci_upper']*100:.1f}%]\")\n",
        "\n",
        "                print(f\"\\n   Test Group (Contrarian):\")\n",
        "                print(f\"      Trades: {weekend_contrarian['total']}\")\n",
        "                print(f\"      Win Rate: {weekend_contrarian['win_rate']*100:.1f}%\")\n",
        "                print(f\"      CI: [{weekend_contrarian['win_rate_ci_lower']*100:.1f}%, {weekend_contrarian['win_rate_ci_upper']*100:.1f}%]\")\n",
        "\n",
        "                print(f\"\\n   üìä Statistical Analysis:\")\n",
        "                if weekend_contrarian['total'] >= 30 and weekend_normal['total'] >= 30:\n",
        "                    diff = (weekend_contrarian['win_rate'] - weekend_normal['win_rate']) * 100\n",
        "                    print(f\"      Difference: {diff:+.1f}%\")\n",
        "\n",
        "                    # Check for CI overlap\n",
        "                    no_overlap = (weekend_contrarian['win_rate_ci_lower'] > weekend_normal['win_rate_ci_upper'] or\n",
        "                                 weekend_normal['win_rate_ci_lower'] > weekend_contrarian['win_rate_ci_upper'])\n",
        "\n",
        "                    if no_overlap:\n",
        "                        print(f\"      Significance: ‚úÖ STATISTICALLY SIGNIFICANT\")\n",
        "                        if diff > 0:\n",
        "                            print(f\"      Winner: üéâ CONTRARIAN STRATEGY\")\n",
        "                            print(f\"      Action: Increase allocation to {min(90, stats['contrarian_allocation']*100 + 10):.0f}%\")\n",
        "                        else:\n",
        "                            print(f\"      Winner: NORMAL STRATEGY\")\n",
        "                            print(f\"      Action: Decrease allocation to {max(10, stats['contrarian_allocation']*100 - 10):.0f}%\")\n",
        "                    elif abs(diff) > 10:\n",
        "                        print(f\"      Significance: ‚ö†Ô∏è TRENDING (not yet conclusive)\")\n",
        "                        print(f\"      Action: Continue testing, gradual allocation shift\")\n",
        "                    else:\n",
        "                        print(f\"      Significance: üî¨ INCONCLUSIVE\")\n",
        "                        print(f\"      Action: Continue collecting data\")\n",
        "                else:\n",
        "                    needed = max(30 - weekend_normal['total'], 30 - weekend_contrarian['total'])\n",
        "                    print(f\"      Status: üî¨ Collecting data...\")\n",
        "                    print(f\"      Needed: ~{needed} more trades per group for significance\")\n",
        "\n",
        "        print(f\"\\nüéØ Next Steps:\")\n",
        "        print(f\"   1. Monitor allocation adjustments (currently {stats['contrarian_allocation']*100:.0f}%)\")\n",
        "        print(f\"   2. System auto-adjusts based on performance\")\n",
        "        print(f\"   3. Target: 30+ trades per group for statistical power\")\n",
        "        print(f\"   4. Check {AB_TEST_METRICS.name} for detailed metrics\")\n",
        "\n",
        "        print(f\"\\n‚ö° v6.4.0 KEY IMPROVEMENTS:\")\n",
        "        print(f\"   ‚úÖ SL/TP properly swapped for contrarian trades\")\n",
        "        print(f\"   ‚úÖ Adaptive volatility-based multipliers (1.5x-2.5x)\")\n",
        "        print(f\"   ‚úÖ Momentum filter (don't fade strong trends)\")\n",
        "        print(f\"   ‚úÖ Statistical confidence intervals (Wilson score)\")\n",
        "        print(f\"   ‚úÖ Performance-based dynamic allocation\")\n",
        "        print(f\"   ‚úÖ Weekend phase tracking\")\n",
        "        print(f\"   ‚úÖ Spread impact analysis\")\n",
        "\n",
        "        # Save signals\n",
        "        if signals:\n",
        "            output_file = DIRECTORIES[\"outputs\"] / f\"signals_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "            with open(output_file, 'w') as f:\n",
        "                json.dump(signals, f, indent=2)\n",
        "            print(f\"\\nüìÑ Signals saved to: {output_file.name}\")\n",
        "\n",
        "        # Show allocation history if exists\n",
        "        if learning_system.allocation_history:\n",
        "            print(f\"\\nüìà Allocation History (last 5):\")\n",
        "            for entry in learning_system.allocation_history[-5:]:\n",
        "                timestamp = datetime.fromisoformat(entry['timestamp']).strftime('%Y-%m-%d %H:%M')\n",
        "                allocation = entry['allocation'] * 100\n",
        "                reason = entry['reason']\n",
        "                print(f\"      {timestamp}: {allocation:.0f}% - {reason}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"Pipeline error: {e}\", \"error\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "Oxmqc37GhzRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "TRADE BEACON v21.1 - REGIME-AWARE TRADING WITH QUALITY FILTERING\n",
        "=================================================================\n",
        "üÜï v21.1 ENHANCEMENTS:\n",
        "‚úÖ All v21.0 features (regime detection, self-learning, etc.)\n",
        "‚úÖ NEW: Self-learning quality scoring system (from Document 3)\n",
        "‚úÖ NEW: Adaptive quality thresholds (improves automatically)\n",
        "‚úÖ NEW: Premium signal filtering (only top 5-10 signals)\n",
        "‚úÖ NEW: Enhanced learning visibility\n",
        "‚úÖ NEW: Quality weight optimization based on outcomes\n",
        "\n",
        "WHAT'S NEW IN v21.1:\n",
        "- Quality scoring layer filters signals before sending\n",
        "- AI learns optimal quality weights from outcomes\n",
        "- You receive ONLY high-probability setups (score >80)\n",
        "- All v21.0 learning preserved and enhanced\n",
        "- Zero risk to existing proven systems\n",
        "\"\"\"\n",
        "import os, sys, json, gzip, random, re, smtplib, subprocess, logging, warnings, shutil, sqlite3\n",
        "from pathlib import Path\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from collections import defaultdict, deque\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "from contextlib import contextmanager\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ENVIRONMENT & CONFIG\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB, IN_GHA, ENV_NAME = True, False, \"Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB, IN_GHA = False, \"GITHUB_ACTIONS\" in os.environ\n",
        "    ENV_NAME = \"GHA\" if IN_GHA else \"Local\"\n",
        "\n",
        "BASE = Path(\"/content\" if IN_COLAB else Path.cwd())\n",
        "SAVE = BASE if IN_GHA else (BASE / \"forex-ai-models\" if IN_COLAB else BASE)\n",
        "DIRS = {k: SAVE / v for k, v in {\"data\": \"data/processed\", \"db\": \"database\", \"logs\": \"logs\",\n",
        "    \"out\": \"outputs\", \"state\": \"omega_state\", \"rl\": \"rl_memory\", \"backup\": \"backups\",\n",
        "    \"learning\": \"learning_data\", \"regime\": \"regime_stats\", \"quality\": \"quality_weights\"}.items()}\n",
        "for d in DIRS.values(): d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Paths\n",
        "DB_FILE = DIRS[\"db\"] / \"memory_v85.db\"\n",
        "RL_MEM = DIRS[\"rl\"] / \"experience_replay.json.gz\"\n",
        "RL_STATS = DIRS[\"rl\"] / \"learning_stats.json\"\n",
        "RL_WEIGHTS = DIRS[\"rl\"] / \"network_weights.json\"\n",
        "SIGNALS = DIRS[\"out\"] / \"omega_signals.json\"\n",
        "ITER_FILE = DIRS[\"state\"] / \"omega_iteration.json\"\n",
        "TRADES = DIRS[\"rl\"] / \"trade_history.json\"\n",
        "VERSION_FILE = DIRS[\"rl\"] / \"version.txt\"\n",
        "PIPELINE_V6_DB = DIRS[\"learning\"] / \"learning_outcomes.json\"\n",
        "REGIME_STATS = DIRS[\"regime\"] / \"regime_performance.json\"\n",
        "QUALITY_WEIGHTS_FILE = DIRS[\"quality\"] / \"learned_weights.json\"  # NEW\n",
        "QUALITY_HISTORY_FILE = DIRS[\"quality\"] / \"quality_history.json\"  # NEW\n",
        "\n",
        "logging.basicConfig(filename=str(DIRS[\"logs\"] / f\"beacon_{datetime.now():%Y%m%d_%H%M%S}.log\"),\n",
        "                   level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
        "\n",
        "def log(msg, lvl=\"info\"):\n",
        "    ico = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\",\"error\":\"‚ùå\",\"rocket\":\"üöÄ\",\"brain\":\"üß†\",\n",
        "           \"money\":\"üí∞\",\"db\":\"üíæ\",\"learning\":\"üéì\",\"regime\":\"üåç\",\"quality\":\"‚≠ê\"}\n",
        "    getattr(logging, \"warning\" if lvl==\"warn\" else lvl, logging.info)(msg)\n",
        "    print(f\"{ico.get(lvl,'‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# Config\n",
        "GH_USER, GH_REPO = \"rahim-dotAI\", \"forex-ai-models\"\n",
        "PAT = os.getenv(\"FOREX_PAT\", \"\").strip()\n",
        "if not PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if PAT: os.environ[\"FOREX_PAT\"] = PAT\n",
        "    except: pass\n",
        "\n",
        "GMAIL = os.getenv(\"GMAIL_USER\", \"nakatonabira3@gmail.com\")\n",
        "GMAIL_PWD = os.getenv(\"GMAIL_APP_PASSWORD\", \"\").strip() or \"rijjykrxamhanovt\"\n",
        "BROWSER_TOKEN = os.getenv(\"BROWSERLESS_TOKEN\", \"\")\n",
        "\n",
        "def load_config():\n",
        "    config_file = SAVE / \"config\" / \"settings.json\"\n",
        "    if config_file.exists():\n",
        "        try:\n",
        "            with open(config_file, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except: pass\n",
        "    return {\n",
        "        'pairs': [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"],\n",
        "        'timeframes': ['1m', '5m', '15m', '1h', '1d'],\n",
        "        'atr_sl_multiplier': 2.0,\n",
        "        'atr_tp_multiplier': 2.5,\n",
        "        'base_capital': 100,\n",
        "        'max_risk_per_trade': 0.02,\n",
        "        'max_positions': 2,\n",
        "        'confidence_threshold': 0.65,\n",
        "        'weekend_contrarian': True,\n",
        "        'regime_filters': True,\n",
        "        'quality_filter_enabled': True,  # NEW\n",
        "        'min_quality_score': 80.0  # NEW\n",
        "    }\n",
        "\n",
        "CONFIG = load_config()\n",
        "PAIRS = CONFIG.get('pairs', [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"])\n",
        "ATR_PER, MIN_ATR, EPS = 14, 1e-5, 1e-8\n",
        "CAPITAL = CONFIG.get('base_capital', 100)\n",
        "RISK = CONFIG.get('max_risk_per_trade', 0.02)\n",
        "MAX_POS = CONFIG.get('max_positions', 2)\n",
        "MAX_CAP = 10.0\n",
        "STATE_SIZE = 35\n",
        "ACTIONS = 3\n",
        "\n",
        "# Hyperparameters\n",
        "LR, GAMMA, TARGET_UPD, BATCH, MEM_CAP = 0.0003, 0.93, 50, 96, 15000\n",
        "PROFIT_SCALE, LOSS_SCALE, WIN_BONUS, LOSS_PEN, SHARPE_SCALE = 10.0, 5.0, 5.0, 5.0, 8.0\n",
        "ATR_SL = CONFIG.get('atr_sl_multiplier', 2.0)\n",
        "ATR_TP = CONFIG.get('atr_tp_multiplier', 2.5)\n",
        "Q_CLIP_MIN, Q_CLIP_MAX = -500.0, 500.0\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# NEW: QUALITY SCORING SYSTEM (From Document 3)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "class QualityScorer:\n",
        "    \"\"\"\n",
        "    Self-learning quality scoring system\n",
        "    Filters signals to only show premium setups\n",
        "    Learns optimal weights from actual trading outcomes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.weights = self._load_weights()\n",
        "        self.history = self._load_history()\n",
        "        log(\"‚≠ê Quality scorer initialized\", \"quality\")\n",
        "\n",
        "    def _load_weights(self) -> Dict:\n",
        "        \"\"\"Load learned quality weights or use smart defaults\"\"\"\n",
        "        if QUALITY_WEIGHTS_FILE.exists():\n",
        "            try:\n",
        "                with open(QUALITY_WEIGHTS_FILE, 'r') as f:\n",
        "                    weights = json.load(f)\n",
        "                    log(f\"‚úÖ Loaded learned quality weights\", \"quality\")\n",
        "                    return weights\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Smart defaults (will be optimized through learning)\n",
        "        return {\n",
        "            # Base scoring components\n",
        "            'model_agreement': 25.0,\n",
        "            'confidence_high': 20.0,\n",
        "            'confidence_medium': 10.0,\n",
        "            'confidence_low_penalty': -15.0,\n",
        "            'regime_alignment': 15.0,\n",
        "            'historical_performance_strong': 20.0,\n",
        "            'historical_performance_good': 10.0,\n",
        "\n",
        "            # Learned regime boosts (from v21 learning)\n",
        "            'london_session_boost': 15.0,  # v21 learned: 75% WR\n",
        "            'overlap_session_boost': 10.0,\n",
        "            'normal_volatility_boost': 10.0,\n",
        "            'strong_trend_boost': 8.0,\n",
        "            'contrarian_weekend_boost': 12.0,  # v21 learned: +33%\n",
        "\n",
        "            # Learned penalties (from v21 learning)\n",
        "            'dead_zone_penalty': -100.0,  # Auto-reject\n",
        "            'extreme_volatility_penalty': -50.0,  # Auto-reject\n",
        "            'weak_confidence_penalty': -20.0,\n",
        "\n",
        "            # Adaptive thresholds (improve automatically)\n",
        "            'min_quality_score': 80.0,\n",
        "            'min_confidence': 0.65,\n",
        "            'optimal_score_target': 85.0,\n",
        "\n",
        "            # Learning metadata\n",
        "            'last_updated': datetime.now(timezone.utc).isoformat(),\n",
        "            'total_signals_scored': 0,\n",
        "            'learning_iterations': 0\n",
        "        }\n",
        "\n",
        "    def _load_history(self) -> List:\n",
        "        \"\"\"Load quality scoring history\"\"\"\n",
        "        if QUALITY_HISTORY_FILE.exists():\n",
        "            try:\n",
        "                with open(QUALITY_HISTORY_FILE, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except:\n",
        "                pass\n",
        "        return []\n",
        "\n",
        "    def calculate_quality_score(self, signal_data: Dict) -> Tuple[float, Dict]:\n",
        "        \"\"\"\n",
        "        Calculate quality score for a signal using learned weights\n",
        "        Returns: (score, components_breakdown)\n",
        "        \"\"\"\n",
        "        score = 0.0\n",
        "        components = {}\n",
        "\n",
        "        # Extract signal data\n",
        "        confidence = signal_data.get('confidence', 0.0)\n",
        "        regimes = signal_data.get('regimes', {})\n",
        "        pair = signal_data.get('pair', '')\n",
        "        timeframe = signal_data.get('timeframe', '')\n",
        "        rb_epsilon = signal_data.get('rb_epsilon', 0.7)\n",
        "\n",
        "        # 1. Confidence scoring\n",
        "        if confidence > 0.75:\n",
        "            points = self.weights['confidence_high']\n",
        "            score += points\n",
        "            components['high_confidence'] = points\n",
        "        elif confidence > 0.65:\n",
        "            points = self.weights['confidence_medium']\n",
        "            score += points\n",
        "            components['medium_confidence'] = points\n",
        "        else:\n",
        "            penalty = self.weights['confidence_low_penalty']\n",
        "            score += penalty\n",
        "            components['low_confidence'] = penalty\n",
        "\n",
        "        # 2. Regime alignment (using v21's learned patterns)\n",
        "\n",
        "        # Session scoring\n",
        "        session = regimes.get('session', 'UNKNOWN')\n",
        "\n",
        "        if session == 'LONDON_SESSION':\n",
        "            boost = self.weights['london_session_boost']\n",
        "            score += boost\n",
        "            components['london_boost'] = boost\n",
        "        elif session == 'OVERLAP_SESSION':\n",
        "            boost = self.weights['overlap_session_boost']\n",
        "            score += boost\n",
        "            components['overlap_boost'] = boost\n",
        "        elif session == 'DEAD_ZONE':\n",
        "            penalty = self.weights['dead_zone_penalty']\n",
        "            score += penalty\n",
        "            components['dead_zone_penalty'] = penalty\n",
        "            return 0.0, components  # Auto-reject\n",
        "\n",
        "        # Volatility scoring\n",
        "        volatility = regimes.get('volatility', 'NORMAL_VOL')\n",
        "\n",
        "        if volatility == 'NORMAL_VOL':\n",
        "            boost = self.weights['normal_volatility_boost']\n",
        "            score += boost\n",
        "            components['normal_vol_boost'] = boost\n",
        "        elif volatility == 'EXTREME_VOL':\n",
        "            penalty = self.weights['extreme_volatility_penalty']\n",
        "            score += penalty\n",
        "            components['extreme_vol_penalty'] = penalty\n",
        "            return 0.0, components  # Auto-reject\n",
        "        elif volatility == 'HIGH_VOL':\n",
        "            # High vol is risky but not auto-reject\n",
        "            score -= 5.0\n",
        "            components['high_vol_caution'] = -5.0\n",
        "\n",
        "        # Trend strength\n",
        "        trend = regimes.get('trend', 'RANGING')\n",
        "\n",
        "        if trend in ['STRONG_UPTREND', 'STRONG_DOWNTREND']:\n",
        "            boost = self.weights['strong_trend_boost']\n",
        "            score += boost\n",
        "            components['strong_trend_boost'] = boost\n",
        "\n",
        "        # Weekend contrarian boost (v21's major discovery)\n",
        "        if self._is_weekend() and trend == 'RANGING':\n",
        "            boost = self.weights['contrarian_weekend_boost']\n",
        "            score += boost\n",
        "            components['contrarian_weekend_boost'] = boost\n",
        "\n",
        "        # 3. Learning stage bonus (higher epsilon = exploration = lower standards)\n",
        "        if rb_epsilon > 0.5:\n",
        "            # Still learning, be more lenient\n",
        "            learning_bonus = (rb_epsilon - 0.5) * 20\n",
        "            score += learning_bonus\n",
        "            components['learning_stage_bonus'] = learning_bonus\n",
        "        elif rb_epsilon < 0.2:\n",
        "            # Mature system, highest standards\n",
        "            maturity_boost = 10.0\n",
        "            score += maturity_boost\n",
        "            components['maturity_boost'] = maturity_boost\n",
        "\n",
        "        # 4. Risk/Reward check\n",
        "        sl = signal_data.get('SL', 0)\n",
        "        tp = signal_data.get('TP', 0)\n",
        "        entry = signal_data.get('last_price', 0)\n",
        "\n",
        "        if sl and tp and entry:\n",
        "            risk = abs(entry - sl)\n",
        "            reward = abs(tp - entry)\n",
        "            rr_ratio = reward / (risk + EPS)\n",
        "\n",
        "            if rr_ratio >= 2.0:\n",
        "                score += 15.0\n",
        "                components['excellent_rr'] = 15.0\n",
        "            elif rr_ratio >= 1.5:\n",
        "                score += 8.0\n",
        "                components['good_rr'] = 8.0\n",
        "            elif rr_ratio < 1.0:\n",
        "                score -= 20.0\n",
        "                components['poor_rr'] = -20.0\n",
        "\n",
        "        # 5. Regime consistency bonus\n",
        "        # If multiple regime factors align, boost score\n",
        "        positive_factors = sum(1 for v in components.values() if v > 0)\n",
        "        if positive_factors >= 4:\n",
        "            alignment_bonus = 10.0\n",
        "            score += alignment_bonus\n",
        "            components['regime_alignment_bonus'] = alignment_bonus\n",
        "\n",
        "        # Normalize score to 0-100 range\n",
        "        score = np.clip(score, 0, 100)\n",
        "\n",
        "        return score, components\n",
        "\n",
        "    def filter_signals(self, signals: List[Dict]) -> Tuple[List[Dict], Dict]:\n",
        "        \"\"\"\n",
        "        Filter signals to only premium quality\n",
        "        Returns: (filtered_signals, statistics)\n",
        "        \"\"\"\n",
        "        if not signals:\n",
        "            return [], {}\n",
        "\n",
        "        scored_signals = []\n",
        "\n",
        "        for signal in signals:\n",
        "            if signal.get('direction') == 'HOLD':\n",
        "                continue\n",
        "\n",
        "            score, components = self.calculate_quality_score(signal)\n",
        "\n",
        "            signal['quality_score'] = score\n",
        "            signal['score_components'] = components\n",
        "\n",
        "            scored_signals.append(signal)\n",
        "\n",
        "        # Sort by quality score\n",
        "        scored_signals.sort(key=lambda x: x['quality_score'], reverse=True)\n",
        "\n",
        "        # Filter by minimum threshold\n",
        "        min_score = self.weights['min_quality_score']\n",
        "        premium_signals = [s for s in scored_signals if s['quality_score'] >= min_score]\n",
        "\n",
        "        # Statistics\n",
        "        stats = {\n",
        "            'total_generated': len(signals),\n",
        "            'total_scored': len(scored_signals),\n",
        "            'premium_count': len(premium_signals),\n",
        "            'filtered_out': len(scored_signals) - len(premium_signals),\n",
        "            'avg_score': np.mean([s['quality_score'] for s in scored_signals]) if scored_signals else 0,\n",
        "            'top_score': scored_signals[0]['quality_score'] if scored_signals else 0,\n",
        "            'min_threshold': min_score\n",
        "        }\n",
        "\n",
        "        log(f\"‚≠ê Quality filter: {len(signals)} ‚Üí {len(premium_signals)} premium (>{min_score})\", \"quality\")\n",
        "\n",
        "        return premium_signals, stats\n",
        "\n",
        "    def learn_from_outcomes(self, outcomes: List[Dict]) -> bool:\n",
        "        \"\"\"\n",
        "        Learn optimal quality weights from trading outcomes\n",
        "        This is PURE autonomous learning - no human feedback\n",
        "        \"\"\"\n",
        "        if len(outcomes) < 10:\n",
        "            log(\"‚ö†Ô∏è Need 10+ outcomes to learn quality weights\", \"warn\")\n",
        "            return False\n",
        "\n",
        "        log(\"üéì Learning optimal quality weights from outcomes...\", \"learning\")\n",
        "\n",
        "        wins = [o for o in outcomes if o.get('was_correct', False)]\n",
        "        losses = [o for o in outcomes if not o.get('was_correct', False)]\n",
        "\n",
        "        if not wins or not losses:\n",
        "            log(\"‚ö†Ô∏è Need both wins and losses to learn\", \"warn\")\n",
        "            return False\n",
        "\n",
        "        # Analyze what quality scores predicted success\n",
        "        win_scores = [o.get('quality_score', 0) for o in wins if 'quality_score' in o]\n",
        "        loss_scores = [o.get('quality_score', 0) for o in losses if 'quality_score' in o]\n",
        "\n",
        "        if win_scores and loss_scores:\n",
        "            avg_win_score = np.mean(win_scores)\n",
        "            avg_loss_score = np.mean(loss_scores)\n",
        "            score_separation = avg_win_score - avg_loss_score\n",
        "\n",
        "            log(f\"   Win avg score: {avg_win_score:.1f}\", \"learning\")\n",
        "            log(f\"   Loss avg score: {avg_loss_score:.1f}\", \"learning\")\n",
        "            log(f\"   Separation: {score_separation:.1f}\", \"learning\")\n",
        "\n",
        "            # Adjust minimum threshold if separation is good\n",
        "            if score_separation > 10 and len(outcomes) > 50:\n",
        "                old_min = self.weights['min_quality_score']\n",
        "                # Gradually increase threshold toward winning score\n",
        "                new_min = min(95, old_min + 1)\n",
        "\n",
        "                if new_min != old_min:\n",
        "                    self.weights['min_quality_score'] = new_min\n",
        "                    log(f\"   üéì LEARNED: Quality threshold {old_min:.0f} ‚Üí {new_min:.0f}\", \"success\")\n",
        "\n",
        "        # Learn regime effectiveness\n",
        "        regime_performance = defaultdict(lambda: {'wins': 0, 'total': 0})\n",
        "\n",
        "        for outcome in outcomes:\n",
        "            regimes = outcome.get('regimes', {})\n",
        "            session = regimes.get('session', 'UNKNOWN')\n",
        "\n",
        "            regime_performance[session]['total'] += 1\n",
        "            if outcome.get('was_correct', False):\n",
        "                regime_performance[session]['wins'] += 1\n",
        "\n",
        "        # Update session boosts\n",
        "        for session, stats in regime_performance.items():\n",
        "            if stats['total'] < 5:\n",
        "                continue\n",
        "\n",
        "            wr = stats['wins'] / stats['total']\n",
        "\n",
        "            if session == 'LONDON_SESSION':\n",
        "                old_boost = self.weights['london_session_boost']\n",
        "\n",
        "                if wr > 0.70:\n",
        "                    new_boost = min(25.0, old_boost + 2.0)\n",
        "                elif wr < 0.50:\n",
        "                    new_boost = max(5.0, old_boost - 2.0)\n",
        "                else:\n",
        "                    new_boost = old_boost\n",
        "\n",
        "                if new_boost != old_boost:\n",
        "                    self.weights['london_session_boost'] = new_boost\n",
        "                    log(f\"   üéì LEARNED: London boost {old_boost:.0f} ‚Üí {new_boost:.0f} (WR: {wr*100:.1f}%)\", \"success\")\n",
        "\n",
        "            elif session == 'OVERLAP_SESSION':\n",
        "                old_boost = self.weights['overlap_session_boost']\n",
        "\n",
        "                if wr > 0.65:\n",
        "                    new_boost = min(20.0, old_boost + 1.5)\n",
        "                elif wr < 0.45:\n",
        "                    new_boost = max(3.0, old_boost - 1.5)\n",
        "                else:\n",
        "                    new_boost = old_boost\n",
        "\n",
        "                if new_boost != old_boost:\n",
        "                    self.weights['overlap_session_boost'] = new_boost\n",
        "                    log(f\"   üéì LEARNED: Overlap boost {old_boost:.0f} ‚Üí {new_boost:.0f} (WR: {wr*100:.1f}%)\", \"success\")\n",
        "\n",
        "        # Learn contrarian effectiveness\n",
        "        contrarian_outcomes = [\n",
        "            o for o in outcomes\n",
        "            if o.get('regimes', {}).get('trend') == 'RANGING'\n",
        "            and self._is_weekend(datetime.fromisoformat(o.get('timestamp', '').replace('Z', '+00:00')))\n",
        "        ]\n",
        "\n",
        "        if len(contrarian_outcomes) >= 10:\n",
        "            contrarian_wins = sum(1 for o in contrarian_outcomes if o.get('was_correct', False))\n",
        "            contrarian_wr = contrarian_wins / len(contrarian_outcomes)\n",
        "\n",
        "            old_boost = self.weights['contrarian_weekend_boost']\n",
        "\n",
        "            if contrarian_wr > 0.40:\n",
        "                new_boost = min(20.0, old_boost + 1.5)\n",
        "            elif contrarian_wr < 0.25:\n",
        "                new_boost = max(3.0, old_boost - 2.0)\n",
        "            else:\n",
        "                new_boost = old_boost\n",
        "\n",
        "            if new_boost != old_boost:\n",
        "                self.weights['contrarian_weekend_boost'] = new_boost\n",
        "                log(f\"   üéì LEARNED: Contrarian boost {old_boost:.0f} ‚Üí {new_boost:.0f} (WR: {contrarian_wr*100:.1f}%)\", \"success\")\n",
        "\n",
        "        # Update metadata\n",
        "        self.weights['last_updated'] = datetime.now(timezone.utc).isoformat()\n",
        "        self.weights['learning_iterations'] += 1\n",
        "\n",
        "        # Save learned weights\n",
        "        self._save_weights()\n",
        "\n",
        "        log(\"‚úÖ Quality weight learning complete\", \"success\")\n",
        "        return True\n",
        "\n",
        "    def _save_weights(self):\n",
        "        \"\"\"Save learned quality weights\"\"\"\n",
        "        try:\n",
        "            with open(QUALITY_WEIGHTS_FILE, 'w') as f:\n",
        "                json.dump(self.weights, f, indent=2)\n",
        "        except Exception as e:\n",
        "            log(f\"Failed to save quality weights: {e}\", \"error\")\n",
        "\n",
        "    def _is_weekend(self, dt=None):\n",
        "        \"\"\"Check if datetime is weekend\"\"\"\n",
        "        if dt is None:\n",
        "            dt = datetime.now()\n",
        "        return dt.weekday() in [5, 6]\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Get quality scorer statistics\"\"\"\n",
        "        return {\n",
        "            'min_quality_score': self.weights['min_quality_score'],\n",
        "            'learning_iterations': self.weights.get('learning_iterations', 0),\n",
        "            'last_updated': self.weights.get('last_updated', 'Never'),\n",
        "            'key_weights': {\n",
        "                'london_boost': self.weights['london_session_boost'],\n",
        "                'contrarian_boost': self.weights['contrarian_weekend_boost'],\n",
        "                'confidence_weight': self.weights['confidence_high']\n",
        "            }\n",
        "        }\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# REGIME DETECTION SYSTEM (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "class RegimeDetector:\n",
        "    \"\"\"Comprehensive market regime detection\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_volatility_regime(df: pd.DataFrame) -> str:\n",
        "        try:\n",
        "            atr_current = df['atr'].iloc[-1]\n",
        "            atr_20 = df['atr'].rolling(20).mean().iloc[-1]\n",
        "            atr_200 = df['atr'].rolling(200).mean().iloc[-1]\n",
        "            volatility_ratio = atr_current / (atr_200 + EPS)\n",
        "            if volatility_ratio < 0.5: return \"LOW_VOL\"\n",
        "            elif volatility_ratio < 1.2: return \"NORMAL_VOL\"\n",
        "            elif volatility_ratio < 2.0: return \"HIGH_VOL\"\n",
        "            else: return \"EXTREME_VOL\"\n",
        "        except: return \"NORMAL_VOL\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_adx(df: pd.DataFrame, period: int = 14) -> float:\n",
        "        try:\n",
        "            high = df['high'].values\n",
        "            low = df['low'].values\n",
        "            close = df['close'].values\n",
        "            plus_dm = np.where((high[1:] - high[:-1]) > (low[:-1] - low[1:]),\n",
        "                              np.maximum(high[1:] - high[:-1], 0), 0)\n",
        "            minus_dm = np.where((low[:-1] - low[1:]) > (high[1:] - high[:-1]),\n",
        "                               np.maximum(low[:-1] - low[1:], 0), 0)\n",
        "            tr1 = high[1:] - low[1:]\n",
        "            tr2 = np.abs(high[1:] - close[:-1])\n",
        "            tr3 = np.abs(low[1:] - close[:-1])\n",
        "            tr = np.maximum(tr1, np.maximum(tr2, tr3))\n",
        "            atr = pd.Series(tr).rolling(period).mean().values\n",
        "            plus_di = 100 * (pd.Series(plus_dm).rolling(period).mean().values / (atr + EPS))\n",
        "            minus_di = 100 * (pd.Series(minus_dm).rolling(period).mean().values / (atr + EPS))\n",
        "            dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + EPS)\n",
        "            adx = pd.Series(dx).rolling(period).mean().iloc[-1]\n",
        "            return float(adx) if not np.isnan(adx) else 20.0\n",
        "        except: return 20.0\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_trend_regime(df: pd.DataFrame) -> str:\n",
        "        try:\n",
        "            ema_12 = df['close'].ewm(span=12).mean().iloc[-1]\n",
        "            ema_26 = df['close'].ewm(span=26).mean().iloc[-1]\n",
        "            ema_50 = df['close'].ewm(span=50).mean().iloc[-1]\n",
        "            ema_200 = df['close'].ewm(span=200).mean().iloc[-1]\n",
        "            price = df['close'].iloc[-1]\n",
        "            adx = RegimeDetector.calculate_adx(df)\n",
        "            ema_50_series = df['close'].ewm(span=50).mean()\n",
        "            ema_50_slope = (ema_50_series.iloc[-1] - ema_50_series.iloc[-20]) / 20\n",
        "            if adx > 25:\n",
        "                if price > ema_12 > ema_26 > ema_50 > ema_200: return \"STRONG_UPTREND\"\n",
        "                elif price < ema_12 < ema_26 < ema_50 < ema_200: return \"STRONG_DOWNTREND\"\n",
        "            if adx < 20: return \"RANGING\"\n",
        "            if ema_12 > ema_26 and ema_50_slope > 0: return \"WEAK_UPTREND\"\n",
        "            elif ema_12 < ema_26 and ema_50_slope < 0: return \"WEAK_DOWNTREND\"\n",
        "            return \"RANGING\"\n",
        "        except: return \"RANGING\"\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_session_regime() -> str:\n",
        "        hour = datetime.now(timezone.utc).hour\n",
        "        if 0 <= hour < 8: return \"ASIA_SESSION\"\n",
        "        elif 8 <= hour < 13: return \"LONDON_SESSION\"\n",
        "        elif 13 <= hour < 16: return \"OVERLAP_SESSION\"\n",
        "        elif 16 <= hour < 21: return \"NY_SESSION\"\n",
        "        else: return \"DEAD_ZONE\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_hurst_exponent(prices: np.ndarray) -> float:\n",
        "        try:\n",
        "            lags = range(2, min(20, len(prices) // 2))\n",
        "            tau = []\n",
        "            for lag in lags:\n",
        "                std = np.std([prices[i] - prices[i - lag] for i in range(lag, len(prices))])\n",
        "                tau.append(std)\n",
        "            if len(tau) > 0 and len(lags) > 0:\n",
        "                poly = np.polyfit(np.log(list(lags)), np.log(tau), 1)\n",
        "                return poly[0]\n",
        "            return 0.5\n",
        "        except: return 0.5\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_behavior_regime(df: pd.DataFrame) -> str:\n",
        "        try:\n",
        "            prices = df['close'].values[-100:]\n",
        "            hurst = RegimeDetector.calculate_hurst_exponent(prices)\n",
        "            if hurst > 0.6: return \"MOMENTUM\"\n",
        "            elif hurst < 0.4: return \"MEAN_REVERT\"\n",
        "            else: return \"NEUTRAL\"\n",
        "        except: return \"NEUTRAL\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_all_regimes(df_1h: pd.DataFrame, df_1d: pd.DataFrame) -> Dict[str, str]:\n",
        "        return {\n",
        "            'volatility': RegimeDetector.detect_volatility_regime(df_1h),\n",
        "            'trend': RegimeDetector.detect_trend_regime(df_1d),\n",
        "            'session': RegimeDetector.detect_session_regime(),\n",
        "            'behavior': RegimeDetector.detect_behavior_regime(df_1h)\n",
        "        }\n",
        "\n",
        "class RegimeStrategy:\n",
        "    \"\"\"Regime-based strategy adjustments\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def adjust_for_volatility(regime: str, base_sl: float, base_tp: float) -> Dict:\n",
        "        multipliers = {\n",
        "            \"LOW_VOL\": {\"sl\": 0.8, \"tp\": 1.5, \"confidence_threshold\": 0.55, \"position_size\": 1.0},\n",
        "            \"NORMAL_VOL\": {\"sl\": 1.0, \"tp\": 1.0, \"confidence_threshold\": 0.65, \"position_size\": 1.0},\n",
        "            \"HIGH_VOL\": {\"sl\": 1.5, \"tp\": 1.2, \"confidence_threshold\": 0.70, \"position_size\": 0.8},\n",
        "            \"EXTREME_VOL\": {\"sl\": 2.5, \"tp\": 1.5, \"confidence_threshold\": 0.80, \"position_size\": 0.5}\n",
        "        }\n",
        "        return multipliers.get(regime, multipliers[\"NORMAL_VOL\"])\n",
        "\n",
        "    @staticmethod\n",
        "    def select_strategy_for_trend(regime: str, is_weekend: bool = False) -> Dict:\n",
        "        strategies = {\n",
        "            \"STRONG_UPTREND\": {\"bias\": \"LONG_ONLY\", \"use_contrarian\": False, \"sl_multiplier\": 1.5, \"tp_multiplier\": 2.0},\n",
        "            \"STRONG_DOWNTREND\": {\"bias\": \"SHORT_ONLY\", \"use_contrarian\": False, \"sl_multiplier\": 1.5, \"tp_multiplier\": 2.0},\n",
        "            \"RANGING\": {\"bias\": \"MEAN_REVERT\", \"use_contrarian\": True, \"sl_multiplier\": 1.0, \"tp_multiplier\": 1.5 if is_weekend else 1.0},\n",
        "            \"WEAK_UPTREND\": {\"bias\": \"NEUTRAL\", \"use_contrarian\": False, \"sl_multiplier\": 1.2, \"tp_multiplier\": 1.5},\n",
        "            \"WEAK_DOWNTREND\": {\"bias\": \"NEUTRAL\", \"use_contrarian\": False, \"sl_multiplier\": 1.2, \"tp_multiplier\": 1.5}\n",
        "        }\n",
        "        strategy = strategies.get(regime, strategies[\"RANGING\"])\n",
        "        if is_weekend and regime == \"RANGING\":\n",
        "            strategy[\"use_contrarian\"] = True\n",
        "            strategy[\"sl_multiplier\"] = 1.5\n",
        "            strategy[\"tp_multiplier\"] = 1.5\n",
        "        return strategy\n",
        "\n",
        "    @staticmethod\n",
        "    def get_optimal_pairs_for_session(session: str) -> List[str]:\n",
        "        session_pairs = {\n",
        "            \"ASIA_SESSION\": [\"USD/JPY\", \"AUD/USD\"],\n",
        "            \"LONDON_SESSION\": [\"EUR/USD\", \"GBP/USD\"],\n",
        "            \"OVERLAP_SESSION\": [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\"],\n",
        "            \"NY_SESSION\": [\"EUR/USD\", \"USD/JPY\"],\n",
        "            \"DEAD_ZONE\": []\n",
        "        }\n",
        "        return session_pairs.get(session, list(PAIRS))\n",
        "\n",
        "    @staticmethod\n",
        "    def should_trade_pair_now(pair: str, df_1h: pd.DataFrame, regimes: Dict) -> Tuple[bool, str]:\n",
        "        session = regimes['session']\n",
        "        vol_regime = regimes['volatility']\n",
        "        if session == \"DEAD_ZONE\": return False, \"Dead zone (21:00-00:00 UTC)\"\n",
        "        if vol_regime == \"EXTREME_VOL\": return False, \"Extreme volatility detected\"\n",
        "        optimal_pairs = RegimeStrategy.get_optimal_pairs_for_session(session)\n",
        "        if optimal_pairs and pair not in optimal_pairs: return False, f\"Pair not optimal for {session}\"\n",
        "        return True, \"All regime checks passed\"\n",
        "\n",
        "class RegimePerformanceTracker:\n",
        "    \"\"\"Track performance by regime type\"\"\"\n",
        "    def __init__(self):\n",
        "        self.stats = self.load()\n",
        "\n",
        "    def load(self) -> Dict:\n",
        "        if REGIME_STATS.exists():\n",
        "            try:\n",
        "                with open(REGIME_STATS, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except: pass\n",
        "        return defaultdict(lambda: {\"wins\": 0, \"total\": 0, \"pnl\": 0.0})\n",
        "\n",
        "    def save(self):\n",
        "        try:\n",
        "            with open(REGIME_STATS, 'w') as f:\n",
        "                json.dump(dict(self.stats), f, indent=2)\n",
        "        except Exception as e:\n",
        "            log(f\"Failed to save regime stats: {e}\", \"warn\")\n",
        "\n",
        "    def record(self, regimes: Dict, pnl: float, hit_tp: bool):\n",
        "        for regime_type, regime_value in regimes.items():\n",
        "            key = f\"{regime_type}_{regime_value}\"\n",
        "            if key not in self.stats:\n",
        "                self.stats[key] = {\"wins\": 0, \"total\": 0, \"pnl\": 0.0}\n",
        "            self.stats[key][\"total\"] += 1\n",
        "            self.stats[key][\"pnl\"] += pnl\n",
        "            if hit_tp: self.stats[key][\"wins\"] += 1\n",
        "        self.save()\n",
        "\n",
        "    def get_win_rate(self, regime_type: str, regime_value: str) -> float:\n",
        "        key = f\"{regime_type}_{regime_value}\"\n",
        "        if key in self.stats and self.stats[key][\"total\"] > 0:\n",
        "            return self.stats[key][\"wins\"] / self.stats[key][\"total\"]\n",
        "        return 0.0\n",
        "\n",
        "    def get_best_regimes(self, min_samples: int = 10) -> List[Tuple[str, float]]:\n",
        "        results = []\n",
        "        for key, data in self.stats.items():\n",
        "            if data[\"total\"] >= min_samples:\n",
        "                win_rate = data[\"wins\"] / data[\"total\"]\n",
        "                results.append((key, win_rate))\n",
        "        return sorted(results, key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PERSISTENCE (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "class Persist:\n",
        "    @staticmethod\n",
        "    def save(path: Path, data: Any, compress=True) -> bool:\n",
        "        try:\n",
        "            if path.exists():\n",
        "                backup = DIRS[\"backup\"] / f\"{path.stem}_backup{path.suffix}\"\n",
        "                try: shutil.copy2(path, backup)\n",
        "                except: pass\n",
        "            tmp = path.parent / f\".tmp_{path.name}\"\n",
        "            opener = gzip.open if compress else open\n",
        "            mode = 'wt' if compress else 'w'\n",
        "            with opener(tmp, mode, encoding='utf-8') as f:\n",
        "                json.dump(data, f, indent=2, default=str)\n",
        "            tmp.replace(path)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            log(f\"Save failed {path.name}: {e}\", \"error\")\n",
        "            if tmp.exists(): tmp.unlink(missing_ok=True)\n",
        "            return False\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path: Path, default=None, compress=True) -> Any:\n",
        "        if not path.exists():\n",
        "            backup = DIRS[\"backup\"] / f\"{path.stem}_backup{path.suffix}\"\n",
        "            path = backup if backup.exists() else path\n",
        "            if not path.exists(): return default\n",
        "        try:\n",
        "            opener = gzip.open if compress else open\n",
        "            mode = 'rt' if compress else 'r'\n",
        "            with opener(path, mode, encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        except: return default\n",
        "\n",
        "P = Persist()\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# DATA CLASSES (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "@dataclass\n",
        "class Experience:\n",
        "    state: List[float]\n",
        "    action: int\n",
        "    reward: float\n",
        "    next_state: List[float]\n",
        "    done: bool\n",
        "    metadata: Dict = field(default_factory=dict)\n",
        "    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n",
        "    def to_dict(self): return asdict(self)\n",
        "    @classmethod\n",
        "    def from_dict(cls, d): return cls(**d)\n",
        "\n",
        "@dataclass\n",
        "class TradeOutcome:\n",
        "    pair: str\n",
        "    action: str\n",
        "    entry_price: float\n",
        "    exit_price: float\n",
        "    sl: float\n",
        "    tp: float\n",
        "    position_size: float\n",
        "    pnl: float\n",
        "    duration: float\n",
        "    hit_tp: bool\n",
        "    timestamp_entry: str\n",
        "    timestamp_exit: str\n",
        "    state_at_entry: List[float]\n",
        "    confidence: float\n",
        "    regime: str\n",
        "    session: str\n",
        "    regimes: Dict = field(default_factory=dict)\n",
        "    quality_score: float = 0.0  # NEW\n",
        "\n",
        "is_weekend = lambda: datetime.now().weekday() in [5, 6]\n",
        "get_mode = lambda: \"WEEKEND_LEARNING\" if is_weekend() else \"LIVE_TRADING\"\n",
        "\n",
        "def load_iter():\n",
        "    data = P.load(ITER_FILE, compress=False)\n",
        "    if not data or not isinstance(data, dict) or 'total' not in data:\n",
        "        return {'total': 0, 'start_date': datetime.now(timezone.utc).isoformat(), 'history': []}\n",
        "    return data\n",
        "\n",
        "def inc_iter():\n",
        "    data = load_iter()\n",
        "    data['total'] += 1\n",
        "    data['last_update'] = datetime.now(timezone.utc).isoformat()\n",
        "    data['history'].append({'iteration': data['total'], 'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "                           'env': ENV_NAME, 'mode': get_mode()})\n",
        "    if len(data['history']) > 1000: data['history'] = data['history'][-1000:]\n",
        "    P.save(ITER_FILE, data, compress=False)\n",
        "    return data['total']\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PIPELINE DATABASE (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "class PipelineDB:\n",
        "    def __init__(self):\n",
        "        self.conn = None\n",
        "        if not DB_FILE.exists():\n",
        "            log(f\"Pipeline DB missing: {DB_FILE}\", \"warn\")\n",
        "            return\n",
        "        try:\n",
        "            self.conn = sqlite3.connect(str(DB_FILE), timeout=30, check_same_thread=False)\n",
        "            log(\"Pipeline DB connected\", \"db\")\n",
        "        except Exception as e:\n",
        "            log(f\"DB connect failed: {e}\", \"error\")\n",
        "\n",
        "    @contextmanager\n",
        "    def cursor(self):\n",
        "        if not self.conn:\n",
        "            yield None\n",
        "            return\n",
        "        c = self.conn.cursor()\n",
        "        try: yield c\n",
        "        finally: c.close()\n",
        "\n",
        "    def get_trades(self, since=None, limit=1000):\n",
        "        if not self.conn: return []\n",
        "        try:\n",
        "            with self.cursor() as c:\n",
        "                q = '''SELECT pair, timeframe, model_used, entry_price, exit_price, sl_price, tp_price,\n",
        "                       prediction, hit_tp, pnl, pnl_percent, duration_hours, created_at, evaluated_at\n",
        "                       FROM completed_trades'''\n",
        "                if since:\n",
        "                    c.execute(q + ' WHERE evaluated_at > ? ORDER BY evaluated_at DESC LIMIT ?', (since, limit))\n",
        "                else:\n",
        "                    c.execute(q + ' ORDER BY evaluated_at DESC LIMIT ?', (limit,))\n",
        "                return c.fetchall()\n",
        "        except Exception as e:\n",
        "            log(f\"Fetch trades failed: {e}\", \"warn\")\n",
        "            return []\n",
        "\n",
        "    def get_stats(self):\n",
        "        if not self.conn: return {}\n",
        "        try:\n",
        "            with self.cursor() as c:\n",
        "                c.execute('''SELECT COUNT(*) as total, SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins,\n",
        "                    SUM(pnl) as pnl, AVG(pnl) as avg, MAX(evaluated_at) as last FROM completed_trades''')\n",
        "                r = c.fetchone()\n",
        "                if r:\n",
        "                    return {'total_trades': r[0] or 0, 'wins': r[1] or 0, 'total_pnl': r[2] or 0.0,\n",
        "                           'avg_pnl': r[3] or 0.0, 'win_rate': (r[1]/r[0]*100) if r[0] else 0.0, 'last_trade': r[4]}\n",
        "        except: pass\n",
        "        return {}\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn: self.conn.close()\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# TECHNICAL INDICATORS (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "def calc_rsi(prices: pd.Series, per=14) -> pd.Series:\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(per, min_periods=1).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(per, min_periods=1).mean()\n",
        "    return 100 - (100 / (1 + gain / (loss + EPS)))\n",
        "\n",
        "def calc_macd(prices: pd.Series, fast=12, slow=26, sig=9):\n",
        "    ema_f = prices.ewm(span=fast, adjust=False).mean()\n",
        "    ema_s = prices.ewm(span=slow, adjust=False).mean()\n",
        "    macd = ema_f - ema_s\n",
        "    signal = macd.ewm(span=sig, adjust=False).mean()\n",
        "    return macd, signal, macd - signal\n",
        "\n",
        "def calc_bb(prices: pd.Series, per=20, std=2):\n",
        "    sma = prices.rolling(per, min_periods=1).mean()\n",
        "    s = prices.rolling(per, min_periods=1).std()\n",
        "    return sma + (s * std), sma, sma - (s * std)\n",
        "\n",
        "def create_state(df_1h: pd.DataFrame, df_1d: pd.DataFrame, pair: str, regimes: Dict) -> np.ndarray:\n",
        "    if len(df_1h) < 50 or len(df_1d) < 30: return np.zeros(STATE_SIZE)\n",
        "    feat = []\n",
        "    try:\n",
        "        close = df_1h['close'].iloc[-1]\n",
        "        h20, l20 = df_1h['high'].iloc[-20:].max(), df_1h['low'].iloc[-20:].min()\n",
        "        feat.append((close - l20) / (h20 - l20 + EPS))\n",
        "        feat.extend(df_1h['close'].pct_change().iloc[-5:].values)\n",
        "        feat.extend([calc_rsi(df_1h['close']).iloc[-1]/100, calc_rsi(df_1d['close']).iloc[-1]/100])\n",
        "        macd, sig, _ = calc_macd(df_1h['close'])\n",
        "        feat.extend([np.tanh(macd.iloc[-1]*100), np.tanh(sig.iloc[-1]*100)])\n",
        "        upper, mid, lower = calc_bb(df_1h['close'])\n",
        "        feat.extend([(close-lower.iloc[-1])/(upper.iloc[-1]-lower.iloc[-1]+EPS),\n",
        "                    (upper.iloc[-1]-lower.iloc[-1])/mid.iloc[-1]])\n",
        "        atr = df_1h['atr'].iloc[-1]\n",
        "        feat.extend([atr/(df_1h['atr'].rolling(20).mean().iloc[-1]+EPS), df_1h['close'].pct_change().std()*100])\n",
        "        ema_f = df_1h['close'].ewm(span=12).mean().iloc[-1]\n",
        "        ema_s = df_1h['close'].ewm(span=26).mean().iloc[-1]\n",
        "        ema_f1d = df_1d['close'].ewm(span=12).mean().iloc[-1]\n",
        "        ema_s1d = df_1d['close'].ewm(span=26).mean().iloc[-1]\n",
        "        feat.extend([(ema_f-ema_s)/ema_s*10, (ema_f1d-ema_s1d)/ema_s1d*10,\n",
        "                    (df_1h['close'].iloc[-1]-df_1h['close'].iloc[-20])/df_1h['close'].iloc[-20]*10])\n",
        "        vol = 1.0\n",
        "        if 'volume' in df_1h.columns and df_1h['volume'].sum() > 0:\n",
        "            vol = df_1h['volume'].iloc[-5:].mean()/(df_1h['volume'].iloc[-50:].mean()+EPS)\n",
        "        feat.append(vol)\n",
        "        h = datetime.now().hour\n",
        "        feat.extend([1.0 if 0<=h<8 else 0.0, 1.0 if 8<=h<16 else 0.0, 1.0 if 16<=h<24 else 0.0])\n",
        "        feat.extend([datetime.now().weekday()/6.0, h/23.0])\n",
        "        closes = df_1h['close'].values[-20:]\n",
        "        mom = (closes[-1]-closes[-10])/(closes[-10]+EPS)\n",
        "        vol_val = np.std(closes)/(np.mean(closes)+EPS)\n",
        "        trend = (closes[-1]-closes[0])/(closes[0]+EPS)\n",
        "        feat.extend([np.tanh(mom*10), vol_val, np.tanh(trend*10)])\n",
        "        regime_vol = {'LOW_VOL': 0.0, 'NORMAL_VOL': 0.33, 'HIGH_VOL': 0.66, 'EXTREME_VOL': 1.0}\n",
        "        feat.append(regime_vol.get(regimes.get('volatility', 'NORMAL_VOL'), 0.33))\n",
        "        regime_trend = {'STRONG_DOWNTREND': -1.0, 'WEAK_DOWNTREND': -0.5, 'RANGING': 0.0,\n",
        "                       'WEAK_UPTREND': 0.5, 'STRONG_UPTREND': 1.0}\n",
        "        feat.append(regime_trend.get(regimes.get('trend', 'RANGING'), 0.0))\n",
        "        regime_session = {'DEAD_ZONE': 0.0, 'ASIA_SESSION': 0.25, 'LONDON_SESSION': 0.5,\n",
        "                         'OVERLAP_SESSION': 0.75, 'NY_SESSION': 1.0}\n",
        "        feat.append(regime_session.get(regimes.get('session', 'DEAD_ZONE'), 0.0))\n",
        "        regime_behavior = {'MEAN_REVERT': -1.0, 'NEUTRAL': 0.0, 'MOMENTUM': 1.0}\n",
        "        feat.append(regime_behavior.get(regimes.get('behavior', 'NEUTRAL'), 0.0))\n",
        "        feat.append(1.0 if is_weekend() else 0.0)\n",
        "        feat = feat[:STATE_SIZE]\n",
        "        while len(feat) < STATE_SIZE: feat.append(0.0)\n",
        "        return np.array(feat, dtype=np.float32)\n",
        "    except:\n",
        "        return np.zeros(STATE_SIZE)\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# Q-NETWORK (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "class QNet:\n",
        "    def __init__(self, state_size=STATE_SIZE, action_size=ACTIONS):\n",
        "        self.ss, self.as_ = state_size, action_size\n",
        "        h1, h2, h3, h4 = 192, 96, 48, 24\n",
        "        self.w1 = np.random.randn(state_size, h1) * np.sqrt(1/state_size)\n",
        "        self.b1 = np.zeros(h1)\n",
        "        self.w2 = np.random.randn(h1, h2) * np.sqrt(1/h1)\n",
        "        self.b2 = np.zeros(h2)\n",
        "        self.w3 = np.random.randn(h2, h3) * np.sqrt(1/h2)\n",
        "        self.b3 = np.zeros(h3)\n",
        "        self.w4 = np.random.randn(h3, h4) * np.sqrt(1/h3)\n",
        "        self.b4 = np.zeros(h4)\n",
        "        self.w5 = np.random.randn(h4, action_size) * np.sqrt(1/h4)\n",
        "        self.b5 = np.zeros(action_size)\n",
        "\n",
        "    def relu(self, x): return np.maximum(0, x)\n",
        "\n",
        "    def forward(self, s):\n",
        "        h1 = self.relu(np.dot(s, self.w1) + self.b1)\n",
        "        h2 = self.relu(np.dot(h1, self.w2) + self.b2)\n",
        "        h3 = self.relu(np.dot(h2, self.w3) + self.b3)\n",
        "        h4 = self.relu(np.dot(h3, self.w4) + self.b4)\n",
        "        return np.dot(h4, self.w5) + self.b5\n",
        "\n",
        "    def predict(self, s):\n",
        "        return self.forward(s[0] if s.ndim > 1 else s)\n",
        "\n",
        "    def update(self, states, targets, lr=LR):\n",
        "        for s, tgt in zip(states, targets):\n",
        "            h1 = self.relu(np.dot(s, self.w1) + self.b1)\n",
        "            h2 = self.relu(np.dot(h1, self.w2) + self.b2)\n",
        "            h3 = self.relu(np.dot(h2, self.w3) + self.b3)\n",
        "            h4 = self.relu(np.dot(h3, self.w4) + self.b4)\n",
        "            q = np.dot(h4, self.w5) + self.b5\n",
        "            err = np.clip(q - tgt, -1, 1)\n",
        "            dw5 = np.clip(np.outer(h4, err), -1, 1)\n",
        "            dh4 = np.dot(err, self.w5.T) * (h4 > 0)\n",
        "            dw4 = np.clip(np.outer(h3, dh4), -1, 1)\n",
        "            dh3 = np.dot(dh4, self.w4.T) * (h3 > 0)\n",
        "            dw3 = np.clip(np.outer(h2, dh3), -1, 1)\n",
        "            dh2 = np.dot(dh3, self.w3.T) * (h2 > 0)\n",
        "            dw2 = np.clip(np.outer(h1, dh2), -1, 1)\n",
        "            dh1 = np.dot(dh2, self.w2.T) * (h1 > 0)\n",
        "            dw1 = np.clip(np.outer(s, dh1), -1, 1)\n",
        "            self.w5 -= lr * dw5\n",
        "            self.b5 -= lr * np.clip(err, -1, 1)\n",
        "            self.w4 -= lr * dw4\n",
        "            self.b4 -= lr * np.clip(dh4, -1, 1)\n",
        "            self.w3 -= lr * dw3\n",
        "            self.b3 -= lr * np.clip(dh3, -1, 1)\n",
        "            self.w2 -= lr * dw2\n",
        "            self.b2 -= lr * np.clip(dh2, -1, 1)\n",
        "            self.w1 -= lr * dw1\n",
        "            self.b1 -= lr * np.clip(dh1, -1, 1)\n",
        "\n",
        "    def clone(self):\n",
        "        new = QNet(self.ss, self.as_)\n",
        "        for attr in ['w1','b1','w2','b2','w3','b3','w4','b4','w5','b5']:\n",
        "            setattr(new, attr, getattr(self, attr).copy())\n",
        "        return new\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: getattr(self, k).tolist() for k in ['w1','b1','w2','b2','w3','b3','w4','b4','w5','b5']}\n",
        "\n",
        "    def from_dict(self, d):\n",
        "        try:\n",
        "            for k in ['w1','b1','w2','b2','w3','b3','w4','b4','w5','b5']:\n",
        "                setattr(self, k, np.array(d[k]))\n",
        "            return True\n",
        "        except: return False\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# RBED & REPLAY BUFFER (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "class RBED:\n",
        "    \"\"\"Reward-Based Epsilon Decay\"\"\"\n",
        "    def __init__(self):\n",
        "        self.eps = 0.7\n",
        "        self.min_eps = 0.10\n",
        "        self.base_decay = 0.9985\n",
        "        self.milestone_decay = 0.95\n",
        "        self.thresh = 0.0\n",
        "        self.inc = 50.0\n",
        "        self.updates = 0\n",
        "        self.last_log = 0\n",
        "        self.pause_decay = False\n",
        "\n",
        "    def update(self, pnl: float, n: int, win_rate: float = 0.0) -> float:\n",
        "        if n < 20: return self.eps\n",
        "        if n >= 50 and win_rate < 0.15:\n",
        "            if not self.pause_decay:\n",
        "                log(f\"‚ö†Ô∏è Win rate {win_rate*100:.1f}% - Pausing epsilon decay\", \"warn\")\n",
        "                self.pause_decay = True\n",
        "            return self.eps\n",
        "        elif self.pause_decay and win_rate >= 0.20:\n",
        "            log(f\"‚úÖ Win rate {win_rate*100:.1f}% - Resuming epsilon decay\", \"success\")\n",
        "            self.pause_decay = False\n",
        "        if self.pause_decay: return self.eps\n",
        "        self.eps = max(self.min_eps, self.eps * self.base_decay)\n",
        "        self.updates += 1\n",
        "        milestone_hit = False\n",
        "        if pnl >= self.thresh:\n",
        "            self.eps = max(self.min_eps, self.eps * self.milestone_decay)\n",
        "            self.thresh += self.inc\n",
        "            milestone_hit = True\n",
        "            log(f\"üéØ Milestone ${self.thresh - self.inc:.0f}‚Üí${self.thresh:.0f} | Œµ={self.eps:.4f}\", \"success\")\n",
        "        if milestone_hit or self.updates - self.last_log >= 10:\n",
        "            progress_pct = ((0.7 - self.eps) / (0.7 - self.min_eps)) * 100\n",
        "            log(f\"üìâ Epsilon: {self.eps:.4f} | Progress: {progress_pct:.1f}%\", \"info\")\n",
        "            self.last_log = self.updates\n",
        "        return self.eps\n",
        "\n",
        "class PriorityReplay:\n",
        "    def __init__(self, cap=MEM_CAP, alpha=0.6):\n",
        "        self.cap, self.alpha = cap, alpha\n",
        "        self.buf, self.pri, self.pos = [], [], 0\n",
        "\n",
        "    def add(self, exp, td=1.0):\n",
        "        p = (abs(td) + 0.01) ** self.alpha\n",
        "        if len(self.buf) < self.cap:\n",
        "            self.buf.append(exp)\n",
        "            self.pri.append(p)\n",
        "        else:\n",
        "            self.buf[self.pos], self.pri[self.pos] = exp, p\n",
        "            self.pos = (self.pos + 1) % self.cap\n",
        "\n",
        "    def sample(self, n):\n",
        "        if len(self.buf) < n: return []\n",
        "        win_idx = [i for i, e in enumerate(self.buf) if e.metadata.get('pnl', 0) > 0]\n",
        "        num_win = min(n//3, len(win_idx))\n",
        "        samples = random.sample(win_idx, num_win) if win_idx else []\n",
        "        rem = n - len(samples)\n",
        "        if rem > 0:\n",
        "            loss_idx = list(set(range(len(self.buf))) - set(samples))\n",
        "            if loss_idx:\n",
        "                pri = np.array(self.pri)\n",
        "                probs = pri[loss_idx] / pri[loss_idx].sum()\n",
        "                samples.extend(np.random.choice(loss_idx, size=rem, replace=False, p=probs))\n",
        "        return [self.buf[i] for i in samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buf)\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# CONFIDENCE SYSTEM (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "class Confidence:\n",
        "    def __init__(self):\n",
        "        self.temp = 1.0\n",
        "\n",
        "    def softmax(self, q):\n",
        "        return np.exp((q-np.max(q))/self.temp) / np.sum(np.exp((q-np.max(q))/self.temp))\n",
        "\n",
        "    def get_conf(self, q: np.ndarray, eps: float, regimes: Dict = None, force=False) -> Tuple[bool, float, Dict]:\n",
        "        sq = np.sort(q)[::-1]\n",
        "        spread = sq[0] - sq[1] if len(sq) > 1 else 0.0\n",
        "        probs = self.softmax(q)\n",
        "        best = np.max(probs)\n",
        "        ent = -np.sum(np.clip(probs, 1e-10, 1) * np.log(np.clip(probs, 1e-10, 1)))\n",
        "        norm_ent = ent / np.log(len(q))\n",
        "        conf = (0.5*best + 0.3*(1-norm_ent) + 0.2*np.tanh(spread*5)) * 100\n",
        "        if regimes:\n",
        "            vol_regime = regimes.get('volatility', 'NORMAL_VOL')\n",
        "            vol_adj = RegimeStrategy.adjust_for_volatility(vol_regime, 1.0, 1.0)\n",
        "            base_thresh = vol_adj['confidence_threshold'] * 100\n",
        "        else:\n",
        "            base_thresh = 65.0\n",
        "        prog = 1 - (eps - 0.10) / 0.6\n",
        "        if force: thresh = 5.0\n",
        "        elif eps > 0.6: thresh = max(10.0, base_thresh * 0.5)\n",
        "        elif eps > 0.4: thresh = max(14.0, base_thresh * 0.7)\n",
        "        elif eps > 0.25: thresh = max(18.0, base_thresh * 0.85)\n",
        "        elif eps > 0.15: thresh = max(21.0, base_thresh * 0.95)\n",
        "        else: thresh = base_thresh\n",
        "        metrics = {'q_spread': float(spread), 'best_prob': float(best), 'entropy': float(norm_ent),\n",
        "                  'confidence': float(np.clip(conf, 0, 100)), 'threshold': float(thresh), 'progress': float(prog)}\n",
        "        trade = conf >= thresh or spread >= 0.03 or force\n",
        "        return trade, conf, metrics\n",
        "\n",
        "    def calc_size(self, base: float, conf: float) -> float:\n",
        "        return base * (0.5 + (conf/100) * 0.5)\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# RL AGENT WITH QUALITY INTEGRATION (v21.1 - ENHANCED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "class RLAgent:\n",
        "    def __init__(self):\n",
        "        self.qnet = QNet()\n",
        "        self.tnet = QNet()\n",
        "        self.mem = PriorityReplay()\n",
        "        self.rbed = RBED()\n",
        "        self.regime_tracker = RegimePerformanceTracker()\n",
        "        self.cnt = 0\n",
        "        self.stats = {\n",
        "            'total_updates': 0, 'total_trades': 0, 'profitable_trades': 0, 'total_pnl': 0.0,\n",
        "            'win_rate': 0.0, 'avg_reward': 0.0, 'epsilon_history': [], 'q_value_history': [],\n",
        "            'pipeline_trades_learned': 0, 'last_pipeline_sync': None, 'rbed_milestones': [],\n",
        "            'consecutive_losses': 0, 'pipeline_v6_learned': 0, 'live_validated': 0,\n",
        "            'backtest_learned': 0, 'total_learning_sources': 0, 'pipeline_v6_contrarian': 0,\n",
        "            'regime_filtered_trades': 0, 'regime_stats': {},\n",
        "            'quality_filtered_trades': 0  # NEW\n",
        "        }\n",
        "        self.consec_loss = 0\n",
        "        self.load()\n",
        "        log(f\"üß† Agent init: {len(self.mem)} exp\", \"brain\")\n",
        "\n",
        "    def select(self, s, greedy=False, bt=False, regimes=None):\n",
        "        eps = self.rbed.update(self.stats['total_pnl'], self.stats['total_trades'], self.stats['win_rate'])\n",
        "        if bt: eps = 0.3\n",
        "        if not greedy and random.random() < eps:\n",
        "            return random.randint(0, ACTIONS-1)\n",
        "        q = self.qnet.predict(s)\n",
        "        if regimes and CONFIG.get('regime_filters', True):\n",
        "            trend = regimes.get('trend', 'RANGING')\n",
        "            strategy = RegimeStrategy.select_strategy_for_trend(trend, is_weekend())\n",
        "            if strategy['bias'] == 'LONG_ONLY' and np.argmax(q) == 1:\n",
        "                q[1] = -999\n",
        "            elif strategy['bias'] == 'SHORT_ONLY' and np.argmax(q) == 0:\n",
        "                q[0] = -999\n",
        "        if bt and np.max(q) - np.sort(q)[-2] < 0.1:\n",
        "            q[2] = -999\n",
        "        return int(np.argmax(q))\n",
        "\n",
        "    def remember(self, exp, td=1.0):\n",
        "        self.mem.add(exp, td)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.mem) < 200: return\n",
        "        batch = self.mem.sample(min(BATCH, len(self.mem)))\n",
        "        if not batch: return\n",
        "        states = np.array([np.array(e.state) for e in batch])\n",
        "        actions = np.array([e.action for e in batch])\n",
        "        rewards = np.array([e.reward for e in batch])\n",
        "        next_states = np.array([np.array(e.next_state) for e in batch])\n",
        "        dones = np.array([e.done for e in batch])\n",
        "        curr_q = np.array([self.qnet.forward(s) for s in states])\n",
        "        next_q = np.array([self.tnet.forward(s) for s in next_states])\n",
        "        tgts = curr_q.copy()\n",
        "        clipped_targets = []\n",
        "        for i in range(len(batch)):\n",
        "            if dones[i]:\n",
        "                target = rewards[i]\n",
        "            else:\n",
        "                target = rewards[i] + GAMMA * np.max(next_q[i])\n",
        "            target = np.clip(target, Q_CLIP_MIN, Q_CLIP_MAX)\n",
        "            clipped_targets.append(target)\n",
        "            tgts[i][actions[i]] = target\n",
        "        self.qnet.update(states, tgts, LR)\n",
        "        self.cnt += 1\n",
        "        self.stats['total_updates'] += 1\n",
        "        avg_q = float(np.mean(clipped_targets))\n",
        "        self.stats['q_value_history'].append(avg_q)\n",
        "        self.stats['epsilon_history'].append(self.rbed.eps)\n",
        "        if self.cnt % TARGET_UPD == 0:\n",
        "            self.tnet = self.qnet.clone()\n",
        "            log(f\"üéØ Target network updated #{self.cnt}\", \"brain\")\n",
        "\n",
        "    def calc_reward(self, t: TradeOutcome) -> float:\n",
        "        r = 0.0\n",
        "        if t.pnl > 0:\n",
        "            r += t.pnl * PROFIT_SCALE + WIN_BONUS\n",
        "        else:\n",
        "            r += t.pnl * LOSS_SCALE - LOSS_PEN\n",
        "        risk = abs(t.entry_price - t.sl) + EPS\n",
        "        r += (t.pnl / risk) * SHARPE_SCALE\n",
        "        if t.pnl < 0 and t.duration > 48:\n",
        "            r -= 5\n",
        "        elif t.pnl > 0 and t.duration < 24:\n",
        "            r += 3\n",
        "        if t.hit_tp:\n",
        "            r += WIN_BONUS * 0.5\n",
        "        if t.pnl < 0:\n",
        "            self.consec_loss += 1\n",
        "            r -= self.consec_loss * 2\n",
        "        else:\n",
        "            self.consec_loss = 0\n",
        "        if t.regimes:\n",
        "            if t.regimes.get('trend') == 'RANGING' and t.pnl > 0:\n",
        "                r += 5\n",
        "            if t.regimes.get('volatility') == 'NORMAL_VOL' and t.pnl > 0:\n",
        "                r += 3\n",
        "        # NEW: Quality score bonus\n",
        "        if t.quality_score > 85:\n",
        "            r += 5\n",
        "            log(f\"‚≠ê High quality signal bonus: +5\", \"quality\")\n",
        "        self.stats['consecutive_losses'] = self.consec_loss\n",
        "        return float(np.clip(r, -200, 200))\n",
        "\n",
        "    def record(self, t: TradeOutcome):\n",
        "        self.stats['total_trades'] += 1\n",
        "        self.stats['total_pnl'] += t.pnl\n",
        "        if t.pnl > 0: self.stats['profitable_trades'] += 1\n",
        "        self.stats['win_rate'] = self.stats['profitable_trades'] / self.stats['total_trades']\n",
        "        if t.regimes:\n",
        "            self.regime_tracker.record(t.regimes, t.pnl, t.hit_tp)\n",
        "        r = self.calc_reward(t)\n",
        "        self.stats['avg_reward'] = (self.stats['avg_reward']*(self.stats['total_trades']-1)+r)/self.stats['total_trades']\n",
        "        act = 0 if t.action == 'BUY' else 1 if t.action == 'SELL' else 2\n",
        "        exp = Experience(state=t.state_at_entry if isinstance(t.state_at_entry, list) else t.state_at_entry.tolist(),\n",
        "            action=act, reward=r, next_state=t.state_at_entry if isinstance(t.state_at_entry, list) else t.state_at_entry.tolist(),\n",
        "            done=True, metadata={'pair': t.pair, 'pnl': t.pnl, 'hit_tp': t.hit_tp, 'duration': t.duration,\n",
        "                                'source': t.regime, 'regimes': t.regimes, 'quality_score': t.quality_score})\n",
        "        td = abs(r - self.qnet.predict(np.array(exp.state))[act])\n",
        "        self.remember(exp, td)\n",
        "        if len(self.mem) >= 200: self.learn()\n",
        "\n",
        "    def learn_from_pipeline_v6(self, learning_db_path: Path) -> int:\n",
        "        if not learning_db_path.exists():\n",
        "            return 0\n",
        "        try:\n",
        "            with open(learning_db_path, 'r') as f:\n",
        "                outcomes = json.load(f)\n",
        "            learned = 0\n",
        "            contrarian_learned = 0\n",
        "            for outcome in outcomes:\n",
        "                try:\n",
        "                    if 'features' not in outcome or 'prediction' not in outcome:\n",
        "                        continue\n",
        "                    features = outcome['features']\n",
        "                    prediction = outcome['prediction']\n",
        "                    pnl = outcome.get('pnl', 0.0)\n",
        "                    hit_tp = outcome.get('hit_tp', False)\n",
        "                    is_contrarian = outcome.get('is_contrarian', False)\n",
        "                    if isinstance(features, list):\n",
        "                        state = features[:STATE_SIZE]\n",
        "                        while len(state) < STATE_SIZE:\n",
        "                            state.append(0.0)\n",
        "                    else:\n",
        "                        state = [0.0] * STATE_SIZE\n",
        "                    if prediction.upper() == 'BUY':\n",
        "                        action = 0\n",
        "                    elif prediction.upper() == 'SELL':\n",
        "                        action = 1\n",
        "                    else:\n",
        "                        continue\n",
        "                    if hit_tp:\n",
        "                        reward = abs(pnl) * PROFIT_SCALE + WIN_BONUS\n",
        "                    elif pnl > 0:\n",
        "                        reward = pnl * PROFIT_SCALE\n",
        "                    else:\n",
        "                        reward = pnl * LOSS_SCALE - LOSS_PEN\n",
        "                    reward = float(np.clip(reward, -800, 800))\n",
        "                    exp = Experience(\n",
        "                        state=state,\n",
        "                        action=action,\n",
        "                        reward=reward,\n",
        "                        next_state=state,\n",
        "                        done=True,\n",
        "                        metadata={\n",
        "                            'pair': outcome.get('pair', 'UNKNOWN'),\n",
        "                            'pnl': pnl,\n",
        "                            'hit_tp': hit_tp,\n",
        "                            'was_correct': outcome.get('was_correct', False),\n",
        "                            'source': 'PIPELINE_V6',\n",
        "                            'is_contrarian': is_contrarian\n",
        "                        }\n",
        "                    )\n",
        "                    td = abs(reward)\n",
        "                    self.remember(exp, td)\n",
        "                    learned += 1\n",
        "                    if is_contrarian:\n",
        "                        contrarian_learned += 1\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "            if learned > 0:\n",
        "                self.learn()\n",
        "                self.stats['pipeline_v6_learned'] = self.stats.get('pipeline_v6_learned', 0) + learned\n",
        "                self.stats['pipeline_v6_contrarian'] = self.stats.get('pipeline_v6_contrarian', 0) + contrarian_learned\n",
        "                log(f\"üìö Pipeline v6: {learned} outcomes ({contrarian_learned} contrarian)\", \"learning\")\n",
        "            return learned\n",
        "        except Exception as e:\n",
        "            log(f\"Failed to load Pipeline v6: {e}\", \"error\")\n",
        "            return 0\n",
        "\n",
        "    def learn_pipeline(self, db: PipelineDB, data: Dict) -> int:\n",
        "        since = self.stats.get('last_pipeline_sync')\n",
        "        since_ts = None\n",
        "        if since:\n",
        "            try: since_ts = int(datetime.fromisoformat(since.replace('Z','+00:00')).timestamp())\n",
        "            except: pass\n",
        "        trades = db.get_trades(since=since_ts, limit=500)\n",
        "        learned = 0\n",
        "        for row in trades:\n",
        "            try:\n",
        "                pair, tf, model, entry, exit, sl, tp, pred, hit_tp, pnl, pnl_pct, dur, created, eval = row\n",
        "                action = 'BUY' if pred.lower() == 'buy' or exit > entry else 'SELL' if pred.lower() == 'sell' or exit < entry else None\n",
        "                if not action: continue\n",
        "                state = np.zeros(STATE_SIZE)\n",
        "                if pair in data and '1h' in data[pair]:\n",
        "                    try:\n",
        "                        regimes = RegimeDetector.get_all_regimes(data[pair]['1h'], data[pair].get('1d'))\n",
        "                        state = create_state(data[pair]['1h'], data[pair]['1d'], pair, regimes)\n",
        "                    except: pass\n",
        "                t = TradeOutcome(pair=pair, action=action, entry_price=float(entry), exit_price=float(exit),\n",
        "                    sl=float(sl), tp=float(tp), position_size=1.0, pnl=float(pnl), duration=float(dur),\n",
        "                    hit_tp=bool(hit_tp), timestamp_entry=created, timestamp_exit=eval,\n",
        "                    state_at_entry=state.tolist(), confidence=0.0, regime='PIPELINE', session='BACKFILL',\n",
        "                    regimes={})\n",
        "                self.record(t)\n",
        "                learned += 1\n",
        "            except: continue\n",
        "        if trades: self.stats['last_pipeline_sync'] = trades[0][-1]\n",
        "        self.stats['pipeline_trades_learned'] = self.stats.get('pipeline_trades_learned', 0) + learned\n",
        "        log(f\"üíæ Pipeline DB: {learned} trades\", \"db\")\n",
        "        return learned\n",
        "\n",
        "    def save(self):\n",
        "        try:\n",
        "            P.save(RL_MEM, [e.to_dict() for e in list(self.mem.buf)], compress=True)\n",
        "            P.save(RL_WEIGHTS, {'q_network': self.qnet.to_dict(), 'target_network': self.tnet.to_dict()}, compress=False)\n",
        "            self.stats['regime_stats'] = dict(self.regime_tracker.stats)\n",
        "            P.save(RL_STATS, self.stats, compress=False)\n",
        "            log(f\"üíæ Saved: {len(self.mem)} exp, {self.stats['total_trades']} trades\", \"success\")\n",
        "        except Exception as e:\n",
        "            log(f\"Save failed: {e}\", \"warn\")\n",
        "\n",
        "    def load(self):\n",
        "        try:\n",
        "            mem_data = P.load(RL_MEM, compress=True)\n",
        "            if mem_data:\n",
        "                valid_exp = []\n",
        "                for e in mem_data:\n",
        "                    try:\n",
        "                        exp = Experience.from_dict(e)\n",
        "                        if len(exp.state) < STATE_SIZE:\n",
        "                            exp.state.extend([0.0] * (STATE_SIZE - len(exp.state)))\n",
        "                        elif len(exp.state) > STATE_SIZE:\n",
        "                            exp.state = exp.state[:STATE_SIZE]\n",
        "                        if len(exp.next_state) < STATE_SIZE:\n",
        "                            exp.next_state.extend([0.0] * (STATE_SIZE - len(exp.next_state)))\n",
        "                        elif len(exp.next_state) > STATE_SIZE:\n",
        "                            exp.next_state = exp.next_state[:STATE_SIZE]\n",
        "                        valid_exp.append(exp)\n",
        "                    except:\n",
        "                        continue\n",
        "                for exp in valid_exp:\n",
        "                    self.mem.add(exp, 1.0)\n",
        "                log(f\"‚úÖ Loaded {len(valid_exp)} experiences\", \"success\")\n",
        "            net_data = P.load(RL_WEIGHTS, compress=False)\n",
        "            if net_data:\n",
        "                try:\n",
        "                    q_net = net_data.get('q_network', {})\n",
        "                    if 'w1' in q_net:\n",
        "                        w1_shape = np.array(q_net['w1']).shape\n",
        "                        if w1_shape[0] != STATE_SIZE:\n",
        "                            log(f\"‚ö†Ô∏è Network dimension mismatch: {w1_shape[0]} != {STATE_SIZE}, reinitializing\", \"warn\")\n",
        "                        elif self.qnet.from_dict(q_net) and self.tnet.from_dict(net_data.get('target_network',{})):\n",
        "                            log(\"‚úÖ Loaded networks\", \"success\")\n",
        "                except Exception as e:\n",
        "                    log(f\"‚ö†Ô∏è Could not load networks: {e}, using fresh init\", \"warn\")\n",
        "            stats = P.load(RL_STATS, compress=False)\n",
        "            if stats:\n",
        "                self.stats = stats\n",
        "                if 'regime_stats' in stats:\n",
        "                    self.regime_tracker.stats = defaultdict(lambda: {\"wins\": 0, \"total\": 0, \"pnl\": 0.0}, stats['regime_stats'])\n",
        "                if self.stats.get('epsilon_history'):\n",
        "                    self.rbed.eps = self.stats['epsilon_history'][-1]\n",
        "                if self.stats.get('rbed_milestones'):\n",
        "                    self.rbed.thresh = self.stats['rbed_milestones'][-1] + 50.0 if self.stats['rbed_milestones'] else 0.0\n",
        "                log(f\"‚úÖ Stats: {self.stats['total_trades']} trades, Œµ={self.rbed.eps:.3f}\", \"success\")\n",
        "        except Exception as e:\n",
        "            log(f\"Load failed: {e}\", \"warn\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# LIVE SIGNAL VALIDATION (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "def load_previous_signals() -> Optional[Dict]:\n",
        "    if SIGNALS.exists():\n",
        "        try:\n",
        "            with open(SIGNALS, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def validate_live_signals(agent: RLAgent, old_signals: Dict, current_prices: Dict, data: Dict) -> int:\n",
        "    if not old_signals or 'signals' not in old_signals:\n",
        "        return 0\n",
        "    validated = 0\n",
        "    for pair, signal in old_signals['signals'].items():\n",
        "        try:\n",
        "            direction = signal.get('direction')\n",
        "            if not direction or direction == 'HOLD':\n",
        "                continue\n",
        "            entry_price = signal.get('last_price', 0)\n",
        "            sl = signal.get('SL', 0)\n",
        "            tp = signal.get('TP', 0)\n",
        "            if not all([entry_price, sl, tp]) or pair not in current_prices:\n",
        "                continue\n",
        "            current_price = current_prices[pair]\n",
        "            hit_tp = False\n",
        "            hit_sl = False\n",
        "            if direction == 'BUY':\n",
        "                hit_tp = current_price >= tp\n",
        "                hit_sl = current_price <= sl\n",
        "            elif direction == 'SELL':\n",
        "                hit_tp = current_price <= tp\n",
        "                hit_sl = current_price >= sl\n",
        "            if not hit_tp and not hit_sl:\n",
        "                continue\n",
        "            if direction == 'BUY':\n",
        "                exit_price = tp if hit_tp else sl\n",
        "                pnl = (exit_price - entry_price) * 1.0\n",
        "            else:\n",
        "                exit_price = tp if hit_tp else sl\n",
        "                pnl = (entry_price - exit_price) * 1.0\n",
        "            state = np.zeros(STATE_SIZE)\n",
        "            if pair in data and '1h' in data[pair] and '1d' in data[pair]:\n",
        "                try:\n",
        "                    regimes = RegimeDetector.get_all_regimes(data[pair]['1h'], data[pair]['1d'])\n",
        "                    state = create_state(data[pair]['1h'], data[pair]['1d'], pair, regimes)\n",
        "                except:\n",
        "                    regimes = {}\n",
        "            timestamp_entry = old_signals.get('timestamp', datetime.now(timezone.utc).isoformat())\n",
        "            timestamp_exit = datetime.now(timezone.utc).isoformat()\n",
        "            try:\n",
        "                entry_dt = datetime.fromisoformat(timestamp_entry.replace('Z', '+00:00'))\n",
        "                exit_dt = datetime.now(timezone.utc)\n",
        "                duration = (exit_dt - entry_dt).total_seconds() / 3600.0\n",
        "            except:\n",
        "                duration = 2.0\n",
        "            outcome = TradeOutcome(\n",
        "                pair=pair,\n",
        "                action=direction,\n",
        "                entry_price=entry_price,\n",
        "                exit_price=exit_price,\n",
        "                sl=sl,\n",
        "                tp=tp,\n",
        "                position_size=1.0,\n",
        "                pnl=pnl,\n",
        "                duration=duration,\n",
        "                hit_tp=hit_tp,\n",
        "                timestamp_entry=timestamp_entry,\n",
        "                timestamp_exit=timestamp_exit,\n",
        "                state_at_entry=state.tolist(),\n",
        "                confidence=signal.get('confidence', 0.0),\n",
        "                regime='LIVE_VALIDATION',\n",
        "                session='VALIDATION',\n",
        "                regimes=regimes if 'regimes' in locals() else {},\n",
        "                quality_score=signal.get('quality_score', 0.0)  # NEW\n",
        "            )\n",
        "            agent.record(outcome)\n",
        "            validated += 1\n",
        "            result = \"WIN\" if hit_tp else \"LOSS\"\n",
        "            log(f\"‚úÖ Validated {pair} {direction}: {result} (${pnl:.2f})\", \"success\" if pnl > 0 else \"warn\")\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    if validated > 0:\n",
        "        agent.stats['live_validated'] = agent.stats.get('live_validated', 0) + validated\n",
        "        log(f\"‚úÖ Live validation: {validated} signals\", \"learning\")\n",
        "    return validated\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# TRADING ENVIRONMENT (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "class TradingEnv:\n",
        "    def __init__(self):\n",
        "        self.active = {}\n",
        "        self.history = P.load(TRADES, compress=False) or []\n",
        "\n",
        "    def save_hist(self):\n",
        "        P.save(TRADES, self.history, compress=False)\n",
        "\n",
        "    def execute(self, pair: str, action: str, price: float, sl: float, tp: float, size: float,\n",
        "                state: np.ndarray, meta: Dict) -> str:\n",
        "        tid = f\"{pair}_{datetime.now():%Y%m%d_%H%M%S}\"\n",
        "        self.active[tid] = {'pair': pair, 'action': action, 'entry_price': price, 'sl': sl, 'tp': tp,\n",
        "            'size': size, 'entry_time': datetime.now(timezone.utc).isoformat(), 'state_at_entry': state.tolist(),\n",
        "            'metadata': meta}\n",
        "        log(f\"üí∞ {tid} - {action} {pair} @ {price:.5f}\", \"money\")\n",
        "        return tid\n",
        "\n",
        "    def check_exits(self, prices: Dict[str, float]) -> List[TradeOutcome]:\n",
        "        completed = []\n",
        "        for tid, trade in list(self.active.items()):\n",
        "            pair = trade['pair']\n",
        "            if pair not in prices: continue\n",
        "            cp = prices[pair]\n",
        "            hit_tp = hit_sl = False\n",
        "            if trade['action'] == 'BUY':\n",
        "                hit_tp, hit_sl = cp >= trade['tp'], cp <= trade['sl']\n",
        "            else:\n",
        "                hit_tp, hit_sl = cp <= trade['tp'], cp >= trade['sl']\n",
        "            if hit_tp or hit_sl:\n",
        "                exit_p = trade['tp'] if hit_tp else trade['sl']\n",
        "                pnl = ((exit_p-trade['entry_price']) if trade['action']=='BUY' else (trade['entry_price']-exit_p))*trade['size']\n",
        "                pnl -= exit_p * 0.0003 + exit_p * trade['size'] * 0.0005\n",
        "                entry_t = datetime.fromisoformat(trade['entry_time'])\n",
        "                exit_t = datetime.now(timezone.utc)\n",
        "                dur = (exit_t - entry_t).total_seconds() / 3600.0\n",
        "                out = TradeOutcome(pair=pair, action=trade['action'], entry_price=trade['entry_price'],\n",
        "                    exit_price=exit_p, sl=trade['sl'], tp=trade['tp'], position_size=trade['size'],\n",
        "                    pnl=pnl, duration=dur, hit_tp=hit_tp, timestamp_entry=trade['entry_time'],\n",
        "                    timestamp_exit=exit_t.isoformat(), state_at_entry=trade['state_at_entry'],\n",
        "                    confidence=trade['metadata'].get('confidence',0), regime=trade['metadata'].get('regime','UNKNOWN'),\n",
        "                    session=trade['metadata'].get('session','UNKNOWN'), regimes=trade['metadata'].get('regimes', {}),\n",
        "                    quality_score=trade['metadata'].get('quality_score', 0.0))  # NEW\n",
        "                completed.append(out)\n",
        "                self.history.append({'trade_id': tid, 'pair': pair, 'action': trade['action'],\n",
        "                    'entry': trade['entry_price'], 'exit': exit_p, 'pnl': pnl,\n",
        "                    'result': 'WIN' if hit_tp else 'LOSS', 'duration_hours': dur,\n",
        "                    'timestamp': exit_t.isoformat(), 'regimes': trade['metadata'].get('regimes', {}),\n",
        "                    'quality_score': trade['metadata'].get('quality_score', 0.0)})  # NEW\n",
        "                del self.active[tid]\n",
        "                log(f\"‚úÖ {tid} - {'WIN' if hit_tp else 'LOSS'} | ${pnl:.2f}\", \"success\" if pnl>0 else \"warn\")\n",
        "        if completed: self.save_hist()\n",
        "        return completed\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# WEEKEND BACKTEST (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "def backtest(data: Dict, agent: RLAgent, conf: Confidence):\n",
        "    log(\"\\nüéì Weekend backtest with regime detection...\", \"brain\")\n",
        "    learned = 0\n",
        "    for pair in PAIRS:\n",
        "        if pair not in data or '1h' not in data[pair]:\n",
        "            continue\n",
        "        df_1h = data[pair]['1h']\n",
        "        df_1d = data[pair].get('1d')\n",
        "        if df_1d is None or len(df_1d) < 30:\n",
        "            continue\n",
        "        start, end = max(100, len(df_1h)-1200), len(df_1h)-10\n",
        "        samples = list(range(start, end, 2))[-400:]\n",
        "        if not samples:\n",
        "            continue\n",
        "        for i in samples:\n",
        "            try:\n",
        "                regimes = RegimeDetector.get_all_regimes(df_1h.iloc[:i], df_1d.iloc[:max(0,i-24)])\n",
        "                state = create_state(df_1h.iloc[:i], df_1d.iloc[:max(0,i-24)], pair, regimes)\n",
        "                act = agent.select(state, greedy=False, bt=True, regimes=regimes)\n",
        "                q = agent.qnet.predict(state)\n",
        "                should, confidence, _ = conf.get_conf(q, agent.rbed.eps, regimes, force=True)\n",
        "                direction = ['BUY','SELL','HOLD'][act]\n",
        "                if direction == 'HOLD': continue\n",
        "                entry = df_1h['close'].iloc[i]\n",
        "                atr = df_1h['atr'].iloc[i]\n",
        "                vol_adj = RegimeStrategy.adjust_for_volatility(regimes['volatility'], ATR_SL, ATR_TP)\n",
        "                trend_strat = RegimeStrategy.select_strategy_for_trend(regimes['trend'], is_weekend())\n",
        "                sl_mult = ATR_SL * vol_adj['sl'] * trend_strat['sl_multiplier']\n",
        "                tp_mult = ATR_TP * vol_adj['tp'] * trend_strat['tp_multiplier']\n",
        "                sl = entry - (atr*sl_mult) if direction=='BUY' else entry + (atr*sl_mult)\n",
        "                tp = entry + (atr*tp_mult) if direction=='BUY' else entry - (atr*tp_mult)\n",
        "                hit_tp = hit_sl = False\n",
        "                exit_i = i + 1\n",
        "                for j in range(i+1, min(i+72, len(df_1h))):\n",
        "                    cp = df_1h['close'].iloc[j]\n",
        "                    if direction == 'BUY':\n",
        "                        if cp >= tp: hit_tp, exit_i = True, j; break\n",
        "                        elif cp <= sl: hit_sl, exit_i = True, j; break\n",
        "                    else:\n",
        "                        if cp <= tp: hit_tp, exit_i = True, j; break\n",
        "                        elif cp >= sl: hit_sl, exit_i = True, j; break\n",
        "                if not hit_tp and not hit_sl: exit_i = min(i+72, len(df_1h)-1)\n",
        "                exit_p = tp if hit_tp else (sl if hit_sl else df_1h['close'].iloc[exit_i])\n",
        "                pnl = ((exit_p-entry) if direction=='BUY' else (entry-exit_p)) * 1.0\n",
        "                out = TradeOutcome(pair=pair, action=direction, entry_price=entry, exit_price=exit_p,\n",
        "                    sl=sl, tp=tp, position_size=1.0, pnl=pnl, duration=(exit_i-i)*1.0, hit_tp=hit_tp,\n",
        "                    timestamp_entry=str(df_1h.index[i]), timestamp_exit=str(df_1h.index[exit_i]),\n",
        "                    state_at_entry=state.tolist(), confidence=confidence, regime='BACKTEST', session='WEEKEND',\n",
        "                    regimes=regimes)\n",
        "                agent.record(out)\n",
        "                learned += 1\n",
        "            except:\n",
        "                continue\n",
        "    agent.stats['backtest_learned'] = agent.stats.get('backtest_learned', 0) + learned\n",
        "    log(f\"‚úÖ Backtest complete: {learned} trades\", \"success\")\n",
        "    return learned\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# DATA LOADING (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "def fetch_price(pair, timeout=10):\n",
        "    if not BROWSER_TOKEN: return None\n",
        "    try:\n",
        "        fc, tc = pair.split(\"/\")\n",
        "        r = requests.post(f\"https://production-sfo.browserless.io/content?token={BROWSER_TOKEN}\",\n",
        "            json={\"url\": f\"https://www.x-rates.com/calculator/?from={fc}&to={tc}&amount=1\"}, timeout=timeout)\n",
        "        m = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', r.text)\n",
        "        return float(m.group(1).replace(\",\", \"\")) if m else None\n",
        "    except: return None\n",
        "\n",
        "def ensure_atr(df):\n",
        "    if \"atr\" in df.columns and df[\"atr\"].median() > MIN_ATR:\n",
        "        return df.assign(atr=df[\"atr\"].fillna(MIN_ATR).clip(lower=MIN_ATR))\n",
        "    h, l, c = df[\"high\"].values, df[\"low\"].values, df[\"close\"].values\n",
        "    tr = np.maximum.reduce([h-l, np.abs(h-np.roll(c,1)), np.abs(l-np.roll(c,1))])\n",
        "    tr[0] = h[0] - l[0] if len(tr) > 0 else MIN_ATR\n",
        "    df[\"atr\"] = pd.Series(tr, index=df.index).rolling(ATR_PER, min_periods=1).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "    return df\n",
        "\n",
        "def update_data():\n",
        "    log(\"üîÑ Updating data...\", \"info\")\n",
        "    cnt = 0\n",
        "    for pair in PAIRS:\n",
        "        p = fetch_price(pair)\n",
        "        if not p or p <= 0: continue\n",
        "        pk = pair.replace(\"/\", \"_\")\n",
        "        for pkl in DIRS[\"data\"].glob(f\"{pk}*.pkl\"):\n",
        "            if any(x in pkl.name for x in ['_model','indicator_cache','.bak']): continue\n",
        "            try:\n",
        "                try: df = pd.read_pickle(pkl, compression='gzip')\n",
        "                except: df = pd.read_pickle(pkl, compression=None)\n",
        "                if not isinstance(df, pd.DataFrame) or len(df) < 10: continue\n",
        "                if not all(c in df.columns for c in ['open','high','low','close']): continue\n",
        "                last_t = df.index[-1]\n",
        "                new_t = datetime.now().replace(second=0, microsecond=0)\n",
        "                if new_t > last_t:\n",
        "                    new_row = pd.DataFrame({'open':[float(p)],'high':[float(p)],'low':[float(p)],\n",
        "                        'close':[float(p)],'volume':[0]}, index=[new_t])\n",
        "                    df = pd.concat([df, new_row]).tail(5000).ffill().bfill()\n",
        "                    df = ensure_atr(df)\n",
        "                    df.to_pickle(pkl, compression='gzip')\n",
        "                    cnt += 1\n",
        "            except: pass\n",
        "    log(f\"‚úÖ Updated {cnt} files\", \"success\")\n",
        "    return cnt\n",
        "\n",
        "def load_data(folder):\n",
        "    log(f\"üìÇ Loading from {folder}\", \"info\")\n",
        "    if not folder.exists(): return {}\n",
        "    all_pkl = [p for p in folder.glob(\"*.pkl\") if not any(s in p.name for s in ['_model','indicator_cache','.bak'])]\n",
        "    pair_files = defaultdict(list)\n",
        "    curr = [\"EUR\",\"GBP\",\"USD\",\"AUD\",\"NZD\",\"CAD\",\"CHF\",\"JPY\"]\n",
        "    for pkl in all_pkl:\n",
        "        parts = pkl.stem.split('_')\n",
        "        if len(parts) >= 2 and parts[0] in curr and parts[1] in curr:\n",
        "            pair_files[f\"{parts[0]}_{parts[1]}\"].append(pkl)\n",
        "    combined = {}\n",
        "    for pk, files in pair_files.items():\n",
        "        pair = f\"{pk[:3]}/{pk[4:]}\"\n",
        "        if pair not in PAIRS: continue\n",
        "        pair_data = {}\n",
        "        for pkl in files:\n",
        "            try:\n",
        "                try: df = pd.read_pickle(pkl, compression='gzip')\n",
        "                except: df = pd.read_pickle(pkl, compression=None)\n",
        "                if not isinstance(df, pd.DataFrame) or len(df) < 50: continue\n",
        "                if not all(c in df.columns for c in ['open','high','low','close']): continue\n",
        "                df = df.ffill().bfill().dropna(subset=['open','high','low','close'])\n",
        "                df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "                if df.index.tz: df.index = df.index.tz_localize(None)\n",
        "                df = df[df.index.notna()]\n",
        "                tf = \"1d\" if \"1d\" in pkl.stem or \"daily\" in pkl.stem else \"1h\"\n",
        "                if tf not in [\"1d\",\"1h\"]: continue\n",
        "                df = ensure_atr(df)\n",
        "                pair_data[tf] = df\n",
        "                log(f\"‚úÖ {pair} [{tf}]: {len(df)} rows\", \"success\")\n",
        "            except: pass\n",
        "        if pair_data: combined[pair] = pair_data\n",
        "    log(f\"‚úÖ Loaded {len(combined)} pairs\", \"success\")\n",
        "    return combined\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# EMAIL REPORTING WITH QUALITY METRICS (v21.1 - ENHANCED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "def send_email(sigs, it, stats, mode, pipe_stats, regime_summary, quality_stats):\n",
        "    if not GMAIL_PWD: return\n",
        "    try:\n",
        "        msg = MIMEMultipart('alternative')\n",
        "        msg['Subject'] = f\"‚≠ê BEACON v21.1 [QUALITY FILTERED] #{it} - {mode}\"\n",
        "        msg['From'] = msg['To'] = GMAIL\n",
        "\n",
        "        active = sum(1 for s in sigs.values() if s.get('direction')!='HOLD')\n",
        "        wr = stats.get('win_rate',0)*100\n",
        "        pnl = stats.get('total_pnl',0.0)\n",
        "        eps = stats.get('epsilon_history', [0.7])[-1]\n",
        "        q_avg = int(stats.get('q_value_history', [0])[-1]) if stats.get('q_value_history') else 0\n",
        "\n",
        "        # Build active signals table with quality scores\n",
        "        active_signals_html = \"\"\n",
        "        for pair, signal in sigs.items():\n",
        "            direction = signal.get('direction', 'HOLD')\n",
        "            if direction == 'HOLD': continue\n",
        "\n",
        "            price = signal.get('last_price', 0)\n",
        "            sl = signal.get('SL', 0)\n",
        "            tp = signal.get('TP', 0)\n",
        "            conf = signal.get('confidence', 0)\n",
        "            quality_score = signal.get('quality_score', 0)\n",
        "            regimes = signal.get('regimes', {})\n",
        "            score_components = signal.get('score_components', {})\n",
        "\n",
        "            if direction == 'BUY':\n",
        "                risk_pips = (price - sl) * 10000\n",
        "                reward_pips = (tp - price) * 10000\n",
        "            else:\n",
        "                risk_pips = (sl - price) * 10000\n",
        "                reward_pips = (price - tp) * 10000\n",
        "            rr_ratio = reward_pips / risk_pips if risk_pips > 0 else 0\n",
        "\n",
        "            direction_color = \"#10b981\" if direction == \"BUY\" else \"#ef4444\"\n",
        "            quality_color = \"#10b981\" if quality_score >= 85 else \"#f59e0b\" if quality_score >= 80 else \"#64748b\"\n",
        "\n",
        "            active_signals_html += f\"\"\"\n",
        "            <div style=\"background:#fff;border-left:4px solid {direction_color};padding:20px;margin:15px 0;border-radius:8px;box-shadow:0 2px 8px rgba(0,0,0,0.1);\">\n",
        "                <div style=\"display:flex;justify-content:space-between;align-items:center;margin-bottom:15px;\">\n",
        "                    <div>\n",
        "                        <h3 style=\"margin:0;font-size:24px;color:#1e293b;\">{pair}</h3>\n",
        "                        <span style=\"background:{direction_color};color:#fff;padding:4px 12px;border-radius:20px;font-size:12px;font-weight:800;\">{direction}</span>\n",
        "                        <span style=\"background:{quality_color};color:#fff;padding:4px 12px;border-radius:20px;font-size:12px;font-weight:800;margin-left:8px;\">‚≠ê {quality_score:.0f}</span>\n",
        "                    </div>\n",
        "                    <div style=\"text-align:right;\">\n",
        "                        <div style=\"font-size:28px;font-weight:900;color:{direction_color};\">{price:.5f}</div>\n",
        "                        <div style=\"font-size:12px;color:#64748b;\">Current Price</div>\n",
        "                    </div>\n",
        "                </div>\n",
        "\n",
        "                <div style=\"display:grid;grid-template-columns:repeat(3,1fr);gap:15px;margin:15px 0;\">\n",
        "                    <div style=\"background:#f1f5f9;padding:12px;border-radius:6px;text-align:center;\">\n",
        "                        <div style=\"font-size:18px;font-weight:800;color:#ef4444;\">{sl:.5f}</div>\n",
        "                        <div style=\"font-size:11px;color:#64748b;margin-top:4px;\">STOP LOSS</div>\n",
        "                        <div style=\"font-size:10px;color:#94a3b8;margin-top:2px;\">-{risk_pips:.1f} pips</div>\n",
        "                    </div>\n",
        "                    <div style=\"background:#f1f5f9;padding:12px;border-radius:6px;text-align:center;\">\n",
        "                        <div style=\"font-size:18px;font-weight:800;color:#8b5cf6;\">{rr_ratio:.2f}:1</div>\n",
        "                        <div style=\"font-size:11px;color:#64748b;margin-top:4px;\">RISK/REWARD</div>\n",
        "                        <div style=\"font-size:10px;color:#94a3b8;margin-top:2px;\">Ratio</div>\n",
        "                    </div>\n",
        "                    <div style=\"background:#f1f5f9;padding:12px;border-radius:6px;text-align:center;\">\n",
        "                        <div style=\"font-size:18px;font-weight:800;color:#10b981;\">{tp:.5f}</div>\n",
        "                        <div style=\"font-size:11px;color:#64748b;margin-top:4px;\">TAKE PROFIT</div>\n",
        "                        <div style=\"font-size:10px;color:#94a3b8;margin-top:2px;\">+{reward_pips:.1f} pips</div>\n",
        "                    </div>\n",
        "                </div>\n",
        "\n",
        "                <div style=\"background:#f8fafc;padding:12px;border-radius:6px;margin-top:12px;\">\n",
        "                    <div style=\"font-size:11px;color:#6d28d9;font-weight:600;margin-bottom:8px;\">‚≠ê QUALITY SCORE BREAKDOWN:</div>\n",
        "                    <div style=\"font-size:12px;color:#475569;line-height:1.6;\">\n",
        "                        {' | '.join([f\"{k.replace('_', ' ').title()}: +{v:.0f}\" if v > 0 else f\"{k.replace('_', ' ').title()}: {v:.0f}\" for k, v in list(score_components.items())[:5]])}\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        if not active_signals_html:\n",
        "            active_signals_html = \"\"\"\n",
        "            <div style=\"background:#fff;padding:30px;text-align:center;border-radius:8px;color:#64748b;\">\n",
        "                <div style=\"font-size:48px;margin-bottom:10px;\">üìä</div>\n",
        "                <div style=\"font-size:18px;font-weight:600;\">No Premium Signals</div>\n",
        "                <div style=\"font-size:14px;margin-top:8px;\">All signals filtered by quality threshold</div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        # Quality filter stats\n",
        "        quality_stats_html = f\"\"\"\n",
        "        <div style=\"display:grid;grid-template-columns:repeat(3,1fr);gap:15px;\">\n",
        "            <div style=\"background:#fff;padding:20px;border-radius:8px;text-align:center;border:2px solid #8b5cf6;\">\n",
        "                <div style=\"font-size:32px;font-weight:900;color:#8b5cf6;\">{quality_stats.get('premium_count', 0)}</div>\n",
        "                <div style=\"font-size:12px;color:#64748b;margin-top:4px;\">PREMIUM SIGNALS</div>\n",
        "                <div style=\"font-size:10px;color:#94a3b8;margin-top:2px;\">Score ‚â• {quality_stats.get('min_threshold', 80)}</div>\n",
        "            </div>\n",
        "            <div style=\"background:#fff;padding:20px;border-radius:8px;text-align:center;\">\n",
        "                <div style=\"font-size:32px;font-weight:900;color:#f59e0b;\">{quality_stats.get('filtered_out', 0)}</div>\n",
        "                <div style=\"font-size:12px;color:#64748b;margin-top:4px;\">FILTERED OUT</div>\n",
        "                <div style=\"font-size:10px;color:#94a3b8;margin-top:2px;\">Low quality</div>\n",
        "            </div>\n",
        "            <div style=\"background:#fff;padding:20px;border-radius:8px;text-align:center;\">\n",
        "                <div style=\"font-size:32px;font-weight:900;color:#10b981;\">{quality_stats.get('avg_score', 0):.0f}</div>\n",
        "                <div style=\"font-size:12px;color:#64748b;margin-top:4px;\">AVG SCORE</div>\n",
        "                <div style=\"font-size:10px;color:#94a3b8;margin-top:2px;\">All signals</div>\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        html = f\"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "<style>\n",
        "body{{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#f8fafc;margin:0;padding:20px}}\n",
        ".container{{max-width:1200px;margin:0 auto;background:#fff;border-radius:16px;box-shadow:0 20px 60px rgba(0,0,0,0.15);overflow:hidden}}\n",
        ".header{{background:linear-gradient(135deg,#8b5cf6,#6d28d9);color:#fff;padding:50px 40px;text-align:center}}\n",
        ".header h1{{margin:0;font-size:42px;font-weight:900;letter-spacing:-1px}}\n",
        ".badge{{background:{\"#f59e0b\" if mode==\"WEEKEND_LEARNING\" else \"#10b981\"};padding:10px 24px;border-radius:30px;margin-top:15px;font-weight:800;display:inline-block;font-size:14px}}\n",
        ".section{{padding:30px 40px}}\n",
        ".section-title{{font-size:20px;font-weight:800;color:#1e293b;margin-bottom:20px;display:flex;align-items:center}}\n",
        ".section-title::before{{content:'';width:4px;height:24px;background:#8b5cf6;margin-right:12px;border-radius:2px}}\n",
        ".stats-grid{{display:grid;grid-template-columns:repeat(auto-fit,minmax(180px,1fr));gap:15px}}\n",
        ".stat-card{{background:linear-gradient(135deg,#f8fafc,#f1f5f9);padding:20px;border-radius:10px;text-align:center;border:1px solid #e2e8f0}}\n",
        ".stat-value{{font-size:32px;font-weight:900;color:#8b5cf6;margin-bottom:5px}}\n",
        ".stat-label{{font-size:12px;color:#64748b;text-transform:uppercase;font-weight:600;letter-spacing:0.5px}}\n",
        ".divider{{height:1px;background:linear-gradient(to right,transparent,#e2e8f0,transparent);margin:30px 0}}\n",
        ".footer{{background:#1e293b;color:#94a3b8;padding:25px 40px;text-align:center;font-size:13px}}\n",
        ".timestamp{{color:#64748b;font-size:14px;margin-top:12px}}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <div class=\"header\">\n",
        "        <h1>‚≠ê TRADE BEACON v21.1</h1>\n",
        "        <div class=\"badge\">QUALITY FILTERED SIGNALS | {mode}</div>\n",
        "        <div class=\"timestamp\">Iteration #{it} ‚Ä¢ {datetime.now(timezone.utc):%Y-%m-%d %H:%M UTC}</div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <div class=\"section-title\">üìä Performance Metrics</div>\n",
        "        <div class=\"stats-grid\">\n",
        "            <div class=\"stat-card\">\n",
        "                <div class=\"stat-value\">{stats.get('total_trades',0)}</div>\n",
        "                <div class=\"stat-label\">Total Trades</div>\n",
        "            </div>\n",
        "            <div class=\"stat-card\">\n",
        "                <div class=\"stat-value\" style=\"color:{'#10b981' if wr >= 50 else '#ef4444'}\">{wr:.1f}%</div>\n",
        "                <div class=\"stat-label\">Win Rate</div>\n",
        "            </div>\n",
        "            <div class=\"stat-card\">\n",
        "                <div class=\"stat-value\" style=\"color:{'#10b981' if pnl >= 0 else '#ef4444'}\">${pnl:.2f}</div>\n",
        "                <div class=\"stat-label\">Total P&L</div>\n",
        "            </div>\n",
        "            <div class=\"stat-card\">\n",
        "                <div class=\"stat-value\">{eps:.3f}</div>\n",
        "                <div class=\"stat-label\">Epsilon</div>\n",
        "            </div>\n",
        "            <div class=\"stat-card\">\n",
        "                <div class=\"stat-value\" style=\"color:#f59e0b\">{active}</div>\n",
        "                <div class=\"stat-label\">Premium Signals</div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"divider\"></div>\n",
        "\n",
        "    <div class=\"section\" style=\"background:#f8fafc;\">\n",
        "        <div class=\"section-title\">‚≠ê Quality Filter Statistics</div>\n",
        "        {quality_stats_html}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"divider\"></div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <div class=\"section-title\">üéØ Premium Trading Signals</div>\n",
        "        {active_signals_html}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"footer\">\n",
        "        <div style=\"font-weight:700;font-size:15px;color:#fff;margin-bottom:8px;\">‚ö†Ô∏è DISCLAIMER</div>\n",
        "        <div>Experimental AI trading system. Past performance does not guarantee future results.</div>\n",
        "        <div style=\"margin-top:15px;padding-top:15px;border-top:1px solid #334155;\">\n",
        "            <div style=\"color:#64748b;\">Trade Beacon v21.1 ‚Ä¢ Quality-Filtered Signals</div>\n",
        "            <div style=\"color:#475569;margin-top:4px;\">Regime Detection + Self-Learning Quality Scoring</div>\n",
        "        </div>\n",
        "    </div>\n",
        "</div>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "        msg.attach(MIMEText(html, 'html'))\n",
        "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as srv:\n",
        "            srv.login(GMAIL, GMAIL_PWD)\n",
        "            srv.send_message(msg)\n",
        "        log(\"‚úÖ Email sent\", \"success\")\n",
        "    except Exception as e:\n",
        "        log(f\"Email failed: {e}\", \"error\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# GIT OPERATIONS (v21.0 - UNCHANGED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "def push_git(files, msg):\n",
        "    if IN_GHA or not PAT: return False\n",
        "    try:\n",
        "        url = f\"https://{GH_USER}:{PAT}@github.com/{GH_USER}/{GH_REPO}.git\"\n",
        "        repo = SAVE if (SAVE/\".git\").exists() else BASE\n",
        "        if not (repo/\".git\").exists():\n",
        "            subprocess.run([\"git\",\"clone\",url,str(repo)], check=True)\n",
        "        os.chdir(repo)\n",
        "        for f in files:\n",
        "            if (repo/f).exists(): subprocess.run([\"git\",\"add\",str(f)], check=False)\n",
        "        subprocess.run([\"git\",\"commit\",\"-m\",msg], check=False)\n",
        "        subprocess.run([\"git\",\"pull\",\"--rebase\",\"origin\",\"main\"], check=False)\n",
        "        return subprocess.run([\"git\",\"push\",\"origin\",\"main\"]).returncode == 0\n",
        "    except: return False\n",
        "    finally:\n",
        "        try: os.chdir(SAVE)\n",
        "        except: pass\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# MAIN EXECUTION (v21.1 - ENHANCED WITH QUALITY FILTERING)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "def main():\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    log(\"=\"*70, \"rocket\")\n",
        "    log(\"‚≠ê TRADE BEACON v21.1 - QUALITY FILTERED SIGNALS\", \"quality\")\n",
        "    log(\"=\"*70, \"rocket\")\n",
        "    mode = get_mode()\n",
        "    log(f\"üìÖ Mode: {mode}\", \"info\")\n",
        "\n",
        "    # Version check\n",
        "    ver = \"21.1\"\n",
        "    needs_reset = False\n",
        "    if VERSION_FILE.exists():\n",
        "        try:\n",
        "            with open(VERSION_FILE, 'r') as f:\n",
        "                if f.read().strip() != ver: needs_reset = True\n",
        "        except: needs_reset = True\n",
        "    else: needs_reset = True\n",
        "\n",
        "    if needs_reset:\n",
        "        log(\"üÜï v21.1: Quality Filtering Enhancement\", \"warn\")\n",
        "        log(\"‚ú® New Features:\", \"info\")\n",
        "        log(\"   - Self-learning quality scoring system\", \"info\")\n",
        "        log(\"   - Premium signal filtering (score >80)\", \"info\")\n",
        "        log(\"   - Adaptive quality thresholds\", \"info\")\n",
        "        log(\"   - Quality weight optimization from outcomes\", \"info\")\n",
        "        log(\"   - Enhanced learning visibility\", \"info\")\n",
        "        log(\"   - All v21.0 features preserved\", \"info\")\n",
        "\n",
        "        # Safe upgrade - no network reset needed\n",
        "        with open(VERSION_FILE, 'w') as f: f.write(ver)\n",
        "        log(\"‚úÖ v21.1 initialized - Quality filtering active!\", \"success\")\n",
        "\n",
        "    it = inc_iter()\n",
        "    agent = RLAgent()\n",
        "    env = TradingEnv()\n",
        "    conf = Confidence()\n",
        "    db = PipelineDB()\n",
        "    quality_scorer = QualityScorer()  # NEW\n",
        "\n",
        "    try:\n",
        "        log(f\"\\nüìä Iteration #{it} | {ENV_NAME} | {mode}\", \"info\")\n",
        "        if mode == \"LIVE_TRADING\": update_data()\n",
        "        data = load_data(DIRS[\"data\"])\n",
        "        if not data: raise ValueError(\"No data\")\n",
        "\n",
        "        # Multi-Source Learning\n",
        "        log(\"\\n\" + \"=\"*70, \"learning\")\n",
        "        log(\"üéì MULTI-SOURCE LEARNING\", \"learning\")\n",
        "        log(\"=\"*70, \"learning\")\n",
        "\n",
        "        total_learned = 0\n",
        "\n",
        "        # Pipeline v6.3\n",
        "        learning_db = DIRS[\"learning\"] / \"learning_outcomes.json\"\n",
        "        if learning_db.exists():\n",
        "            learned = agent.learn_from_pipeline_v6(learning_db)\n",
        "            total_learned += learned\n",
        "\n",
        "            # NEW: Learn quality weights from v6 outcomes\n",
        "            try:\n",
        "                with open(learning_db, 'r') as f:\n",
        "                    v6_outcomes = json.load(f)\n",
        "                if len(v6_outcomes) > 0:\n",
        "                    quality_scorer.learn_from_outcomes(v6_outcomes[-100:])  # Last 100\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Live validation\n",
        "        old_signals = load_previous_signals()\n",
        "        prices = {}\n",
        "        for pair in PAIRS:\n",
        "            if mode == \"WEEKEND_LEARNING\":\n",
        "                if pair in data and '1h' in data[pair]:\n",
        "                    prices[pair] = data[pair]['1h'].iloc[-1]['close']\n",
        "            else:\n",
        "                p = fetch_price(pair) or (data[pair]['1h'].iloc[-1]['close'] if pair in data and '1h' in data[pair] else None)\n",
        "                if p: prices[pair] = p\n",
        "\n",
        "        if old_signals and prices:\n",
        "            validated = validate_live_signals(agent, old_signals, prices, data)\n",
        "            total_learned += validated\n",
        "\n",
        "        # Pipeline DB\n",
        "        if db.conn:\n",
        "            pipe_learned = agent.learn_pipeline(db, data)\n",
        "            total_learned += pipe_learned\n",
        "            pipe_stats = db.get_stats()\n",
        "        else:\n",
        "            pipe_learned, pipe_stats = 0, {}\n",
        "\n",
        "        # Weekend backtest\n",
        "        if mode == \"WEEKEND_LEARNING\":\n",
        "            bt_trades = backtest(data, agent, conf)\n",
        "            total_learned += bt_trades\n",
        "\n",
        "        agent.stats['total_learning_sources'] = total_learned\n",
        "        log(f\"\\nüß† TOTAL LEARNING: {total_learned} experiences!\", \"success\")\n",
        "        log(\"=\"*70, \"learning\")\n",
        "\n",
        "        # Check active trades\n",
        "        completed = env.check_exits(prices)\n",
        "        for t in completed:\n",
        "            agent.record(t)\n",
        "\n",
        "        # Generate signals with regime awareness\n",
        "        log(\"\\n\" + \"=\"*70, \"regime\")\n",
        "        log(\"‚≠ê QUALITY-FILTERED SIGNAL GENERATION\", \"quality\")\n",
        "        log(\"=\"*70, \"regime\")\n",
        "\n",
        "        raw_signals = {}\n",
        "        regime_summary = {}\n",
        "\n",
        "        for pair in PAIRS:\n",
        "            if pair not in data or '1h' not in data[pair] or '1d' not in data[pair]:\n",
        "                raw_signals[pair] = {'direction': 'HOLD', 'last_price': prices.get(pair, 0)}\n",
        "                continue\n",
        "\n",
        "            # Detect all regimes\n",
        "            regimes = RegimeDetector.get_all_regimes(data[pair]['1h'], data[pair]['1d'])\n",
        "\n",
        "            # Check if we should trade this pair now\n",
        "            should_trade, reason = RegimeStrategy.should_trade_pair_now(pair, data[pair]['1h'], regimes)\n",
        "\n",
        "            if not should_trade:\n",
        "                log(f\"‚è≠Ô∏è  {pair} skipped: {reason}\", \"info\")\n",
        "                raw_signals[pair] = {'direction': 'HOLD', 'last_price': prices.get(pair, 0),\n",
        "                             'skip_reason': reason, 'regimes': regimes}\n",
        "                agent.stats['regime_filtered_trades'] = agent.stats.get('regime_filtered_trades', 0) + 1\n",
        "                continue\n",
        "\n",
        "            state = create_state(data[pair]['1h'], data[pair]['1d'], pair, regimes)\n",
        "            q = agent.qnet.predict(state)\n",
        "            should, confidence, metrics = conf.get_conf(q, agent.rbed.eps, regimes)\n",
        "            best = np.argmax(q)\n",
        "            direction = ['BUY','SELL','HOLD'][best] if should else 'HOLD'\n",
        "\n",
        "            # Apply regime-based strategy\n",
        "            trend_strategy = RegimeStrategy.select_strategy_for_trend(regimes['trend'], is_weekend())\n",
        "\n",
        "            # Use contrarian for ranging markets on weekends\n",
        "            if is_weekend() and regimes['trend'] == 'RANGING' and CONFIG.get('weekend_contrarian', True):\n",
        "                if direction == 'BUY':\n",
        "                    direction = 'SELL'\n",
        "                    log(f\"üîÑ {pair}: Contrarian SELL (was BUY) - Ranging weekend\", \"regime\")\n",
        "                elif direction == 'SELL':\n",
        "                    direction = 'BUY'\n",
        "                    log(f\"üîÑ {pair}: Contrarian BUY (was SELL) - Ranging weekend\", \"regime\")\n",
        "\n",
        "            # Apply trend bias filter\n",
        "            if trend_strategy['bias'] == 'LONG_ONLY' and direction == 'SELL':\n",
        "                direction = 'HOLD'\n",
        "                log(f\"üö´ {pair}: Filtered SELL in LONG_ONLY regime\", \"regime\")\n",
        "            elif trend_strategy['bias'] == 'SHORT_ONLY' and direction == 'BUY':\n",
        "                direction = 'HOLD'\n",
        "                log(f\"üö´ {pair}: Filtered BUY in SHORT_ONLY regime\", \"regime\")\n",
        "\n",
        "            price = prices.get(pair, 0)\n",
        "            atr = data[pair]['1h']['atr'].iloc[-1]\n",
        "\n",
        "            # Regime-adjusted SL/TP\n",
        "            vol_adj = RegimeStrategy.adjust_for_volatility(regimes['volatility'], ATR_SL, ATR_TP)\n",
        "            sl_mult = ATR_SL * vol_adj['sl'] * trend_strategy['sl_multiplier']\n",
        "            tp_mult = ATR_TP * vol_adj['tp'] * trend_strategy['tp_multiplier']\n",
        "\n",
        "            if direction == 'BUY':\n",
        "                sl, tp = price - (atr*sl_mult), price + (atr*tp_mult)\n",
        "            elif direction == 'SELL':\n",
        "                sl, tp = price + (atr*sl_mult), price - (atr*tp_mult)\n",
        "            else:\n",
        "                sl = tp = price\n",
        "\n",
        "            raw_signals[pair] = {\n",
        "                'direction': direction,\n",
        "                'last_price': price,\n",
        "                'SL': float(sl),\n",
        "                'TP': float(tp),\n",
        "                'confidence': confidence,\n",
        "                'threshold': metrics['threshold'],\n",
        "                'rb_epsilon': agent.rbed.eps,\n",
        "                'regimes': regimes,\n",
        "                'sl_multiplier': float(sl_mult),\n",
        "                'tp_multiplier': float(tp_mult),\n",
        "                'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "                'pair': pair,\n",
        "                'timeframe': '1h'\n",
        "            }\n",
        "\n",
        "            # Store regime summary\n",
        "            for reg_type, reg_val in regimes.items():\n",
        "                regime_summary[f\"{pair}_{reg_type}\"] = reg_val\n",
        "\n",
        "            log(f\"üåç {pair}: {direction} | Vol={regimes['volatility']} Trend={regimes['trend']}\", \"regime\")\n",
        "\n",
        "        # NEW: Apply quality filtering\n",
        "        log(\"\\n\" + \"=\"*70, \"quality\")\n",
        "        log(\"‚≠ê APPLYING QUALITY FILTER\", \"quality\")\n",
        "        log(\"=\"*70, \"quality\")\n",
        "\n",
        "        filtered_signals, quality_stats = quality_scorer.filter_signals(list(raw_signals.values()))\n",
        "\n",
        "        # Convert back to dict format\n",
        "        final_signals = {}\n",
        "        for sig in filtered_signals:\n",
        "            pair = sig['pair']\n",
        "            final_signals[pair] = sig\n",
        "\n",
        "        # Add HOLD signals for pairs that didn't pass filter\n",
        "        for pair in PAIRS:\n",
        "            if pair not in final_signals:\n",
        "                final_signals[pair] = raw_signals.get(pair, {\n",
        "                    'direction': 'HOLD',\n",
        "                    'last_price': prices.get(pair, 0),\n",
        "                    'quality_filtered': True\n",
        "                })\n",
        "\n",
        "        log(f\"\\n‚≠ê Quality filter results:\", \"quality\")\n",
        "        log(f\"   Generated: {quality_stats.get('total_scored', 0)}\", \"info\")\n",
        "        log(f\"   Premium (‚â•{quality_stats.get('min_threshold', 80)}): {quality_stats.get('premium_count', 0)}\", \"success\")\n",
        "        log(f\"   Filtered out: {quality_stats.get('filtered_out', 0)}\", \"info\")\n",
        "        log(f\"   Avg score: {quality_stats.get('avg_score', 0):.1f}\", \"info\")\n",
        "\n",
        "        # Execute premium trades\n",
        "        for pair, signal in final_signals.items():\n",
        "            direction = signal.get('direction', 'HOLD')\n",
        "\n",
        "            if direction != 'HOLD' and len(env.active) < MAX_POS and mode == \"LIVE_TRADING\":\n",
        "                price = signal.get('last_price', 0)\n",
        "                sl = signal.get('SL', 0)\n",
        "                tp = signal.get('TP', 0)\n",
        "                confidence = signal.get('confidence', 0)\n",
        "                quality_score = signal.get('quality_score', 0)\n",
        "                regimes = signal.get('regimes', {})\n",
        "\n",
        "                state = create_state(data[pair]['1h'], data[pair]['1d'], pair, regimes)\n",
        "\n",
        "                vol_adj = RegimeStrategy.adjust_for_volatility(\n",
        "                    regimes.get('volatility', 'NORMAL_VOL'), ATR_SL, ATR_TP\n",
        "                )\n",
        "\n",
        "                base = (CAPITAL * RISK) / (abs(price - sl) + EPS)\n",
        "                size = conf.calc_size(base, confidence)\n",
        "                size = min(size, MAX_CAP / price)\n",
        "                size *= vol_adj['position_size']\n",
        "\n",
        "                env.execute(pair, direction, price, sl, tp, size, state,\n",
        "                    {'confidence': confidence, 'rb_epsilon': agent.rbed.eps,\n",
        "                     'regime': 'RL_v21.1_QUALITY', 'session': regimes.get('session', 'UNKNOWN'),\n",
        "                     'regimes': regimes, 'quality_score': quality_score})\n",
        "\n",
        "        # Output\n",
        "        output = {\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "            'iteration': it,\n",
        "            'version': 'v21.1-quality-filtered',\n",
        "            'mode': mode,\n",
        "            'signals': final_signals,\n",
        "            'rl_stats': agent.stats,\n",
        "            'active_trades': len(env.active),\n",
        "            'pipeline_stats': pipe_stats,\n",
        "            'regime_summary': regime_summary,\n",
        "            'quality_stats': quality_stats,\n",
        "            'quality_scorer_stats': quality_scorer.get_stats(),\n",
        "            'regime_filters_applied': agent.stats.get('regime_filtered_trades', 0),\n",
        "            'learning_summary': {\n",
        "                'pipeline_v6': agent.stats.get('pipeline_v6_learned', 0),\n",
        "                'pipeline_v6_contrarian': agent.stats.get('pipeline_v6_contrarian', 0),\n",
        "                'live_validated': agent.stats.get('live_validated', 0),\n",
        "                'pipeline_db': agent.stats.get('pipeline_trades_learned', 0),\n",
        "                'backtest': agent.stats.get('backtest_learned', 0),\n",
        "                'total_this_run': total_learned\n",
        "            }\n",
        "        }\n",
        "\n",
        "        P.save(SIGNALS, output, compress=False)\n",
        "        agent.save()\n",
        "        quality_scorer._save_weights()\n",
        "\n",
        "        # Regime performance report\n",
        "        log(\"\\n\" + \"=\"*70, \"regime\")\n",
        "        log(\"üìä REGIME PERFORMANCE SUMMARY\", \"regime\")\n",
        "        log(\"=\"*70, \"regime\")\n",
        "\n",
        "        best_regimes = agent.regime_tracker.get_best_regimes(min_samples=5)\n",
        "        if best_regimes:\n",
        "            log(\"üèÜ Top Performing Regimes:\", \"regime\")\n",
        "            for i, (regime, win_rate) in enumerate(best_regimes[:5], 1):\n",
        "                log(f\"   {i}. {regime}: {win_rate*100:.1f}% win rate\", \"success\" if win_rate > 0.5 else \"info\")\n",
        "\n",
        "        log(f\"\\nüìä Stats: {agent.stats['total_trades']} trades | {agent.stats['win_rate']*100:.1f}% WR | ${agent.stats['total_pnl']:.2f} P&L | Œµ={agent.rbed.eps:.4f}\", \"brain\")\n",
        "        log(f\"üåç Regime filters: {agent.stats.get('regime_filtered_trades', 0)} trades filtered\", \"regime\")\n",
        "        log(f\"‚≠ê Quality filters: {quality_stats.get('filtered_out', 0)} signals filtered\", \"quality\")\n",
        "\n",
        "        # Diagnostics\n",
        "        log(\"\\n\" + \"=\"*70, \"brain\")\n",
        "        log(\"üìä SYSTEM DIAGNOSTICS\", \"brain\")\n",
        "        log(\"=\"*70, \"brain\")\n",
        "        log(f\"üéØ Performance: {agent.stats['total_trades']} trades | {agent.stats['win_rate']*100:.1f}% WR | ${agent.stats['total_pnl']:.2f} P&L\", \"brain\")\n",
        "        log(f\"üß† Learning: Œµ={agent.rbed.eps:.4f} | Updates={agent.stats['total_updates']} | Memory={len(agent.mem)}/{MEM_CAP}\", \"brain\")\n",
        "\n",
        "        total_v6 = agent.stats.get('pipeline_v6_learned', 0)\n",
        "        contrarian = agent.stats.get('pipeline_v6_contrarian', 0)\n",
        "        if total_v6 > 0:\n",
        "            contrarian_pct = (contrarian / total_v6) * 100\n",
        "            log(f\"üß™ Contrarian: {contrarian}/{total_v6} ({contrarian_pct:.1f}%)\", \"learning\")\n",
        "\n",
        "        if agent.stats.get('q_value_history'):\n",
        "            recent_q = agent.stats['q_value_history'][-100:]\n",
        "            q_mean = np.mean(recent_q)\n",
        "            q_std = np.std(recent_q)\n",
        "            log(f\"üìà Q-Values: Œº={q_mean:.2f}, œÉ={q_std:.2f}\", \"brain\")\n",
        "\n",
        "        log(\"\\n‚≠ê Quality Scoring:\", \"quality\")\n",
        "        log(f\"   Min threshold: {quality_stats.get('min_threshold', 80):.0f}\", \"info\")\n",
        "        log(f\"   Signals generated: {quality_stats.get('total_scored', 0)}\", \"info\")\n",
        "        log(f\"   Premium signals: {quality_stats.get('premium_count', 0)}\", \"success\")\n",
        "        log(f\"   Filter rate: {(quality_stats.get('filtered_out', 0) / max(quality_stats.get('total_scored', 1), 1) * 100):.1f}%\", \"info\")\n",
        "\n",
        "        log(\"=\"*70, \"brain\")\n",
        "\n",
        "        # Email\n",
        "        if mode == \"LIVE_TRADING\":\n",
        "            send_email(final_signals, it, agent.stats, mode, pipe_stats, regime_summary, quality_stats)\n",
        "\n",
        "        # Git push\n",
        "        files = [\n",
        "            f\"outputs/{SIGNALS.name}\",\n",
        "            f\"omega_state/{ITER_FILE.name}\",\n",
        "            f\"rl_memory/{RL_MEM.name}\",\n",
        "            f\"rl_memory/{RL_STATS.name}\",\n",
        "            f\"rl_memory/{TRADES.name}\",\n",
        "            f\"rl_memory/{RL_WEIGHTS.name}\",\n",
        "            f\"rl_memory/version.txt\",\n",
        "            f\"regime_stats/{REGIME_STATS.name}\",\n",
        "            f\"quality_weights/learned_weights.json\"\n",
        "        ]\n",
        "\n",
        "        commit = f\"‚≠ê v21.1 #{it} [{mode}] Œµ={agent.rbed.eps:.3f} WR={agent.stats['win_rate']*100:.1f}% Q={quality_stats.get('premium_count', 0)}\"\n",
        "        push_git(files, commit)\n",
        "\n",
        "        duration = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "        log(\"\\n\" + \"=\"*70, \"success\")\n",
        "        log(\"‚úÖ CYCLE COMPLETE - QUALITY FILTERING ACTIVE!\", \"success\")\n",
        "        log(\"=\"*70, \"success\")\n",
        "        log(f\"Iteration: #{it} ({ENV_NAME})\", \"info\")\n",
        "        log(f\"Mode: {mode}\", \"info\")\n",
        "        log(f\"Duration: {duration:.1f}s\", \"info\")\n",
        "        log(f\"Total Learning: {total_learned} experiences\", \"learning\")\n",
        "        log(f\"RL Trades: {agent.stats['total_trades']}\", \"brain\")\n",
        "        log(f\"Win Rate: {agent.stats['win_rate']*100:.1f}%\", \"info\")\n",
        "        log(f\"Total P&L: ${agent.stats['total_pnl']:.2f}\", \"money\")\n",
        "        log(f\"Premium Signals: {quality_stats.get('premium_count', 0)}\", \"quality\")\n",
        "        log(f\"Active Trades: {len(env.active)}\", \"info\")\n",
        "        log(f\"Epsilon: {agent.rbed.eps:.4f}\", \"success\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"\\n‚ùå Error: {e}\", \"error\")\n",
        "        logging.exception(\"Fatal error\")\n",
        "        raise\n",
        "    finally:\n",
        "        if db.conn:\n",
        "            db.close()\n",
        "        log(f\"\\n‚≠ê Cycle complete #{it} - Quality filtering active!\", \"quality\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    log(\"\\n‚≠ê Trade Beacon v21.1 - Premium Quality Signals\", \"success\")"
      ],
      "metadata": {
        "id": "CWFe1Brsq4Bk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}