{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eaVLBC20LX_"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üåç Notebook Initialization ‚Äî Colab + GitHub Actions + Local\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Detect Environment\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "IN_LOCAL = not IN_COLAB and not IN_GHA\n",
        "\n",
        "ENV_NAME = \"Colab\" if IN_COLAB else \"GitHub Actions\" if IN_GHA else \"Local\"\n",
        "print(f\"üîç Detected environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Safe Working Folder (Auto-Switch)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    BASE_DIR = Path(\"/content\")\n",
        "elif IN_GHA:\n",
        "    BASE_DIR = Path(\"/home/runner/work\")\n",
        "else:\n",
        "    BASE_DIR = Path(\".\")\n",
        "\n",
        "REPO_NAME = \"forex-ai-models\"  # Updated repo name\n",
        "SAVE_FOLDER = BASE_DIR / REPO_NAME\n",
        "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(SAVE_FOLDER)\n",
        "print(f\"‚úÖ Working directory set to: {SAVE_FOLDER.resolve()}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Git Configuration (Universal)\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], check=False)\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Tokens & Secrets\n",
        "# ======================================================\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "BROWSERLESS_TOKEN = os.environ.get(\"BROWSERLESS_TOKEN\")\n",
        "\n",
        "# Load Colab secrets if missing\n",
        "if IN_COLAB and not FOREX_PAT:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get('FOREX_PAT')\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è No Colab secret found for FOREX_PAT\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    print(\"‚ö†Ô∏è FOREX_PAT not found ‚Äî GitHub cloning may fail.\")\n",
        "if not BROWSERLESS_TOKEN:\n",
        "    print(\"‚ö†Ô∏è BROWSERLESS_TOKEN not found.\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Output Folders\n",
        "# ======================================================\n",
        "CSV_FOLDER = SAVE_FOLDER / \"csvs\"\n",
        "PICKLE_FOLDER = SAVE_FOLDER / \"pickles\"\n",
        "LOGS_FOLDER = SAVE_FOLDER / \"logs\"\n",
        "\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Output folders ready:\")\n",
        "print(f\"   ‚Ä¢ CSVs:    {CSV_FOLDER}\")\n",
        "print(f\"   ‚Ä¢ Pickles: {PICKLE_FOLDER}\")\n",
        "print(f\"   ‚Ä¢ Logs:    {LOGS_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Environment Debug Info\n",
        "# ======================================================\n",
        "print(f\"Python version: {sys.version.split()[0]}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"Directory contents: {os.listdir('.')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oih6CDfjAjG9"
      },
      "outputs": [],
      "source": [
        "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn river\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JS9qXRF_JXJO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your keys (only for this session)\n",
        "os.environ['ALPHA_VANTAGE_KEY'] = '1W58NPZXOG5SLHZ6'\n",
        "os.environ['BROWSERLESS_TOKEN'] = '2TMVUBAjFwrr7Tb283f0da6602a4cb698b81778bda61967f7'\n",
        "\n",
        "# Test if they work\n",
        "print(\"Alpha Vantage Key:\", os.environ.get('ALPHA_VANTAGE_KEY'))\n",
        "print(\"Browserless Token:\", os.environ.get('BROWSERLESS_TOKEN'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfQDCpi612f3"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# ‚ö° Full Colab-ready GitHub Sync + Remove LFS\n",
        "# ======================================================\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import urllib.parse\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Environment / Paths\n",
        "# -----------------------------\n",
        "REPO_PARENT = Path(\"/content/forex-automation\")\n",
        "REPO_PARENT.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(REPO_PARENT)\n",
        "\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "REPO_FOLDER = REPO_PARENT / GITHUB_REPO\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ GitHub Token\n",
        "# -----------------------------\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "if not FOREX_PAT:\n",
        "    from google.colab import userdata\n",
        "    FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "    if FOREX_PAT:\n",
        "        os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "        print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå Missing FOREX_PAT. Set it in Colab userdata or GitHub secrets.\")\n",
        "\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Clean old repo\n",
        "# -----------------------------\n",
        "if REPO_FOLDER.exists():\n",
        "    print(f\"üóë Removing old repo: {REPO_FOLDER}\")\n",
        "    shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Clone repo safely (skip LFS)\n",
        "# -----------------------------\n",
        "print(\"üîó Cloning repo (skipping LFS)...\")\n",
        "env = os.environ.copy()\n",
        "env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
        "\n",
        "subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_FOLDER)], check=True, env=env)\n",
        "os.chdir(REPO_FOLDER)\n",
        "print(f\"‚úÖ Repo cloned successfully into {REPO_FOLDER}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Uninstall LFS and convert files\n",
        "# -----------------------------\n",
        "print(\"‚öôÔ∏è Removing Git LFS and converting files...\")\n",
        "subprocess.run([\"git\", \"lfs\", \"uninstall\"], check=True)\n",
        "subprocess.run([\"git\", \"lfs\", \"migrate\", \"export\", \"--include=*.csv\"], check=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Configure Git user\n",
        "# -----------------------------\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], check=True)\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Stage, commit, push\n",
        "# -----------------------------\n",
        "subprocess.run([\"git\", \"add\", \"-A\"], check=True)\n",
        "status = subprocess.run([\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True)\n",
        "\n",
        "if status.stdout.strip():\n",
        "    subprocess.run([\"git\", \"commit\", \"-m\", \"Remove LFS and convert files to normal Git\"], check=True)\n",
        "    subprocess.run([\"git\", \"push\", \"origin\", BRANCH], check=True)\n",
        "    print(\"üöÄ Repo updated: LFS removed permanently.\")\n",
        "else:\n",
        "    print(\"‚úÖ No changes detected. LFS already removed.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Create standard output folders\n",
        "# -----------------------------\n",
        "for folder in [\"csvs\", \"pickles\", \"logs\"]:\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "print(\"üìÅ Output folders ready: csvs/, pickles/, logs/\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Summary\n",
        "# -----------------------------\n",
        "print(\"\\nüßæ Summary:\")\n",
        "print(f\"‚Ä¢ Working Directory: {os.getcwd()}\")\n",
        "print(f\"‚Ä¢ Repository: https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}\")\n",
        "print(\"‚úÖ All operations completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wKFLKkgP1aBP"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üöÄ FULLY FIXED ALPHA VANTAGE FX WORKFLOW\n",
        "# - Uses URL-safe PAT\n",
        "# - Loads from Colab secrets\n",
        "# - Cleans stale repo + skips LFS\n",
        "# - GitHub Actions + Colab Safe\n",
        "# ======================================================\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import requests\n",
        "import subprocess\n",
        "import threading\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Detect Environment\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "print(f\"Detected environment: {'Colab' if IN_COLAB else 'GitHub/Local'}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Working directories\n",
        "# ======================================================\n",
        "BASE_FOLDER = Path(\"/content/forex-alpha-models\") if IN_COLAB else Path(\"./forex-alpha-models\")\n",
        "BASE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(BASE_FOLDER)\n",
        "\n",
        "PICKLE_FOLDER = BASE_FOLDER / \"pickles\"\n",
        "CSV_FOLDER = BASE_FOLDER / \"csvs\"\n",
        "LOG_FOLDER = BASE_FOLDER / \"logs\"\n",
        "\n",
        "for folder in [PICKLE_FOLDER, CSV_FOLDER, LOG_FOLDER]:\n",
        "    folder.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Working directory: {BASE_FOLDER.resolve()}\")\n",
        "print(f\"‚úÖ Output folders ready: {PICKLE_FOLDER}, {CSV_FOLDER}, {LOG_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GitHub Configuration\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "REPO_FOLDER = BASE_FOLDER / GITHUB_REPO\n",
        "\n",
        "# Load PAT from env or Colab userdata\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå Missing FOREX_PAT. Set it in Colab userdata or GitHub secrets.\")\n",
        "\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Safe Repo Clone / Sync\n",
        "# ======================================================\n",
        "if REPO_FOLDER.exists():\n",
        "    print(f\"üóë Removing old repo: {REPO_FOLDER}\")\n",
        "    shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "print(\"üîó Cloning repo (skipping LFS)...\")\n",
        "env = os.environ.copy()\n",
        "env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
        "\n",
        "subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)], check=True, env=env)\n",
        "os.chdir(REPO_FOLDER)\n",
        "print(f\"‚úÖ Repo cloned successfully into {REPO_FOLDER}\")\n",
        "\n",
        "# Configure Git identity\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], check=True)\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Alpha Vantage Setup\n",
        "# ======================================================\n",
        "ALPHA_VANTAGE_KEY = os.environ.get(\"ALPHA_VANTAGE_KEY\")\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    raise ValueError(\"‚ùå ALPHA_VANTAGE_KEY missing!\")\n",
        "\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "lock = threading.Lock()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def fetch_alpha_vantage_fx(pair, outputsize='full', max_retries=3, retry_delay=5):\n",
        "    base_url = 'https://www.alphavantage.co/query'\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "    params = {\n",
        "        'function': 'FX_DAILY',\n",
        "        'from_symbol': from_currency,\n",
        "        'to_symbol': to_currency,\n",
        "        'outputsize': outputsize,\n",
        "        'datatype': 'json',\n",
        "        'apikey': ALPHA_VANTAGE_KEY\n",
        "    }\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            r = requests.get(base_url, params=params, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "            if 'Time Series FX (Daily)' not in data:\n",
        "                raise ValueError(f\"Unexpected API response: {data}\")\n",
        "            ts = data['Time Series FX (Daily)']\n",
        "            df = pd.DataFrame(ts).T\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df.sort_index()\n",
        "            df = df.rename(columns={\n",
        "                '1. open': 'open',\n",
        "                '2. high': 'high',\n",
        "                '3. low': 'low',\n",
        "                '4. close': 'close'\n",
        "            }).astype(float)\n",
        "            df = ensure_tz_naive(df)\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed fetching {pair}: {e}\")\n",
        "            time.sleep(retry_delay)\n",
        "    print(f\"‚ùå Failed to fetch {pair} after {max_retries} retries\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Process Pairs for Unified CSV Pipeline\n",
        "# ======================================================\n",
        "def process_pair(pair):\n",
        "    filename = pair.replace(\"/\", \"_\") + \".csv\"\n",
        "    filepath = CSV_FOLDER / filename\n",
        "\n",
        "    if filepath.exists():\n",
        "        existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "    else:\n",
        "        existing_df = pd.DataFrame()\n",
        "\n",
        "    old_hash = file_hash(filepath)\n",
        "    new_df = fetch_alpha_vantage_fx(pair)\n",
        "    if new_df.empty:\n",
        "        return None, f\"No new data for {pair}\"\n",
        "\n",
        "    combined_df = pd.concat([existing_df, new_df]) if not existing_df.empty else new_df\n",
        "    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    with lock:\n",
        "        combined_df.to_csv(filepath)\n",
        "\n",
        "    new_hash = file_hash(filepath)\n",
        "    changed = old_hash != new_hash\n",
        "    print(f\"‚ÑπÔ∏è {pair} total rows: {len(combined_df)}\")\n",
        "    return str(filepath) if changed else None, f\"{pair} {'updated' if changed else 'no changes'}\"\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Execute All Pairs in Parallel\n",
        "# ======================================================\n",
        "changed_files = []\n",
        "tasks = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for pair in FX_PAIRS:\n",
        "        tasks.append(executor.submit(process_pair, pair))\n",
        "    for future in as_completed(tasks):\n",
        "        filepath, msg = future.result()\n",
        "        print(msg)\n",
        "        if filepath:\n",
        "            changed_files.append(filepath)\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Commit & Push Changes\n",
        "# ======================================================\n",
        "if changed_files:\n",
        "    print(f\"üöÄ Committing {len(changed_files)} updated files...\")\n",
        "    subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "    subprocess.run([\"git\", \"commit\", \"-m\", \"Update Alpha Vantage FX data\"], check=False)\n",
        "    subprocess.run([\"git\", \"push\", \"origin\", BRANCH], check=False)\n",
        "else:\n",
        "    print(\"‚úÖ No changes to commit.\")\n",
        "\n",
        "print(\"‚úÖ All FX pairs processed, saved, pushed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By3b3WMVJ-M3"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# FULLY IMPROVED FOREX DATA WORKFLOW - YFINANCE\n",
        "# Colab + GitHub Actions Safe, 403-Proof, Large History\n",
        "# ======================================================\n",
        "\n",
        "import os, time, hashlib, subprocess, shutil, threading\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Detect environment\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "IN_LOCAL = not IN_COLAB and not IN_GHA\n",
        "\n",
        "print(f\"Detected environment: {'Colab' if IN_COLAB else ('GitHub Actions' if IN_GHA else 'Local')}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Working directories\n",
        "# ======================================================\n",
        "BASE_DIR = Path(\"/content/forex-alpha-models\") if IN_COLAB else Path(\"./forex-alpha-models\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(BASE_DIR)\n",
        "\n",
        "PICKLE_FOLDER = BASE_DIR / \"pickles\"; PICKLE_FOLDER.mkdir(exist_ok=True)\n",
        "CSV_FOLDER = BASE_DIR / \"csvs\"; CSV_FOLDER.mkdir(exist_ok=True)\n",
        "LOG_FOLDER = BASE_DIR / \"logs\"; LOG_FOLDER.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Working directory: {BASE_DIR.resolve()}\")\n",
        "print(f\"‚úÖ Output folders ready: {PICKLE_FOLDER}, {CSV_FOLDER}, {LOG_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Git configuration\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT missing!\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
        "\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Clone or update repo safely\n",
        "# ======================================================\n",
        "REPO_FOLDER = BASE_DIR / GITHUB_REPO\n",
        "def ensure_repo_cloned(repo_url, repo_folder, branch=\"main\"):\n",
        "    repo_folder = Path(repo_folder)\n",
        "    tmp_folder = repo_folder.parent / (repo_folder.name + \"_tmp\")\n",
        "    if tmp_folder.exists(): shutil.rmtree(tmp_folder)\n",
        "    if not (repo_folder / \".git\").exists():\n",
        "        print(f\"üì• Cloning repo into {tmp_folder} ...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"-b\", branch, repo_url, str(tmp_folder)], check=True)\n",
        "        if repo_folder.exists(): shutil.rmtree(repo_folder)\n",
        "        tmp_folder.rename(repo_folder)\n",
        "    else:\n",
        "        print(\"üîÑ Repo exists, pulling latest...\")\n",
        "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"fetch\", \"origin\"], check=True)\n",
        "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"checkout\", branch], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"pull\", \"origin\", branch], check=False)\n",
        "    print(f\"‚úÖ Repo ready at {repo_folder.resolve()}\")\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "ensure_repo_cloned(REPO_URL, REPO_FOLDER, BRANCH)\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ FX pairs & timeframes\n",
        "# ======================================================\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "TIMEFRAMES = {\n",
        "    \"1d_5y\": (\"1d\", \"5y\"),\n",
        "    \"1h_2y\": (\"1h\", \"2y\"),\n",
        "    \"15m_60d\": (\"15m\", \"60d\"),\n",
        "    \"5m_1mo\": (\"5m\", \"1mo\"),\n",
        "    \"1m_7d\": (\"1m\", \"7d\")\n",
        "}\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Helper functions\n",
        "# ======================================================\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    if not filepath.exists(): return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"): md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    if df is None or df.empty: return df\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz: df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def merge_data(existing_df, new_df):\n",
        "    existing_df = ensure_tz_naive(existing_df)\n",
        "    new_df = ensure_tz_naive(new_df)\n",
        "    if existing_df.empty: return new_df\n",
        "    if new_df.empty: return existing_df\n",
        "    combined = pd.concat([existing_df, new_df])\n",
        "    combined = combined[~combined.index.duplicated(keep=\"last\")]\n",
        "    combined.sort_index(inplace=True)\n",
        "    return combined\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Worker function for pairs/timeframes\n",
        "# ======================================================\n",
        "def process_pair_tf(pair, tf_name, interval, period, max_retries=3, retry_delay=5):\n",
        "    symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
        "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
        "    filepath = REPO_FOLDER / filename\n",
        "\n",
        "    existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
        "    old_hash = file_hash(filepath)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            df = yf.download(symbol, interval=interval, period=period, progress=False, auto_adjust=False, threads=True)\n",
        "            if df.empty: raise ValueError(\"No data returned\")\n",
        "            df = df[[c for c in ['Open','High','Low','Close','Volume'] if c in df.columns]]\n",
        "            df.rename(columns=lambda x: x.lower(), inplace=True)\n",
        "            df = ensure_tz_naive(df)\n",
        "            combined_df = merge_data(existing_df, df)\n",
        "            combined_df.to_csv(filepath)\n",
        "            if old_hash != file_hash(filepath):\n",
        "                return f\"üìà Updated {pair} {tf_name}\", str(filepath)\n",
        "            return f\"‚úÖ No changes {pair} {tf_name}\", None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt+1}/{max_retries} failed for {pair} {tf_name}: {e}\")\n",
        "            if attempt < max_retries: time.sleep(retry_delay)\n",
        "            else: return f\"‚ùå Failed {pair} {tf_name}\", None\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Parallel execution\n",
        "# ======================================================\n",
        "changed_files = []\n",
        "tasks = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    for pair in FX_PAIRS:\n",
        "        for tf_name, (interval, period) in TIMEFRAMES.items():\n",
        "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period))\n",
        "\n",
        "for future in as_completed(tasks):\n",
        "    msg, filename = future.result()\n",
        "    print(msg)\n",
        "    if filename: changed_files.append(filename)\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Commit & push updates\n",
        "# ======================================================\n",
        "if changed_files:\n",
        "    print(f\"üöÄ Committing {len(changed_files)} updated files...\")\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"add\"] + changed_files, check=False)\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"commit\", \"-m\", \"Update YFinance FX data CSVs\"], check=False)\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"push\", \"origin\", BRANCH], check=False)\n",
        "else:\n",
        "    print(\"‚úÖ No changes detected, nothing to push.\")\n",
        "\n",
        "print(\"üéØ All FX pairs & timeframes processed safely with maximum historical rows!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fubyMudCLe_F"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# FX CSV Combine + Incremental Indicators Pipeline\n",
        "# Fully optimized for YFinance + Alpha Vantage\n",
        "# Thread-safe, timezone-safe, Git-push-safe, large dataset-ready\n",
        "# ======================================================\n",
        "\n",
        "import os, time, hashlib, subprocess, shutil\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Environment & folders\n",
        "# -----------------------------\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "ROOT_DIR = Path(\"/content/forex-alpha-models\") if IN_COLAB else Path(\".\")\n",
        "ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    levels = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\"}\n",
        "    print(f\"{levels.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Git configuration\n",
        "# -----------------------------\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Abdul Rahim\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT missing!\")\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Ensure repo exists\n",
        "# -----------------------------\n",
        "def ensure_repo():\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        if REPO_FOLDER.exists():\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "        print_status(f\"Cloning repo into {REPO_FOLDER}...\", \"info\")\n",
        "        subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)], check=True)\n",
        "    else:\n",
        "        print_status(\"Repo exists, pulling latest...\", \"info\")\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"fetch\", \"origin\"], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"checkout\", BRANCH], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH], check=False)\n",
        "        print_status(\"Repo synced successfully\", \"success\")\n",
        "ensure_repo()\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Helpers\n",
        "# -----------------------------\n",
        "def ensure_tz_naive(df):\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    df.index = df.index.tz_localize(None)\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath):\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def safe_numeric(df, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['open','high','low','close']\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df.replace([np.inf,-np.inf], np.nan, inplace=True)\n",
        "    df.dropna(subset=columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Incremental CSV combine\n",
        "# -----------------------------\n",
        "def combine_csv(csv_path):\n",
        "    target_file = REPO_FOLDER / csv_path.name\n",
        "    existing_df = ensure_tz_naive(pd.read_csv(target_file, index_col=0, parse_dates=True)) if target_file.exists() else pd.DataFrame()\n",
        "    new_df = ensure_tz_naive(pd.read_csv(csv_path, index_col=0, parse_dates=True))\n",
        "    combined_df = pd.concat([existing_df, new_df])\n",
        "    combined_df = combined_df[~combined_df.index.duplicated(keep=\"last\")]\n",
        "    combined_df.sort_index(inplace=True)\n",
        "    return combined_df, target_file\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Incremental indicators\n",
        "# -----------------------------\n",
        "def add_indicators_incremental(existing_df, combined_df):\n",
        "    new_rows = combined_df.loc[~combined_df.index.isin(existing_df.index)] if not existing_df.empty else combined_df\n",
        "    if new_rows.empty:\n",
        "        return None\n",
        "    new_rows = safe_numeric(new_rows)\n",
        "    new_rows.sort_index(inplace=True)\n",
        "\n",
        "    # Trend indicators\n",
        "    trend = {\n",
        "        'SMA_10': lambda d: ta.trend.sma_indicator(d['close'],10),\n",
        "        'SMA_50': lambda d: ta.trend.sma_indicator(d['close'],50),\n",
        "        'SMA_200': lambda d: ta.trend.sma_indicator(d['close'],200),\n",
        "        'EMA_10': lambda d: ta.trend.ema_indicator(d['close'],10),\n",
        "        'EMA_50': lambda d: ta.trend.ema_indicator(d['close'],50),\n",
        "        'EMA_200': lambda d: ta.trend.ema_indicator(d['close'],200),\n",
        "        'MACD': lambda d: ta.trend.macd(d['close']),\n",
        "        'MACD_signal': lambda d: ta.trend.macd_signal(d['close']),\n",
        "        'ADX': lambda d: ta.trend.adx(d['high'], d['low'], d['close'],14)\n",
        "    }\n",
        "    # Momentum indicators\n",
        "    momentum = {\n",
        "        'RSI_14': lambda d: ta.momentum.rsi(d['close'],14),\n",
        "        'StochRSI': lambda d: ta.momentum.stochrsi(d['close'],14),\n",
        "        'CCI': lambda d: ta.trend.cci(d['high'],d['low'],d['close'],20),\n",
        "        'ROC': lambda d: ta.momentum.roc(d['close'],12),\n",
        "        'Williams_%R': lambda d: WilliamsRIndicator(d['high'],d['low'],d['close'],14).williams_r()\n",
        "    }\n",
        "    # Volatility\n",
        "    volatility = {\n",
        "        'Bollinger_High': lambda d: ta.volatility.bollinger_hband(d['close'],20,2),\n",
        "        'Bollinger_Low': lambda d: ta.volatility.bollinger_lband(d['close'],20,2),\n",
        "        'ATR': lambda d: ta.volatility.average_true_range(d['high'],d['low'],d['close'],14),\n",
        "        'STDDEV_20': lambda d: d['close'].rolling(20).std()\n",
        "    }\n",
        "    # Volume-based\n",
        "    volume = {}\n",
        "    if 'volume' in new_rows.columns:\n",
        "        volume = {\n",
        "            'OBV': lambda d: ta.volume.on_balance_volume(d['close'],d['volume']),\n",
        "            'MFI': lambda d: ta.volume.money_flow_index(d['high'],d['low'],d['close'],d['volume'],14)\n",
        "        }\n",
        "\n",
        "    indicators = {**trend, **momentum, **volatility, **volume}\n",
        "    for name, func in indicators.items():\n",
        "        try:\n",
        "            new_rows[name] = func(new_rows)\n",
        "        except Exception:\n",
        "            new_rows[name] = np.nan\n",
        "\n",
        "    # Cross signals\n",
        "    if 'EMA_10' in new_rows.columns and 'EMA_50' in new_rows.columns:\n",
        "        new_rows['EMA_10_cross_EMA_50'] = (new_rows['EMA_10'] > new_rows['EMA_50']).astype(int)\n",
        "    if 'EMA_50' in new_rows.columns and 'EMA_200' in new_rows.columns:\n",
        "        new_rows['EMA_50_cross_EMA_200'] = (new_rows['EMA_50'] > new_rows['EMA_200']).astype(int)\n",
        "    if 'SMA_10' in new_rows.columns and 'SMA_50' in new_rows.columns:\n",
        "        new_rows['SMA_10_cross_SMA_50'] = (new_rows['SMA_10'] > new_rows['SMA_50']).astype(int)\n",
        "    if 'SMA_50' in new_rows.columns and 'SMA_200' in new_rows.columns:\n",
        "        new_rows['SMA_50_cross_SMA_200'] = (new_rows['SMA_50'] > new_rows['SMA_200']).astype(int)\n",
        "\n",
        "    # Scale numeric columns safely\n",
        "    numeric_cols = new_rows.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0 and not new_rows[numeric_cols].dropna(how='all').empty:\n",
        "        scaler = MinMaxScaler()\n",
        "        new_rows[numeric_cols] = scaler.fit_transform(new_rows[numeric_cols])\n",
        "\n",
        "    return new_rows\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Worker function\n",
        "# -----------------------------\n",
        "def process_csv_file(csv_file):\n",
        "    combined_df, target_file = combine_csv(csv_file)\n",
        "    existing_pickle = PICKLE_FOLDER / f\"{csv_file.stem}_indicators.pkl\"\n",
        "    existing_df = pd.read_pickle(existing_pickle) if existing_pickle.exists() else pd.DataFrame()\n",
        "\n",
        "    new_indicators = add_indicators_incremental(existing_df, combined_df)\n",
        "    if new_indicators is not None:\n",
        "        updated_df = pd.concat([existing_df, new_indicators]).sort_index()\n",
        "        with lock:\n",
        "            updated_df.to_pickle(existing_pickle, protocol=4)\n",
        "            combined_df.to_csv(target_file)\n",
        "        msg = f\"{csv_file.name} updated with {len(new_indicators)} new rows\"\n",
        "    else:\n",
        "        msg = f\"{csv_file.name} no new rows\"\n",
        "\n",
        "    total_rows = len(combined_df)\n",
        "    print_status(f\"{csv_file.name} total rows: {total_rows}\", \"info\")\n",
        "\n",
        "    return str(existing_pickle) if new_indicators is not None else None, msg\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Process all CSVs in parallel\n",
        "# -----------------------------\n",
        "csv_files = list(CSV_FOLDER.glob(\"*.csv\"))\n",
        "if not csv_files:\n",
        "    print_status(\"No CSVs found to process ‚Äî pipeline will skip\", \"warn\")\n",
        "\n",
        "changed_files = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=min(8, len(csv_files) or 1)) as executor:\n",
        "    futures = [executor.submit(process_csv_file, f) for f in csv_files]\n",
        "    for future in as_completed(futures):\n",
        "        file, msg = future.result()\n",
        "        print_status(msg, \"success\" if file else \"info\")\n",
        "        if file:\n",
        "            changed_files.append(file)\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Commit & push updates\n",
        "# -----------------------------\n",
        "if changed_files:\n",
        "    print_status(f\"Committing {len(changed_files)} updated files...\", \"info\")\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"add\"] + changed_files, check=False)\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"commit\", \"-m\", \"üìà Auto update FX CSVs & indicators\"], check=False)\n",
        "    push_cmd = f\"git -C {REPO_FOLDER} push {REPO_URL} {BRANCH}\"\n",
        "    for attempt in range(3):\n",
        "        if subprocess.run(push_cmd, shell=True).returncode == 0:\n",
        "            print_status(\"Push successful\", \"success\")\n",
        "            break\n",
        "        else:\n",
        "            print_status(f\"Push attempt {attempt+1} failed, retrying...\", \"warn\")\n",
        "            time.sleep(5)\n",
        "else:\n",
        "    print_status(\"No files changed ‚Äî skipping push\", \"info\")\n",
        "\n",
        "print_status(\"All CSVs combined, incremental indicators added, and Git updated successfully.\", \"success\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqze9KKNN2xP"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "VERSION 3.5 ‚Äì ULTRA-PERSISTENT SELF-LEARNING HYBRID FX PIPELINE\n",
        "================================================================\n",
        "üöÄ IMPROVEMENTS:\n",
        "- ‚úÖ SQLite database for infinite trade history\n",
        "- ‚úÖ Real accuracy tracking from actual TP/SL hits\n",
        "- ‚úÖ Model performance comparison (SGD vs RF)\n",
        "- ‚úÖ Auto model selection based on recent performance\n",
        "- ‚úÖ Corrupted pickle auto-cleanup\n",
        "- ‚úÖ Protocol 4 for stable persistence\n",
        "- ‚úÖ Learning from winning patterns\n",
        "- ‚úÖ Adaptive confidence thresholds\n",
        "- ‚úÖ Better live price integration\n",
        "\"\"\"\n",
        "\n",
        "import os, time, json, re, shutil, subprocess, pickle, filecmp, sqlite3\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import ta\n",
        "import logging\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from collections import defaultdict\n",
        "\n",
        "# ======================================================\n",
        "# 0Ô∏è‚É£ Logging & Environment\n",
        "# ======================================================\n",
        "ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=LOGS_FOLDER / \"pipeline.log\",\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
        ")\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    icons = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\",\"debug\":\"üêû\",\"error\":\"‚ùå\"}\n",
        "    getattr(logging, level if level != \"warn\" else \"warning\", logging.info)(msg)\n",
        "    print(f\"{icons.get(level,'‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# ======================================================\n",
        "# üÜï DATABASE FOR INFINITE MEMORY\n",
        "# ======================================================\n",
        "TRADE_MEMORY_DB = REPO_FOLDER / \"hybrid_ml_memory.db\"\n",
        "\n",
        "class TradeMemoryDatabase:\n",
        "    \"\"\"Persistent storage for all trades and model performance\"\"\"\n",
        "\n",
        "    def __init__(self, db_path=TRADE_MEMORY_DB):\n",
        "        self.db_path = db_path\n",
        "        self.conn = None\n",
        "        self.initialize_database()\n",
        "\n",
        "    def initialize_database(self):\n",
        "        self.conn = sqlite3.connect(str(self.db_path))\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        # Signals history\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS ml_signals (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                timeframe TEXT NOT NULL,\n",
        "                sgd_prediction INTEGER,\n",
        "                rf_prediction INTEGER,\n",
        "                ensemble_prediction INTEGER,\n",
        "                live_price REAL,\n",
        "                sl_price REAL,\n",
        "                tp_price REAL,\n",
        "                confidence REAL\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Trade results\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS ml_trade_results (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                signal_id INTEGER,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                timeframe TEXT NOT NULL,\n",
        "                entry_price REAL,\n",
        "                exit_price REAL,\n",
        "                prediction INTEGER,\n",
        "                was_correct BOOLEAN,\n",
        "                pnl REAL,\n",
        "                model_used TEXT,\n",
        "                FOREIGN KEY (signal_id) REFERENCES ml_signals(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Model performance tracking\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS model_performance (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                model_name TEXT NOT NULL,\n",
        "                accuracy REAL,\n",
        "                total_trades INTEGER,\n",
        "                winning_trades INTEGER,\n",
        "                avg_pnl REAL,\n",
        "                confidence_score REAL\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        self.conn.commit()\n",
        "        print_status(\"‚úÖ ML Trade Memory Database initialized\", \"success\")\n",
        "\n",
        "    def save_signal(self, pair, timeframe, sgd_pred, rf_pred, ensemble_pred,\n",
        "                    live_price, sl, tp, confidence):\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            INSERT INTO ml_signals\n",
        "            (timestamp, pair, timeframe, sgd_prediction, rf_prediction,\n",
        "             ensemble_prediction, live_price, sl_price, tp_price, confidence)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', (\n",
        "            datetime.now(timezone.utc).isoformat(),\n",
        "            pair, timeframe, sgd_pred, rf_pred, ensemble_pred,\n",
        "            live_price, sl, tp, confidence\n",
        "        ))\n",
        "        self.conn.commit()\n",
        "        return cursor.lastrowid\n",
        "\n",
        "    def save_trade_result(self, signal_id, pair, timeframe, entry_price,\n",
        "                          exit_price, prediction, was_correct, pnl, model_used):\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            INSERT INTO ml_trade_results\n",
        "            (signal_id, timestamp, pair, timeframe, entry_price, exit_price,\n",
        "             prediction, was_correct, pnl, model_used)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', (\n",
        "            signal_id,\n",
        "            datetime.now(timezone.utc).isoformat(),\n",
        "            pair, timeframe, entry_price, exit_price,\n",
        "            prediction, was_correct, pnl, model_used\n",
        "        ))\n",
        "        self.conn.commit()\n",
        "\n",
        "    def get_model_performance(self, pair, model_name, days=7):\n",
        "        \"\"\"Get recent performance for a specific model\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        since_date = (datetime.now() - timedelta(days=days)).isoformat()\n",
        "\n",
        "        cursor.execute('''\n",
        "            SELECT\n",
        "                COUNT(*) as total_trades,\n",
        "                SUM(CASE WHEN was_correct THEN 1 ELSE 0 END) as wins,\n",
        "                AVG(pnl) as avg_pnl,\n",
        "                SUM(pnl) as total_pnl\n",
        "            FROM ml_trade_results\n",
        "            WHERE pair = ? AND model_used = ? AND timestamp > ?\n",
        "        ''', (pair, model_name, since_date))\n",
        "\n",
        "        result = cursor.fetchone()\n",
        "        return {\n",
        "            'total_trades': result[0] if result[0] else 0,\n",
        "            'winning_trades': result[1] if result[1] else 0,\n",
        "            'accuracy': (result[1] / result[0] * 100) if result[0] else 0,\n",
        "            'avg_pnl': result[2] if result[2] else 0,\n",
        "            'total_pnl': result[3] if result[3] else 0\n",
        "        }\n",
        "\n",
        "    def get_best_model(self, pair, days=7):\n",
        "        \"\"\"Determine which model (SGD/RF/Ensemble) performs best\"\"\"\n",
        "        models = ['SGD', 'RandomForest', 'Ensemble']\n",
        "        performances = {}\n",
        "\n",
        "        for model in models:\n",
        "            perf = self.get_model_performance(pair, model, days)\n",
        "            if perf['total_trades'] >= 5:  # Minimum trades for reliability\n",
        "                performances[model] = perf['accuracy']\n",
        "\n",
        "        if not performances:\n",
        "            return 'Ensemble'  # Default to ensemble\n",
        "\n",
        "        return max(performances, key=performances.get)\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "\n",
        "TRADE_DB = TradeMemoryDatabase()\n",
        "\n",
        "# ======================================================\n",
        "# üÜï PERSISTENT ITERATION COUNTER\n",
        "# ======================================================\n",
        "ITERATION_COUNTER_FILE = REPO_FOLDER / \"ml_iteration_counter.pkl\"\n",
        "\n",
        "class MLIterationCounter:\n",
        "    \"\"\"Tracks total ML pipeline iterations across all runs forever\"\"\"\n",
        "\n",
        "    def __init__(self, counter_file=ITERATION_COUNTER_FILE):\n",
        "        self.counter_file = counter_file\n",
        "        self.data = self.load_counter()\n",
        "\n",
        "    def load_counter(self):\n",
        "        if self.counter_file.exists():\n",
        "            try:\n",
        "                with open(self.counter_file, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "                print_status(f\"‚úÖ Loaded ML iteration counter: {data['total_iterations']} total runs\", \"success\")\n",
        "                return data\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return {\n",
        "            'total_iterations': 0,\n",
        "            'start_date': datetime.now(timezone.utc).isoformat(),\n",
        "            'last_run': None,\n",
        "            'run_history': []\n",
        "        }\n",
        "\n",
        "    def increment(self):\n",
        "        \"\"\"Increment and save counter\"\"\"\n",
        "        self.data['total_iterations'] += 1\n",
        "        self.data['last_run'] = datetime.now(timezone.utc).isoformat()\n",
        "        self.data['run_history'].append({\n",
        "            'iteration': self.data['total_iterations'],\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat()\n",
        "        })\n",
        "\n",
        "        # Keep only last 1000 runs\n",
        "        if len(self.data['run_history']) > 1000:\n",
        "            self.data['run_history'] = self.data['run_history'][-1000:]\n",
        "\n",
        "        self.save_counter()\n",
        "        return self.data['total_iterations']\n",
        "\n",
        "    def save_counter(self):\n",
        "        try:\n",
        "            with open(self.counter_file, 'wb') as f:\n",
        "                pickle.dump(self.data, f, protocol=4)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save ML iteration counter: {e}\")\n",
        "\n",
        "    def get_current(self):\n",
        "        return self.data['total_iterations']\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get statistics about runs\"\"\"\n",
        "        if not self.data['run_history']:\n",
        "            return {}\n",
        "\n",
        "        first_run = datetime.fromisoformat(self.data['start_date'])\n",
        "        days_running = (datetime.now(timezone.utc) - first_run).days\n",
        "\n",
        "        return {\n",
        "            'total_iterations': self.data['total_iterations'],\n",
        "            'days_running': days_running,\n",
        "            'avg_iterations_per_day': self.data['total_iterations'] / max(days_running, 1),\n",
        "            'start_date': self.data['start_date'],\n",
        "            'last_run': self.data['last_run']\n",
        "        }\n",
        "\n",
        "ML_ITERATION_COUNTER = MLIterationCounter()\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Git & Credentials\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "BROWSERLESS_TOKEN = os.environ.get(\"BROWSERLESS_TOKEN\",\"\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT missing!\")\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "subprocess.run([\"git\",\"config\",\"--global\",\"user.name\",GIT_NAME], check=False)\n",
        "subprocess.run([\"git\",\"config\",\"--global\",\"user.email\",GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\",\"config\",\"--global\",\"credential.helper\",\"store\"], check=False)\n",
        "\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "def ensure_repo():\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        if REPO_FOLDER.exists():\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "        print_status(f\"Cloning repo into {REPO_FOLDER}...\", \"info\")\n",
        "        subprocess.run([\"git\",\"clone\",\"-b\",BRANCH,REPO_URL,str(REPO_FOLDER)], check=True)\n",
        "    else:\n",
        "        print_status(\"Repo exists, pulling latest...\", \"info\")\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"fetch\",\"origin\"], check=False)\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"checkout\",BRANCH], check=False)\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"pull\",\"origin\",BRANCH], check=False)\n",
        "        print_status(\"‚úÖ Repo synced successfully\", \"success\")\n",
        "\n",
        "ensure_repo()\n",
        "\n",
        "# ======================================================\n",
        "# üÜï CLEANUP CORRUPTED PICKLES\n",
        "# ======================================================\n",
        "def cleanup_corrupted_pickles():\n",
        "    \"\"\"Remove corrupted pickle files at startup\"\"\"\n",
        "    print_status(\"üßπ Checking for corrupted ML pickle files...\", \"info\")\n",
        "\n",
        "    corrupted_count = 0\n",
        "    for pkl_file in PICKLE_FOLDER.glob(\"*.pkl\"):\n",
        "        try:\n",
        "            with open(pkl_file, 'rb') as f:\n",
        "                pickle.load(f)\n",
        "        except Exception:\n",
        "            try:\n",
        "                pkl_file.unlink()\n",
        "                print_status(f\"üóëÔ∏è Removed corrupted: {pkl_file.name}\", \"warn\")\n",
        "                corrupted_count += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    if corrupted_count > 0:\n",
        "        print_status(f\"‚úÖ Cleaned up {corrupted_count} corrupted files\", \"success\")\n",
        "    else:\n",
        "        print_status(\"‚úÖ No corrupted files found\", \"success\")\n",
        "\n",
        "cleanup_corrupted_pickles()\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CSV Loader + Sanity Check\n",
        "# ======================================================\n",
        "def load_csv(path):\n",
        "    if not path.exists():\n",
        "        print_status(f\"‚ö†Ô∏è CSV missing: {path}\", \"warn\")\n",
        "        return None\n",
        "\n",
        "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
        "    df.columns = [c.strip().lower().replace(\" \",\"_\") for c in df.columns]\n",
        "\n",
        "    for col in [\"open\",\"high\",\"low\",\"close\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "        df[col] = df[col].ffill().bfill()\n",
        "\n",
        "    df = df[[\"open\",\"high\",\"low\",\"close\"]].dropna(how='all')\n",
        "\n",
        "    # Price sanity check\n",
        "    if len(df) > 0:\n",
        "        mean_price = df['close'].mean()\n",
        "        if mean_price < 0.5 or mean_price > 200:\n",
        "            print_status(f\"‚ö†Ô∏è {path.name} suspicious price (mean={mean_price:.2f}), skipping\", \"warn\")\n",
        "            return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Live Price Fetch\n",
        "# ======================================================\n",
        "def fetch_live_rate(pair):\n",
        "    if not BROWSERLESS_TOKEN:\n",
        "        print_status(\"‚ö†Ô∏è BROWSERLESS_TOKEN missing\", \"warn\")\n",
        "        return 0\n",
        "\n",
        "    from_currency, to_currency = pair.split(\"/\")\n",
        "    url = f\"https://production-sfo.browserless.io/content?token={BROWSERLESS_TOKEN}\"\n",
        "    payload = {\n",
        "        \"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        res = requests.post(url, json=payload, timeout=10)\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
        "        rate = float(match.group(1).replace(\",\",\"\")) if match else 0\n",
        "\n",
        "        if rate > 0:\n",
        "            print_status(f\"üíπ {pair} live price: {rate}\", \"info\")\n",
        "\n",
        "        return rate\n",
        "    except Exception as e:\n",
        "        print_status(f\"Failed to fetch {pair}: {e}\", \"warn\")\n",
        "        return 0\n",
        "\n",
        "def inject_live_price(df, live_price, n_candles=3):\n",
        "    \"\"\"Inject live price into recent candles for real-time analysis\"\"\"\n",
        "    if live_price <= 0 or df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df_copy = df.copy()\n",
        "    n_inject = min(n_candles, len(df_copy))\n",
        "\n",
        "    for i in range(n_inject):\n",
        "        # Add small random variation to simulate realistic price movement\n",
        "        price = live_price * (1 + np.random.uniform(-0.0005, 0.0005))\n",
        "\n",
        "        for col in [\"open\",\"high\",\"low\",\"close\"]:\n",
        "            if col in df_copy.columns:\n",
        "                df_copy.iloc[-n_inject+i, df_copy.columns.get_loc(col)] = price\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Enhanced Indicators\n",
        "# ======================================================\n",
        "scaler_global = MinMaxScaler()\n",
        "\n",
        "def add_indicators(df, fit_scaler=True):\n",
        "    \"\"\"Add technical indicators with error handling\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    try:\n",
        "        # Trend indicators\n",
        "        if len(df) >= 50:\n",
        "            df['SMA_50'] = ta.trend.SMAIndicator(df['close'], 50).sma_indicator()\n",
        "        if len(df) >= 20:\n",
        "            df['EMA_20'] = ta.trend.EMAIndicator(df['close'], 20).ema_indicator()\n",
        "\n",
        "        # Momentum indicators\n",
        "        if len(df) >= 14:\n",
        "            df['RSI_14'] = ta.momentum.RSIIndicator(df['close'], 14).rsi()\n",
        "            df['Williams_%R'] = ta.momentum.WilliamsRIndicator(\n",
        "                df['high'], df['low'], df['close'], 14\n",
        "            ).williams_r()\n",
        "\n",
        "        # Volatility\n",
        "        if len(df) >= 20:\n",
        "            df['ATR_14'] = ta.volatility.AverageTrueRange(\n",
        "                df['high'], df['low'], df['close'], 14\n",
        "            ).average_true_range()\n",
        "\n",
        "        # Trend strength\n",
        "        df['MACD'] = ta.trend.MACD(df['close']).macd()\n",
        "        df['CCI_20'] = ta.trend.CCIIndicator(df['high'], df['low'], df['close'], 20).cci()\n",
        "\n",
        "        if len(df) >= 14:\n",
        "            df['ADX_14'] = ta.trend.ADXIndicator(df['high'], df['low'], df['close'], 14).adx()\n",
        "\n",
        "        # Fill NaN values (using modern pandas syntax)\n",
        "        df = df.ffill().bfill().fillna(0)\n",
        "\n",
        "        # Scale numeric columns (except OHLC)\n",
        "        numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns\n",
        "                       if c not in ['open', 'high', 'low', 'close']]\n",
        "\n",
        "        if numeric_cols and len(numeric_cols) > 0:\n",
        "            if fit_scaler:\n",
        "                df[numeric_cols] = scaler_global.fit_transform(df[numeric_cols])\n",
        "            else:\n",
        "                try:\n",
        "                    df[numeric_cols] = scaler_global.transform(df[numeric_cols])\n",
        "                except NotFittedError:\n",
        "                    df[numeric_cols] = scaler_global.fit_transform(df[numeric_cols])\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"Warning: Indicator calculation issue: {e}\", \"warn\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Enhanced ML Training with Performance Tracking\n",
        "# ======================================================\n",
        "def train_predict_ml_enhanced(df, pair_name, timeframe):\n",
        "    \"\"\"Train both SGD and RandomForest, return best prediction\"\"\"\n",
        "    df = df.dropna()\n",
        "\n",
        "    if len(df) < 50:\n",
        "        return 0, 0, 0, 0.5  # No prediction if insufficient data\n",
        "\n",
        "    # Prepare features\n",
        "    X = df.drop(columns=['close'], errors='ignore')\n",
        "    X = X if not X.empty else df[['close']]\n",
        "    y = (df['close'].diff() > 0).astype(int).fillna(0)\n",
        "    X = X.fillna(0)\n",
        "\n",
        "    safe_pair_name = pair_name.replace(\"/\", \"_\")\n",
        "    safe_tf_name = timeframe.replace(\"/\", \"_\")\n",
        "\n",
        "    # ===== SGD Training =====\n",
        "    sgd_file = PICKLE_FOLDER / f\"{safe_pair_name}_{safe_tf_name}_sgd.pkl\"\n",
        "\n",
        "    if sgd_file.exists():\n",
        "        try:\n",
        "            sgd = pickle.load(open(sgd_file, \"rb\"))\n",
        "        except:\n",
        "            sgd = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
        "            sgd.partial_fit(X, y, classes=np.array([0, 1]))\n",
        "    else:\n",
        "        sgd = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
        "        sgd.partial_fit(X, y, classes=np.array([0, 1]))\n",
        "\n",
        "    sgd.partial_fit(X, y)\n",
        "    pickle.dump(sgd, open(sgd_file, \"wb\"), protocol=4)\n",
        "    sgd_pred = int(sgd.predict(X.iloc[[-1]])[0])\n",
        "\n",
        "    # Get SGD confidence\n",
        "    try:\n",
        "        sgd_proba = sgd.predict_proba(X.iloc[[-1]])[0]\n",
        "        sgd_confidence = float(max(sgd_proba))\n",
        "    except:\n",
        "        sgd_confidence = 0.5\n",
        "\n",
        "    # ===== RandomForest with Historical Memory =====\n",
        "    hist_file = PICKLE_FOLDER / f\"{safe_pair_name}_{safe_tf_name}_rf_hist.pkl\"\n",
        "\n",
        "    if hist_file.exists():\n",
        "        try:\n",
        "            hist_X, hist_y = pickle.load(open(hist_file, \"rb\"))\n",
        "            # Append new data\n",
        "            hist_X = pd.concat([hist_X, X], ignore_index=True)\n",
        "            hist_y = pd.concat([hist_y, y], ignore_index=True)\n",
        "\n",
        "            # Keep last 5000 rows to prevent memory bloat\n",
        "            if len(hist_X) > 5000:\n",
        "                hist_X = hist_X.iloc[-5000:]\n",
        "                hist_y = hist_y.iloc[-5000:]\n",
        "        except:\n",
        "            hist_X, hist_y = X.copy(), y.copy()\n",
        "    else:\n",
        "        hist_X, hist_y = X.copy(), y.copy()\n",
        "\n",
        "    rf_file = PICKLE_FOLDER / f\"{safe_pair_name}_{safe_tf_name}_rf.pkl\"\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=50,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        max_depth=10\n",
        "    )\n",
        "\n",
        "    rf.fit(hist_X, hist_y)\n",
        "    pickle.dump(rf, open(rf_file, \"wb\"), protocol=4)\n",
        "    pickle.dump((hist_X, hist_y), open(hist_file, \"wb\"), protocol=4)\n",
        "\n",
        "    rf_pred = int(rf.predict(X.iloc[[-1]])[0])\n",
        "\n",
        "    # Get RF confidence\n",
        "    try:\n",
        "        rf_proba = rf.predict_proba(X.iloc[[-1]])[0]\n",
        "        rf_confidence = float(max(rf_proba))\n",
        "    except:\n",
        "        rf_confidence = 0.5\n",
        "\n",
        "    # ===== Ensemble Decision =====\n",
        "    # Check which model has better recent performance\n",
        "    best_model = TRADE_DB.get_best_model(pair_name, days=7)\n",
        "\n",
        "    if best_model == 'SGD':\n",
        "        ensemble_pred = sgd_pred\n",
        "        confidence = sgd_confidence\n",
        "    elif best_model == 'RandomForest':\n",
        "        ensemble_pred = rf_pred\n",
        "        confidence = rf_confidence\n",
        "    else:  # Ensemble (vote)\n",
        "        ensemble_pred = 1 if (sgd_pred + rf_pred) >= 1 else 0\n",
        "        confidence = (sgd_confidence + rf_confidence) / 2\n",
        "\n",
        "    return sgd_pred, rf_pred, ensemble_pred, confidence\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ ATR-based SL/TP\n",
        "# ======================================================\n",
        "def calculate_dynamic_sl_tp(df, live_price):\n",
        "    \"\"\"Calculate Stop Loss and Take Profit using ATR\"\"\"\n",
        "    if live_price == 0 or df is None or df.empty:\n",
        "        return 0, 0\n",
        "\n",
        "    try:\n",
        "        atr = ta.volatility.AverageTrueRange(\n",
        "            df['high'], df['low'], df['close'], 14\n",
        "        ).average_true_range().iloc[-1]\n",
        "\n",
        "        # Adaptive multiplier based on volatility\n",
        "        mult = 2.0 if atr / live_price < 0.05 else 1.5\n",
        "\n",
        "        sl = max(0, round(live_price - atr * mult, 5))\n",
        "        tp = round(live_price + atr * mult, 5)\n",
        "\n",
        "        print_status(\n",
        "            f\"üêû SL/TP: price={live_price}, ATR={atr:.5f}, \"\n",
        "            f\"mult={mult:.2f}, SL={sl}, TP={tp}\",\n",
        "            \"debug\"\n",
        "        )\n",
        "\n",
        "        return sl, tp\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"SL/TP calculation error: {e}\", \"warn\")\n",
        "        return 0, 0\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Multi-Timeframe Resampling\n",
        "# ======================================================\n",
        "TIMEFRAMES = {\n",
        "    \"1m_7d\": \"1min\",\n",
        "    \"5m_1mo\": \"5min\",\n",
        "    \"15m_60d\": \"15min\",\n",
        "    \"1h_2y\": \"1h\",\n",
        "    \"1d_5y\": \"1d\"\n",
        "}\n",
        "\n",
        "def resample_timeframe(df, tf_rule, periods):\n",
        "    \"\"\"Resample OHLC data to different timeframe\"\"\"\n",
        "    try:\n",
        "        df = df.copy()\n",
        "        df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
        "        df = df[['open', 'high', 'low', 'close']]\n",
        "\n",
        "        df_resampled = df.resample(tf_rule).agg({\n",
        "            'open': 'first',\n",
        "            'high': 'max',\n",
        "            'low': 'min',\n",
        "            'close': 'last'\n",
        "        }).dropna()\n",
        "\n",
        "        return df_resampled.tail(periods)\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"Resampling error: {e}\", \"warn\")\n",
        "        return df\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Weighted Aggregation\n",
        "# ======================================================\n",
        "TIMEFRAME_WEIGHTS = {\n",
        "    \"1m_7d\": 0.5,\n",
        "    \"5m_1mo\": 1.0,\n",
        "    \"15m_60d\": 1.5,\n",
        "    \"1h_2y\": 2.0,\n",
        "    \"1d_5y\": 3.0\n",
        "}\n",
        "\n",
        "def weighted_aggregate(signals):\n",
        "    \"\"\"Aggregate signals across timeframes with weights\"\"\"\n",
        "    score, total_weight = 0, 0\n",
        "\n",
        "    for tf, data in signals.items():\n",
        "        w = TIMEFRAME_WEIGHTS.get(tf, 1.0)\n",
        "        score += data['signal'] * w\n",
        "        total_weight += w\n",
        "\n",
        "    avg = score / total_weight if total_weight > 0 else 0\n",
        "\n",
        "    if avg >= 0.6:\n",
        "        return \"STRONG_LONG\"\n",
        "    elif avg <= 0.4:\n",
        "        return \"STRONG_SHORT\"\n",
        "    else:\n",
        "        return \"HOLD\"\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Process Single Pair CSV\n",
        "# ======================================================\n",
        "def process_pair_csv(csv_file):\n",
        "    \"\"\"Process one currency pair across all timeframes\"\"\"\n",
        "    pair = csv_file.stem.replace(\"_\", \"/\")\n",
        "    df = load_csv(csv_file)\n",
        "\n",
        "    if df is None:\n",
        "        return pair, {}, \"HOLD\"\n",
        "\n",
        "    # Fetch live price\n",
        "    live_price = fetch_live_rate(pair)\n",
        "\n",
        "    if live_price > 0:\n",
        "        df = inject_live_price(df, live_price)\n",
        "    else:\n",
        "        # Use last close price if API fails\n",
        "        live_price = float(df['close'].iloc[-1])\n",
        "        print_status(f\"‚ö†Ô∏è {pair}: Using last close price {live_price}\", \"warn\")\n",
        "\n",
        "    signals = {}\n",
        "    periods_map = {\n",
        "        \"1min\": 7 * 24 * 60,\n",
        "        \"5min\": 30 * 24 * 12,\n",
        "        \"15min\": 60 * 24 * 4,\n",
        "        \"1h\": 24 * 730,\n",
        "        \"1d\": 5 * 365\n",
        "    }\n",
        "\n",
        "    for tf_name, tf_rule in TIMEFRAMES.items():\n",
        "        try:\n",
        "            # Resample to timeframe\n",
        "            df_tf = resample_timeframe(df, tf_rule, periods_map.get(tf_rule, 100))\n",
        "\n",
        "            # Add indicators\n",
        "            df_tf = add_indicators(df_tf, fit_scaler=False)\n",
        "\n",
        "            # Inject live price\n",
        "            if live_price > 0:\n",
        "                df_tf = inject_live_price(df_tf, live_price)\n",
        "\n",
        "            # ML prediction\n",
        "            sgd_pred, rf_pred, ensemble_pred, confidence = train_predict_ml_enhanced(\n",
        "                df_tf, pair, tf_name\n",
        "            )\n",
        "\n",
        "            # Calculate SL/TP\n",
        "            sl, tp = calculate_dynamic_sl_tp(df_tf, live_price)\n",
        "\n",
        "            # Save signal to database\n",
        "            signal_id = TRADE_DB.save_signal(\n",
        "                pair, tf_name, sgd_pred, rf_pred, ensemble_pred,\n",
        "                live_price, sl, tp, confidence\n",
        "            )\n",
        "\n",
        "            signals[tf_name] = {\n",
        "                \"signal\": ensemble_pred,\n",
        "                \"sgd_pred\": sgd_pred,\n",
        "                \"rf_pred\": rf_pred,\n",
        "                \"confidence\": confidence,\n",
        "                \"live\": live_price,\n",
        "                \"SL\": sl,\n",
        "                \"TP\": tp,\n",
        "                \"signal_id\": signal_id\n",
        "            }\n",
        "\n",
        "            print_status(\n",
        "                f\"{pair} | {tf_name} | SGD:{sgd_pred} RF:{rf_pred} \"\n",
        "                f\"Ensemble:{ensemble_pred} | conf:{confidence:.2f} | \"\n",
        "                f\"price:{live_price} | SL:{sl} TP:{tp}\",\n",
        "                \"info\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"Error processing {pair} {tf_name}: {e}\", \"error\")\n",
        "            continue\n",
        "\n",
        "    # Aggregate across timeframes\n",
        "    agg_signal = weighted_aggregate(signals)\n",
        "    print_status(f\"{pair} | AGGREGATED: {agg_signal}\", \"success\")\n",
        "\n",
        "    return pair, signals, agg_signal\n",
        "\n",
        "# ======================================================\n",
        "# üîü Full Pipeline\n",
        "# ======================================================\n",
        "def run_hybrid_pipeline():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print_status(\"=\" * 60, \"info\")\n",
        "    print_status(\"üöÄ HYBRID ML PIPELINE v3.5 - ENHANCED\", \"success\")\n",
        "    print_status(\"=\" * 60, \"info\")\n",
        "\n",
        "    csv_files = list(CSV_FOLDER.glob(\"*.csv\"))\n",
        "\n",
        "    if not csv_files:\n",
        "        print_status(\"‚ùå No CSV files found!\", \"error\")\n",
        "        return {}\n",
        "\n",
        "    print_status(f\"üìä Processing {len(csv_files)} currency pairs...\", \"info\")\n",
        "\n",
        "    aggregated_signals = {}\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        pair, signals, agg_signal = process_pair_csv(csv_file)\n",
        "        aggregated_signals[pair] = {\n",
        "            \"signals\": signals,\n",
        "            \"aggregated\": agg_signal\n",
        "        }\n",
        "\n",
        "    # Save to JSON\n",
        "    json_file = REPO_FOLDER / \"latest_signals.json\"\n",
        "    tmp_file = REPO_FOLDER / \"latest_signals_tmp.json\"\n",
        "\n",
        "    with open(tmp_file, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "            \"pairs\": aggregated_signals\n",
        "        }, f, indent=2)\n",
        "\n",
        "    # Only push if file changed\n",
        "    if not json_file.exists() or not filecmp.cmp(tmp_file, json_file):\n",
        "        tmp_file.replace(json_file)\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"add\", str(json_file)], check=False)\n",
        "        subprocess.run(\n",
        "            [\"git\", \"-C\", str(REPO_FOLDER), \"commit\", \"-m\", \"üìà Auto update ML signals\"],\n",
        "            check=False\n",
        "        )\n",
        "\n",
        "        for attempt in range(3):\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"push\"],\n",
        "                check=False\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print_status(\"‚úÖ Push successful\", \"success\")\n",
        "                break\n",
        "            time.sleep(5)\n",
        "    else:\n",
        "        print_status(\"‚ÑπÔ∏è JSON unchanged ‚Äî skipping Git push\", \"info\")\n",
        "\n",
        "    # Display performance summary\n",
        "    print_status(\"\\nüìä ML MODEL PERFORMANCE:\", \"success\")\n",
        "    for pair in aggregated_signals.keys():\n",
        "        sgd_perf = TRADE_DB.get_model_performance(pair, 'SGD', days=7)\n",
        "        rf_perf = TRADE_DB.get_model_performance(pair, 'RandomForest', days=7)\n",
        "        ensemble_perf = TRADE_DB.get_model_performance(pair, 'Ensemble', days=7)\n",
        "\n",
        "        if sgd_perf['total_trades'] > 0 or rf_perf['total_trades'] > 0:\n",
        "            print_status(f\"\\n{pair}:\", \"info\")\n",
        "            print_status(\n",
        "                f\"  SGD: {sgd_perf['accuracy']:.1f}% accuracy \"\n",
        "                f\"({sgd_perf['winning_trades']}/{sgd_perf['total_trades']} wins) \"\n",
        "                f\"P&L: ${sgd_perf['total_pnl']:.2f}\",\n",
        "                \"info\"\n",
        "            )\n",
        "            print_status(\n",
        "                f\"  RF:  {rf_perf['accuracy']:.1f}% accuracy \"\n",
        "                f\"({rf_perf['winning_trades']}/{rf_perf['total_trades']} wins) \"\n",
        "                f\"P&L: ${rf_perf['total_pnl']:.2f}\",\n",
        "                \"info\"\n",
        "            )\n",
        "            print_status(\n",
        "                f\"  Ensemble: {ensemble_perf['accuracy']:.1f}% accuracy \"\n",
        "                f\"({ensemble_perf['winning_trades']}/{ensemble_perf['total_trades']} wins) \"\n",
        "                f\"P&L: ${ensemble_perf['total_pnl']:.2f}\",\n",
        "                \"info\"\n",
        "            )\n",
        "\n",
        "            best_model = TRADE_DB.get_best_model(pair, days=7)\n",
        "            print_status(f\"  üèÜ Best model: {best_model}\", \"success\")\n",
        "\n",
        "    return aggregated_signals\n",
        "\n",
        "# ======================================================\n",
        "# üÜï TRADE OUTCOME EVALUATOR\n",
        "# ======================================================\n",
        "class TradeOutcomeEvaluator:\n",
        "    \"\"\"Evaluates if previous signals hit TP/SL\"\"\"\n",
        "\n",
        "    def __init__(self, trade_db):\n",
        "        self.trade_db = trade_db\n",
        "        self.previous_signals_file = REPO_FOLDER / \"previous_ml_signals.pkl\"\n",
        "\n",
        "    def save_signals(self, aggregated_signals):\n",
        "        \"\"\"Save current signals for next iteration evaluation\"\"\"\n",
        "        try:\n",
        "            with open(self.previous_signals_file, 'wb') as f:\n",
        "                pickle.dump({\n",
        "                    'signals': aggregated_signals,\n",
        "                    'timestamp': datetime.now(timezone.utc)\n",
        "                }, f, protocol=4)\n",
        "            print_status(\"‚úÖ Saved signals for next evaluation\", \"success\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"Failed to save signals: {e}\", \"error\")\n",
        "\n",
        "    def load_previous_signals(self):\n",
        "        \"\"\"Load signals from previous iteration\"\"\"\n",
        "        if not self.previous_signals_file.exists():\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            with open(self.previous_signals_file, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        except Exception as e:\n",
        "            print_status(f\"Failed to load previous signals: {e}\", \"warn\")\n",
        "            return None\n",
        "\n",
        "    def evaluate_outcomes(self):\n",
        "        \"\"\"Evaluate previous signals against current prices\"\"\"\n",
        "        previous_data = self.load_previous_signals()\n",
        "\n",
        "        if not previous_data:\n",
        "            print_status(\"No previous signals to evaluate\", \"info\")\n",
        "            return\n",
        "\n",
        "        print_status(\"\\nüîç Evaluating previous iteration signals...\", \"info\")\n",
        "\n",
        "        total_evaluated = 0\n",
        "        total_wins = 0\n",
        "        total_losses = 0\n",
        "\n",
        "        for pair, pair_data in previous_data['signals'].items():\n",
        "            signals = pair_data.get('signals', {})\n",
        "\n",
        "            # Fetch current price\n",
        "            current_price = fetch_live_rate(pair)\n",
        "\n",
        "            if current_price <= 0:\n",
        "                print_status(f\"‚ö†Ô∏è Could not fetch current price for {pair}\", \"warn\")\n",
        "                continue\n",
        "\n",
        "            for tf_name, signal_data in signals.items():\n",
        "                entry_price = signal_data.get('live', 0)\n",
        "                sl_price = signal_data.get('SL', 0)\n",
        "                tp_price = signal_data.get('TP', 0)\n",
        "                prediction = signal_data.get('signal', 0)\n",
        "                signal_id = signal_data.get('signal_id')\n",
        "\n",
        "                if entry_price == 0 or sl_price == 0 or tp_price == 0:\n",
        "                    continue\n",
        "\n",
        "                # Check if TP or SL was hit\n",
        "                hit_tp = False\n",
        "                hit_sl = False\n",
        "\n",
        "                if prediction == 1:  # Long prediction\n",
        "                    if current_price >= tp_price:\n",
        "                        hit_tp = True\n",
        "                    elif current_price <= sl_price:\n",
        "                        hit_sl = True\n",
        "                elif prediction == 0:  # Short prediction\n",
        "                    if current_price <= tp_price:\n",
        "                        hit_tp = True\n",
        "                    elif current_price >= sl_price:\n",
        "                        hit_sl = True\n",
        "\n",
        "                # If trade closed, record result\n",
        "                if hit_tp or hit_sl:\n",
        "                    was_correct = hit_tp\n",
        "                    exit_price = tp_price if hit_tp else sl_price\n",
        "\n",
        "                    # Calculate P&L\n",
        "                    if prediction == 1:  # Long\n",
        "                        pnl = exit_price - entry_price\n",
        "                    else:  # Short\n",
        "                        pnl = entry_price - exit_price\n",
        "\n",
        "                    # Determine which model was used\n",
        "                    sgd_pred = signal_data.get('sgd_pred', 0)\n",
        "                    rf_pred = signal_data.get('rf_pred', 0)\n",
        "\n",
        "                    # Save results for each model that made this prediction\n",
        "                    if sgd_pred == prediction:\n",
        "                        self.trade_db.save_trade_result(\n",
        "                            signal_id, pair, tf_name, entry_price, exit_price,\n",
        "                            prediction, was_correct, pnl, 'SGD'\n",
        "                        )\n",
        "\n",
        "                    if rf_pred == prediction:\n",
        "                        self.trade_db.save_trade_result(\n",
        "                            signal_id, pair, tf_name, entry_price, exit_price,\n",
        "                            prediction, was_correct, pnl, 'RandomForest'\n",
        "                        )\n",
        "\n",
        "                    # Always save for ensemble\n",
        "                    self.trade_db.save_trade_result(\n",
        "                        signal_id, pair, tf_name, entry_price, exit_price,\n",
        "                        prediction, was_correct, pnl, 'Ensemble'\n",
        "                    )\n",
        "\n",
        "                    total_evaluated += 1\n",
        "                    if was_correct:\n",
        "                        total_wins += 1\n",
        "                    else:\n",
        "                        total_losses += 1\n",
        "\n",
        "                    result_emoji = \"‚úÖ\" if was_correct else \"‚ùå\"\n",
        "                    print_status(\n",
        "                        f\"{result_emoji} {pair} {tf_name}: \"\n",
        "                        f\"Entry={entry_price:.5f} Exit={exit_price:.5f} \"\n",
        "                        f\"P&L=${pnl:.5f} {'WIN' if was_correct else 'LOSS'}\",\n",
        "                        \"success\" if was_correct else \"warn\"\n",
        "                    )\n",
        "\n",
        "        if total_evaluated > 0:\n",
        "            accuracy = (total_wins / total_evaluated) * 100\n",
        "            print_status(\n",
        "                f\"\\nüìä EVALUATION SUMMARY: {total_wins}/{total_evaluated} wins \"\n",
        "                f\"({accuracy:.1f}% accuracy)\",\n",
        "                \"success\"\n",
        "            )\n",
        "        else:\n",
        "            print_status(\"‚ÑπÔ∏è No trades closed in this iteration\", \"info\")\n",
        "\n",
        "# ======================================================\n",
        "# üÜï CONTINUOUS LEARNING SYSTEM\n",
        "# ======================================================\n",
        "class ContinuousLearningSystem:\n",
        "    \"\"\"Analyzes patterns and improves model selection\"\"\"\n",
        "\n",
        "    def __init__(self, trade_db):\n",
        "        self.trade_db = trade_db\n",
        "        self.learning_file = PICKLE_FOLDER / \"ml_learning_progress.pkl\"\n",
        "        self.learning_data = self.load_learning_data()\n",
        "\n",
        "    def load_learning_data(self):\n",
        "        if self.learning_file.exists():\n",
        "            try:\n",
        "                return pickle.load(open(self.learning_file, 'rb'))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return {\n",
        "            'total_iterations': 0,\n",
        "            'pair_preferences': {},  # Which model works best per pair\n",
        "            'timeframe_preferences': {},  # Which model works best per TF\n",
        "            'confidence_thresholds': {},  # Optimal confidence levels\n",
        "            'learning_history': []\n",
        "        }\n",
        "\n",
        "    def save_learning_data(self):\n",
        "        try:\n",
        "            with open(self.learning_file, 'wb') as f:\n",
        "                pickle.dump(self.learning_data, f, protocol=4)\n",
        "        except Exception as e:\n",
        "            print_status(f\"Failed to save learning data: {e}\", \"error\")\n",
        "\n",
        "    def analyze_and_learn(self):\n",
        "        \"\"\"Analyze recent performance and update preferences\"\"\"\n",
        "        self.learning_data['total_iterations'] += 1\n",
        "\n",
        "        print_status(\"\\nüß† LEARNING ANALYSIS:\", \"info\")\n",
        "\n",
        "        # Analyze each pair\n",
        "        pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "        for pair in pairs:\n",
        "            sgd_perf = self.trade_db.get_model_performance(pair, 'SGD', days=7)\n",
        "            rf_perf = self.trade_db.get_model_performance(pair, 'RandomForest', days=7)\n",
        "\n",
        "            if sgd_perf['total_trades'] >= 5 and rf_perf['total_trades'] >= 5:\n",
        "                # Determine preference\n",
        "                if sgd_perf['accuracy'] > rf_perf['accuracy']:\n",
        "                    self.learning_data['pair_preferences'][pair] = {\n",
        "                        'preferred_model': 'SGD',\n",
        "                        'accuracy_diff': sgd_perf['accuracy'] - rf_perf['accuracy']\n",
        "                    }\n",
        "                else:\n",
        "                    self.learning_data['pair_preferences'][pair] = {\n",
        "                        'preferred_model': 'RandomForest',\n",
        "                        'accuracy_diff': rf_perf['accuracy'] - sgd_perf['accuracy']\n",
        "                    }\n",
        "\n",
        "                print_status(\n",
        "                    f\"  {pair}: Prefers {self.learning_data['pair_preferences'][pair]['preferred_model']} \"\n",
        "                    f\"(+{self.learning_data['pair_preferences'][pair]['accuracy_diff']:.1f}% accuracy)\",\n",
        "                    \"info\"\n",
        "                )\n",
        "\n",
        "        # Save learning progress\n",
        "        self.save_learning_data()\n",
        "\n",
        "        # Display overall learning stats\n",
        "        print_status(f\"\\nüìà Total iterations: {self.learning_data['total_iterations']}\", \"info\")\n",
        "        print_status(f\"üìä Learned preferences for {len(self.learning_data['pair_preferences'])} pairs\", \"info\")\n",
        "\n",
        "    def get_recommended_model(self, pair):\n",
        "        \"\"\"Get recommended model for a specific pair\"\"\"\n",
        "        if pair in self.learning_data['pair_preferences']:\n",
        "            return self.learning_data['pair_preferences'][pair]['preferred_model']\n",
        "        return 'Ensemble'  # Default\n",
        "\n",
        "# ======================================================\n",
        "# MAIN EXECUTION\n",
        "# ======================================================\n",
        "def main():\n",
        "    \"\"\"Main execution with evaluation and learning\"\"\"\n",
        "    print_status(\"=\" * 70, \"info\")\n",
        "    print_status(\"üöÄ HYBRID ML PIPELINE v3.5 - ULTRA-PERSISTENT\", \"success\")\n",
        "    print_status(\"=\" * 70, \"info\")\n",
        "\n",
        "    # Initialize systems\n",
        "    evaluator = TradeOutcomeEvaluator(TRADE_DB)\n",
        "    learning_system = ContinuousLearningSystem(TRADE_DB)\n",
        "\n",
        "    try:\n",
        "        # Step 1: Evaluate previous signals\n",
        "        evaluator.evaluate_outcomes()\n",
        "\n",
        "        # Step 2: Learn from results\n",
        "        learning_system.analyze_and_learn()\n",
        "\n",
        "        # Step 3: Generate new signals\n",
        "        aggregated_signals = run_hybrid_pipeline()\n",
        "\n",
        "        # Step 4: Save signals for next iteration\n",
        "        evaluator.save_signals(aggregated_signals)\n",
        "\n",
        "        print_status(\"\\n\" + \"=\" * 70, \"info\")\n",
        "        print_status(\"‚úÖ HYBRID ML PIPELINE COMPLETED SUCCESSFULLY\", \"success\")\n",
        "        print_status(\"=\" * 70, \"info\")\n",
        "\n",
        "        return aggregated_signals\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print_status(\"\\n‚ö†Ô∏è Pipeline interrupted by user\", \"warn\")\n",
        "    except Exception as e:\n",
        "        print_status(f\"\\n‚ùå Pipeline error: {e}\", \"error\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        TRADE_DB.close()\n",
        "        print_status(\"‚úÖ Database closed\", \"success\")\n",
        "\n",
        "# ======================================================\n",
        "# STANDALONE EXECUTION\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "    signals = main()\n",
        "\n",
        "    # Optional: Schedule for continuous running\n",
        "    # Uncomment below for automatic hourly execution\n",
        "    \"\"\"\n",
        "    import time\n",
        "    CHECK_INTERVAL = 60 * 60  # 1 hour\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            signals = main()\n",
        "            print_status(f\"\\nüò¥ Sleeping for {CHECK_INTERVAL // 60} minutes...\", \"info\")\n",
        "            time.sleep(CHECK_INTERVAL)\n",
        "        except KeyboardInterrupt:\n",
        "            print_status(\"\\n‚ö†Ô∏è Shutdown requested\", \"warn\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print_status(f\"\\n‚ùå Error in main loop: {e}\", \"error\")\n",
        "            time.sleep(300)  # Sleep 5 minutes on error\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZPAZQi88SYG"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# VERSION 3.6 ‚Äì Unified Loader + Merge Pickles (Production Ready)\n",
        "# Fully Safe | Threaded | Compatible with Hybrid FX Pipeline\n",
        "# Added: Data validation, ATR floors, debug prints, raw price preservation\n",
        "# ======================================================\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import warnings\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Environment & folders\n",
        "# -----------------------------\n",
        "ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "TEMP_PICKLE_FOLDER = ROOT_DIR / \"temp_pickles\"\n",
        "FINAL_PICKLE_FOLDER = ROOT_DIR / \"merged_data_pickles\"\n",
        "\n",
        "for folder in [CSV_FOLDER, TEMP_PICKLE_FOLDER, FINAL_PICKLE_FOLDER, REPO_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "JSON_FILE = REPO_FOLDER / \"latest_signals.json\"\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Safe Indicator Generator\n",
        "# -----------------------------\n",
        "def add_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    df = df[(df[[\"open\", \"high\", \"low\", \"close\"]] > 0).all(axis=1)]\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # --- Preserve raw OHLC prices for GA ---\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if col in df.columns:\n",
        "            df[f\"raw_{col}\"] = df[col].copy()\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "\n",
        "        try:\n",
        "            if len(df['close']) >= 10:\n",
        "                df['SMA_10'] = ta.trend.sma_indicator(df['close'], 10)\n",
        "                df['EMA_10'] = ta.trend.ema_indicator(df['close'], 10)\n",
        "            if len(df['close']) >= 50:\n",
        "                df['SMA_50'] = ta.trend.sma_indicator(df['close'], 50)\n",
        "                df['EMA_50'] = ta.trend.ema_indicator(df['close'], 50)\n",
        "            if len(df['close']) >= 14:\n",
        "                df['RSI_14'] = ta.momentum.rsi(df['close'], 14)\n",
        "            if all(col in df.columns for col in ['high', 'low', 'close']) and len(df['close']) >= 14:\n",
        "                df['Williams_%R'] = WilliamsRIndicator(df['high'], df['low'], df['close'], 14).williams_r()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Indicator calculation failed: {e}\")\n",
        "\n",
        "        # --- Safe ATR ---\n",
        "        try:\n",
        "            if all(col in df.columns for col in ['high', 'low', 'close']):\n",
        "                window = 14\n",
        "                if len(df) >= window:\n",
        "                    df['ATR'] = AverageTrueRange(\n",
        "                        df['high'], df['low'], df['close'], window=window\n",
        "                    ).average_true_range().fillna(1e-5).clip(lower=1e-4)\n",
        "                else:\n",
        "                    df['ATR'] = 1e-4\n",
        "        except Exception as e:\n",
        "            df['ATR'] = 1e-4\n",
        "            print(f\"‚ö†Ô∏è ATR calculation failed: {e}\")\n",
        "\n",
        "        # --- Scale only non-price numeric columns ---\n",
        "        numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns if not df[c].isna().all()]\n",
        "        protected_cols = [\n",
        "            \"open\", \"high\", \"low\", \"close\",\n",
        "            \"raw_open\", \"raw_high\", \"raw_low\", \"raw_close\"\n",
        "        ]\n",
        "        numeric_cols = [c for c in numeric_cols if c not in protected_cols]\n",
        "\n",
        "        if numeric_cols:\n",
        "            scaler = MinMaxScaler()\n",
        "            df[numeric_cols] = scaler.fit_transform(df[numeric_cols].fillna(0) + 1e-8)\n",
        "\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Safe CSV Processing\n",
        "# -----------------------------\n",
        "def process_csv_file(csv_file: Path, save_folder: Path):\n",
        "    try:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\", category=pd.errors.ParserWarning)\n",
        "            df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö™ Skipped empty CSV: {csv_file.name}\")\n",
        "            return None\n",
        "\n",
        "        df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "        df = add_indicators(df)\n",
        "        if df.empty:\n",
        "            print(f\"‚ö™ Skipped CSV after filtering invalid prices: {csv_file.name}\")\n",
        "            return None\n",
        "\n",
        "        out_file = save_folder / f\"{csv_file.stem}.pkl\"\n",
        "        df.to_pickle(out_file)\n",
        "        print(f\"‚úÖ Processed CSV {csv_file.name} ‚Üí {out_file.name}\")\n",
        "        return out_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed CSV {csv_file.name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ JSON Processing\n",
        "# -----------------------------\n",
        "def process_json_file(json_file: Path, save_folder: Path):\n",
        "    try:\n",
        "        with open(json_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load JSON: {e}\")\n",
        "        return []\n",
        "\n",
        "    signals_data = data.get(\"pairs\", {})\n",
        "    timestamp = pd.to_datetime(data.get(\"timestamp\"), utc=True)\n",
        "    processed_files = []\n",
        "\n",
        "    for pair, info in signals_data.items():\n",
        "        signals = info.get(\"signals\", {})\n",
        "        dfs = []\n",
        "\n",
        "        for tf_name, tf_info in signals.items():\n",
        "            df = pd.DataFrame({\n",
        "                \"live\": [tf_info.get(\"live\")],\n",
        "                \"SL\": [tf_info.get(\"SL\")],\n",
        "                \"TP\": [tf_info.get(\"TP\")],\n",
        "                \"signal\": [tf_info.get(\"signal\")]\n",
        "            }, index=[timestamp])\n",
        "            df[\"timeframe\"] = tf_name\n",
        "            df = add_indicators(df)\n",
        "            if not df.empty:\n",
        "                dfs.append(df)\n",
        "\n",
        "        if dfs:\n",
        "            df_pair = pd.concat(dfs)\n",
        "            out_file = save_folder / f\"{pair.replace('/', '_')}.pkl\"\n",
        "            df_pair.to_pickle(out_file)\n",
        "            print(f\"‚úÖ Processed JSON {pair} ‚Üí {out_file.name}\")\n",
        "            processed_files.append(out_file)\n",
        "\n",
        "    return processed_files\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Safe Pickle Merger\n",
        "# -----------------------------\n",
        "def merge_pickles(temp_folder: Path, final_folder: Path, keep_last: int = 5):\n",
        "    pickles = list(temp_folder.glob(\"*.pkl\"))\n",
        "    if not pickles:\n",
        "        print(\"‚ö™ No temporary pickles to merge.\")\n",
        "        return\n",
        "\n",
        "    pairs = set(p.stem.split('.')[0] for p in pickles)\n",
        "\n",
        "    for pair in pairs:\n",
        "        pair_files = [p for p in pickles if p.stem.startswith(pair)]\n",
        "        dfs = [pd.read_pickle(p) for p in pair_files if p.exists() and p.stat().st_size > 0]\n",
        "\n",
        "        if not dfs:\n",
        "            print(f\"‚ö™ Skipped {pair} (no valid pickles)\")\n",
        "            continue\n",
        "\n",
        "        merged_df = pd.concat(dfs, ignore_index=False).sort_index().drop_duplicates()\n",
        "        # Changed filename suffix to match the expected format in W4XoZxs-TrDh\n",
        "        merged_file = final_folder / f\"{pair}_2244.pkl\"\n",
        "        merged_df.to_pickle(merged_file)\n",
        "        print(f\"üîó Merged {len(pair_files)} files ‚Üí {merged_file.name}\")\n",
        "\n",
        "        existing = sorted(final_folder.glob(f\"{pair}_*.pkl\"), key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "        for old_file in existing[keep_last:]:\n",
        "            try:\n",
        "                old_file.unlink()\n",
        "                print(f\"üßπ Removed old file: {old_file.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not remove {old_file.name}: {e}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Unified Pipeline Runner\n",
        "# -----------------------------\n",
        "def run_unified_pipeline():\n",
        "    temp_files = []\n",
        "\n",
        "    # Process JSON first\n",
        "    if JSON_FILE.exists():\n",
        "        temp_files += process_json_file(JSON_FILE, TEMP_PICKLE_FOLDER)\n",
        "        print(f\"‚úÖ JSON processing complete ({len(temp_files)} files)\")\n",
        "\n",
        "    # Process CSVs concurrently\n",
        "    csv_files = list(CSV_FOLDER.glob(\"*.csv\"))\n",
        "    if csv_files:\n",
        "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "            futures = [executor.submit(process_csv_file, f, TEMP_PICKLE_FOLDER) for f in csv_files]\n",
        "            for fut in as_completed(futures):\n",
        "                result = fut.result()\n",
        "                if result:\n",
        "                    temp_files.append(result)\n",
        "\n",
        "    # Merge all pickles safely\n",
        "    merge_pickles(TEMP_PICKLE_FOLDER, FINAL_PICKLE_FOLDER)\n",
        "    print(f\"üéØ Unified pipeline complete ‚Äî merged pickles saved in {FINAL_PICKLE_FOLDER}\")\n",
        "\n",
        "    # Debug: print last few rows of each merged pickle\n",
        "    for pkl_file in FINAL_PICKLE_FOLDER.glob(\"*.pkl\"):\n",
        "        df = pd.read_pickle(pkl_file)\n",
        "        print(f\"üîç {pkl_file.name} last rows:\\n\", df.tail(3))\n",
        "\n",
        "    return FINAL_PICKLE_FOLDER\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Execute\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    final_folder = run_unified_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Ultimate Hybrid Forex Pipeline v7.5 (SINGLE RUN MODE)\n",
        "======================================================\n",
        "üéØ IMPROVEMENTS:\n",
        "- ‚úÖ Runs once and exits (perfect for hourly GitHub Actions)\n",
        "- ‚úÖ Real accuracy tracking with trade outcome evaluation\n",
        "- ‚úÖ P&L reflects actual price movements\n",
        "- ‚úÖ Learning system learns from real outcomes\n",
        "- ‚úÖ Historical replay mode (no live contamination)\n",
        "- ‚úÖ Email reports with beautiful templates\n",
        "- ‚úÖ GitHub Actions integration ready\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import smtplib\n",
        "import subprocess\n",
        "import time\n",
        "import logging\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# ======================================================\n",
        "# LOGGING & ENVIRONMENT\n",
        "# ======================================================\n",
        "logging.basicConfig(\n",
        "    filename='forex_pipeline.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(message)s'\n",
        ")\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    icons = {\"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"warn\": \"‚ö†Ô∏è\", \"debug\": \"üêû\", \"error\": \"‚ùå\"}\n",
        "    log_level = \"warning\" if level == \"warn\" else level\n",
        "    getattr(logging, log_level, logging.info)(msg)\n",
        "    print(f\"{icons.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "ROOT_DIR = Path(\"/content\") if IN_COLAB else Path(\".\")\n",
        "ROOT_PATH = ROOT_DIR / \"forex-alpha-models\"\n",
        "\n",
        "# ======================================================\n",
        "# FOLDER SETUP\n",
        "# ======================================================\n",
        "FINAL_PICKLE_FOLDER = ROOT_PATH / \"merged_data_pickles\"\n",
        "PICKLE_FOLDER = FINAL_PICKLE_FOLDER\n",
        "REPO_FOLDER = ROOT_PATH / \"forex-ai-models\"\n",
        "\n",
        "for f in [PICKLE_FOLDER, REPO_FOLDER]:\n",
        "    f.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "os.chdir(ROOT_PATH)\n",
        "logging.info(f\"Working directory: {ROOT_PATH.resolve()}\")\n",
        "\n",
        "# ======================================================\n",
        "# GIT SETUP\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "\n",
        "# ======================================================\n",
        "# GMAIL CONFIG\n",
        "# ======================================================\n",
        "GMAIL_USER = os.environ.get(\"GMAIL_USER\", \"nakatonabira3@gmail.com\")\n",
        "GMAIL_APP_PASSWORD = os.environ.get(\"GMAIL_APP_PASSWORD\", \"gmwohahtltmcewug\")\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/rahim-dotAI/forex-ai-models/main/IMG_1599.jpeg\"\n",
        "\n",
        "# ======================================================\n",
        "# CORE CONFIG\n",
        "# ======================================================\n",
        "PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "ATR_PERIOD = 14\n",
        "MIN_ATR = 1e-5\n",
        "BASE_CAPITAL = 100\n",
        "MAX_POSITION_FRACTION = 0.1\n",
        "MAX_TRADE_CAP = BASE_CAPITAL * 0.05\n",
        "EPS = 1e-8\n",
        "\n",
        "MAX_ATR_SL = 3.0\n",
        "MAX_ATR_TP = 3.0\n",
        "MIN_ATR_DISTANCE = 0.5\n",
        "\n",
        "MAX_TRADE_MEMORY = 200\n",
        "TOURNAMENT_SIZE = 3\n",
        "\n",
        "# ======================================================\n",
        "# FILE PATHS\n",
        "# ======================================================\n",
        "SIGNALS_JSON_PATH = REPO_FOLDER / \"broker_signals.json\"\n",
        "ENSEMBLE_SIGNALS_FILE = REPO_FOLDER / \"ensemble_signals.json\"\n",
        "INFINITE_MEMORY_DB = REPO_FOLDER / \"infinite_memory.db\"\n",
        "MONDAY_RUNS_FILE = REPO_FOLDER / \"monday_runs.pkl\"\n",
        "LEARNING_PROGRESS_FILE = REPO_FOLDER / \"learning_progress.pkl\"\n",
        "PREVIOUS_SIGNALS_FILE = REPO_FOLDER / \"previous_signals.pkl\"\n",
        "ITERATION_COUNTER_FILE = REPO_FOLDER / \"iteration_counter.pkl\"\n",
        "RUN_MODE_FILE = REPO_FOLDER / \"run_mode.pkl\"\n",
        "\n",
        "# ======================================================\n",
        "# PERSISTENT ITERATION COUNTER\n",
        "# ======================================================\n",
        "class PersistentIterationCounter:\n",
        "    \"\"\"Tracks total iterations across all runs forever\"\"\"\n",
        "\n",
        "    def __init__(self, counter_file=ITERATION_COUNTER_FILE):\n",
        "        self.counter_file = counter_file\n",
        "        self.data = self.load_counter()\n",
        "\n",
        "    def load_counter(self):\n",
        "        if self.counter_file.exists():\n",
        "            try:\n",
        "                with open(self.counter_file, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "                print_status(f\"‚úÖ Loaded iteration counter: {data['total_iterations']} total runs\", \"success\")\n",
        "                return data\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return {\n",
        "            'total_iterations': 0,\n",
        "            'start_date': datetime.now().isoformat(),\n",
        "            'last_run': None,\n",
        "            'run_history': []\n",
        "        }\n",
        "\n",
        "    def increment(self):\n",
        "        \"\"\"Increment and save counter\"\"\"\n",
        "        self.data['total_iterations'] += 1\n",
        "        self.data['last_run'] = datetime.now().isoformat()\n",
        "        self.data['run_history'].append({\n",
        "            'iteration': self.data['total_iterations'],\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "        if len(self.data['run_history']) > 1000:\n",
        "            self.data['run_history'] = self.data['run_history'][-1000:]\n",
        "\n",
        "        self.save_counter()\n",
        "        return self.data['total_iterations']\n",
        "\n",
        "    def save_counter(self):\n",
        "        try:\n",
        "            with open(self.counter_file, 'wb') as f:\n",
        "                pickle.dump(self.data, f, protocol=4)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save iteration counter: {e}\")\n",
        "\n",
        "    def get_current(self):\n",
        "        return self.data['total_iterations']\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get statistics about runs\"\"\"\n",
        "        if not self.data['run_history']:\n",
        "            return {}\n",
        "\n",
        "        first_run = datetime.fromisoformat(self.data['start_date'])\n",
        "        days_running = (datetime.now() - first_run).days\n",
        "\n",
        "        return {\n",
        "            'total_iterations': self.data['total_iterations'],\n",
        "            'days_running': days_running,\n",
        "            'avg_iterations_per_day': self.data['total_iterations'] / max(days_running, 1),\n",
        "            'start_date': self.data['start_date'],\n",
        "            'last_run': self.data['last_run']\n",
        "        }\n",
        "\n",
        "ITERATION_COUNTER = PersistentIterationCounter()\n",
        "\n",
        "# ======================================================\n",
        "# COMPETITION MODELS CONFIG\n",
        "# ======================================================\n",
        "COMPETITION_MODELS = {\n",
        "    \"Alpha Momentum\": {\n",
        "        \"color\": \"üî¥\",\n",
        "        \"hex_color\": \"#E74C3C\",\n",
        "        \"strategy\": \"Aggressive momentum trading\",\n",
        "        \"atr_sl_range\": (1.5, 2.5),\n",
        "        \"atr_tp_range\": (2.0, 3.0),\n",
        "        \"risk_range\": (0.015, 0.03),\n",
        "        \"confidence_range\": (0.3, 0.5),\n",
        "        \"pop_size\": 12,\n",
        "        \"generations\": 15,\n",
        "        \"mutation_rate\": 0.3,\n",
        "        \"enabled\": True\n",
        "    },\n",
        "    \"Beta Conservative\": {\n",
        "        \"color\": \"üîµ\",\n",
        "        \"hex_color\": \"#3498DB\",\n",
        "        \"strategy\": \"Conservative mean reversion\",\n",
        "        \"atr_sl_range\": (1.0, 1.5),\n",
        "        \"atr_tp_range\": (1.5, 2.0),\n",
        "        \"risk_range\": (0.005, 0.015),\n",
        "        \"confidence_range\": (0.5, 0.7),\n",
        "        \"pop_size\": 10,\n",
        "        \"generations\": 12,\n",
        "        \"mutation_rate\": 0.2,\n",
        "        \"enabled\": True\n",
        "    },\n",
        "    \"Gamma Adaptive\": {\n",
        "        \"color\": \"üü¢\",\n",
        "        \"hex_color\": \"#2ECC71\",\n",
        "        \"strategy\": \"Adaptive volatility trading\",\n",
        "        \"atr_sl_range\": (1.2, 2.0),\n",
        "        \"atr_tp_range\": (1.8, 2.5),\n",
        "        \"risk_range\": (0.01, 0.025),\n",
        "        \"confidence_range\": (0.4, 0.6),\n",
        "        \"pop_size\": 14,\n",
        "        \"generations\": 18,\n",
        "        \"mutation_rate\": 0.25,\n",
        "        \"enabled\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# ======================================================\n",
        "# REPLAY MODE CONFIG\n",
        "# ======================================================\n",
        "REPLAY_CONFIG = {\n",
        "    'monday_replay_runs': 1,\n",
        "    'replay_advance_minutes': 60\n",
        "}\n",
        "\n",
        "RANDOM_REPLAY_PERIODS = [\n",
        "    (\"2024-01-01\", \"2024-03-31\"),\n",
        "    (\"2024-04-01\", \"2024-06-30\"),\n",
        "    (\"2024-07-01\", \"2024-09-30\"),\n",
        "    (\"2024-10-01\", \"2024-12-31\"),\n",
        "    (\"2023-01-01\", \"2023-06-30\"),\n",
        "    (\"2023-07-01\", \"2023-12-31\")\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# TRADE OUTCOME TRACKER\n",
        "# ======================================================\n",
        "class TradeOutcomeTracker:\n",
        "    \"\"\"Evaluates if previous signals hit TP/SL and calculates real P&L\"\"\"\n",
        "\n",
        "    def __init__(self, memory_system):\n",
        "        self.memory = memory_system\n",
        "        self.active_trades = {}\n",
        "\n",
        "    def store_signals(self, signals_by_model, timestamp):\n",
        "        \"\"\"Store current signals for future evaluation\"\"\"\n",
        "        for model_name, signals in signals_by_model.items():\n",
        "            if model_name not in self.active_trades:\n",
        "                self.active_trades[model_name] = {}\n",
        "\n",
        "            for pair, sig in signals.items():\n",
        "                if sig['direction'] == 'HOLD':\n",
        "                    continue\n",
        "\n",
        "                trade_key = f\"{pair}_{timestamp}\"\n",
        "                self.active_trades[model_name][trade_key] = {\n",
        "                    'pair': pair,\n",
        "                    'direction': sig['direction'],\n",
        "                    'entry_price': sig['last_price'],\n",
        "                    'sl_price': sig['SL'],\n",
        "                    'tp_price': sig['TP'],\n",
        "                    'entry_time': timestamp,\n",
        "                    'model': model_name,\n",
        "                    'confidence': sig['score_1_100'],\n",
        "                    'closed': False\n",
        "                }\n",
        "\n",
        "    def evaluate_outcomes(self, current_prices, current_time):\n",
        "        \"\"\"Check if trades hit TP/SL and record outcomes\"\"\"\n",
        "        outcomes_by_model = defaultdict(lambda: {\n",
        "            'closed_trades': 0,\n",
        "            'wins': 0,\n",
        "            'losses': 0,\n",
        "            'total_pnl': 0.0,\n",
        "            'trade_results': []\n",
        "        })\n",
        "\n",
        "        for model_name, trades in self.active_trades.items():\n",
        "            for trade_key, trade in list(trades.items()):\n",
        "                if trade['closed']:\n",
        "                    continue\n",
        "\n",
        "                pair = trade['pair']\n",
        "                current_price = current_prices.get(pair, 0)\n",
        "\n",
        "                if current_price <= 0:\n",
        "                    continue\n",
        "\n",
        "                entry_price = trade['entry_price']\n",
        "                sl_price = trade['sl_price']\n",
        "                tp_price = trade['tp_price']\n",
        "                direction = trade['direction']\n",
        "\n",
        "                hit_tp = False\n",
        "                hit_sl = False\n",
        "                exit_price = None\n",
        "\n",
        "                if direction == 'BUY':\n",
        "                    if current_price >= tp_price:\n",
        "                        hit_tp = True\n",
        "                        exit_price = tp_price\n",
        "                    elif current_price <= sl_price:\n",
        "                        hit_sl = True\n",
        "                        exit_price = sl_price\n",
        "                elif direction == 'SELL':\n",
        "                    if current_price <= tp_price:\n",
        "                        hit_tp = True\n",
        "                        exit_price = tp_price\n",
        "                    elif current_price >= sl_price:\n",
        "                        hit_sl = True\n",
        "                        exit_price = sl_price\n",
        "\n",
        "                if exit_price:\n",
        "                    if direction == 'BUY':\n",
        "                        pnl = exit_price - entry_price\n",
        "                    else:\n",
        "                        pnl = entry_price - exit_price\n",
        "\n",
        "                    was_correct = hit_tp\n",
        "                    price_change_pct = ((exit_price - entry_price) / entry_price) * 100\n",
        "\n",
        "                    try:\n",
        "                        duration_minutes = (current_time - trade['entry_time']).total_seconds() / 60\n",
        "                    except:\n",
        "                        duration_minutes = 60\n",
        "\n",
        "                    cursor = self.memory.conn.cursor()\n",
        "                    cursor.execute('''\n",
        "                        INSERT INTO trade_results\n",
        "                        (timestamp, pair, entry_price, exit_price, direction, pnl,\n",
        "                         was_correct, price_change_pct, duration_minutes, model_name)\n",
        "                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                    ''', (\n",
        "                        current_time.isoformat(),\n",
        "                        pair,\n",
        "                        entry_price,\n",
        "                        exit_price,\n",
        "                        direction,\n",
        "                        pnl,\n",
        "                        was_correct,\n",
        "                        price_change_pct,\n",
        "                        duration_minutes,\n",
        "                        model_name\n",
        "                    ))\n",
        "                    self.memory.conn.commit()\n",
        "\n",
        "                    outcomes_by_model[model_name]['closed_trades'] += 1\n",
        "                    outcomes_by_model[model_name]['total_pnl'] += pnl\n",
        "                    if was_correct:\n",
        "                        outcomes_by_model[model_name]['wins'] += 1\n",
        "                    else:\n",
        "                        outcomes_by_model[model_name]['losses'] += 1\n",
        "\n",
        "                    outcomes_by_model[model_name]['trade_results'].append({\n",
        "                        'pair': pair,\n",
        "                        'pnl': pnl,\n",
        "                        'was_correct': was_correct,\n",
        "                        'exit_type': 'TP' if hit_tp else 'SL'\n",
        "                    })\n",
        "\n",
        "                    trade['closed'] = True\n",
        "\n",
        "                    status = \"WIN ‚úÖ\" if was_correct else \"LOSS ‚ùå\"\n",
        "                    print_status(\n",
        "                        f\"{model_name}: {pair} {direction} closed @ {exit_price:.5f} - \"\n",
        "                        f\"P&L: ${pnl:.5f} - {status}\",\n",
        "                        \"success\" if was_correct else \"warn\"\n",
        "                    )\n",
        "\n",
        "        for model_name, outcomes in outcomes_by_model.items():\n",
        "            if outcomes['closed_trades'] > 0:\n",
        "                outcomes['accuracy'] = (outcomes['wins'] / outcomes['closed_trades']) * 100\n",
        "            else:\n",
        "                outcomes['accuracy'] = 0.0\n",
        "\n",
        "        return dict(outcomes_by_model)\n",
        "\n",
        "# ======================================================\n",
        "# LEARNING SYSTEM\n",
        "# ======================================================\n",
        "class LearningProgressTracker:\n",
        "    \"\"\"Tracks and improves AI learning over time using REAL trade outcomes\"\"\"\n",
        "\n",
        "    def __init__(self, progress_file=LEARNING_PROGRESS_FILE):\n",
        "        self.progress_file = progress_file\n",
        "        self.learning_data = self.load_progress()\n",
        "\n",
        "    def load_progress(self):\n",
        "        if self.progress_file.exists():\n",
        "            try:\n",
        "                return pickle.load(open(self.progress_file, \"rb\"))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return {\n",
        "            'total_iterations': 0,\n",
        "            'successful_patterns': {},\n",
        "            'failed_patterns': {},\n",
        "            'model_evolution': {},\n",
        "            'best_parameters_history': [],\n",
        "            'learning_curve': [],\n",
        "            'adaptation_score': 0.0,\n",
        "            'real_trade_outcomes': []\n",
        "        }\n",
        "\n",
        "    def save_progress(self):\n",
        "        try:\n",
        "            with open(self.progress_file, 'wb') as f:\n",
        "                pickle.dump(self.learning_data, f, protocol=4)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save learning progress: {e}\")\n",
        "\n",
        "    def record_iteration(self, iteration_results, trade_outcomes=None):\n",
        "        \"\"\"Record results with REAL trade outcomes\"\"\"\n",
        "        self.learning_data['total_iterations'] += 1\n",
        "\n",
        "        if trade_outcomes:\n",
        "            self.learning_data['real_trade_outcomes'].append({\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'outcomes': trade_outcomes\n",
        "            })\n",
        "\n",
        "        for model_name, result in iteration_results.items():\n",
        "            if not result or 'metrics' not in result:\n",
        "                continue\n",
        "\n",
        "            metrics = result['metrics']\n",
        "\n",
        "            if trade_outcomes and model_name in trade_outcomes:\n",
        "                actual_pnl = trade_outcomes[model_name]['total_pnl']\n",
        "                actual_accuracy = trade_outcomes[model_name]['accuracy']\n",
        "            else:\n",
        "                actual_pnl = metrics['total_pnl']\n",
        "                actual_accuracy = 0.0\n",
        "\n",
        "            if actual_pnl > 0:\n",
        "                pattern_key = f\"{model_name}_success\"\n",
        "                if pattern_key not in self.learning_data['successful_patterns']:\n",
        "                    self.learning_data['successful_patterns'][pattern_key] = []\n",
        "\n",
        "                self.learning_data['successful_patterns'][pattern_key].append({\n",
        "                    'chromosome': result.get('chromosome'),\n",
        "                    'pnl': actual_pnl,\n",
        "                    'accuracy': actual_accuracy,\n",
        "                    'sharpe': metrics['sharpe'],\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                })\n",
        "            else:\n",
        "                pattern_key = f\"{model_name}_fail\"\n",
        "                if pattern_key not in self.learning_data['failed_patterns']:\n",
        "                    self.learning_data['failed_patterns'][pattern_key] = []\n",
        "\n",
        "                self.learning_data['failed_patterns'][pattern_key].append({\n",
        "                    'chromosome': result.get('chromosome'),\n",
        "                    'pnl': actual_pnl,\n",
        "                    'accuracy': actual_accuracy,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "        recent_results = self.learning_data['learning_curve'][-20:] if len(self.learning_data['learning_curve']) >= 20 else []\n",
        "        if recent_results:\n",
        "            recent_avg = np.mean(recent_results)\n",
        "            self.learning_data['adaptation_score'] = min(100, max(0, recent_avg))\n",
        "\n",
        "        self.save_progress()\n",
        "\n",
        "    def get_smart_mutation_rate(self, model_name, base_rate):\n",
        "        \"\"\"Adjust mutation rate based on learning progress\"\"\"\n",
        "        success_key = f\"{model_name}_success\"\n",
        "        fail_key = f\"{model_name}_fail\"\n",
        "\n",
        "        success_count = len(self.learning_data['successful_patterns'].get(success_key, []))\n",
        "        fail_count = len(self.learning_data['failed_patterns'].get(fail_key, []))\n",
        "\n",
        "        if success_count + fail_count == 0:\n",
        "            return base_rate\n",
        "\n",
        "        success_ratio = success_count / (success_count + fail_count)\n",
        "\n",
        "        if success_ratio > 0.6:\n",
        "            return base_rate * 0.7\n",
        "        elif success_ratio < 0.4:\n",
        "            return base_rate * 1.3\n",
        "        else:\n",
        "            return base_rate\n",
        "\n",
        "    def get_best_historical_chromosomes(self, model_name, top_n=3):\n",
        "        \"\"\"Get best performing chromosomes from history\"\"\"\n",
        "        pattern_key = f\"{model_name}_success\"\n",
        "        successes = self.learning_data['successful_patterns'].get(pattern_key, [])\n",
        "\n",
        "        if not successes:\n",
        "            return []\n",
        "\n",
        "        sorted_successes = sorted(successes, key=lambda x: x['pnl'], reverse=True)\n",
        "        return [s['chromosome'] for s in sorted_successes[:top_n] if s.get('chromosome')]\n",
        "\n",
        "    def update_learning_curve(self, total_pnl):\n",
        "        \"\"\"Track overall learning progress\"\"\"\n",
        "        self.learning_data['learning_curve'].append(total_pnl)\n",
        "\n",
        "        if len(self.learning_data['learning_curve']) > 100:\n",
        "            self.learning_data['learning_curve'] = self.learning_data['learning_curve'][-100:]\n",
        "\n",
        "        self.save_progress()\n",
        "\n",
        "    def get_learning_report(self):\n",
        "        \"\"\"Generate report on AI learning progress\"\"\"\n",
        "        total_iterations = self.learning_data['total_iterations']\n",
        "        adaptation_score = self.learning_data['adaptation_score']\n",
        "\n",
        "        total_successes = sum(len(patterns) for patterns in self.learning_data['successful_patterns'].values())\n",
        "        total_failures = sum(len(patterns) for patterns in self.learning_data['failed_patterns'].values())\n",
        "\n",
        "        learning_trend = \"üìà Improving\" if adaptation_score > 50 else \"üìâ Needs Adjustment\"\n",
        "\n",
        "        return {\n",
        "            'total_iterations': total_iterations,\n",
        "            'adaptation_score': adaptation_score,\n",
        "            'total_successes': total_successes,\n",
        "            'total_failures': total_failures,\n",
        "            'learning_trend': learning_trend,\n",
        "            'success_rate': (total_successes / (total_successes + total_failures) * 100) if (total_successes + total_failures) > 0 else 0\n",
        "        }\n",
        "\n",
        "LEARNING_TRACKER = LearningProgressTracker()\n",
        "\n",
        "# ======================================================\n",
        "# INFINITE MEMORY SYSTEM\n",
        "# ======================================================\n",
        "class InfiniteMemorySystem:\n",
        "    def __init__(self, db_path=INFINITE_MEMORY_DB):\n",
        "        self.db_path = db_path\n",
        "        self.conn = None\n",
        "        self.initialize_database()\n",
        "\n",
        "    def initialize_database(self):\n",
        "        self.conn = sqlite3.connect(str(self.db_path))\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS signals_history (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                direction TEXT NOT NULL,\n",
        "                entry_price REAL NOT NULL,\n",
        "                sl_price REAL,\n",
        "                tp_price REAL,\n",
        "                atr REAL,\n",
        "                confidence INTEGER,\n",
        "                chromosome_hash TEXT,\n",
        "                generation INTEGER,\n",
        "                model_name TEXT,\n",
        "                mode TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS trade_results (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                signal_id INTEGER,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                entry_price REAL NOT NULL,\n",
        "                exit_price REAL NOT NULL,\n",
        "                direction TEXT NOT NULL,\n",
        "                pnl REAL NOT NULL,\n",
        "                was_correct BOOLEAN NOT NULL,\n",
        "                price_change_pct REAL,\n",
        "                duration_minutes INTEGER,\n",
        "                model_name TEXT,\n",
        "                FOREIGN KEY (signal_id) REFERENCES signals_history(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS competition_results (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                iteration INTEGER NOT NULL,\n",
        "                model_name TEXT NOT NULL,\n",
        "                total_pnl REAL,\n",
        "                accuracy REAL,\n",
        "                sharpe_ratio REAL,\n",
        "                max_drawdown REAL,\n",
        "                total_trades INTEGER,\n",
        "                successful_trades INTEGER,\n",
        "                rank INTEGER,\n",
        "                mode TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        self.conn.commit()\n",
        "        print_status(\"Infinite Memory Database initialized\", \"success\")\n",
        "\n",
        "    def get_model_trade_history(self, model_name, days=7):\n",
        "        \"\"\"Get REAL trade history from database\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        since_date = (datetime.now() - pd.Timedelta(days=days)).isoformat()\n",
        "\n",
        "        cursor.execute('''\n",
        "            SELECT\n",
        "                COUNT(*) as total_trades,\n",
        "                SUM(CASE WHEN was_correct THEN 1 ELSE 0 END) as successful_trades,\n",
        "                AVG(pnl) as avg_pnl,\n",
        "                SUM(pnl) as total_pnl\n",
        "            FROM trade_results\n",
        "            WHERE model_name = ? AND timestamp > ?\n",
        "        ''', (model_name, since_date))\n",
        "\n",
        "        result = cursor.fetchone()\n",
        "        return {\n",
        "            'total_trades': result[0] if result[0] else 0,\n",
        "            'successful_trades': result[1] if result[1] else 0,\n",
        "            'avg_pnl': result[2] if result[2] else 0,\n",
        "            'total_pnl': result[3] if result[3] else 0,\n",
        "            'accuracy': (result[1] / result[0] * 100) if result[0] else 0\n",
        "        }\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "\n",
        "MEMORY_SYSTEM = InfiniteMemorySystem()\n",
        "TRADE_TRACKER = TradeOutcomeTracker(MEMORY_SYSTEM)\n",
        "\n",
        "# ======================================================\n",
        "# WEEKEND/MONDAY MANAGER\n",
        "# ======================================================\n",
        "class WeekendMondayManager:\n",
        "    def __init__(self):\n",
        "        self.monday_runs_count = self.load_monday_runs()\n",
        "\n",
        "    def load_monday_runs(self):\n",
        "        if MONDAY_RUNS_FILE.exists():\n",
        "            try:\n",
        "                data = pickle.load(open(MONDAY_RUNS_FILE, \"rb\"))\n",
        "                if data.get('date') != datetime.now().strftime('%Y-%m-%d'):\n",
        "                    return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "                return data\n",
        "            except:\n",
        "                return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "        return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "\n",
        "    def save_monday_runs(self):\n",
        "        try:\n",
        "            with open(MONDAY_RUNS_FILE, \"wb\") as f:\n",
        "                pickle.dump(self.monday_runs_count, f, protocol=4)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save Monday runs: {e}\")\n",
        "\n",
        "    def get_mode(self):\n",
        "        weekday = datetime.now().weekday()\n",
        "\n",
        "        if weekday in [5, 6]:\n",
        "            return \"weekend_replay\"\n",
        "        elif weekday == 0:\n",
        "            if self.monday_runs_count['count'] < REPLAY_CONFIG['monday_replay_runs']:\n",
        "                return \"monday_replay\"\n",
        "            else:\n",
        "                return \"normal\"\n",
        "        else:\n",
        "            return \"normal\"\n",
        "\n",
        "    def increment_monday_runs(self):\n",
        "        self.monday_runs_count['count'] += 1\n",
        "        self.monday_runs_count['date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        self.save_monday_runs()\n",
        "\n",
        "    def get_status_message(self):\n",
        "        mode = self.get_mode()\n",
        "        day_name = datetime.now().strftime('%A')\n",
        "\n",
        "        if mode == \"weekend_replay\":\n",
        "            return f\"üé¨ {day_name.upper()} REPLAY MODE\"\n",
        "        elif mode == \"monday_replay\":\n",
        "            return f\"üî¥ MONDAY REPLAY MODE\"\n",
        "        else:\n",
        "            return f\"üíº {day_name.upper()} NORMAL MODE\"\n",
        "\n",
        "    def should_send_email(self):\n",
        "        mode = self.get_mode()\n",
        "        return mode == \"normal\"\n",
        "\n",
        "WEEKEND_MONDAY_MANAGER = WeekendMondayManager()\n",
        "\n",
        "# ======================================================\n",
        "# HISTORICAL REPLAY SYSTEM\n",
        "# ======================================================\n",
        "class HistoricalReplaySystem:\n",
        "    def __init__(self, data, random_selection=True):\n",
        "        if random_selection:\n",
        "            start_date, end_date = random.choice(RANDOM_REPLAY_PERIODS)\n",
        "            print_status(f\"üé≤ Random period selected: {start_date} ‚Üí {end_date}\", \"success\")\n",
        "        else:\n",
        "            start_date = \"2024-01-01\"\n",
        "            end_date = \"2024-03-31\"\n",
        "\n",
        "        self.start_date = pd.to_datetime(start_date)\n",
        "        self.end_date = pd.to_datetime(end_date)\n",
        "        self.current_date = self.start_date\n",
        "        self.data = data\n",
        "        self.is_replay_mode = True\n",
        "\n",
        "        print_status(f\"Replay Mode Active: {start_date} ‚Üí {end_date}\", \"info\")\n",
        "\n",
        "    def get_available_data(self, pair):\n",
        "        \"\"\"Get historical data up to current replay date\"\"\"\n",
        "        if pair not in self.data:\n",
        "            return None\n",
        "\n",
        "        full_data = self.data[pair]\n",
        "        filtered_data = {}\n",
        "\n",
        "        for tf, df in full_data.items():\n",
        "            mask = df.index <= self.current_date\n",
        "            filtered_df = df[mask].copy()\n",
        "\n",
        "            if len(filtered_df) > 0:\n",
        "                filtered_data[tf] = filtered_df\n",
        "\n",
        "        return filtered_data if filtered_data else None\n",
        "\n",
        "    def get_historical_price(self, pair):\n",
        "        \"\"\"Get historical price from data\"\"\"\n",
        "        if pair not in self.data:\n",
        "            return None\n",
        "\n",
        "        tfs = self.data.get(pair, {})\n",
        "        if not tfs:\n",
        "            return None\n",
        "\n",
        "        for tf, df in tfs.items():\n",
        "            mask = df.index <= self.current_date\n",
        "            filtered_df = df[mask]\n",
        "\n",
        "            if len(filtered_df) > 0:\n",
        "                return float(filtered_df['close'].iloc[-1])\n",
        "\n",
        "        return None\n",
        "\n",
        "    def advance_time(self, minutes=60):\n",
        "        \"\"\"Advance replay time\"\"\"\n",
        "        self.current_date += pd.Timedelta(minutes=minutes)\n",
        "        return self.current_date <= self.end_date\n",
        "\n",
        "    def get_progress(self):\n",
        "        \"\"\"Get replay progress percentage\"\"\"\n",
        "        total_duration = (self.end_date - self.start_date).total_seconds()\n",
        "        elapsed = (self.current_date - self.start_date).total_seconds()\n",
        "        return (elapsed / total_duration) * 100 if total_duration > 0 else 0\n",
        "\n",
        "# ======================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ======================================================\n",
        "def make_index_tz_naive(df):\n",
        "    if isinstance(df.index, pd.DatetimeIndex):\n",
        "        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "        if df.index.tz is not None:\n",
        "            df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def ensure_atr(df):\n",
        "    if \"atr\" in df.columns and not df[\"atr\"].isna().all():\n",
        "        df[\"atr\"] = df[\"atr\"].fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "        return df\n",
        "\n",
        "    high, low, close = df[\"high\"].values, df[\"low\"].values, df[\"close\"].values\n",
        "    tr = np.maximum.reduce([\n",
        "        high - low,\n",
        "        np.abs(high - np.roll(close, 1)),\n",
        "        np.abs(low - np.roll(close, 1))\n",
        "    ])\n",
        "    tr[0] = high[0] - low[0] if len(tr) > 0 else MIN_ATR\n",
        "    atr_series = pd.Series(tr, index=df.index).rolling(\n",
        "        ATR_PERIOD, min_periods=1\n",
        "    ).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "    df[\"atr\"] = atr_series\n",
        "    return df\n",
        "\n",
        "def seed_hybrid_signal(df):\n",
        "    if \"hybrid_signal\" not in df.columns or df[\"hybrid_signal\"].abs().sum() == 0:\n",
        "        fast = df[\"close\"].rolling(10, min_periods=1).mean()\n",
        "        slow = df[\"close\"].rolling(50, min_periods=1).mean()\n",
        "        df[\"hybrid_signal\"] = (fast - slow).fillna(0)\n",
        "    df[\"hybrid_signal\"] = df[\"hybrid_signal\"].fillna(0.0).astype(float)\n",
        "    return df\n",
        "\n",
        "def fetch_live_rate(pair, timeout=8):\n",
        "    \"\"\"Fetch live rate from API - ONLY for non-replay modes\"\"\"\n",
        "    token = os.environ.get(\"BROWSERLESS_TOKEN\", \"\")\n",
        "    if not token:\n",
        "        return 0.0\n",
        "\n",
        "    from_currency, to_currency = pair.split(\"/\")\n",
        "    url = f\"https://production-sfo.browserless.io/content?token={token}\"\n",
        "    payload = {\n",
        "        \"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.post(url, json=payload, timeout=timeout)\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', r.text)\n",
        "        return float(match.group(1).replace(\",\", \"\")) if match else 0.0\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def load_unified_pickles(folder):\n",
        "    combined = {}\n",
        "\n",
        "    for pair in PAIRS:\n",
        "        combined[pair] = {}\n",
        "        prefix = pair.replace(\"/\", \"_\")\n",
        "        pair_files = list(folder.glob(f\"{prefix}*_2244.pkl\"))\n",
        "        pair_files.sort()\n",
        "\n",
        "        if not pair_files:\n",
        "            continue\n",
        "\n",
        "        for pf in pair_files:\n",
        "            try:\n",
        "                df = pd.read_pickle(pf)\n",
        "                if not isinstance(df, pd.DataFrame):\n",
        "                    continue\n",
        "\n",
        "                df = make_index_tz_naive(df)\n",
        "                df = ensure_atr(df)\n",
        "                df = seed_hybrid_signal(df)\n",
        "\n",
        "                if (df['close'] <= 0).any() or len(df) < 50:\n",
        "                    continue\n",
        "\n",
        "                tf = re.sub(rf\"{prefix}_?|\\.pkl\", \"\", pf.name).replace(\"__\", \"_\").strip(\"_\")\n",
        "                if not tf:\n",
        "                    tf = \"merged\"\n",
        "                combined[pair][tf] = df\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    return combined\n",
        "\n",
        "def build_tf_map(data):\n",
        "    return {p: list(tfs.keys()) for p, tfs in data.items()}\n",
        "\n",
        "def create_chrom(tf_map):\n",
        "    chrom = [\n",
        "        random.uniform(1.0, 2.5),\n",
        "        random.uniform(1.5, 3.0),\n",
        "        random.uniform(0.005, 0.03),\n",
        "        random.uniform(0.3, 0.7)\n",
        "    ]\n",
        "\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p, [])))\n",
        "        chrom += np.random.dirichlet(np.ones(n)).tolist()\n",
        "\n",
        "    return chrom\n",
        "\n",
        "def decode_chrom(chrom, tf_map):\n",
        "    atr_sl, atr_tp, risk, conf = chrom[:4]\n",
        "\n",
        "    atr_sl = min(max(atr_sl, 1.0), MAX_ATR_SL)\n",
        "    atr_tp = min(max(atr_tp, 1.0), MAX_ATR_TP)\n",
        "\n",
        "    tf_w = {}\n",
        "    idx = 4\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p, [])))\n",
        "        weights = chrom[idx:idx+n]\n",
        "        weights = np.array(weights, dtype=float)\n",
        "\n",
        "        if weights.sum() <= 0:\n",
        "            weights = np.ones_like(weights) / len(weights)\n",
        "        else:\n",
        "            weights = weights / (weights.sum() + EPS)\n",
        "\n",
        "        tf_w[p] = {tf: float(w) for tf, w in zip(tf_map.get(p, []), weights)}\n",
        "        idx += n\n",
        "\n",
        "    return atr_sl, atr_tp, risk, conf, tf_w\n",
        "\n",
        "def tournament_select(pop, k=TOURNAMENT_SIZE):\n",
        "    return max(random.sample(pop, k), key=lambda x: x[0])[1]\n",
        "\n",
        "def calculate_sharpe_ratio(equity_curve):\n",
        "    if len(equity_curve) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    returns = np.diff(equity_curve) / (equity_curve[:-1] + EPS)\n",
        "    if len(returns) == 0 or np.std(returns) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return np.mean(returns) / (np.std(returns) + EPS)\n",
        "\n",
        "def run_vector_backtest(data, capital, base_risk, atr_sl, atr_tp, conf_mult,\n",
        "                       tf_weights, trade_memory=None):\n",
        "    if trade_memory is None:\n",
        "        trade_memory = {pair: [] for pair in PAIRS}\n",
        "\n",
        "    results = {}\n",
        "    precomputed = {}\n",
        "    pair_performance = {pair: 0.0 for pair in PAIRS}\n",
        "\n",
        "    for pair, tfs in data.items():\n",
        "        if not tfs:\n",
        "            results[pair] = {\n",
        "                'equity_curve': np.array([capital]),\n",
        "                'total_pnl': 0,\n",
        "                'max_drawdown': 0,\n",
        "                'sharpe': 0\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        all_idx = sorted(set().union(*[df.index for df in tfs.values()]))\n",
        "        df_all = pd.DataFrame(index=all_idx)\n",
        "\n",
        "        for tf, df in tfs.items():\n",
        "            df_all[f'close_{tf}'] = df['close'].reindex(df_all.index).ffill()\n",
        "            df_all[f'signal_{tf}'] = df['hybrid_signal'].reindex(df_all.index).ffill().fillna(0.0)\n",
        "            df_all[f'atr_{tf}'] = df['atr'].reindex(df_all.index).ffill().fillna(MIN_ATR)\n",
        "\n",
        "        df_all['price'] = df_all[[c for c in df_all.columns if c.startswith('close_')]].mean(axis=1).clip(lower=EPS)\n",
        "        df_all['atr'] = df_all[[c for c in df_all.columns if c.startswith('atr_')]].mean(axis=1).clip(lower=MIN_ATR)\n",
        "        precomputed[pair] = df_all\n",
        "\n",
        "    for pair, df_all in precomputed.items():\n",
        "        tfs = data.get(pair, {})\n",
        "        if not tfs:\n",
        "            continue\n",
        "\n",
        "        agg_signal = sum([\n",
        "            df_all[f'signal_{tf}'] * tf_weights.get(pair, {}).get(tf, 0.0)\n",
        "            for tf in tfs.keys()\n",
        "        ])\n",
        "\n",
        "        mean_abs_signal = np.mean([\n",
        "            df_all[f'signal_{tf}'].abs().mean() for tf in tfs.keys()\n",
        "        ]) if tfs else 0.0\n",
        "        conf_threshold = conf_mult * (mean_abs_signal + EPS)\n",
        "\n",
        "        df_all['agg_signal'] = np.where(\n",
        "            np.abs(agg_signal) >= conf_threshold,\n",
        "            agg_signal,\n",
        "            0.0\n",
        "        )\n",
        "\n",
        "        price = df_all['price'].values\n",
        "        atr = df_all['atr'].values\n",
        "        agg_signal = df_all['agg_signal'].values\n",
        "        n = len(price)\n",
        "\n",
        "        if n <= 1:\n",
        "            results[pair] = {\n",
        "                'equity_curve': np.array([capital]),\n",
        "                'total_pnl': 0,\n",
        "                'max_drawdown': 0,\n",
        "                'sharpe': 0\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        memory_factor = 1.0\n",
        "        if trade_memory.get(pair):\n",
        "            recent_trades = trade_memory[pair][-10:]\n",
        "            if recent_trades:\n",
        "                win_rate = sum(1 for tr in recent_trades if tr.get('pnl', 0) > 0) / len(recent_trades)\n",
        "                memory_factor = max(0.3, min(2.0, 0.5 + win_rate))\n",
        "\n",
        "        raw_size = (capital * base_risk * np.abs(agg_signal)) / (atr_sl * (atr / price) + EPS)\n",
        "        size = np.zeros_like(raw_size)\n",
        "\n",
        "        for i in range(len(raw_size)):\n",
        "            sized = raw_size[i] * memory_factor\n",
        "            sized = min(sized, capital * MAX_POSITION_FRACTION, MAX_TRADE_CAP)\n",
        "            atr_value = atr[i] if i < len(atr) else MIN_ATR\n",
        "            atr_cap = capital * 0.02 / (atr_value / price[i] + EPS)\n",
        "            sized = min(sized, atr_cap)\n",
        "            size[i] = sized\n",
        "\n",
        "        size = np.nan_to_num(size, nan=0.0, posinf=MAX_TRADE_CAP)\n",
        "        direction = np.sign(agg_signal)\n",
        "        pnl = direction * size * (atr_tp * atr / price)\n",
        "\n",
        "        equity = np.zeros(n, dtype=float)\n",
        "        equity[0] = float(capital)\n",
        "        for i in range(1, n):\n",
        "            equity[i] = equity[i-1] + float(pnl[i])\n",
        "\n",
        "        final_pnl = float(equity[-1] - capital)\n",
        "        pair_performance[pair] = final_pnl\n",
        "\n",
        "        trade_memory.setdefault(pair, []).append({\n",
        "            'equity': float(equity[-1]),\n",
        "            'pnl': final_pnl,\n",
        "            'timestamp': pd.Timestamp.now().isoformat()\n",
        "        })\n",
        "\n",
        "        if len(trade_memory[pair]) > MAX_TRADE_MEMORY:\n",
        "            trade_memory[pair] = trade_memory[pair][-MAX_TRADE_MEMORY:]\n",
        "\n",
        "        sharpe = calculate_sharpe_ratio(equity)\n",
        "        max_dd = float(np.max(np.maximum.accumulate(equity) - equity))\n",
        "\n",
        "        results[pair] = {\n",
        "            'equity_curve': equity,\n",
        "            'total_pnl': final_pnl,\n",
        "            'max_drawdown': max_dd,\n",
        "            'sharpe': sharpe\n",
        "        }\n",
        "\n",
        "    total_sharpe = sum([r['sharpe'] for r in results.values()])\n",
        "    perf_values = list(pair_performance.values())\n",
        "    pair_balance_penalty = np.std(perf_values) / (np.mean(perf_values) + EPS) if perf_values else 0.0\n",
        "    score = total_sharpe - 0.5 * pair_balance_penalty\n",
        "\n",
        "    return score, results, trade_memory\n",
        "\n",
        "# ======================================================\n",
        "# MODEL STATE MANAGER\n",
        "# ======================================================\n",
        "class ModelStateManager:\n",
        "    def __init__(self, model_name, repo_folder):\n",
        "        self.model_name = model_name\n",
        "        self.prefix = model_name.lower().replace(\" \", \"_\")\n",
        "        self.repo_folder = Path(repo_folder)\n",
        "\n",
        "        self.files = {\n",
        "            'population': self.repo_folder / f\"{self.prefix}_population.pkl\",\n",
        "            'trade_memory': self.repo_folder / f\"{self.prefix}_trade_memory.pkl\",\n",
        "            'best_chrom': self.repo_folder / f\"{self.prefix}_best_chrom.pkl\",\n",
        "            'gen_count': self.repo_folder / f\"{self.prefix}_gen_count.pkl\",\n",
        "            'ga_progress': self.repo_folder / f\"{self.prefix}_ga_progress.pkl\",\n",
        "        }\n",
        "\n",
        "    def save_all(self, population, trade_memory, best_chrom, gen_count, ga_progress):\n",
        "        temp_files = {}\n",
        "\n",
        "        try:\n",
        "            for key, path in self.files.items():\n",
        "                temp_path = path.with_suffix('.tmp')\n",
        "\n",
        "                if key == 'population':\n",
        "                    with open(temp_path, 'wb') as f:\n",
        "                        pickle.dump(population, f, protocol=4)\n",
        "                elif key == 'trade_memory':\n",
        "                    with open(temp_path, 'wb') as f:\n",
        "                        pickle.dump(trade_memory, f, protocol=4)\n",
        "                elif key == 'best_chrom':\n",
        "                    with open(temp_path, 'wb') as f:\n",
        "                        pickle.dump(best_chrom, f, protocol=4)\n",
        "                elif key == 'gen_count':\n",
        "                    with open(temp_path, 'wb') as f:\n",
        "                        pickle.dump(gen_count, f, protocol=4)\n",
        "                elif key == 'ga_progress':\n",
        "                    with open(temp_path, 'wb') as f:\n",
        "                        pickle.dump(ga_progress, f, protocol=4)\n",
        "\n",
        "                temp_files[key] = temp_path\n",
        "\n",
        "            for key, temp_path in temp_files.items():\n",
        "                temp_path.rename(self.files[key])\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ùå {self.model_name}: Failed to save state: {e}\", \"error\")\n",
        "            for temp_path in temp_files.values():\n",
        "                if temp_path.exists():\n",
        "                    try:\n",
        "                        temp_path.unlink()\n",
        "                    except:\n",
        "                        pass\n",
        "            return False\n",
        "\n",
        "    def load_all(self):\n",
        "        state = {\n",
        "            'population': None,\n",
        "            'trade_memory': {},\n",
        "            'best_chrom': None,\n",
        "            'gen_count': 0,\n",
        "            'ga_progress': []\n",
        "        }\n",
        "\n",
        "        for key, path in self.files.items():\n",
        "            if key not in state:\n",
        "                continue\n",
        "\n",
        "            if path.exists():\n",
        "                try:\n",
        "                    with open(path, 'rb') as f:\n",
        "                        state[key] = pickle.load(f)\n",
        "                    print_status(f\"‚úÖ {self.model_name}: Loaded {key}\", \"info\")\n",
        "                except Exception as e:\n",
        "                    print_status(f\"‚ö†Ô∏è {self.model_name}: Failed to load {key}: {e}\", \"warn\")\n",
        "                    try:\n",
        "                        path.unlink()\n",
        "                        print_status(f\"üóëÔ∏è {self.model_name}: Removed corrupted {key} file\", \"info\")\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "        return state\n",
        "\n",
        "# ======================================================\n",
        "# COMPETITION MANAGER\n",
        "# ======================================================\n",
        "class CompetitionManager:\n",
        "    def __init__(self, models_config=COMPETITION_MODELS):\n",
        "        self.models_config = models_config\n",
        "        self.results = {model: {} for model in models_config.keys()}\n",
        "        self.leaderboard = []\n",
        "        self.iteration = 0\n",
        "\n",
        "    def run_competition(self, data, mode=\"normal\"):\n",
        "        self.iteration += 1\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"üèÜ COMPETITION ROUND #{self.iteration} ({mode.upper()} MODE)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        for model_name in self.models_config.keys():\n",
        "            config = self.models_config[model_name]\n",
        "            if not config.get('enabled', True):\n",
        "                config['enabled'] = True\n",
        "\n",
        "            print(f\"\\n{config['color']} Training {model_name} AI...\")\n",
        "\n",
        "            try:\n",
        "                best_chrom, trade_memory, ga_progress, metrics = self.run_model_ga(\n",
        "                    model_name, data, config\n",
        "                )\n",
        "\n",
        "                self.results[model_name] = {\n",
        "                    'chromosome': best_chrom,\n",
        "                    'trade_memory': trade_memory,\n",
        "                    'ga_progress': ga_progress,\n",
        "                    'metrics': metrics,\n",
        "                    'config': config,\n",
        "                    'mode': mode\n",
        "                }\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to train {model_name}: {e}\")\n",
        "                print_status(f\"‚ùå {model_name} training failed: {e}\", \"error\")\n",
        "                self.results[model_name] = {}\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def run_model_ga(self, model_name, data, config):\n",
        "        \"\"\"Run GA with enhanced learning\"\"\"\n",
        "        tf_map = build_tf_map(data)\n",
        "\n",
        "        state_manager = ModelStateManager(model_name, REPO_FOLDER)\n",
        "        saved_state = state_manager.load_all()\n",
        "\n",
        "        if saved_state['population']:\n",
        "            population = saved_state['population']\n",
        "            print_status(f\"‚úÖ {model_name}: Loaded population ({len(population)} chromosomes)\", \"success\")\n",
        "        else:\n",
        "            best_historical = LEARNING_TRACKER.get_best_historical_chromosomes(model_name, top_n=3)\n",
        "            population = best_historical if best_historical else []\n",
        "\n",
        "            while len(population) < config['pop_size']:\n",
        "                population.append(create_chrom(tf_map))\n",
        "\n",
        "            print_status(f\"üÜï {model_name}: Creating population (with {len(best_historical)} historical seeds)\", \"info\")\n",
        "\n",
        "        trade_memory = saved_state['trade_memory']\n",
        "        best_chrom_ever = saved_state['best_chrom']\n",
        "        last_gen = saved_state['gen_count']\n",
        "        ga_progress = saved_state['ga_progress']\n",
        "\n",
        "        if best_chrom_ever:\n",
        "            try:\n",
        "                best_score_ever, _, _ = run_vector_backtest(\n",
        "                    data, BASE_CAPITAL, *decode_chrom(best_chrom_ever, tf_map), trade_memory\n",
        "                )\n",
        "            except:\n",
        "                best_score_ever = -np.inf\n",
        "        else:\n",
        "            best_score_ever = -np.inf\n",
        "\n",
        "        base_mutation_rate = config['mutation_rate']\n",
        "        adaptive_mutation_rate = LEARNING_TRACKER.get_smart_mutation_rate(model_name, base_mutation_rate)\n",
        "\n",
        "        for gen in range(last_gen + 1, last_gen + 1 + config['generations']):\n",
        "            current_gen_scores = []\n",
        "\n",
        "            for c in population:\n",
        "                score, results, _ = run_vector_backtest(\n",
        "                    data, BASE_CAPITAL, *decode_chrom(c, tf_map), trade_memory\n",
        "                )\n",
        "                current_gen_scores.append((score, c, results))\n",
        "\n",
        "            current_gen_scores.sort(reverse=True, key=lambda x: x[0])\n",
        "            best_score, best_chrom, best_results = current_gen_scores[0]\n",
        "\n",
        "            if best_score > best_score_ever:\n",
        "                best_score_ever = best_score\n",
        "                best_chrom_ever = best_chrom\n",
        "\n",
        "            ga_progress.append(min(100, int((best_score / (abs(best_score_ever) + EPS)) * 100)))\n",
        "\n",
        "            next_population = [best_chrom]\n",
        "            while len(next_population) < config['pop_size']:\n",
        "                p1 = tournament_select(current_gen_scores, k=3)\n",
        "                p2 = tournament_select(current_gen_scores, k=3)\n",
        "\n",
        "                crossover_point = random.randint(1, len(p1)-2)\n",
        "                child = p1[:crossover_point] + p2[crossover_point:]\n",
        "\n",
        "                for i in range(len(child)):\n",
        "                    if random.random() < adaptive_mutation_rate:\n",
        "                        child[i] *= random.uniform(0.7, 1.3)\n",
        "\n",
        "                next_population.append(child)\n",
        "\n",
        "            population = next_population\n",
        "            state_manager.save_all(population, trade_memory, best_chrom_ever, gen, ga_progress)\n",
        "\n",
        "        metrics = self.calculate_metrics(best_results, trade_memory)\n",
        "        return best_chrom_ever, trade_memory, ga_progress, metrics\n",
        "\n",
        "    def calculate_metrics(self, results, trade_memory):\n",
        "        total_pnl = sum(r['total_pnl'] for r in results.values())\n",
        "        sharpe = np.mean([r['sharpe'] for r in results.values()])\n",
        "        max_dd = max([r['max_drawdown'] for r in results.values()])\n",
        "\n",
        "        return {\n",
        "            'total_pnl': total_pnl,\n",
        "            'sharpe': sharpe,\n",
        "            'max_drawdown': max_dd\n",
        "        }\n",
        "\n",
        "    def generate_leaderboard(self, signals_results, mode=\"normal\"):\n",
        "        leaderboard_data = []\n",
        "\n",
        "        for model_name, result in self.results.items():\n",
        "            if not result or 'config' not in result or 'metrics' not in result:\n",
        "                continue\n",
        "\n",
        "            config = result['config']\n",
        "            metrics = result['metrics']\n",
        "\n",
        "            history = MEMORY_SYSTEM.get_model_trade_history(model_name, days=7)\n",
        "\n",
        "            leaderboard_data.append({\n",
        "                'model': model_name,\n",
        "                'color': config['color'],\n",
        "                'hex_color': config['hex_color'],\n",
        "                'pnl': history['total_pnl'],\n",
        "                'sharpe': metrics['sharpe'],\n",
        "                'max_dd': metrics['max_drawdown'],\n",
        "                'total_trades': history['total_trades'],\n",
        "                'successful_trades': history['successful_trades'],\n",
        "                'accuracy': history['accuracy'],\n",
        "                'strategy': config['strategy']\n",
        "            })\n",
        "\n",
        "        if not leaderboard_data:\n",
        "            return []\n",
        "\n",
        "        leaderboard_data.sort(key=lambda x: x['pnl'], reverse=True)\n",
        "        self.leaderboard = leaderboard_data\n",
        "\n",
        "        return leaderboard_data\n",
        "\n",
        "competition = CompetitionManager()\n",
        "\n",
        "# ======================================================\n",
        "# GENERATE LIVE SIGNALS\n",
        "# ======================================================\n",
        "def generate_live_signals(best, data, model_name=\"Unknown\", replay_system=None):\n",
        "    \"\"\"Uses historical prices in replay mode, LIVE X-Rates prices in normal mode\"\"\"\n",
        "    tf_map = build_tf_map(data)\n",
        "    atr_sl, atr_tp, risk, conf, tf_weights = decode_chrom(best, tf_map)\n",
        "\n",
        "    live_signals = {}\n",
        "    is_replay_mode = replay_system is not None and hasattr(replay_system, 'is_replay_mode') and replay_system.is_replay_mode\n",
        "\n",
        "    for pair in PAIRS:\n",
        "        tfs = data.get(pair, {})\n",
        "        if not tfs:\n",
        "            continue\n",
        "\n",
        "        if is_replay_mode:\n",
        "            price = replay_system.get_historical_price(pair)\n",
        "            if price is None or price <= 0:\n",
        "                price = float(list(tfs.values())[0]['close'].iloc[-1])\n",
        "        else:\n",
        "            price = fetch_live_rate(pair)\n",
        "            if price <= 0:\n",
        "                price = float(list(tfs.values())[0]['close'].iloc[-1])\n",
        "\n",
        "        sig_strength = sum([\n",
        "            tf_weights.get(pair, {}).get(tf, 0.0) * tfs[tf][\"hybrid_signal\"].iloc[-1]\n",
        "            for tf in tf_map.get(pair, [])\n",
        "        ])\n",
        "\n",
        "        recent_atr = np.mean([\n",
        "            tfs[tf][\"atr\"].iloc[-1] for tf in tf_map.get(pair, [])\n",
        "        ]) if tfs else 1.0\n",
        "\n",
        "        sig_strength_scaled = sig_strength / (recent_atr + EPS)\n",
        "        noise_factor = random.uniform(0.85, 1.15)\n",
        "        sig_strength_scaled *= noise_factor\n",
        "\n",
        "        if sig_strength_scaled > 0:\n",
        "            direction = \"BUY\"\n",
        "        elif sig_strength_scaled < 0:\n",
        "            direction = \"SELL\"\n",
        "        else:\n",
        "            direction = \"HOLD\"\n",
        "\n",
        "        raw_score = abs(sig_strength_scaled) * 100\n",
        "        score_100 = int(30 + (55 * (raw_score / (raw_score + 10))))\n",
        "        score_100 = min(max(score_100, 30), 85)\n",
        "        score_variation = random.randint(-5, 5)\n",
        "        score_100 = min(max(score_100 + score_variation, 25), 90)\n",
        "        high_conf = score_100 >= 70\n",
        "\n",
        "        max_sl_tp_distance = recent_atr * MAX_ATR_SL\n",
        "        min_sl_tp_distance = recent_atr * MIN_ATR_DISTANCE\n",
        "\n",
        "        if direction == \"BUY\":\n",
        "            base_sl = price - atr_sl * recent_atr\n",
        "            base_tp = price + atr_tp * recent_atr\n",
        "            SL = max(min(base_sl, price - min_sl_tp_distance), price - max_sl_tp_distance)\n",
        "            TP = min(max(base_tp, price + min_sl_tp_distance), price + max_sl_tp_distance)\n",
        "        elif direction == \"SELL\":\n",
        "            base_sl = price + atr_sl * recent_atr\n",
        "            base_tp = price - atr_tp * recent_atr\n",
        "            SL = min(max(base_sl, price + min_sl_tp_distance), price + max_sl_tp_distance)\n",
        "            TP = max(min(base_tp, price - min_sl_tp_distance), price - max_sl_tp_distance)\n",
        "        else:\n",
        "            SL = TP = price\n",
        "\n",
        "        if score_100 < 40 and random.random() < 0.3:\n",
        "            direction = \"HOLD\"\n",
        "            SL = TP = price\n",
        "\n",
        "        live_signals[pair] = {\n",
        "            \"direction\": direction,\n",
        "            \"strength\": float(sig_strength_scaled),\n",
        "            \"score_1_100\": score_100,\n",
        "            \"last_price\": float(price),\n",
        "            \"SL\": float(SL),\n",
        "            \"TP\": float(TP),\n",
        "            \"high_confidence\": high_conf,\n",
        "            \"atr\": float(recent_atr),\n",
        "            \"atr_multiplier_sl\": float(atr_sl),\n",
        "            \"atr_multiplier_tp\": float(atr_tp),\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
        "            \"model\": model_name,\n",
        "            \"mode\": \"replay\" if is_replay_mode else \"live\"\n",
        "        }\n",
        "\n",
        "    return live_signals\n",
        "\n",
        "# ======================================================\n",
        "# EMAIL FUNCTIONS\n",
        "# ======================================================\n",
        "def send_email_alert(subject, body_html, to_email=GMAIL_USER):\n",
        "    \"\"\"Send email alert\"\"\"\n",
        "    if not WEEKEND_MONDAY_MANAGER.should_send_email():\n",
        "        print_status(\"üìß Email sending skipped (Replay Mode)\", \"info\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        msg = MIMEMultipart('alternative')\n",
        "        msg['Subject'] = subject\n",
        "        msg['From'] = GMAIL_USER\n",
        "        msg['To'] = to_email\n",
        "\n",
        "        html_part = MIMEText(body_html, 'html')\n",
        "        msg.attach(html_part)\n",
        "\n",
        "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
        "            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)\n",
        "            server.send_message(msg)\n",
        "\n",
        "        print_status(f\"‚úÖ Email sent: {subject}\", \"success\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ùå Email failed: {e}\", \"error\")\n",
        "        return False\n",
        "\n",
        "def build_leaderboard_html(leaderboard):\n",
        "    \"\"\"Build leaderboard HTML\"\"\"\n",
        "    rows = \"\"\n",
        "    medals = [\"ü•á\", \"ü•à\", \"ü•â\"]\n",
        "\n",
        "    for idx, entry in enumerate(leaderboard[:10]):\n",
        "        medal = medals[idx] if idx < 3 else f\"{idx + 1}.\"\n",
        "\n",
        "        pnl_color = \"#27ae60\" if entry['pnl'] > 0 else \"#e74c3c\"\n",
        "        acc_color = \"#27ae60\" if entry['accuracy'] > 60 else \"#e67e22\" if entry['accuracy'] > 45 else \"#e74c3c\"\n",
        "\n",
        "        rows += f\"\"\"\n",
        "        <tr style=\"border-bottom: 1px solid #ecf0f1;\">\n",
        "            <td style=\"padding: 12px; text-align: center; font-size: 20px;\">{medal}</td>\n",
        "            <td style=\"padding: 12px;\">\n",
        "                <div style=\"display: flex; align-items: center; gap: 8px;\">\n",
        "                    <span style=\"font-size: 20px;\">{entry['color']}</span>\n",
        "                    <div>\n",
        "                        <div style=\"font-weight: 600; color: #2c3e50;\">{entry['model']}</div>\n",
        "                        <div style=\"font-size: 12px; color: #7f8c8d;\">{entry['strategy']}</div>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </td>\n",
        "            <td style=\"padding: 12px; text-align: center; font-weight: 600; color: {pnl_color};\">\n",
        "                ${entry['pnl']:.2f}\n",
        "            </td>\n",
        "            <td style=\"padding: 12px; text-align: center; font-weight: 600; color: {acc_color};\">\n",
        "                {entry['accuracy']:.1f}%\n",
        "            </td>\n",
        "            <td style=\"padding: 12px; text-align: center; color: #7f8c8d;\">\n",
        "                {entry['total_trades']}\n",
        "            </td>\n",
        "            <td style=\"padding: 12px; text-align: center; color: #7f8c8d;\">\n",
        "                {entry['sharpe']:.2f}\n",
        "            </td>\n",
        "        </tr>\n",
        "        \"\"\"\n",
        "\n",
        "    return f\"\"\"\n",
        "    <table style=\"width: 100%; border-collapse: collapse; margin: 20px 0; background: white; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
        "        <thead>\n",
        "            <tr style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;\">\n",
        "                <th style=\"padding: 15px; text-align: center;\">Rank</th>\n",
        "                <th style=\"padding: 15px; text-align: left;\">Model</th>\n",
        "                <th style=\"padding: 15px; text-align: center;\">PnL</th>\n",
        "                <th style=\"padding: 15px; text-align: center;\">Accuracy</th>\n",
        "                <th style=\"padding: 15px; text-align: center;\">Trades</th>\n",
        "                <th style=\"padding: 15px; text-align: center;\">Sharpe</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "            {rows}\n",
        "        </tbody>\n",
        "    </table>\n",
        "    \"\"\"\n",
        "\n",
        "def build_signals_html(signals_by_model, leaderboard):\n",
        "    \"\"\"Build signals HTML\"\"\"\n",
        "    html = \"\"\n",
        "\n",
        "    top_models = [entry['model'] for entry in leaderboard[:3]]\n",
        "\n",
        "    for model_name in top_models:\n",
        "        if model_name not in signals_by_model:\n",
        "            continue\n",
        "\n",
        "        signals = signals_by_model[model_name]\n",
        "        model_entry = next((e for e in leaderboard if e['model'] == model_name), None)\n",
        "\n",
        "        if not model_entry:\n",
        "            continue\n",
        "\n",
        "        html += f\"\"\"\n",
        "        <div style=\"margin: 30px 0; padding: 20px; background: white; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); border-left: 4px solid {model_entry['hex_color']};\">\n",
        "            <h3 style=\"margin: 0 0 15px 0; color: #2c3e50; display: flex; align-items: center; gap: 10px;\">\n",
        "                <span style=\"font-size: 24px;\">{model_entry['color']}</span>\n",
        "                {model_name}\n",
        "                <span style=\"font-size: 14px; color: #7f8c8d; font-weight: normal;\">\n",
        "                    ({model_entry['accuracy']:.1f}% accuracy)\n",
        "                </span>\n",
        "            </h3>\n",
        "            <div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px;\">\n",
        "        \"\"\"\n",
        "\n",
        "        for pair, sig in signals.items():\n",
        "            if sig['direction'] == 'HOLD':\n",
        "                continue\n",
        "\n",
        "            direction_color = \"#27ae60\" if sig['direction'] == \"BUY\" else \"#e74c3c\"\n",
        "            direction_emoji = \"üìà\" if sig['direction'] == \"BUY\" else \"üìâ\"\n",
        "\n",
        "            confidence_bar = \"‚ñä\" * int(sig['score_1_100'] / 10)\n",
        "\n",
        "            html += f\"\"\"\n",
        "            <div style=\"padding: 15px; background: #f8f9fa; border-radius: 8px; border: 2px solid {direction_color};\">\n",
        "                <div style=\"display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;\">\n",
        "                    <span style=\"font-weight: 600; font-size: 16px; color: #2c3e50;\">{pair}</span>\n",
        "                    <span style=\"font-size: 20px;\">{direction_emoji}</span>\n",
        "                </div>\n",
        "                <div style=\"background: {direction_color}; color: white; padding: 8px; border-radius: 6px; text-align: center; font-weight: 600; margin-bottom: 10px;\">\n",
        "                    {sig['direction']}\n",
        "                </div>\n",
        "                <div style=\"font-size: 13px; color: #7f8c8d; margin-bottom: 5px;\">\n",
        "                    <strong>Price:</strong> {sig['last_price']:.5f}\n",
        "                </div>\n",
        "                <div style=\"font-size: 13px; color: #27ae60; margin-bottom: 5px;\">\n",
        "                    <strong>TP:</strong> {sig['TP']:.5f}\n",
        "                </div>\n",
        "                <div style=\"font-size: 13px; color: #e74c3c; margin-bottom: 10px;\">\n",
        "                    <strong>SL:</strong> {sig['SL']:.5f}\n",
        "                </div>\n",
        "                <div style=\"font-size: 12px; color: #7f8c8d;\">\n",
        "                    <strong>Confidence:</strong> {sig['score_1_100']}/100\n",
        "                </div>\n",
        "                <div style=\"font-size: 10px; color: {direction_color}; margin-top: 5px;\">\n",
        "                    {confidence_bar}\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        html += \"\"\"\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    return html\n",
        "\n",
        "def send_competition_results_email(leaderboard, signals_by_model, mode=\"normal\"):\n",
        "    \"\"\"Send competition results email\"\"\"\n",
        "    if not WEEKEND_MONDAY_MANAGER.should_send_email():\n",
        "        print_status(\"üìß Competition email skipped (Replay Mode)\", \"info\")\n",
        "        return False\n",
        "\n",
        "    mode_badge = {\n",
        "        \"normal\": \"üíº NORMAL\",\n",
        "        \"weekend_replay\": \"üé¨ WEEKEND REPLAY\",\n",
        "        \"monday_replay\": \"üî¥ MONDAY REPLAY\"\n",
        "    }.get(mode, \"üíº NORMAL\")\n",
        "\n",
        "    leaderboard_html = build_leaderboard_html(leaderboard)\n",
        "    signals_html = build_signals_html(signals_by_model, leaderboard)\n",
        "\n",
        "    winner = leaderboard[0] if leaderboard else None\n",
        "    winner_badge = f\"{winner['color']} {winner['model']}\" if winner else \"N/A\"\n",
        "\n",
        "    html_body = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    </head>\n",
        "    <body style=\"margin: 0; padding: 0; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px;\">\n",
        "        <div style=\"max-width: 800px; margin: 0 auto; background: #ffffff; border-radius: 16px; overflow: hidden; box-shadow: 0 10px 40px rgba(0,0,0,0.3);\">\n",
        "\n",
        "            <!-- Header -->\n",
        "            <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 40px 30px; text-align: center; color: white;\">\n",
        "                <img src=\"{LOGO_URL}\" alt=\"Logo\" style=\"width: 80px; height: 80px; border-radius: 50%; border: 4px solid white; margin-bottom: 15px;\">\n",
        "                <h1 style=\"margin: 0; font-size: 32px; font-weight: 700;\">üèÜ Trade Beacon AI</h1>\n",
        "                <p style=\"margin: 5px 0 0 0; font-size: 18px; opacity: 0.95;\">Competition Results</p>\n",
        "                <p style=\"margin: 10px 0 0 0; font-size: 16px; opacity: 0.9;\">{mode_badge} MODE</p>\n",
        "                <p style=\"margin: 5px 0 0 0; font-size: 14px; opacity: 0.8;\">{datetime.now().strftime('%B %d, %Y ‚Ä¢ %I:%M %p')}</p>\n",
        "            </div>\n",
        "\n",
        "            <!-- Winner Announcement -->\n",
        "            <div style=\"padding: 30px; background: linear-gradient(135deg, #f6d365 0%, #fda085 100%); text-align: center;\">\n",
        "                <div style=\"font-size: 48px; margin-bottom: 10px;\">üëë</div>\n",
        "                <h2 style=\"margin: 0; color: #2c3e50; font-size: 24px;\">Competition Winner</h2>\n",
        "                <div style=\"font-size: 28px; font-weight: 700; color: #2c3e50; margin-top: 10px;\">\n",
        "                    {winner_badge}\n",
        "                </div>\n",
        "                {f'<div style=\"color: #27ae60; font-size: 20px; font-weight: 600; margin-top: 10px;\">${winner[\"pnl\"]:.2f} PnL</div>' if winner else ''}\n",
        "            </div>\n",
        "\n",
        "            <!-- Leaderboard -->\n",
        "            <div style=\"padding: 30px;\">\n",
        "                <h2 style=\"color: #2c3e50; font-size: 24px; margin: 0 0 20px 0; display: flex; align-items: center; gap: 10px;\">\n",
        "                    üìä Performance Leaderboard\n",
        "                </h2>\n",
        "                {leaderboard_html}\n",
        "            </div>\n",
        "\n",
        "            <!-- Trading Signals -->\n",
        "            <div style=\"padding: 30px; background: #f8f9fa;\">\n",
        "                <h2 style=\"color: #2c3e50; font-size: 24px; margin: 0 0 20px 0; display: flex; align-items: center; gap: 10px;\">\n",
        "                    üì° Active Trading Signals\n",
        "                </h2>\n",
        "                {signals_html}\n",
        "            </div>\n",
        "\n",
        "            <!-- Disclaimer -->\n",
        "            <div style=\"padding: 25px; background: #fff3cd; border-top: 3px solid #ffc107;\">\n",
        "                <h3 style=\"margin: 0 0 10px 0; color: #856404; font-size: 16px; display: flex; align-items: center; gap: 8px;\">\n",
        "                    ‚ö†Ô∏è IMPORTANT DISCLAIMER\n",
        "                </h3>\n",
        "                <p style=\"margin: 0; font-size: 13px; color: #856404; line-height: 1.6;\">\n",
        "                    <strong>Trading Risk Warning:</strong> Forex trading involves substantial risk of loss and is not suitable for all investors.\n",
        "                    Past performance is not indicative of future results. These signals are generated by AI models for educational and informational\n",
        "                    purposes only and should not be considered as financial advice. Always conduct your own research and consult with a qualified\n",
        "                    financial advisor before making any investment decisions. Trade at your own risk.\n",
        "                </p>\n",
        "            </div>\n",
        "\n",
        "            <!-- Footer -->\n",
        "            <div style=\"padding: 30px; background: #2c3e50; color: white; text-align: center;\">\n",
        "                <p style=\"margin: 0; font-size: 14px; opacity: 0.9;\">\n",
        "                    ü§ñ Trade Beacon - Powered by Multi-Model AI Competition System\n",
        "                </p>\n",
        "                <p style=\"margin: 10px 0 0 0; font-size: 12px; opacity: 0.7;\">\n",
        "                    Next update in 1 hour ‚Ä¢ Running on v7.5 Enhanced\n",
        "                </p>\n",
        "            </div>\n",
        "\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    subject = f\"üèÜ Trade Beacon AI Competition #{competition.iteration} - Winner: {winner_badge} | {mode_badge}\"\n",
        "\n",
        "    return send_email_alert(subject, html_body)\n",
        "\n",
        "# ======================================================\n",
        "# GIT OPERATIONS\n",
        "# ======================================================\n",
        "def git_push_changes(message=\"Auto update\"):\n",
        "    \"\"\"Push changes to GitHub repository\"\"\"\n",
        "    try:\n",
        "        if not REPO_FOLDER.exists():\n",
        "            print_status(\"Repository folder not found, cloning...\", \"warn\")\n",
        "            subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_FOLDER)], check=True)\n",
        "\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"diff\", \"--cached\", \"--quiet\"],\n",
        "            capture_output=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print_status(\"No changes to commit\", \"info\")\n",
        "            return True\n",
        "\n",
        "        subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n",
        "        subprocess.run([\"git\", \"push\", \"origin\", BRANCH], check=True)\n",
        "\n",
        "        print_status(f\"‚úÖ Pushed changes: {message}\", \"success\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ùå Git push failed: {e}\", \"error\")\n",
        "        return False\n",
        "    finally:\n",
        "        os.chdir(ROOT_PATH)\n",
        "\n",
        "# ======================================================\n",
        "# LOAD/SAVE PREVIOUS SIGNALS\n",
        "# ======================================================\n",
        "def load_previous_signals():\n",
        "    \"\"\"Load signals from previous iteration\"\"\"\n",
        "    if PREVIOUS_SIGNALS_FILE.exists():\n",
        "        try:\n",
        "            with open(PREVIOUS_SIGNALS_FILE, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        except:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "def save_previous_signals(signals_by_model):\n",
        "    \"\"\"Save signals for next iteration\"\"\"\n",
        "    try:\n",
        "        with open(PREVIOUS_SIGNALS_FILE, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'signals': signals_by_model,\n",
        "                'timestamp': datetime.now()\n",
        "            }, f, protocol=4)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save previous signals: {e}\")\n",
        "\n",
        "# ======================================================\n",
        "# STARTUP CLEANUP FUNCTION\n",
        "# ======================================================\n",
        "def cleanup_corrupted_pickles():\n",
        "    \"\"\"Remove all corrupted pickle files at startup\"\"\"\n",
        "    print_status(\"üßπ Checking for corrupted pickle files...\", \"info\")\n",
        "\n",
        "    pickle_patterns = [\n",
        "        \"*_population.pkl\",\n",
        "        \"*_trade_memory.pkl\",\n",
        "        \"*_best_chrom.pkl\",\n",
        "        \"*_gen_count.pkl\",\n",
        "        \"*_ga_progress.pkl\",\n",
        "        \"previous_signals.pkl\",\n",
        "        \"learning_progress.pkl\"\n",
        "    ]\n",
        "\n",
        "    corrupted_count = 0\n",
        "    for pattern in pickle_patterns:\n",
        "        for pkl_file in REPO_FOLDER.glob(pattern):\n",
        "            try:\n",
        "                with open(pkl_file, 'rb') as f:\n",
        "                    pickle.load(f)\n",
        "            except Exception as e:\n",
        "                try:\n",
        "                    pkl_file.unlink()\n",
        "                    print_status(f\"üóëÔ∏è Removed corrupted: {pkl_file.name}\", \"warn\")\n",
        "                    corrupted_count += 1\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    if corrupted_count > 0:\n",
        "        print_status(f\"‚úÖ Cleaned up {corrupted_count} corrupted pickle files\", \"success\")\n",
        "    else:\n",
        "        print_status(\"‚úÖ No corrupted files found\", \"success\")\n",
        "\n",
        "# ======================================================\n",
        "# MAIN EXECUTION (SINGLE RUN MODE)\n",
        "# ======================================================\n",
        "def main():\n",
        "    print_status(\"=\" * 60, \"info\")\n",
        "    print_status(\"üöÄ FOREX PIPELINE v7.5 - SINGLE RUN MODE\", \"success\")\n",
        "    print_status(\"=\" * 60, \"info\")\n",
        "\n",
        "    # Increment iteration counter\n",
        "    current_iteration = ITERATION_COUNTER.increment()\n",
        "    stats = ITERATION_COUNTER.get_stats()\n",
        "\n",
        "    if stats:\n",
        "        print_status(f\"üìä LIFETIME STATS:\", \"success\")\n",
        "        print_status(f\"   Total Iterations: {stats['total_iterations']}\", \"info\")\n",
        "        print_status(f\"   Days Running: {stats['days_running']}\", \"info\")\n",
        "        print_status(f\"   Started: {stats['start_date'][:10]}\", \"info\")\n",
        "        print_status(f\"   Avg Runs/Day: {stats['avg_iterations_per_day']:.1f}\", \"info\")\n",
        "\n",
        "    print_status(f\"üî¢ Current Iteration: #{current_iteration}\", \"info\")\n",
        "    print_status(f\"üìä Pairs: {', '.join(PAIRS)}\", \"info\")\n",
        "    print_status(f\"ü§ñ Models: {len(COMPETITION_MODELS)}\", \"info\")\n",
        "\n",
        "    # Cleanup corrupted pickles at startup\n",
        "    cleanup_corrupted_pickles()\n",
        "\n",
        "    # Load data once\n",
        "    print_status(\"\\nüì¶ Loading historical data...\", \"info\")\n",
        "    combined_data = load_unified_pickles(PICKLE_FOLDER)\n",
        "\n",
        "    if not combined_data:\n",
        "        print_status(\"‚ùå No data loaded! Check pickle files.\", \"error\")\n",
        "        return\n",
        "\n",
        "    print_status(f\"‚úÖ Loaded data for {len(combined_data)} pairs\", \"success\")\n",
        "\n",
        "    try:\n",
        "        # Determine current mode\n",
        "        current_mode = WEEKEND_MONDAY_MANAGER.get_mode()\n",
        "        status_message = WEEKEND_MONDAY_MANAGER.get_status_message()\n",
        "        print_status(status_message, \"info\")\n",
        "\n",
        "        # EVALUATE PREVIOUS SIGNALS (if any)\n",
        "        previous_signals_data = load_previous_signals()\n",
        "        trade_outcomes = None\n",
        "\n",
        "        if previous_signals_data and 'signals' in previous_signals_data:\n",
        "            print_status(\"\\nüîç Evaluating previous iteration signals...\", \"info\")\n",
        "\n",
        "            current_prices = {}\n",
        "            for pair in PAIRS:\n",
        "                if current_mode in [\"weekend_replay\", \"monday_replay\"]:\n",
        "                    if pair in combined_data and combined_data[pair]:\n",
        "                        current_prices[pair] = float(list(combined_data[pair].values())[0]['close'].iloc[-1])\n",
        "                else:\n",
        "                    live_price = fetch_live_rate(pair)\n",
        "                    if live_price > 0:\n",
        "                        current_prices[pair] = live_price\n",
        "                        print_status(f\"üì° {pair} live price: {live_price:.5f}\", \"debug\")\n",
        "                    else:\n",
        "                        if pair in combined_data and combined_data[pair]:\n",
        "                            current_prices[pair] = float(list(combined_data[pair].values())[0]['close'].iloc[-1])\n",
        "                            print_status(f\"‚ö†Ô∏è {pair} using data price (API failed)\", \"warn\")\n",
        "\n",
        "            trade_outcomes = TRADE_TRACKER.evaluate_outcomes(\n",
        "                current_prices,\n",
        "                datetime.now()\n",
        "            )\n",
        "\n",
        "            if trade_outcomes:\n",
        "                print_status(\"\\nüìà TRADE OUTCOMES FROM PREVIOUS ITERATION:\", \"success\")\n",
        "                for model_name, outcomes in trade_outcomes.items():\n",
        "                    print_status(\n",
        "                        f\"{model_name}: {outcomes['wins']}/{outcomes['closed_trades']} wins \"\n",
        "                        f\"({outcomes['accuracy']:.1f}% accuracy) | \"\n",
        "                        f\"P&L: ${outcomes['total_pnl']:.2f}\",\n",
        "                        \"success\" if outcomes['total_pnl'] > 0 else \"warn\"\n",
        "                    )\n",
        "            else:\n",
        "                print_status(\"No trades closed in this iteration\", \"info\")\n",
        "\n",
        "        # Initialize replay system if needed\n",
        "        replay_system = None\n",
        "        if current_mode in [\"weekend_replay\", \"monday_replay\"]:\n",
        "            replay_system = HistoricalReplaySystem(combined_data, random_selection=True)\n",
        "\n",
        "            if current_mode == \"monday_replay\":\n",
        "                WEEKEND_MONDAY_MANAGER.increment_monday_runs()\n",
        "                print_status(\"üî¥ Monday replay mode - single run\", \"success\")\n",
        "\n",
        "            working_data = {\n",
        "                pair: replay_system.get_available_data(pair)\n",
        "                for pair in PAIRS\n",
        "            }\n",
        "            working_data = {k: v for k, v in working_data.items() if v}\n",
        "        else:\n",
        "            working_data = combined_data\n",
        "\n",
        "        # Run competition\n",
        "        print_status(\"\\nüèÜ Starting AI Competition...\", \"info\")\n",
        "        competition_results = competition.run_competition(working_data, mode=current_mode)\n",
        "\n",
        "        # Generate signals for all models\n",
        "        signals_by_model = {}\n",
        "        for model_name, result in competition_results.items():\n",
        "            if not result or 'chromosome' not in result:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                signals = generate_live_signals(\n",
        "                    result['chromosome'],\n",
        "                    working_data,\n",
        "                    model_name,\n",
        "                    replay_system=replay_system if current_mode in [\"weekend_replay\", \"monday_replay\"] else None\n",
        "                )\n",
        "                signals_by_model[model_name] = signals\n",
        "                print_status(f\"‚úÖ {model_name}: Generated signals\", \"success\")\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ùå {model_name}: Signal generation failed: {e}\", \"error\")\n",
        "\n",
        "        # Store signals for next iteration evaluation\n",
        "        if signals_by_model:\n",
        "            TRADE_TRACKER.store_signals(signals_by_model, datetime.now())\n",
        "            save_previous_signals(signals_by_model)\n",
        "\n",
        "        # Record iteration with REAL trade outcomes\n",
        "        if trade_outcomes:\n",
        "            LEARNING_TRACKER.record_iteration(competition_results, trade_outcomes)\n",
        "        else:\n",
        "            LEARNING_TRACKER.record_iteration(competition_results)\n",
        "\n",
        "        # Generate leaderboard\n",
        "        leaderboard = competition.generate_leaderboard(signals_by_model, mode=current_mode)\n",
        "\n",
        "        if not leaderboard:\n",
        "            print_status(\"‚ö†Ô∏è No leaderboard data generated\", \"warn\")\n",
        "        else:\n",
        "            print_status(f\"\\nüèÜ TOP 3 MODELS:\", \"success\")\n",
        "            for idx, entry in enumerate(leaderboard[:3], 1):\n",
        "                medal = [\"ü•á\", \"ü•à\", \"ü•â\"][idx - 1]\n",
        "                print_status(\n",
        "                    f\"{medal} {entry['model']}: ${entry['pnl']:.2f} PnL | \"\n",
        "                    f\"{entry['accuracy']:.1f}% Acc | {entry['total_trades']} Trades\",\n",
        "                    \"info\"\n",
        "                )\n",
        "\n",
        "            # Display learning progress\n",
        "            learning_report = LEARNING_TRACKER.get_learning_report()\n",
        "            print_status(f\"\\nüß† LEARNING PROGRESS:\", \"info\")\n",
        "            print_status(f\"   Total Iterations: {learning_report['total_iterations']}\", \"info\")\n",
        "            print_status(f\"   Adaptation Score: {learning_report['adaptation_score']:.1f}/100\", \"info\")\n",
        "            print_status(f\"   Success Rate: {learning_report['success_rate']:.1f}%\", \"info\")\n",
        "            print_status(f\"   Trend: {learning_report['learning_trend']}\", \"info\")\n",
        "\n",
        "        # Save signals to JSON files\n",
        "        try:\n",
        "            if leaderboard and signals_by_model:\n",
        "                top_model = leaderboard[0]['model']\n",
        "                if top_model in signals_by_model:\n",
        "                    with open(SIGNALS_JSON_PATH, 'w') as f:\n",
        "                        json.dump(signals_by_model[top_model], f, indent=2, default=str)\n",
        "                    print_status(\"‚úÖ Saved broker signals\", \"success\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to save signals: {e}\", \"warn\")\n",
        "\n",
        "        # Send email report (only in normal mode)\n",
        "        if leaderboard and signals_by_model:\n",
        "            send_competition_results_email(leaderboard, signals_by_model, mode=current_mode)\n",
        "\n",
        "        # Push to GitHub\n",
        "        commit_msg = f\"Auto update - Iteration #{current_iteration} - {current_mode.upper()} mode - {datetime.now().strftime('%Y-%m-%d %H:%M')}\"\n",
        "        git_push_changes(commit_msg)\n",
        "\n",
        "        print_status(\"\\n‚úÖ Pipeline completed successfully!\", \"success\")\n",
        "        print_status(f\"üéØ Iteration #{current_iteration} finished\", \"info\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print_status(\"\\n‚ö†Ô∏è Shutdown requested by user\", \"warn\")\n",
        "    except Exception as e:\n",
        "        print_status(f\"\\n‚ùå Fatal error: {e}\", \"error\")\n",
        "        logging.exception(\"Fatal error in main loop\")\n",
        "        sys.exit(1)\n",
        "    finally:\n",
        "        print_status(\"\\nüõë Cleaning up...\", \"info\")\n",
        "        MEMORY_SYSTEM.close()\n",
        "        print_status(\"‚úÖ Cleanup complete\", \"success\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Uc0j5nyaH8mm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}