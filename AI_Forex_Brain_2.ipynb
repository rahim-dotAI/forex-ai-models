{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr_DWDx4-LLJ"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üîë API Keys Configuration\n",
        "# ======================================================\n",
        "import os\n",
        "\n",
        "# Set API keys from environment variables or defaults\n",
        "ALPHA_VANTAGE_KEY = os.environ.get('ALPHA_VANTAGE_KEY', '1W58NPZXOG5SLHZ6')\n",
        "BROWSERLESS_TOKEN = os.environ.get('BROWSERLESS_TOKEN', '2TMVUBAjFwrr7Tb283f0da6602a4cb698b81778bda61967f7')\n",
        "\n",
        "# Set environment variables for downstream code\n",
        "os.environ['ALPHA_VANTAGE_KEY'] = ALPHA_VANTAGE_KEY\n",
        "os.environ['BROWSERLESS_TOKEN'] = BROWSERLESS_TOKEN\n",
        "\n",
        "# Validate\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    print(\"‚ö†Ô∏è Warning: ALPHA_VANTAGE_KEY not set!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Alpha Vantage Key: {ALPHA_VANTAGE_KEY[:4]}...{ALPHA_VANTAGE_KEY[-4:]}\")\n",
        "\n",
        "if not BROWSERLESS_TOKEN:\n",
        "    print(\"‚ö†Ô∏è Warning: BROWSERLESS_TOKEN not set!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Browserless Token: {BROWSERLESS_TOKEN[:4]}...{BROWSERLESS_TOKEN[-4:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H52F2WkfvWOc"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üåç Environment Detection & Setup (MUST RUN FIRST!)\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "# Override ENV_NAME if in GitHub Actions\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "# Set base paths based on environment\n",
        "if IN_COLAB:\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "elif IN_GHA:\n",
        "    # GitHub Actions already checks out the repo\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    # Local development\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "\n",
        "# Create necessary directories with organized structure\n",
        "DIRECTORIES = {\n",
        "    \"data_raw\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "}\n",
        "\n",
        "# Create all directories\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Display environment info\n",
        "print(\"=\" * 60)\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üîß Python: {sys.version.split()[0]}\")\n",
        "print(f\"üìç Working Dir: {os.getcwd()}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Validate critical environment variables for GitHub Actions\n",
        "if IN_GHA:\n",
        "    required_vars = [\"FOREX_PAT\", \"GIT_USER_NAME\", \"GIT_USER_EMAIL\"]\n",
        "    missing = [v for v in required_vars if not os.environ.get(v)]\n",
        "    if missing:\n",
        "        print(f\"‚ö†Ô∏è  Warning: Missing environment variables: {', '.join(missing)}\")\n",
        "        sys.exit(1)  # Fail fast in CI if critical vars missing\n",
        "    else:\n",
        "        print(\"‚úÖ All required environment variables present\")\n",
        "\n",
        "# Export commonly used paths as globals\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw\"]\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "DB_PATH = DIRECTORIES[\"database\"] / \"memory_v85.db\"\n",
        "LOG_PATH = DIRECTORIES[\"logs\"] / \"pipeline.log\"\n",
        "OUTPUT_PATH = DIRECTORIES[\"outputs\"] / \"signals.json\"\n",
        "\n",
        "print(f\"\\nüìÅ Key Paths:\")\n",
        "print(f\"   CSV: {CSV_FOLDER}\")\n",
        "print(f\"   Pickles: {PICKLE_FOLDER}\")\n",
        "print(f\"   Database: {DB_PATH}\")\n",
        "print(f\"   Logs: {LOG_PATH}\")\n",
        "print(f\"   Signals: {OUTPUT_PATH}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMHCk7ldwo3p"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üìÑ GitHub Sync (Environment-Aware) - ALIGNED VERSION\n",
        "# ======================================================\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import urllib.parse\n",
        "import sys\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Environment Detection (MUST MATCH YOUR FIRST CELL!)\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "# Override ENV_NAME if in GitHub Actions\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CRITICAL FIX: Use SAME paths as environment detection\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    # ‚úÖ MATCHES YOUR ENVIRONMENT DETECTION\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"  # Same as env detection!\n",
        "    REPO_FOLDER = SAVE_FOLDER  # Repo IS the save folder\n",
        "    print(\"‚òÅÔ∏è Colab Mode: Cloning directly to /content/forex-ai-models\")\n",
        "\n",
        "elif IN_GHA:\n",
        "    # ‚úÖ GitHub Actions: Use current directory (already in repo)\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER  # We're already in the repo!\n",
        "    print(\"ü§ñ GitHub Actions Mode: Using current directory\")\n",
        "\n",
        "else:\n",
        "    # ‚úÖ Local: Use current directory\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "    print(\"üíª Local Mode: Using current directory\")\n",
        "\n",
        "# Create necessary directories WITH your organized structure\n",
        "DIRECTORIES = {\n",
        "    \"data_raw\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"üîß Running in: {ENV_NAME}\")\n",
        "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
        "print(f\"üíæ Save folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo folder: {REPO_FOLDER}\")\n",
        "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GitHub Configuration\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GitHub Token (Multi-Source)\n",
        "# ======================================================\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "# Try Colab secrets if in Colab and PAT not found\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except ImportError:\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load Colab secret: {e}\")\n",
        "\n",
        "# Validate PAT\n",
        "if not FOREX_PAT:\n",
        "    print(\"‚ö†Ô∏è Warning: FOREX_PAT not found. Git operations may fail.\")\n",
        "    print(\"   Set FOREX_PAT in:\")\n",
        "    print(\"   - GitHub Secrets (for Actions)\")\n",
        "    print(\"   - Colab Secrets (for Colab)\")\n",
        "    print(\"   - Environment variable (for local)\")\n",
        "    REPO_URL = None\n",
        "else:\n",
        "    SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "    REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "    print(\"‚úÖ GitHub token configured\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Handle Repository Based on Environment\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    # ===== GitHub Actions =====\n",
        "    print(\"\\nü§ñ GitHub Actions Mode\")\n",
        "    print(\"‚úÖ Repository already checked out by actions/checkout\")\n",
        "    print(f\"üìÇ Current directory: {Path.cwd()}\")\n",
        "\n",
        "    # Verify .git exists\n",
        "    if not (Path.cwd() / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Warning: .git directory not found!\")\n",
        "        print(\"   Make sure actions/checkout@v4 is in your workflow\")\n",
        "    else:\n",
        "        print(\"‚úÖ Git repository confirmed\")\n",
        "\n",
        "elif IN_COLAB:\n",
        "    # ===== Google Colab =====\n",
        "    print(\"\\n‚òÅÔ∏è Google Colab Mode\")\n",
        "\n",
        "    if not REPO_URL:\n",
        "        print(\"‚ùå Cannot clone repository: FOREX_PAT not available\")\n",
        "    elif not (REPO_FOLDER / \".git\").exists():\n",
        "        # Check if directory exists but isn't a git repo\n",
        "        if REPO_FOLDER.exists():\n",
        "            print(f\"‚ö†Ô∏è Directory exists but is not a git repo. Removing...\")\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "            print(\"‚úÖ Cleaned up non-git directory\")\n",
        "\n",
        "        # Clone repository\n",
        "        print(f\"üì• Cloning repository to {REPO_FOLDER}...\")\n",
        "        env = os.environ.copy()\n",
        "        env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"  # Skip LFS files\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                check=True,\n",
        "                env=env,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            print(\"‚úÖ Repository cloned successfully\")\n",
        "\n",
        "            # Change to repo directory\n",
        "            os.chdir(REPO_FOLDER)\n",
        "            print(f\"üìÇ Changed directory to: {os.getcwd()}\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Clone failed: {e.stderr}\")\n",
        "            print(\"Creating directory structure manually...\")\n",
        "            REPO_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ùå Clone timed out after 60 seconds\")\n",
        "            REPO_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "    else:\n",
        "        # Repository exists, pull latest\n",
        "        print(\"‚úÖ Repository already exists, pulling latest changes...\")\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"pull\", \"origin\", BRANCH],\n",
        "                check=True,\n",
        "                cwd=REPO_FOLDER,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            print(\"‚úÖ Successfully pulled latest changes\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Pull failed: {e.stderr}\")\n",
        "            print(\"Continuing with existing files...\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ö†Ô∏è Pull timed out, continuing anyway...\")\n",
        "\n",
        "    # Configure Git LFS (disable for Colab)\n",
        "    print(\"‚öôÔ∏è Configuring Git LFS...\")\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\"git\", \"lfs\", \"uninstall\"],\n",
        "            check=False,\n",
        "            cwd=REPO_FOLDER,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"‚úÖ LFS disabled for Colab\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è LFS setup warning: {e}\")\n",
        "\n",
        "else:\n",
        "    # ===== Local Environment =====\n",
        "    print(\"\\nüíª Local Development Mode\")\n",
        "    print(f\"üìÇ Working in: {SAVE_FOLDER}\")\n",
        "\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Not a git repository\")\n",
        "        print(\"   Run: git clone https://github.com/rahim-dotAI/forex-ai-models.git\")\n",
        "    else:\n",
        "        print(\"‚úÖ Git repository found\")\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Create Organized Directory Structure\n",
        "# ======================================================\n",
        "print(\"\\nüìÅ Creating organized directory structure...\")\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"   ‚úÖ {dir_name}: {dir_path}\")\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Git Global Configuration\n",
        "# ======================================================\n",
        "print(\"\\nüîß Configuring Git...\")\n",
        "\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "# Set git config\n",
        "git_configs = [\n",
        "    ([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], \"User name\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], \"User email\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], \"Detached HEAD warning\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"init.defaultBranch\", \"main\"], \"Default branch\")\n",
        "]\n",
        "\n",
        "for cmd, description in git_configs:\n",
        "    try:\n",
        "        subprocess.run(cmd, check=False, capture_output=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not set {description}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Export Path Constants (MATCH YOUR ENVIRONMENT DETECTION!)\n",
        "# ======================================================\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw\"]\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "DB_PATH = DIRECTORIES[\"database\"] / \"memory_v85.db\"\n",
        "LOG_PATH = DIRECTORIES[\"logs\"] / \"pipeline.log\"\n",
        "OUTPUT_PATH = DIRECTORIES[\"outputs\"] / \"signals.json\"\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Environment Summary & Validation\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üßæ ENVIRONMENT SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment:      {ENV_NAME}\")\n",
        "print(f\"Working Dir:      {os.getcwd()}\")\n",
        "print(f\"Save Folder:      {SAVE_FOLDER}\")\n",
        "print(f\"Repo Folder:      {REPO_FOLDER}\")\n",
        "print(f\"Repository:       https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}\")\n",
        "print(f\"Branch:           {BRANCH}\")\n",
        "print(f\"Git Repo Exists:  {(REPO_FOLDER / '.git').exists()}\")\n",
        "print(f\"FOREX_PAT Set:    {'‚úÖ Yes' if FOREX_PAT else '‚ùå No'}\")\n",
        "\n",
        "# Check critical paths\n",
        "print(\"\\nüìã Critical Paths:\")\n",
        "print(f\"   CSV Folder:    {CSV_FOLDER}\")\n",
        "print(f\"   Pickle Folder: {PICKLE_FOLDER}\")\n",
        "print(f\"   Database:      {DB_PATH}\")\n",
        "print(f\"   Logs:          {LOG_PATH}\")\n",
        "print(f\"   Signals:       {OUTPUT_PATH}\")\n",
        "\n",
        "print(\"\\nüìÇ Directory Status:\")\n",
        "critical_paths = {\n",
        "    \"Repo .git\": REPO_FOLDER / \".git\",\n",
        "    \"Data Raw\": CSV_FOLDER,\n",
        "    \"Data Processed\": PICKLE_FOLDER,\n",
        "    \"Database\": DIRECTORIES[\"database\"],\n",
        "    \"Logs\": DIRECTORIES[\"logs\"],\n",
        "    \"Outputs\": DIRECTORIES[\"outputs\"]\n",
        "}\n",
        "\n",
        "for name, path in critical_paths.items():\n",
        "    exists = path.exists()\n",
        "    icon = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"  {icon} {name}: {path}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ Setup completed successfully!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# üîü Export Variables for Downstream Cells\n",
        "# ======================================================\n",
        "# These variables are now available in subsequent cells:\n",
        "# - ENV_NAME: Environment name\n",
        "# - IN_COLAB: Boolean for Colab detection\n",
        "# - IN_GHA: Boolean for GitHub Actions detection\n",
        "# - SAVE_FOLDER: Path to save files (same as REPO_FOLDER in Colab)\n",
        "# - REPO_FOLDER: Path to git repository\n",
        "# - CSV_FOLDER, PICKLE_FOLDER, DB_PATH, LOG_PATH, OUTPUT_PATH: Organized paths\n",
        "# - GITHUB_USERNAME, GITHUB_REPO, BRANCH: Git config\n",
        "# - FOREX_PAT: GitHub token (if available)\n",
        "\n",
        "print(\"\\n‚úÖ All environment variables exported for downstream cells\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oih6CDfjAjG9"
      },
      "outputs": [],
      "source": [
        "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn river\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVMes9cDyXky"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ALPHA VANTAGE FX DATA FETCHER - OPTIMIZED FOR DAILY USE\n",
        "=======================================================\n",
        "‚úÖ Designed to run ONCE per day (not every 2 hours)\n",
        "‚úÖ Reduces API usage from 48/day to 4/day\n",
        "‚úÖ Environment variable SKIP_ALPHA_VANTAGE support\n",
        "‚úÖ Data quality validation before saving\n",
        "‚úÖ Works in GitHub Actions, Google Colab, and Local\n",
        "‚úÖ Thread-safe operations with retry logic\n",
        "‚úÖ Clear naming: pair_daily_av.csv (av = Alpha Vantage)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import hashlib\n",
        "import requests\n",
        "import subprocess\n",
        "import threading\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# üÜï SKIP CHECK - Exit early if not needed\n",
        "# ======================================================\n",
        "SKIP_ALPHA_VANTAGE = os.environ.get(\"SKIP_ALPHA_VANTAGE\", \"false\").lower() == \"true\"\n",
        "\n",
        "if SKIP_ALPHA_VANTAGE:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"‚è≠Ô∏è  ALPHA VANTAGE SKIPPED (runs separately at midnight)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"‚ÑπÔ∏è  Alpha Vantage daily data doesn't change hourly\")\n",
        "    print(\"‚ÑπÔ∏è  Using existing data from last midnight run\")\n",
        "    print(\"=\" * 70)\n",
        "    sys.exit(0)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION\n",
        "# ======================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ Alpha Vantage FX Data Fetcher - Daily Optimized v2.0\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üìç Environment: {ENV_NAME}\")\n",
        "print(f\"‚è∞ Current Time: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
        "print(f\"üîÑ Fetch Mode: Daily (saves API calls)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ PATH CONFIGURATION\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "# Directory structure\n",
        "DIRECTORIES = {\n",
        "    \"data_raw_alpha\": SAVE_FOLDER / \"data\" / \"raw\" / \"alpha_vantage\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "    \"quarantine\": SAVE_FOLDER / \"data\" / \"quarantine\" / \"alpha_vantage\",\n",
        "}\n",
        "\n",
        "for dir_path in DIRECTORIES.values():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw_alpha\"]\n",
        "QUARANTINE_FOLDER = DIRECTORIES[\"quarantine\"]\n",
        "LOG_FOLDER = DIRECTORIES[\"logs\"]\n",
        "\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üìä Alpha Vantage CSV: {CSV_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ DATA QUALITY VALIDATOR\n",
        "# ======================================================\n",
        "class DataQualityValidator:\n",
        "    \"\"\"Validate data quality before saving\"\"\"\n",
        "\n",
        "    MIN_ROWS = 50\n",
        "    MIN_PRICE_CV = 0.01  # 0.01% minimum variation\n",
        "    MIN_UNIQUE_RATIO = 0.01  # 1% unique prices\n",
        "    MIN_TRUE_RANGE = 1e-10\n",
        "    MIN_QUALITY_SCORE = 40.0\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df, pair):\n",
        "        \"\"\"\n",
        "        Validate DataFrame quality\n",
        "        Returns: (is_valid, quality_score, metrics, issues)\n",
        "        \"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return False, 0.0, {}, [\"Empty DataFrame\"]\n",
        "\n",
        "        issues = []\n",
        "        metrics = {}\n",
        "\n",
        "        metrics['row_count'] = len(df)\n",
        "        if len(df) < DataQualityValidator.MIN_ROWS:\n",
        "            issues.append(f\"Too few rows: {len(df)}\")\n",
        "\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            issues.append(f\"Missing columns: {missing_cols}\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        ohlc_data = df[required_cols].dropna()\n",
        "        if len(ohlc_data) == 0:\n",
        "            issues.append(\"No valid OHLC data\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        metrics['valid_rows'] = len(ohlc_data)\n",
        "        metrics['valid_ratio'] = len(ohlc_data) / len(df)\n",
        "\n",
        "        close_prices = ohlc_data['close']\n",
        "        metrics['price_mean'] = float(close_prices.mean())\n",
        "        metrics['price_std'] = float(close_prices.std())\n",
        "        metrics['price_cv'] = (metrics['price_std'] / metrics['price_mean']) * 100 if metrics['price_mean'] > 0 else 0.0\n",
        "\n",
        "        metrics['unique_prices'] = close_prices.nunique()\n",
        "        metrics['unique_ratio'] = metrics['unique_prices'] / len(close_prices)\n",
        "\n",
        "        high = ohlc_data['high'].values\n",
        "        low = ohlc_data['low'].values\n",
        "        close = ohlc_data['close'].values\n",
        "\n",
        "        tr = np.maximum.reduce([\n",
        "            high - low,\n",
        "            np.abs(high - np.roll(close, 1)),\n",
        "            np.abs(low - np.roll(close, 1))\n",
        "        ])\n",
        "        tr[0] = high[0] - low[0]\n",
        "\n",
        "        metrics['true_range_median'] = float(np.median(tr))\n",
        "        metrics['true_range_mean'] = float(np.mean(tr))\n",
        "\n",
        "        # Quality score (0-100)\n",
        "        quality_score = 0.0\n",
        "        quality_score += metrics['valid_ratio'] * 30\n",
        "\n",
        "        if metrics['price_cv'] >= 1.0:\n",
        "            quality_score += 30\n",
        "        elif metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV:\n",
        "            quality_score += (metrics['price_cv'] / 1.0) * 30\n",
        "\n",
        "        quality_score += min(metrics['unique_ratio'] * 20, 20)\n",
        "\n",
        "        if metrics['true_range_median'] >= 1e-5:\n",
        "            quality_score += 20\n",
        "        elif metrics['true_range_median'] >= DataQualityValidator.MIN_TRUE_RANGE:\n",
        "            quality_score += (metrics['true_range_median'] / 1e-5) * 20\n",
        "\n",
        "        metrics['quality_score'] = quality_score\n",
        "        is_valid = (quality_score >= DataQualityValidator.MIN_QUALITY_SCORE)\n",
        "\n",
        "        return is_valid, quality_score, metrics, issues\n",
        "\n",
        "validator = DataQualityValidator()\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GITHUB CONFIGURATION\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "if FOREX_PAT:\n",
        "    print(\"‚úÖ GitHub credentials configured\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Warning: FOREX_PAT not found\")\n",
        "\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "               capture_output=True, check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "               capture_output=True, check=False)\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ ALPHA VANTAGE CONFIGURATION\n",
        "# ======================================================\n",
        "ALPHA_VANTAGE_KEY = os.environ.get(\"ALPHA_VANTAGE_KEY\")\n",
        "\n",
        "if not ALPHA_VANTAGE_KEY and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        ALPHA_VANTAGE_KEY = userdata.get(\"ALPHA_VANTAGE_KEY\")\n",
        "        if ALPHA_VANTAGE_KEY:\n",
        "            os.environ[\"ALPHA_VANTAGE_KEY\"] = ALPHA_VANTAGE_KEY\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    raise ValueError(\"‚ùå ALPHA_VANTAGE_KEY is required\")\n",
        "\n",
        "print(f\"‚úÖ Alpha Vantage API key: {ALPHA_VANTAGE_KEY[:4]}...{ALPHA_VANTAGE_KEY[-4:]}\")\n",
        "\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "print(f\"üìä Fetching {len(FX_PAIRS)} pairs: {', '.join(FX_PAIRS)}\")\n",
        "print(f\"üí° Daily API usage: {len(FX_PAIRS)} requests/day (16% of 25 limit)\")\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    \"\"\"Calculate MD5 hash of file to detect changes\"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def fetch_alpha_vantage_fx(pair, outputsize='full', max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Fetch FX data from Alpha Vantage API with retry logic\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with OHLC data or empty DataFrame on failure\n",
        "    \"\"\"\n",
        "    base_url = 'https://www.alphavantage.co/query'\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "\n",
        "    params = {\n",
        "        'function': 'FX_DAILY',\n",
        "        'from_symbol': from_currency,\n",
        "        'to_symbol': to_currency,\n",
        "        'outputsize': outputsize,\n",
        "        'datatype': 'json',\n",
        "        'apikey': ALPHA_VANTAGE_KEY\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  üîΩ Fetching {pair} (attempt {attempt + 1}/{max_retries})...\")\n",
        "\n",
        "            r = requests.get(base_url, params=params, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "\n",
        "            if 'Error Message' in data:\n",
        "                raise ValueError(f\"API Error: {data['Error Message']}\")\n",
        "\n",
        "            if 'Note' in data:\n",
        "                print(f\"  ‚ö†Ô∏è API rate limit reached for {pair}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay * 2)\n",
        "                    continue\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            if 'Time Series FX (Daily)' not in data:\n",
        "                raise ValueError(f\"Unexpected response format: {list(data.keys())}\")\n",
        "\n",
        "            ts = data['Time Series FX (Daily)']\n",
        "            df = pd.DataFrame(ts).T\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df.sort_index()\n",
        "\n",
        "            df = df.rename(columns={\n",
        "                '1. open': 'open',\n",
        "                '2. high': 'high',\n",
        "                '3. low': 'low',\n",
        "                '4. close': 'close'\n",
        "            })\n",
        "\n",
        "            df = df.astype(float)\n",
        "            df = ensure_tz_naive(df)\n",
        "\n",
        "            print(f\"  ‚úÖ Fetched {len(df)} rows for {pair}\")\n",
        "            return df\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"  ‚ö†Ô∏è Network error: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Error: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ PAIR PROCESSING WITH QUALITY VALIDATION\n",
        "# ======================================================\n",
        "def process_pair(pair):\n",
        "    \"\"\"\n",
        "    Process single FX pair: fetch, validate quality, merge, save\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (filepath if changed, status message, quality_score)\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîÑ Processing {pair}...\")\n",
        "\n",
        "    filename = pair.replace(\"/\", \"_\") + \"_daily_av.csv\"\n",
        "    file_path = CSV_FOLDER / filename\n",
        "\n",
        "    # Load existing data\n",
        "    existing_df = pd.DataFrame()\n",
        "    if file_path.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
        "            existing_df = ensure_tz_naive(existing_df)\n",
        "            print(f\"  üìä Loaded {len(existing_df)} existing rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not load existing data: {e}\")\n",
        "\n",
        "    old_hash = file_hash(file_path)\n",
        "\n",
        "    # Fetch new data\n",
        "    new_df = fetch_alpha_vantage_fx(pair)\n",
        "\n",
        "    if new_df.empty:\n",
        "        return None, f\"‚ùå {pair}: No data fetched\", 0.0\n",
        "\n",
        "    # Merge with existing data\n",
        "    if not existing_df.empty:\n",
        "        combined_df = pd.concat([existing_df, new_df])\n",
        "        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
        "    else:\n",
        "        combined_df = new_df\n",
        "\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    # Validate quality\n",
        "    is_valid, quality_score, metrics, issues = validator.validate_dataframe(\n",
        "        combined_df, pair\n",
        "    )\n",
        "\n",
        "    print(f\"  üìä Quality score: {quality_score:.1f}/100\")\n",
        "\n",
        "    if not is_valid:\n",
        "        print(f\"  ‚ö†Ô∏è Quality issues: {'; '.join(issues[:2])}\")\n",
        "        print(f\"     CV: {metrics.get('price_cv', 0):.4f}%, Unique: {metrics.get('unique_ratio', 0):.1%}\")\n",
        "\n",
        "        if quality_score < DataQualityValidator.MIN_QUALITY_SCORE:\n",
        "            print(f\"  ‚ùå Data quality too low - quarantining\")\n",
        "\n",
        "            quarantine_file = QUARANTINE_FOLDER / f\"{filename}.bad\"\n",
        "            with lock:\n",
        "                combined_df.to_csv(quarantine_file)\n",
        "\n",
        "                report_file = QUARANTINE_FOLDER / f\"{filename}.quality.txt\"\n",
        "                with open(report_file, 'w') as f:\n",
        "                    f.write(f\"Quality Report for {pair} (Alpha Vantage)\\n\")\n",
        "                    f.write(f\"{'='*50}\\n\")\n",
        "                    f.write(f\"Quality Score: {quality_score:.1f}/100\\n\")\n",
        "                    f.write(f\"Issues: {'; '.join(issues)}\\n\")\n",
        "                    f.write(f\"\\nMetrics:\\n\")\n",
        "                    for k, v in metrics.items():\n",
        "                        f.write(f\"  {k}: {v}\\n\")\n",
        "\n",
        "            return None, f\"‚ùå {pair}: Quality too low ({quality_score:.1f}/100)\", quality_score\n",
        "\n",
        "    # Save the file\n",
        "    with lock:\n",
        "        combined_df.to_csv(file_path)\n",
        "\n",
        "    new_hash = file_hash(file_path)\n",
        "    changed = (old_hash != new_hash)\n",
        "\n",
        "    status = \"‚úÖ Updated\" if changed else \"‚ÑπÔ∏è No changes\"\n",
        "    print(f\"  {status} - {len(combined_df)} rows, quality: {quality_score:.1f}/100\")\n",
        "\n",
        "    return (str(file_path) if changed else None), f\"{status} {pair} ({len(combined_df)} rows, Q:{quality_score:.0f})\", quality_score\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ EXECUTION WITH RATE LIMITING\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Fetching FX data with quality validation...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "changed_files = []\n",
        "results = []\n",
        "quality_scores = {}\n",
        "\n",
        "# Sequential processing with delays to respect rate limits\n",
        "for pair in FX_PAIRS:\n",
        "    try:\n",
        "        filepath, message, quality = process_pair(pair)\n",
        "        results.append(message)\n",
        "        if filepath:\n",
        "            changed_files.append(filepath)\n",
        "            quality_scores[filepath] = quality\n",
        "\n",
        "        # Rate limiting: Wait 15 seconds between requests\n",
        "        if pair != FX_PAIRS[-1]:  # Don't wait after last pair\n",
        "            print(f\"\\n‚è≥ Waiting 15 seconds (rate limiting)...\")\n",
        "            time.sleep(15)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {pair} processing failed: {e}\")\n",
        "        results.append(f\"‚ùå {pair}: Failed\")\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ RESULTS SUMMARY\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PROCESSING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "print(f\"\\nTotal pairs processed: {len(FX_PAIRS)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"API calls made: {len(FX_PAIRS)}\")\n",
        "\n",
        "if quality_scores:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä QUALITY REPORT\")\n",
        "    print(\"=\" * 70)\n",
        "    avg_quality = sum(quality_scores.values()) / len(quality_scores)\n",
        "    print(f\"Average quality score: {avg_quality:.1f}/100\")\n",
        "\n",
        "    print(f\"\\nFiles by quality:\")\n",
        "    for fname, score in sorted(quality_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {'‚úÖ' if score >= 60 else '‚ö†Ô∏è'} {Path(fname).name}: {score:.1f}/100\")\n",
        "\n",
        "quarantined = list(QUARANTINE_FOLDER.glob(\"*.bad\"))\n",
        "if quarantined:\n",
        "    print(f\"\\n‚ö†Ô∏è  QUARANTINED FILES: {len(quarantined)}\")\n",
        "    for qfile in quarantined:\n",
        "        print(f\"  ‚ùå {qfile.stem}\")\n",
        "\n",
        "# ======================================================\n",
        "# üîü GIT COMMIT & PUSH\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Handled by workflow\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files and FOREX_PAT:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        commit_msg = f\"üìä Alpha Vantage daily update - {len(changed_files)} files\"\n",
        "        if quality_scores:\n",
        "            commit_msg += f\" (Avg Q:{avg_quality:.0f})\"\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Changes committed\")\n",
        "\n",
        "            SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "            REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "            for attempt in range(3):\n",
        "                print(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/3)...\")\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"push\", REPO_URL, BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(\"‚úÖ Successfully pushed to GitHub\")\n",
        "                    break\n",
        "                elif attempt < 2:\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", REPO_URL, BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Git error: {e}\")\n",
        "    finally:\n",
        "        os.chdir(SAVE_FOLDER)\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è No changes to commit\")\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALPHA VANTAGE WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Quality validated: ‚úÖ\")\n",
        "if quality_scores:\n",
        "    print(f\"Average quality: {avg_quality:.1f}/100\")\n",
        "print(f\"API calls: {len(FX_PAIRS)}/25 daily limit\")\n",
        "print(f\"Status: {'‚úÖ Success' if len(results) == len(FX_PAIRS) else '‚ö†Ô∏è Partial'}\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüí° Optimization Summary:\")\n",
        "print(\"   ‚Ä¢ Runs once daily at midnight\")\n",
        "print(\"   ‚Ä¢ Uses 4 API calls/day (16% of limit)\")\n",
        "print(\"   ‚Ä¢ Saves 44 calls/day compared to hourly fetching\")\n",
        "print(\"   ‚Ä¢ Daily OHLC data doesn't change intraday\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBW31rh39aMb"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "YFINANCE FX DATA FETCHER - CLEAN STRUCTURE EDITION\n",
        "===================================================\n",
        "‚úÖ Aligned with clean repo structure (data/raw/yfinance)\n",
        "‚úÖ Relaxed quality thresholds for more data acceptance\n",
        "‚úÖ Automatic OHLC logic fixing\n",
        "‚úÖ Enhanced fallback options\n",
        "‚úÖ Smart data cleaning before validation\n",
        "‚úÖ Better symbol format handling\n",
        "‚úÖ Multi-environment support (Colab, GHA, Local)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import subprocess\n",
        "import shutil\n",
        "import threading\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ YFinance FX Data Fetcher - Clean Structure Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION (MATCHES YOUR SETUP!)\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ UNIFIED PATH CONFIGURATION (MATCHES CLEAN STRUCTURE!)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    print(\"‚òÅÔ∏è Google Colab detected - using clean structure\")\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"  # ‚úÖ MATCHES!\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    print(\"ü§ñ GitHub Actions detected - using repository root\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    print(\"üíª Local environment detected - using clean structure\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "# ‚úÖ CREATE ORGANIZED DIRECTORY STRUCTURE\n",
        "DIRECTORIES = {\n",
        "    \"data_raw_yfinance\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "    \"quarantine\": SAVE_FOLDER / \"data\" / \"quarantine\" / \"yfinance\",\n",
        "}\n",
        "\n",
        "# Create all directories\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Export key paths\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw_yfinance\"]  # ‚úÖ YFinance CSVs here\n",
        "QUARANTINE_FOLDER = DIRECTORIES[\"quarantine\"]\n",
        "LOG_FOLDER = DIRECTORIES[\"logs\"]\n",
        "\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo Folder: {REPO_FOLDER}\")\n",
        "print(f\"üìä YFinance CSV: {CSV_FOLDER}\")\n",
        "print(f\"üóëÔ∏è Quarantine: {QUARANTINE_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GIT CONFIGURATION\n",
        "# ======================================================\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "# Try Colab secrets if in Colab and PAT not found\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not access Colab secrets: {e}\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT is required!\")\n",
        "\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# Configure git\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "               capture_output=True, check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "               capture_output=True, check=False)\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ REPOSITORY MANAGEMENT (SIMPLIFIED)\n",
        "# ======================================================\n",
        "def ensure_repository():\n",
        "    \"\"\"Ensure repository is available and up-to-date\"\"\"\n",
        "    if IN_GHA:\n",
        "        print(\"\\nü§ñ GitHub Actions: Repository already available\")\n",
        "        if not (REPO_FOLDER / \".git\").exists():\n",
        "            print(\"‚ö†Ô∏è Warning: .git directory not found\")\n",
        "        else:\n",
        "            print(\"‚úÖ Git repository verified\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüì• Managing repository...\")\n",
        "\n",
        "    if REPO_FOLDER.exists() and not (REPO_FOLDER / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Directory exists but is not a git repository\")\n",
        "        return\n",
        "\n",
        "    if (REPO_FOLDER / \".git\").exists():\n",
        "        print(f\"üîÑ Pulling latest changes...\")\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Repository updated successfully\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Pull had issues, continuing anyway\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Update failed: {e} - continuing with existing repo\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Repository not found. This script expects the repo to be set up first.\")\n",
        "        print(\"   Please run the GitHub Sync script first!\")\n",
        "\n",
        "ensure_repository()\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ RATE LIMITER\n",
        "# ======================================================\n",
        "class RateLimiter:\n",
        "    \"\"\"Rate limiter for API calls\"\"\"\n",
        "    def __init__(self, requests_per_minute=10, requests_per_hour=350):\n",
        "        self.rpm = requests_per_minute\n",
        "        self.rph = requests_per_hour\n",
        "        self.request_times = []\n",
        "        self.hourly_request_times = []\n",
        "        self.lock = threading.Lock()\n",
        "        self.total_requests = 0\n",
        "\n",
        "    def wait_if_needed(self):\n",
        "        with self.lock:\n",
        "            now = time.time()\n",
        "            self.request_times = [t for t in self.request_times if now - t < 60]\n",
        "            self.hourly_request_times = [t for t in self.hourly_request_times if now - t < 3600]\n",
        "\n",
        "            if len(self.request_times) >= self.rpm:\n",
        "                wait_time = 60 - (now - self.request_times[0])\n",
        "                if wait_time > 0:\n",
        "                    time.sleep(wait_time + 1)\n",
        "                    self.request_times = []\n",
        "\n",
        "            if len(self.hourly_request_times) >= self.rph:\n",
        "                wait_time = 3600 - (now - self.hourly_request_times[0])\n",
        "                if wait_time > 0:\n",
        "                    time.sleep(wait_time + 1)\n",
        "                    self.hourly_request_times = []\n",
        "\n",
        "            self.request_times.append(now)\n",
        "            self.hourly_request_times.append(now)\n",
        "            self.total_requests += 1\n",
        "            time.sleep(1.0 + (hash(str(now)) % 20) / 10)\n",
        "\n",
        "    def get_stats(self):\n",
        "        with self.lock:\n",
        "            return {'total_requests': self.total_requests}\n",
        "\n",
        "rate_limiter = RateLimiter()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ DATA CLEANING & VALIDATION\n",
        "# ======================================================\n",
        "def fix_ohlc_logic(df):\n",
        "    \"\"\"Fix impossible OHLC relationships\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df = df.copy()\n",
        "    required_cols = ['open', 'high', 'low', 'close']\n",
        "\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        return df\n",
        "\n",
        "    # Fix High: should be maximum of OHLC\n",
        "    df['high'] = df[required_cols].max(axis=1)\n",
        "\n",
        "    # Fix Low: should be minimum of OHLC\n",
        "    df['low'] = df[required_cols].min(axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "class DataQualityValidator:\n",
        "    \"\"\"RELAXED validation for more data acceptance\"\"\"\n",
        "\n",
        "    # ‚úÖ RELAXED THRESHOLDS\n",
        "    MIN_ROWS = 5  # Down from 10\n",
        "    MIN_PRICE_CV = 0.01  # Down from 0.1 (1% instead of 10%)\n",
        "    MIN_UNIQUE_RATIO = 0.005  # Down from 0.05 (0.5% instead of 5%)\n",
        "    MIN_TRUE_RANGE = 1e-12  # More lenient\n",
        "    MIN_QUALITY_SCORE = 20.0  # Down from 40.0\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df, pair, tf_name):\n",
        "        \"\"\"Validate with relaxed criteria\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return False, 0.0, {}, [\"Empty DataFrame\"]\n",
        "\n",
        "        issues = []\n",
        "        metrics = {}\n",
        "\n",
        "        metrics['row_count'] = len(df)\n",
        "        if len(df) < DataQualityValidator.MIN_ROWS:\n",
        "            return False, 0.0, metrics, [f\"Too few rows: {len(df)}\"]\n",
        "\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        if not all(col in df.columns for col in required_cols):\n",
        "            return False, 0.0, metrics, [\"Missing OHLC columns\"]\n",
        "\n",
        "        ohlc_data = df[required_cols].dropna()\n",
        "        if len(ohlc_data) == 0:\n",
        "            return False, 0.0, metrics, [\"No valid OHLC data\"]\n",
        "\n",
        "        metrics['valid_rows'] = len(ohlc_data)\n",
        "        metrics['valid_ratio'] = len(ohlc_data) / len(df)\n",
        "\n",
        "        close_prices = ohlc_data['close']\n",
        "        metrics['price_mean'] = float(close_prices.mean())\n",
        "        metrics['price_std'] = float(close_prices.std())\n",
        "        metrics['price_cv'] = (metrics['price_std'] / metrics['price_mean']) * 100 if metrics['price_mean'] > 0 else 0.0\n",
        "\n",
        "        metrics['unique_prices'] = close_prices.nunique()\n",
        "        metrics['unique_ratio'] = metrics['unique_prices'] / len(close_prices)\n",
        "\n",
        "        # Calculate true range\n",
        "        high = ohlc_data['high'].values\n",
        "        low = ohlc_data['low'].values\n",
        "        close = ohlc_data['close'].values\n",
        "\n",
        "        tr = np.maximum.reduce([\n",
        "            high - low,\n",
        "            np.abs(high - np.roll(close, 1)),\n",
        "            np.abs(low - np.roll(close, 1))\n",
        "        ])\n",
        "        tr[0] = high[0] - low[0]\n",
        "\n",
        "        metrics['true_range_median'] = float(np.median(tr))\n",
        "\n",
        "        # Quality score calculation (more lenient)\n",
        "        quality_score = metrics['valid_ratio'] * 30\n",
        "\n",
        "        if metrics['price_cv'] >= 0.5:\n",
        "            quality_score += 40\n",
        "        elif metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV:\n",
        "            quality_score += (metrics['price_cv'] / 0.5) * 40\n",
        "\n",
        "        if metrics['unique_ratio'] >= 0.1:\n",
        "            quality_score += 30\n",
        "        elif metrics['unique_ratio'] >= DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "            quality_score += (metrics['unique_ratio'] / 0.1) * 30\n",
        "\n",
        "        metrics['quality_score'] = quality_score\n",
        "\n",
        "        # Relaxed validation - accept if meets minimum thresholds\n",
        "        is_valid = (\n",
        "            quality_score >= DataQualityValidator.MIN_QUALITY_SCORE and\n",
        "            metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV and\n",
        "            metrics['unique_ratio'] >= DataQualityValidator.MIN_UNIQUE_RATIO\n",
        "        )\n",
        "\n",
        "        if not is_valid:\n",
        "            if metrics['price_cv'] < DataQualityValidator.MIN_PRICE_CV:\n",
        "                issues.append(f\"Low CV: {metrics['price_cv']:.4f}%\")\n",
        "            if metrics['unique_ratio'] < DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "                issues.append(f\"Low unique: {metrics['unique_ratio']:.3%}\")\n",
        "\n",
        "        return is_valid, quality_score, metrics, issues\n",
        "\n",
        "validator = DataQualityValidator()\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ CONFIGURATION\n",
        "# ======================================================\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "# ‚úÖ ENHANCED with more fallback options\n",
        "TIMEFRAMES = {\n",
        "    \"1d_5y\": [\n",
        "        (\"1d\", \"5y\"),\n",
        "        (\"1d\", \"max\"),  # Try max available\n",
        "        (\"1d\", \"3y\"),\n",
        "        (\"1d\", \"2y\"),\n",
        "    ],\n",
        "    \"1h_2y\": [\n",
        "        (\"1h\", \"2y\"),\n",
        "        (\"1h\", \"1y\"),\n",
        "        (\"1h\", \"730d\"),  # Exactly 2 years in days\n",
        "        (\"1h\", \"6mo\")\n",
        "    ],\n",
        "    \"15m_60d\": [\n",
        "        (\"15m\", \"60d\"),\n",
        "        (\"15m\", \"2mo\"),\n",
        "        (\"15m\", \"30d\"),\n",
        "    ],\n",
        "    \"5m_1mo\": [\n",
        "        (\"5m\", \"1mo\"),\n",
        "        (\"5m\", \"30d\"),\n",
        "        (\"5m\", \"14d\"),\n",
        "    ],\n",
        "    \"1m_7d\": [\n",
        "        (\"1m\", \"7d\"),\n",
        "        (\"1m\", \"5d\"),\n",
        "        (\"1m\", \"3d\"),\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"\\nüìä Configuration:\")\n",
        "print(f\"   Pairs: {len(FX_PAIRS)}\")\n",
        "print(f\"   Timeframes: {len(TIMEFRAMES)}\")\n",
        "print(f\"   Total tasks: {len(FX_PAIRS) * len(TIMEFRAMES)}\")\n",
        "print(f\"   Quality threshold: {validator.MIN_QUALITY_SCORE}/100 (RELAXED)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def file_hash(filepath):\n",
        "    \"\"\"Calculate MD5 hash of file\"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def merge_data(existing_df, new_df):\n",
        "    \"\"\"Merge existing and new data, removing duplicates\"\"\"\n",
        "    existing_df = ensure_tz_naive(existing_df)\n",
        "    new_df = ensure_tz_naive(new_df)\n",
        "    if existing_df.empty:\n",
        "        return new_df\n",
        "    if new_df.empty:\n",
        "        return existing_df\n",
        "    combined = pd.concat([existing_df, new_df])\n",
        "    combined = combined[~combined.index.duplicated(keep=\"last\")]\n",
        "    combined.sort_index(inplace=True)\n",
        "    return combined\n",
        "\n",
        "def get_symbol_variants(pair, interval):\n",
        "    \"\"\"Get multiple symbol format variations\"\"\"\n",
        "    base_symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
        "    variants = [base_symbol]\n",
        "\n",
        "    # Additional formats\n",
        "    if interval in [\"1d\", \"1h\"]:\n",
        "        from_curr, to_curr = pair.split(\"/\")\n",
        "        variants.append(f\"{from_curr}{to_curr}=X\")  # No separator\n",
        "        variants.append(f\"{from_curr}=X\")  # Just base currency\n",
        "\n",
        "    return variants\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ WORKER FUNCTION\n",
        "# ======================================================\n",
        "def process_pair_tf(pair, tf_name, interval_period_options, max_retries=3):\n",
        "    \"\"\"\n",
        "    Download YFinance data with OHLC fixing and validation\n",
        "\n",
        "    ‚úÖ Saves to data/raw/yfinance/ with clear naming\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (message, filepath if changed, quality_score)\n",
        "    \"\"\"\n",
        "    # ‚úÖ Save to YFinance folder\n",
        "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
        "    filepath = CSV_FOLDER / filename\n",
        "\n",
        "    existing_df = pd.DataFrame()\n",
        "    if filepath.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "            existing_df = ensure_tz_naive(existing_df)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not load existing data: {e}\")\n",
        "\n",
        "    old_hash = file_hash(filepath)\n",
        "\n",
        "    for option_idx, (interval, period) in enumerate(interval_period_options):\n",
        "        symbol_variants = get_symbol_variants(pair, interval)\n",
        "\n",
        "        for symbol in symbol_variants:\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    rate_limiter.wait_if_needed()\n",
        "\n",
        "                    ticker = yf.Ticker(symbol)\n",
        "                    df = ticker.history(\n",
        "                        period=period,\n",
        "                        interval=interval,\n",
        "                        auto_adjust=False,\n",
        "                        prepost=False,\n",
        "                        actions=False,\n",
        "                        raise_errors=False\n",
        "                    )\n",
        "\n",
        "                    if df.empty:\n",
        "                        raise ValueError(\"Empty data\")\n",
        "\n",
        "                    available_cols = [c for c in ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "                                     if c in df.columns]\n",
        "                    df = df[available_cols]\n",
        "                    df.rename(columns=lambda x: x.lower(), inplace=True)\n",
        "                    df = ensure_tz_naive(df)\n",
        "\n",
        "                    combined_df = merge_data(existing_df, df)\n",
        "\n",
        "                    # ‚úÖ FIX OHLC LOGIC BEFORE VALIDATION\n",
        "                    combined_df = fix_ohlc_logic(combined_df)\n",
        "\n",
        "                    is_valid, quality_score, metrics, issues = validator.validate_dataframe(\n",
        "                        combined_df, pair, tf_name\n",
        "                    )\n",
        "\n",
        "                    if not is_valid:\n",
        "                        if attempt < max_retries - 1:\n",
        "                            time.sleep(3 * (2 ** attempt))\n",
        "                            continue\n",
        "                        elif option_idx < len(interval_period_options) - 1:\n",
        "                            break  # Try next option\n",
        "                        else:\n",
        "                            # Save anyway but mark as low quality\n",
        "                            print(f\"  ‚ö†Ô∏è Low quality ({quality_score:.1f}) but saving: {pair} {tf_name}\")\n",
        "\n",
        "                    # Save the file\n",
        "                    with lock:\n",
        "                        combined_df.to_csv(filepath)\n",
        "\n",
        "                    new_hash = file_hash(filepath)\n",
        "                    changed = (old_hash != new_hash)\n",
        "\n",
        "                    status = \"‚úÖ\" if quality_score >= 50 else \"‚ö†Ô∏è\"\n",
        "                    msg = f\"{status} {pair} {tf_name} - {len(combined_df)} rows, Q:{quality_score:.0f}\"\n",
        "                    print(f\"  {msg}\")\n",
        "                    return msg, str(filepath) if changed else None, quality_score\n",
        "\n",
        "                except Exception as e:\n",
        "                    if attempt < max_retries - 1:\n",
        "                        time.sleep(3 * (2 ** attempt))\n",
        "                    else:\n",
        "                        if option_idx < len(interval_period_options) - 1:\n",
        "                            break  # Try next option\n",
        "\n",
        "    return f\"‚ùå Failed {pair} {tf_name}\", None, 0.0\n",
        "\n",
        "# ======================================================\n",
        "# üîü PARALLEL EXECUTION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Starting YFinance data download...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "changed_files = []\n",
        "results = []\n",
        "quality_scores = {}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    tasks = []\n",
        "    for pair in FX_PAIRS:\n",
        "        for tf_name, options in TIMEFRAMES.items():\n",
        "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, options))\n",
        "\n",
        "    for future in as_completed(tasks):\n",
        "        try:\n",
        "            msg, filename, quality = future.result()\n",
        "            results.append(msg)\n",
        "            if filename:\n",
        "                changed_files.append(filename)\n",
        "                quality_scores[filename] = quality\n",
        "        except Exception as e:\n",
        "            results.append(f\"‚ùå Error: {e}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ SUMMARY\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PROCESSING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "success_count = len([r for r in results if \"‚úÖ\" in r or \"‚ö†Ô∏è\" in r])\n",
        "print(f\"\\nTotal tasks: {len(results)}\")\n",
        "print(f\"Successful: {success_count}/{len(results)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Time: {elapsed_time/60:.1f} min\")\n",
        "\n",
        "if quality_scores:\n",
        "    avg_q = sum(quality_scores.values()) / len(quality_scores)\n",
        "    print(f\"Average quality: {avg_q:.1f}/100\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä QUALITY REPORT\")\n",
        "    print(\"=\" * 70)\n",
        "    for fname, score in sorted(quality_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "        status = \"‚úÖ\" if score >= 50 else \"‚ö†Ô∏è\"\n",
        "        print(f\"  {status} {Path(fname).name}: {score:.1f}/100\")\n",
        "\n",
        "# Check quarantine\n",
        "quarantined = list(QUARANTINE_FOLDER.glob(\"*.bad\"))\n",
        "if quarantined:\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚ö†Ô∏è  QUARANTINED FILES: {len(quarantined)}\")\n",
        "    print(\"=\" * 70)\n",
        "    for qfile in quarantined:\n",
        "        print(f\"  ‚ùå {qfile.stem}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ GIT COMMIT & PUSH\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        commit_msg = f\"Update YFinance data - {len(changed_files)} files\"\n",
        "        if quality_scores:\n",
        "            commit_msg += f\" (Avg Q:{avg_q:.0f})\"\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Changes committed\")\n",
        "\n",
        "            for attempt in range(3):\n",
        "                print(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/3)...\")\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(\"‚úÖ Successfully pushed to GitHub\")\n",
        "                    break\n",
        "                elif attempt < 2:\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  No changes to commit\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Git error: {e}\")\n",
        "    finally:\n",
        "        os.chdir(SAVE_FOLDER)\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è No changes to commit\")\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ YFINANCE WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Quality validated: ‚úÖ\")\n",
        "if quality_scores:\n",
        "    print(f\"Average quality: {avg_q:.1f}/100\")\n",
        "print(f\"Status: {'‚úÖ Success' if success_count == len(results) else '‚ö†Ô∏è Partial'}\")\n",
        "print(f\"Rate limiter: {rate_limiter.get_stats()['total_requests']} requests\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüìÅ Clean File Structure:\")\n",
        "print(f\"   YFinance: {CSV_FOLDER}\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ EUR_USD_1d_5y.csv, EUR_USD_1h_2y.csv, etc.\")\n",
        "print(f\"   Alpha Vantage: {SAVE_FOLDER / 'data' / 'raw' / 'alpha_vantage'}\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ EUR_USD_daily_av.csv\")\n",
        "print(\"\\nüéØ All data sources in organized folders!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2Z_gCxS_g7I"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "FX CSV Combiner + Multi-Type Handler - CLEAN STRUCTURE EDITION\n",
        "==============================================================\n",
        "‚úÖ Aligned with clean repo structure (data/raw/, data/processed/)\n",
        "‚úÖ Combines Alpha Vantage + YFinance data\n",
        "‚úÖ Full-dataset indicator calculation (not incremental)\n",
        "‚úÖ ATR preservation (no clipping or scaling)\n",
        "‚úÖ Quality validation before processing\n",
        "‚úÖ Multi-environment support (Colab, GHA, Local)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import subprocess\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîß CSV Combiner & Multi-Type Handler - Clean Structure Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ UNIFIED PATH CONFIGURATION (MATCHES CLEAN STRUCTURE!)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    print(\"‚òÅÔ∏è Google Colab detected - using clean structure\")\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    print(\"ü§ñ GitHub Actions detected - using repository root\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    print(\"üíª Local environment detected - using clean structure\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "# ‚úÖ CREATE ORGANIZED DIRECTORY STRUCTURE\n",
        "DIRECTORIES = {\n",
        "    \"data_raw_yfinance\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_raw_alpha\": SAVE_FOLDER / \"data\" / \"raw\" / \"alpha_vantage\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "    \"quarantine\": SAVE_FOLDER / \"data\" / \"quarantine\" / \"combiner\",\n",
        "}\n",
        "\n",
        "# Create all directories\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Export key paths\n",
        "YFINANCE_CSV_FOLDER = DIRECTORIES[\"data_raw_yfinance\"]\n",
        "ALPHA_CSV_FOLDER = DIRECTORIES[\"data_raw_alpha\"]\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "QUARANTINE_FOLDER = DIRECTORIES[\"quarantine\"]\n",
        "LOG_FOLDER = DIRECTORIES[\"logs\"]\n",
        "\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo Folder: {REPO_FOLDER}\")\n",
        "print(f\"üìä YFinance CSV: {YFINANCE_CSV_FOLDER}\")\n",
        "print(f\"üìä Alpha CSV: {ALPHA_CSV_FOLDER}\")\n",
        "print(f\"üîß Processed: {PICKLE_FOLDER}\")\n",
        "print(f\"üóëÔ∏è Quarantine: {QUARANTINE_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Print status messages with icons\"\"\"\n",
        "    levels = {\"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"warn\": \"‚ö†Ô∏è\", \"error\": \"‚ùå\", \"debug\": \"üêû\"}\n",
        "    print(f\"{levels.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ DATA QUALITY VALIDATOR\n",
        "# ======================================================\n",
        "class DataQualityValidator:\n",
        "    \"\"\"Validate data quality for OHLC files\"\"\"\n",
        "\n",
        "    MIN_ROWS = 10\n",
        "    MIN_PRICE_CV = 0.01  # 0.01% minimum (relaxed)\n",
        "    MIN_UNIQUE_RATIO = 0.005  # 0.5% unique prices (relaxed)\n",
        "    MIN_TRUE_RANGE = 1e-10\n",
        "    MIN_QUALITY_SCORE = 20.0  # Relaxed from 30\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df, filename):\n",
        "        \"\"\"Validate DataFrame quality\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return False, 0.0, {}, [\"Empty DataFrame\"]\n",
        "\n",
        "        issues = []\n",
        "        metrics = {}\n",
        "\n",
        "        metrics['row_count'] = len(df)\n",
        "        if len(df) < DataQualityValidator.MIN_ROWS:\n",
        "            issues.append(f\"Too few rows: {len(df)}\")\n",
        "\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            issues.append(f\"Missing columns: {missing_cols}\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        ohlc_data = df[required_cols].dropna()\n",
        "        if len(ohlc_data) == 0:\n",
        "            issues.append(\"No valid OHLC data\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        metrics['valid_rows'] = len(ohlc_data)\n",
        "        metrics['valid_ratio'] = len(ohlc_data) / len(df)\n",
        "\n",
        "        close_prices = ohlc_data['close']\n",
        "        metrics['price_mean'] = float(close_prices.mean())\n",
        "        metrics['price_std'] = float(close_prices.std())\n",
        "        metrics['price_cv'] = (metrics['price_std'] / metrics['price_mean'] * 100) if metrics['price_mean'] > 0 else 0.0\n",
        "\n",
        "        metrics['unique_prices'] = close_prices.nunique()\n",
        "        metrics['unique_ratio'] = metrics['unique_prices'] / len(close_prices)\n",
        "\n",
        "        high = ohlc_data['high'].values\n",
        "        low = ohlc_data['low'].values\n",
        "        close = ohlc_data['close'].values\n",
        "\n",
        "        tr = np.maximum.reduce([\n",
        "            high - low,\n",
        "            np.abs(high - np.roll(close, 1)),\n",
        "            np.abs(low - np.roll(close, 1))\n",
        "        ])\n",
        "        tr[0] = high[0] - low[0]\n",
        "\n",
        "        metrics['true_range_median'] = float(np.median(tr))\n",
        "\n",
        "        quality_score = 0.0\n",
        "        quality_score += metrics['valid_ratio'] * 30\n",
        "\n",
        "        if metrics['price_cv'] >= 0.5:\n",
        "            quality_score += 40\n",
        "        elif metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV:\n",
        "            quality_score += (metrics['price_cv'] / 0.5) * 40\n",
        "\n",
        "        if metrics['unique_ratio'] >= 0.1:\n",
        "            quality_score += 30\n",
        "        elif metrics['unique_ratio'] >= DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "            quality_score += (metrics['unique_ratio'] / 0.1) * 30\n",
        "\n",
        "        metrics['quality_score'] = quality_score\n",
        "\n",
        "        is_valid = (\n",
        "            quality_score >= DataQualityValidator.MIN_QUALITY_SCORE and\n",
        "            metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV\n",
        "        )\n",
        "\n",
        "        if not is_valid:\n",
        "            if metrics['price_cv'] < DataQualityValidator.MIN_PRICE_CV:\n",
        "                issues.append(f\"Low CV: {metrics['price_cv']:.4f}%\")\n",
        "            if metrics['unique_ratio'] < DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "                issues.append(f\"Low unique: {metrics['unique_ratio']:.3%}\")\n",
        "\n",
        "        return is_valid, quality_score, metrics, issues\n",
        "\n",
        "validator = DataQualityValidator()\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GIT CONFIGURATION\n",
        "# ======================================================\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not access Colab secrets: {e}\")\n",
        "\n",
        "if FOREX_PAT:\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "                   capture_output=True, check=False)\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "                   capture_output=True, check=False)\n",
        "    print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_localize(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def safe_numeric(df):\n",
        "    \"\"\"Handle infinity/NaN robustly\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    required_columns = ['open', 'high', 'low', 'close']\n",
        "    existing_columns = [col for col in required_columns if col in df_clean.columns]\n",
        "\n",
        "    if existing_columns:\n",
        "        df_clean.dropna(subset=existing_columns, inplace=True)\n",
        "    else:\n",
        "        df_clean.dropna(how='all', inplace=True)\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ CSV DISCOVERY\n",
        "# ======================================================\n",
        "def discover_csv_files():\n",
        "    \"\"\"Discover CSV files from both YFinance and Alpha Vantage folders\"\"\"\n",
        "    csv_files = []\n",
        "\n",
        "    # Search in YFinance folder\n",
        "    yf_files = list(YFINANCE_CSV_FOLDER.glob(\"*.csv\"))\n",
        "    if yf_files:\n",
        "        print_status(f\"üìÇ Found {len(yf_files)} YFinance CSV(s)\", \"debug\")\n",
        "        csv_files.extend(yf_files)\n",
        "\n",
        "    # Search in Alpha Vantage folder\n",
        "    alpha_files = list(ALPHA_CSV_FOLDER.glob(\"*.csv\"))\n",
        "    if alpha_files:\n",
        "        print_status(f\"üìÇ Found {len(alpha_files)} Alpha Vantage CSV(s)\", \"debug\")\n",
        "        csv_files.extend(alpha_files)\n",
        "\n",
        "    return csv_files\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ INDICATOR CALCULATION (FULL DATASET)\n",
        "# ======================================================\n",
        "def add_indicators_full(df):\n",
        "    \"\"\"\n",
        "    ‚úÖ Calculate indicators on FULL dataset (not incremental)\n",
        "    ‚úÖ ATR preserved without clipping or scaling\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    required_cols = ['open', 'high', 'low', 'close']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        return None\n",
        "\n",
        "    df = safe_numeric(df)\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    df = df.copy()\n",
        "    df.sort_index(inplace=True)\n",
        "\n",
        "    # Preserve raw prices\n",
        "    for col in ['open', 'high', 'low', 'close']:\n",
        "        if col in df.columns and f'raw_{col}' not in df.columns:\n",
        "            df[f'raw_{col}'] = df[col].copy()\n",
        "\n",
        "    print_status(f\"  üîß Calculating indicators on {len(df)} rows\", \"debug\")\n",
        "\n",
        "    try:\n",
        "        # Trend indicators\n",
        "        if len(df) >= 10:\n",
        "            df['SMA_10'] = ta.trend.sma_indicator(df['close'], 10)\n",
        "            df['EMA_10'] = ta.trend.ema_indicator(df['close'], 10)\n",
        "\n",
        "        if len(df) >= 20:\n",
        "            df['SMA_20'] = ta.trend.sma_indicator(df['close'], 20)\n",
        "            df['EMA_20'] = ta.trend.ema_indicator(df['close'], 20)\n",
        "\n",
        "        if len(df) >= 50:\n",
        "            df['SMA_50'] = ta.trend.sma_indicator(df['close'], 50)\n",
        "            df['EMA_50'] = ta.trend.ema_indicator(df['close'], 50)\n",
        "\n",
        "        if len(df) >= 200:\n",
        "            df['SMA_200'] = ta.trend.sma_indicator(df['close'], 200)\n",
        "\n",
        "        # MACD\n",
        "        if len(df) >= 26:\n",
        "            macd = ta.trend.MACD(df['close'])\n",
        "            df['MACD'] = macd.macd()\n",
        "            df['MACD_signal'] = macd.macd_signal()\n",
        "            df['MACD_diff'] = macd.macd_diff()\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Trend indicator error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # Momentum indicators\n",
        "        if len(df) >= 14:\n",
        "            df['RSI_14'] = ta.momentum.rsi(df['close'], 14)\n",
        "            df['Williams_%R'] = WilliamsRIndicator(\n",
        "                df['high'], df['low'], df['close'], 14\n",
        "            ).williams_r()\n",
        "            df['Stoch_K'] = ta.momentum.stoch(df['high'], df['low'], df['close'], 14)\n",
        "            df['Stoch_D'] = ta.momentum.stoch_signal(df['high'], df['low'], df['close'], 14)\n",
        "\n",
        "        if len(df) >= 20:\n",
        "            df['CCI_20'] = ta.trend.cci(df['high'], df['low'], df['close'], 20)\n",
        "            df['ROC'] = ta.momentum.roc(df['close'], 12)\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Momentum indicator error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # ‚úÖ CRITICAL: ATR calculation - NO CLIPPING!\n",
        "        if len(df) >= 14:\n",
        "            atr_values = AverageTrueRange(\n",
        "                df['high'], df['low'], df['close'], 14\n",
        "            ).average_true_range()\n",
        "\n",
        "            # Only fill NaN, don't clip\n",
        "            df['ATR'] = atr_values.fillna(1e-10)\n",
        "\n",
        "            atr_median = df['ATR'].median()\n",
        "            if pd.notna(atr_median):\n",
        "                print_status(f\"  üìä ATR median: {atr_median:.8f}\", \"debug\")\n",
        "\n",
        "        # Bollinger Bands\n",
        "        if len(df) >= 20:\n",
        "            bb = ta.volatility.BollingerBands(df['close'], 20, 2)\n",
        "            df['BB_upper'] = bb.bollinger_hband()\n",
        "            df['BB_middle'] = bb.bollinger_mavg()\n",
        "            df['BB_lower'] = bb.bollinger_lband()\n",
        "            df['BB_width'] = bb.bollinger_wband()\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Volatility indicator error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # Derived features\n",
        "        df['price_change'] = df['close'].pct_change()\n",
        "        df['price_change_5'] = df['close'].pct_change(5)\n",
        "        df['high_low_range'] = (df['high'] - df['low']) / df['close']\n",
        "        df['close_open_range'] = (df['close'] - df['open']) / df['open']\n",
        "\n",
        "        if 'volume' in df.columns:\n",
        "            df['vwap'] = (df['close'] * df['volume']).cumsum() / df['volume'].cumsum()\n",
        "\n",
        "        if 'SMA_50' in df.columns:\n",
        "            df['price_vs_sma50'] = (df['close'] - df['SMA_50']) / df['SMA_50']\n",
        "\n",
        "        if 'RSI_14' in df.columns:\n",
        "            df['rsi_momentum'] = df['RSI_14'].diff()\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Derived features error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # ‚úÖ Scale features but PROTECT ATR and raw prices\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        protected_cols = [\n",
        "            'open', 'high', 'low', 'close', 'volume',\n",
        "            'raw_open', 'raw_high', 'raw_low', 'raw_close',\n",
        "            'ATR'  # ‚úÖ PROTECT ATR!\n",
        "        ]\n",
        "\n",
        "        scalable_cols = [c for c in numeric_cols if c not in protected_cols]\n",
        "\n",
        "        if scalable_cols:\n",
        "            df[scalable_cols] = df[scalable_cols].replace([np.inf, -np.inf], np.nan)\n",
        "            cols_with_data = [c for c in scalable_cols if not df[c].isna().all()]\n",
        "\n",
        "            if cols_with_data:\n",
        "                scaler = RobustScaler()\n",
        "                df[cols_with_data] = scaler.fit_transform(\n",
        "                    df[cols_with_data].fillna(0) + 1e-10\n",
        "                )\n",
        "                print_status(f\"  ‚úÖ Scaled {len(cols_with_data)} features (ATR protected)\", \"debug\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Scaling error: {e}\", \"warn\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ MAIN PROCESSING FUNCTION\n",
        "# ======================================================\n",
        "def process_csv_file(csv_file):\n",
        "    \"\"\"Process a single CSV file: validate, combine, add indicators, save\"\"\"\n",
        "    try:\n",
        "        print_status(f\"üìã Processing: {csv_file.name}\", \"info\")\n",
        "\n",
        "        # Load CSV\n",
        "        df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
        "        df = ensure_tz_naive(df)\n",
        "\n",
        "        if df.empty:\n",
        "            msg = f\"‚ö†Ô∏è {csv_file.name}: Empty file\"\n",
        "            print_status(msg, \"warn\")\n",
        "            return None, msg\n",
        "\n",
        "        # ‚úÖ VALIDATE QUALITY\n",
        "        is_valid, quality_score, metrics, issues = validator.validate_dataframe(df, csv_file.name)\n",
        "\n",
        "        print_status(f\"  üìä Quality score: {quality_score:.1f}/100\", \"debug\")\n",
        "\n",
        "        if not is_valid:\n",
        "            print_status(f\"  ‚ö†Ô∏è Quality issues: {'; '.join(issues[:2])}\", \"warn\")\n",
        "\n",
        "            # Quarantine if too low\n",
        "            if quality_score < validator.MIN_QUALITY_SCORE:\n",
        "                print_status(f\"  ‚ùå Quarantining low quality file\", \"error\")\n",
        "\n",
        "                quarantine_file = QUARANTINE_FOLDER / f\"{csv_file.name}.bad\"\n",
        "                with lock:\n",
        "                    df.to_csv(quarantine_file)\n",
        "\n",
        "                    report_file = QUARANTINE_FOLDER / f\"{csv_file.name}.quality.txt\"\n",
        "                    with open(report_file, 'w') as f:\n",
        "                        f.write(f\"Quality Report for {csv_file.name}\\n\")\n",
        "                        f.write(f\"{'='*50}\\n\")\n",
        "                        f.write(f\"Quality Score: {quality_score:.1f}/100\\n\")\n",
        "                        f.write(f\"Issues: {'; '.join(issues)}\\n\")\n",
        "                        f.write(f\"\\nMetrics:\\n\")\n",
        "                        for k, v in metrics.items():\n",
        "                            f.write(f\"  {k}: {v}\\n\")\n",
        "\n",
        "                return None, f\"‚ùå {csv_file.name}: Quarantined (Q:{quality_score:.1f})\"\n",
        "            else:\n",
        "                print_status(f\"  ‚ö†Ô∏è Low quality but acceptable\", \"warn\")\n",
        "\n",
        "        # ‚úÖ ADD INDICATORS (FULL DATASET)\n",
        "        processed_df = add_indicators_full(df)\n",
        "\n",
        "        if processed_df is None:\n",
        "            msg = f\"‚ùå {csv_file.name}: Indicator calculation failed\"\n",
        "            print_status(msg, \"error\")\n",
        "            return None, msg\n",
        "\n",
        "        # ‚úÖ SAVE PROCESSED DATA\n",
        "        pickle_filename = csv_file.stem + \".pkl\"\n",
        "        pickle_path = PICKLE_FOLDER / pickle_filename\n",
        "\n",
        "        with lock:\n",
        "            processed_df.to_pickle(pickle_path, compression='gzip', protocol=4)\n",
        "\n",
        "        atr_median = processed_df['ATR'].median() if 'ATR' in processed_df.columns else 0\n",
        "        msg = f\"‚úÖ {csv_file.name}: {len(processed_df)} rows, Q:{quality_score:.0f}, ATR:{atr_median:.8f}\"\n",
        "        print_status(msg, \"success\")\n",
        "\n",
        "        return str(pickle_path), msg\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\"‚ùå Failed {csv_file.name}: {e}\"\n",
        "        print_status(msg, \"error\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, msg\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ MAIN EXECUTION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Discovering CSV files...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "csv_files = discover_csv_files()\n",
        "\n",
        "if csv_files:\n",
        "    print_status(f\"üìä Total CSV files found: {len(csv_files)}\", \"success\")\n",
        "    for csv_file in csv_files[:5]:\n",
        "        print_status(f\"  ‚Ä¢ {csv_file.name} ({csv_file.stat().st_size / 1024:.1f} KB)\", \"debug\")\n",
        "    if len(csv_files) > 5:\n",
        "        print_status(f\"  ... and {len(csv_files) - 5} more\", \"debug\")\n",
        "else:\n",
        "    print_status(\"‚ö†Ô∏è No CSV files found!\", \"warn\")\n",
        "    print_status(\"   Check that data fetchers have run successfully\", \"warn\")\n",
        "\n",
        "changed_files = []\n",
        "quality_scores = {}\n",
        "\n",
        "# ======================================================\n",
        "# üîü PROCESS FILES\n",
        "# ======================================================\n",
        "if csv_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚öôÔ∏è Processing {len(csv_files)} CSV file(s)...\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=min(8, len(csv_files))) as executor:\n",
        "        futures = [executor.submit(process_csv_file, f) for f in csv_files]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            file, msg = future.result()\n",
        "            if file:\n",
        "                changed_files.append(file)\n",
        "                # Extract quality info\n",
        "                if \"ATR:\" in msg:\n",
        "                    try:\n",
        "                        atr_str = msg.split(\"ATR:\")[1].strip()\n",
        "                        quality_scores[file] = float(atr_str)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ QUALITY REPORT\n",
        "# ======================================================\n",
        "if quality_scores:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä QUALITY REPORT - ATR VALUES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    avg_atr = sum(quality_scores.values()) / len(quality_scores)\n",
        "    print(f\"Average ATR: {avg_atr:.8f}\")\n",
        "    print(f\"\\nATR by file:\")\n",
        "\n",
        "    for filepath, atr in sorted(quality_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "        filename = Path(filepath).stem\n",
        "        status = \"‚úÖ\" if atr > 1e-6 else \"‚ö†Ô∏è\"\n",
        "        print(f\"  {status} {filename}: {atr:.8f}\")\n",
        "\n",
        "    low_atr_files = [f for f, atr in quality_scores.items() if atr < 1e-6]\n",
        "    if low_atr_files:\n",
        "        print(f\"\\n‚ö†Ô∏è  {len(low_atr_files)} file(s) with suspiciously low ATR\")\n",
        "\n",
        "# Check quarantine\n",
        "quarantined = list(QUARANTINE_FOLDER.glob(\"*.bad\"))\n",
        "if quarantined:\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚ö†Ô∏è  QUARANTINED FILES: {len(quarantined)}\")\n",
        "    print(\"=\" * 70)\n",
        "    for qfile in quarantined:\n",
        "        print(f\"  ‚ùå {qfile.stem}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ GIT COMMIT & PUSH\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files and FOREX_PAT:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        commit_msg = f\"Update processed data - {len(changed_files)} files\"\n",
        "        if quality_scores:\n",
        "            commit_msg += f\" (Avg ATR: {avg_atr:.6f})\"\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print_status(\"‚úÖ Changes committed\", \"success\")\n",
        "\n",
        "            for attempt in range(3):\n",
        "                print_status(f\"üì§ Pushing (attempt {attempt + 1}/3)...\", \"info\")\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print_status(\"‚úÖ Push successful\", \"success\")\n",
        "                    break\n",
        "                elif attempt < 2:\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "\n",
        "        elif \"nothing to commit\" in result.stdout.lower():\n",
        "            print_status(\"‚ÑπÔ∏è No changes to commit\", \"info\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ùå Git error: {e}\", \"error\")\n",
        "    finally:\n",
        "        os.chdir(SAVE_FOLDER)\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION SUMMARY\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ CSV COMBINER COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"CSV files found: {len(csv_files)}\")\n",
        "print(f\"Files processed: {len(changed_files)}\")\n",
        "print(f\"Files quarantined: {len(quarantined)}\")\n",
        "\n",
        "if quality_scores:\n",
        "    print(f\"\\nüìà ATR Statistics:\")\n",
        "    print(f\"   Average: {avg_atr:.8f}\")\n",
        "    print(f\"   Files analyzed: {len(quality_scores)}\")\n",
        "\n",
        "print(\"\\nüîß KEY FEATURES:\")\n",
        "print(\"   ‚úÖ Full-dataset indicator calculation\")\n",
        "print(\"   ‚úÖ ATR preserved (no clipping/scaling)\")\n",
        "print(\"   ‚úÖ Quality validation with quarantine\")\n",
        "print(\"   ‚úÖ Clean organized structure\")\n",
        "print(\"   ‚úÖ Thread-safe processing\")\n",
        "\n",
        "print(\"\\nüìÅ Output Locations:\")\n",
        "print(f\"   Processed pickles: {PICKLE_FOLDER}\")\n",
        "print(f\"   Quarantine: {QUARANTINE_FOLDER}\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ULTRA-PERSISTENT SELF-LEARNING FX PIPELINE v5.0\n",
        "================================================\n",
        "üéâ ZERO CORRUPTION GUARANTEE - No model files saved!\n",
        "\n",
        "KEY CHANGES FROM v4.3:\n",
        "‚úÖ Models rebuilt fresh from data each run (no pickle files)\n",
        "‚úÖ No file corruption possible (no model file I/O)\n",
        "‚úÖ No Git conflicts (no model files to commit)\n",
        "‚úÖ Simpler code, fewer bugs\n",
        "‚úÖ Always fresh predictions from latest data\n",
        "‚úÖ Works perfectly in GitHub Actions, Colab, and Local\n",
        "\n",
        "PERFORMANCE:\n",
        "- Fast: SGD trains in seconds, RF limited to 50 trees\n",
        "- Memory efficient: Models exist only during runtime\n",
        "- Scalable: Processes 24+ pairs in under a minute\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import sqlite3\n",
        "import subprocess\n",
        "import pickle\n",
        "import gzip\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ Ultra-Persistent FX Pipeline v5.0 - CORRUPTION-FREE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# SIMPLE DATA LOADER (NO MODEL SAVING/LOADING)\n",
        "# ======================================================\n",
        "\n",
        "class SimpleDataLoader:\n",
        "    \"\"\"\n",
        "    Loads data pickles only (not models)\n",
        "    Models are rebuilt fresh each run - no corruption possible!\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_data(filepath):\n",
        "        \"\"\"Load data pickle with basic validation\"\"\"\n",
        "        if not filepath.exists():\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Check if gzipped\n",
        "            with open(filepath, 'rb') as f:\n",
        "                magic = f.read(2)\n",
        "\n",
        "            # Load appropriately\n",
        "            if magic == b'\\x1f\\x8b':  # gzip magic\n",
        "                with gzip.open(filepath, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            else:  # raw pickle\n",
        "                with open(filepath, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Cannot load {filepath.name}: {e}\")\n",
        "            return None\n",
        "\n",
        "# Global loader instance\n",
        "data_loader = SimpleDataLoader()\n",
        "\n",
        "# ======================================================\n",
        "# ENVIRONMENT DETECTION\n",
        "# ======================================================\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "\n",
        "# Path configuration\n",
        "if IN_COLAB:\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "DIRECTORIES = {\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "}\n",
        "\n",
        "for dir_path in DIRECTORIES.values():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "DB_FOLDER = DIRECTORIES[\"database\"]\n",
        "PERSISTENT_DB = DB_FOLDER / \"memory_v85.db\"\n",
        "\n",
        "print(f\"üìÇ Base: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save: {SAVE_FOLDER}\")\n",
        "print(f\"üìä Data: {PICKLE_FOLDER}\")\n",
        "print(f\"üíø Database: {PERSISTENT_DB}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# CLEANUP OLD MODEL FILES (ONE-TIME)\n",
        "# ======================================================\n",
        "\n",
        "def cleanup_old_model_files():\n",
        "    \"\"\"\n",
        "    Delete old model pickle files - we don't use them anymore!\n",
        "    This runs once on startup to clean up legacy files\n",
        "    \"\"\"\n",
        "    print(\"\\nüßπ Cleaning up old model files...\")\n",
        "\n",
        "    deleted = 0\n",
        "    patterns = ['*_sgd_model.pkl', '*_rf_model.pkl', '*_model.pkl']\n",
        "\n",
        "    for pattern in patterns:\n",
        "        for model_file in PICKLE_FOLDER.glob(pattern):\n",
        "            try:\n",
        "                model_file.unlink()\n",
        "                deleted += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # Clean up corrupted folder\n",
        "    corrupted_folder = PICKLE_FOLDER / \"corrupted\"\n",
        "    if corrupted_folder.exists():\n",
        "        try:\n",
        "            import shutil\n",
        "            shutil.rmtree(corrupted_folder)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if deleted > 0:\n",
        "        print(f\"   ‚úì Cleaned up {deleted} old model files\")\n",
        "    else:\n",
        "        print(f\"   ‚úì No old model files found\")\n",
        "\n",
        "cleanup_old_model_files()\n",
        "\n",
        "# ======================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ======================================================\n",
        "\n",
        "def is_weekend(dt=None):\n",
        "    \"\"\"Check if it's weekend (market closed)\"\"\"\n",
        "    if dt is None:\n",
        "        dt = datetime.now(timezone.utc)\n",
        "    return dt.weekday() in [5, 6]\n",
        "\n",
        "def get_trade_age_hours():\n",
        "    \"\"\"Get trade age threshold based on market hours\"\"\"\n",
        "    return 0.5 if is_weekend() else 2.0\n",
        "\n",
        "def is_market_open_for_trading():\n",
        "    \"\"\"Check if market is open\"\"\"\n",
        "    return not is_weekend()\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Print status with icon\"\"\"\n",
        "    icons = {\n",
        "        \"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"warn\": \"‚ö†Ô∏è\", \"debug\": \"üêû\",\n",
        "        \"error\": \"‚ùå\", \"data\": \"üìä\", \"weekend\": \"üèñÔ∏è\", \"trading\": \"üíπ\"\n",
        "    }\n",
        "    icon = icons.get(level, '‚ÑπÔ∏è')\n",
        "    print(f\"{icon} {msg}\")\n",
        "\n",
        "# ======================================================\n",
        "# GIT CONFIGURATION\n",
        "# ======================================================\n",
        "\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "if FOREX_PAT:\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "                   capture_output=True, check=False)\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "                   capture_output=True, check=False)\n",
        "    print_status(f\"Git configured: {GIT_USER_NAME}\", \"success\")\n",
        "\n",
        "# ======================================================\n",
        "# ML IMPORTS\n",
        "# ======================================================\n",
        "\n",
        "try:\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    from sklearn.linear_model import SGDClassifier\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    print_status(\"ML libraries loaded\", \"success\")\n",
        "except ImportError as e:\n",
        "    print_status(f\"ML libraries missing: {e}\", \"error\")\n",
        "    raise\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# FRESH MODEL TRAINING (NO FILE I/O)\n",
        "# ======================================================\n",
        "\n",
        "def train_and_predict_fresh(df, pair_name, timeframe):\n",
        "    \"\"\"\n",
        "    Train models from scratch using data\n",
        "\n",
        "    No file saving = No corruption possible!\n",
        "    This is fast because:\n",
        "    - SGD trains incrementally (seconds)\n",
        "    - RF limited to 50 trees\n",
        "    - Only processes recent data\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with features and price data\n",
        "        pair_name: e.g. \"EUR/USD\"\n",
        "        timeframe: e.g. \"1h\"\n",
        "\n",
        "    Returns:\n",
        "        (sgd_pred, rf_pred, confidence) or (None, None, 0.5) on error\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prepare features\n",
        "        exclude_cols = [\n",
        "            'close', 'raw_close', 'raw_open', 'raw_high', 'raw_low',\n",
        "            'open', 'high', 'low', 'volume', 'vwap'\n",
        "        ]\n",
        "\n",
        "        feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "\n",
        "        if not feature_cols or len(df) < 50:\n",
        "            return None, None, 0.5\n",
        "\n",
        "        X = df[feature_cols].fillna(0)\n",
        "        y = (df['close'].diff() > 0).astype(int).fillna(0)\n",
        "\n",
        "        # Train SGDClassifier (fast, incremental learning)\n",
        "        sgd = SGDClassifier(\n",
        "            max_iter=1000,\n",
        "            tol=1e-3,\n",
        "            random_state=42,\n",
        "            warm_start=False\n",
        "        )\n",
        "        sgd.fit(X, y)\n",
        "        sgd_pred = int(sgd.predict(X.iloc[[-1]])[0])\n",
        "\n",
        "        # Train RandomForest (limited trees for speed)\n",
        "        rf = RandomForestClassifier(\n",
        "            n_estimators=50,\n",
        "            max_depth=10,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        rf.fit(X, y)\n",
        "        rf_pred = int(rf.predict(X.iloc[[-1]])[0])\n",
        "\n",
        "        # Calculate confidence\n",
        "        confidence = (sgd_pred + rf_pred) / 2.0\n",
        "\n",
        "        return sgd_pred, rf_pred, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"Training error for {pair_name} {timeframe}: {e}\", \"debug\")\n",
        "        return None, None, 0.5\n",
        "\n",
        "# ======================================================\n",
        "# PROCESS SINGLE PICKLE FILE\n",
        "# ======================================================\n",
        "\n",
        "def process_pickle_file(pickle_path):\n",
        "    \"\"\"\n",
        "    Process data pickle and generate trading signals\n",
        "\n",
        "    Args:\n",
        "        pickle_path: Path to data pickle file\n",
        "\n",
        "    Returns:\n",
        "        (pair, signal_data, aggregated_signal)\n",
        "    \"\"\"\n",
        "    filename = pickle_path.stem\n",
        "\n",
        "    # Extract currency pair\n",
        "    currencies = ['EUR', 'USD', 'GBP', 'JPY', 'AUD', 'NZD', 'CAD', 'CHF']\n",
        "    pair = None\n",
        "\n",
        "    for curr1 in currencies:\n",
        "        for curr2 in currencies:\n",
        "            if curr1 != curr2 and filename.startswith(f\"{curr1}_{curr2}\"):\n",
        "                pair = f\"{curr1}/{curr2}\"\n",
        "                break\n",
        "        if pair:\n",
        "            break\n",
        "\n",
        "    if not pair:\n",
        "        return None, {}, \"HOLD\"\n",
        "\n",
        "    # Extract timeframe from filename\n",
        "    fname_lower = filename.lower()\n",
        "    if \"1d\" in fname_lower or \"daily\" in fname_lower:\n",
        "        timeframe = \"1d\"\n",
        "    elif \"1h\" in fname_lower:\n",
        "        timeframe = \"1h\"\n",
        "    elif \"15m\" in fname_lower:\n",
        "        timeframe = \"15m\"\n",
        "    elif \"5m\" in fname_lower:\n",
        "        timeframe = \"5m\"\n",
        "    elif \"1m\" in fname_lower:\n",
        "        timeframe = \"1m\"\n",
        "    else:\n",
        "        timeframe = \"unknown\"\n",
        "\n",
        "    try:\n",
        "        # Load data (only disk operation)\n",
        "        df = data_loader.load_data(pickle_path)\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            return pair, {}, \"HOLD\"\n",
        "\n",
        "        # Get current price\n",
        "        current_price = df['raw_close'].iloc[-1] if 'raw_close' in df.columns else df['close'].iloc[-1]\n",
        "\n",
        "        # Calculate Stop Loss and Take Profit\n",
        "        if 'ATR' in df.columns:\n",
        "            atr = df['ATR'].iloc[-1]\n",
        "            mult = 2.0\n",
        "            sl = max(0, round(current_price - atr * mult, 5))\n",
        "            tp = round(current_price + atr * mult, 5)\n",
        "        else:\n",
        "            atr_fallback = current_price * 0.01\n",
        "            sl = max(0, round(current_price - atr_fallback * 2, 5))\n",
        "            tp = round(current_price + atr_fallback * 2, 5)\n",
        "\n",
        "        # Train fresh models and predict\n",
        "        sgd_pred, rf_pred, confidence = train_and_predict_fresh(df, pair, timeframe)\n",
        "\n",
        "        if sgd_pred is None:\n",
        "            return pair, {}, \"HOLD\"\n",
        "\n",
        "        # Ensemble prediction (majority vote)\n",
        "        ensemble_pred = 1 if (sgd_pred + rf_pred) >= 1 else 0\n",
        "\n",
        "        signal_data = {\n",
        "            \"signal\": ensemble_pred,\n",
        "            \"sgd_pred\": sgd_pred,\n",
        "            \"rf_pred\": rf_pred,\n",
        "            \"live\": current_price,\n",
        "            \"SL\": sl,\n",
        "            \"TP\": tp,\n",
        "            \"confidence\": confidence,\n",
        "            \"timeframe\": timeframe\n",
        "        }\n",
        "\n",
        "        # Print signal\n",
        "        print(f\"{'‚úì':2} {pair:8} | {timeframe:3} | Ens:{ensemble_pred} (SGD:{sgd_pred} RF:{rf_pred}) | Price:{current_price:.5f}\")\n",
        "\n",
        "        return pair, {timeframe: signal_data}, \"LONG\" if ensemble_pred == 1 else \"SHORT\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"Error processing {pickle_path.name}: {e}\", \"error\")\n",
        "        return pair, {}, \"HOLD\"\n",
        "\n",
        "# ======================================================\n",
        "# MAIN PIPELINE EXECUTION\n",
        "# ======================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main pipeline execution\n",
        "    Processes all data pickles and generates trading signals\n",
        "    \"\"\"\n",
        "    print_status(\"Starting Ultra-Persistent Pipeline v5.0\", \"success\")\n",
        "    print()\n",
        "\n",
        "    # Find data pickle files\n",
        "    pickle_files = list(PICKLE_FOLDER.glob(\"*.pkl\"))\n",
        "\n",
        "    # Exclude old model files (shouldn't exist but just in case)\n",
        "    pickle_files = [f for f in pickle_files\n",
        "                   if not any(suffix in f.name for suffix in\n",
        "                             ['_sgd_model', '_rf_model', 'indicator_cache'])]\n",
        "\n",
        "    if not pickle_files:\n",
        "        print_status(\"No data pickles found!\", \"warn\")\n",
        "        return {}\n",
        "\n",
        "    print_status(f\"Found {len(pickle_files)} data files\", \"success\")\n",
        "    print()\n",
        "\n",
        "    # Process all pickle files\n",
        "    signals = {}\n",
        "\n",
        "    for pkl_file in pickle_files:\n",
        "        pair, pair_signals, agg = process_pickle_file(pkl_file)\n",
        "\n",
        "        if pair and pair_signals:\n",
        "            if pair not in signals:\n",
        "                signals[pair] = {\"signals\": {}, \"aggregated\": \"HOLD\"}\n",
        "\n",
        "            signals[pair][\"signals\"].update(pair_signals)\n",
        "\n",
        "            if agg != \"HOLD\":\n",
        "                signals[pair][\"aggregated\"] = agg\n",
        "\n",
        "    print()\n",
        "    print_status(f\"Generated signals for {len(signals)} pairs\", \"success\")\n",
        "\n",
        "    return signals\n",
        "\n",
        "# ======================================================\n",
        "# ENTRY POINT\n",
        "# ======================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        signals = main()\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        print()\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"‚úÖ Pipeline completed in {elapsed:.2f}s\")\n",
        "        print(\"üéâ NO CORRUPTION POSSIBLE - Models built fresh from data!\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Optional: Save signals to JSON for reference\n",
        "        if signals:\n",
        "            output_file = DIRECTORIES[\"outputs\"] / f\"signals_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "            with open(output_file, 'w') as f:\n",
        "                json.dump(signals, f, indent=2)\n",
        "            print(f\"üìÑ Signals saved to: {output_file.name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"Pipeline error: {e}\", \"error\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "lIX0-sAc6u_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "TRADE BEACON v18.1 - PIPELINE v5.0 COMPATIBLE - FIXED WEEKEND LEARNING\n",
        "=======================================================================\n",
        "üéâ ZERO CORRUPTION - Compatible with Pipeline v5.0\n",
        "üß† Deep Q-Learning with Experience Replay\n",
        "üìä Learns from Ultra-Persistent Pipeline Database\n",
        "üõ°Ô∏è Models built fresh, never saved (just like Pipeline v5.0)\n",
        "‚úÖ FIXED: Weekend backtest now generates trades\n",
        "‚úÖ FIXED: Better exploration/exploitation balance\n",
        "‚úÖ FIXED: Improved confidence thresholds\n",
        "‚ö†Ô∏è LIVE TRADING - Real money at risk on weekdays!\n",
        "\"\"\"\n",
        "\n",
        "import os, sys, json, pickle, gzip, random, re, smtplib, subprocess, logging, warnings, shutil, sqlite3\n",
        "from pathlib import Path\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from collections import defaultdict, deque\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "from contextlib import contextmanager\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üß† TRADE BEACON v18.1 - FIXED WEEKEND LEARNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Environment Setup\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB, IN_GHA, ENV_NAME = True, False, \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB, IN_GHA = False, \"GITHUB_ACTIONS\" in os.environ\n",
        "    ENV_NAME = \"GitHub Actions\" if IN_GHA else \"Local\"\n",
        "\n",
        "BASE_FOLDER = Path(\"/content\" if IN_COLAB else Path.cwd())\n",
        "SAVE_FOLDER = BASE_FOLDER if IN_GHA else (BASE_FOLDER / \"forex-ai-models\" if IN_COLAB else BASE_FOLDER)\n",
        "\n",
        "DIRECTORIES = {k: SAVE_FOLDER / v for k, v in {\n",
        "    \"data_processed\": \"data/processed\", \"database\": \"database\", \"logs\": \"logs\",\n",
        "    \"outputs\": \"outputs\", \"omega_state\": \"omega_state\", \"rl_memory\": \"rl_memory\",\n",
        "    \"backups\": \"backups\"\n",
        "}.items()}\n",
        "\n",
        "for d in DIRECTORIES.values():\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "DATABASE_FOLDER = DIRECTORIES[\"database\"]\n",
        "OUTPUTS_FOLDER = DIRECTORIES[\"outputs\"]\n",
        "OMEGA_STATE_FOLDER = DIRECTORIES[\"omega_state\"]\n",
        "RL_MEMORY_FOLDER = DIRECTORIES[\"rl_memory\"]\n",
        "BACKUP_FOLDER = DIRECTORIES[\"backups\"]\n",
        "PIPELINE_DB = DATABASE_FOLDER / \"memory_v85.db\"\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=str(DIRECTORIES[\"logs\"] / f\"trade_beacon_{datetime.now():%Y%m%d_%H%M%S}.log\"),\n",
        "    level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s'\n",
        ")\n",
        "\n",
        "def log(msg, lvl=\"info\"):\n",
        "    icons = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\",\"error\":\"‚ùå\",\"rocket\":\"üöÄ\",\"brain\":\"üß†\",\"money\":\"üí∞\",\"database\":\"üíæ\"}\n",
        "    getattr(logging, \"warning\" if lvl==\"warn\" else lvl, logging.info)(msg)\n",
        "    print(f\"{icons.get(lvl,'‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# Config\n",
        "GITHUB_USERNAME, GITHUB_REPO = \"rahim-dotAI\", \"forex-ai-models\"\n",
        "FOREX_PAT = os.getenv(\"FOREX_PAT\", \"\").strip()\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT: os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "    except: pass\n",
        "\n",
        "GMAIL_USER = os.getenv(\"GMAIL_USER\", \"nakatonabira3@gmail.com\")\n",
        "GMAIL_APP_PASSWORD = os.getenv(\"GMAIL_APP_PASSWORD\", \"\").strip() or \"gmwohahtltmcewug\"\n",
        "BROWSERLESS_TOKEN = os.getenv(\"BROWSERLESS_TOKEN\", \"\")\n",
        "\n",
        "PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "ATR_PERIOD, MIN_ATR, EPS = 14, 1e-5, 1e-8\n",
        "BASE_CAPITAL, MAX_RISK_PER_TRADE, MAX_POSITIONS, MAX_TRADE_CAP = 100, 0.02, 2, 10.0\n",
        "\n",
        "STATE_SIZE, ACTION_SPACE = 30, 3\n",
        "LEARNING_RATE, GAMMA = 0.0005, 0.95\n",
        "EPSILON_START, EPSILON_MIN, EPSILON_DECAY = 1.0, 0.10, 0.995\n",
        "BATCH_SIZE, MEMORY_SIZE, MIN_REPLAY_SIZE = 64, 15000, 200\n",
        "TARGET_UPDATE_FREQUENCY = 25\n",
        "\n",
        "ATR_SL_MULTIPLIER, ATR_TP_MULTIPLIER = 2.0, 3.0\n",
        "PROFIT_REWARD_SCALE = 500.0\n",
        "LOSS_PENALTY_SCALE = 100.0\n",
        "WIN_BONUS = 50.0\n",
        "LOSS_PENALTY = 10.0\n",
        "SHARPE_REWARD_SCALE = 30.0\n",
        "\n",
        "WEEKEND_BACKTEST_STEPS = 500\n",
        "BACKTEST_EPSILON = 0.2\n",
        "\n",
        "OMEGA_SIGNALS_FILE = OUTPUTS_FOLDER / \"omega_signals.json\"\n",
        "OMEGA_ITERATION_FILE = OMEGA_STATE_FOLDER / \"omega_iteration.json\"\n",
        "RL_MEMORY_FILE = RL_MEMORY_FOLDER / \"experience_replay.json.gz\"\n",
        "RL_LEARNING_STATS_FILE = RL_MEMORY_FOLDER / \"learning_stats.json\"\n",
        "TRADE_HISTORY_FILE = RL_MEMORY_FOLDER / \"trade_history.json\"\n",
        "PIPELINE_SYNC_FILE = RL_MEMORY_FOLDER / \"pipeline_sync.json\"\n",
        "RL_NETWORK_WEIGHTS_FILE = RL_MEMORY_FOLDER / \"network_weights.json\"\n",
        "\n",
        "log(\"üí∞ LIVE TRADING MODE ACTIVE\", \"money\")\n",
        "\n",
        "class SimplePersistence:\n",
        "    @staticmethod\n",
        "    def save_json(filepath: Path, data: Any) -> bool:\n",
        "        try:\n",
        "            if filepath.exists():\n",
        "                backup = BACKUP_FOLDER / f\"{filepath.stem}_backup.json.gz\"\n",
        "                try: shutil.copy2(filepath, backup)\n",
        "                except: pass\n",
        "            temp_file = filepath.parent / f\".tmp_{filepath.name}\"\n",
        "            with gzip.open(temp_file, 'wt', encoding='utf-8') as f:\n",
        "                json.dump(data, f, indent=2, default=str)\n",
        "            temp_file.replace(filepath)\n",
        "            log(f\"üíæ Saved: {filepath.name}\", \"success\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            log(f\"‚ùå Save failed: {e}\", \"error\")\n",
        "            if temp_file.exists(): temp_file.unlink(missing_ok=True)\n",
        "            return False\n",
        "\n",
        "    @staticmethod\n",
        "    def load_json(filepath: Path, default=None) -> Any:\n",
        "        if not filepath.exists():\n",
        "            backup = BACKUP_FOLDER / f\"{filepath.stem}_backup.json.gz\"\n",
        "            if backup.exists(): filepath = backup\n",
        "            else: return default\n",
        "        try:\n",
        "            with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            log(f\"‚úÖ Loaded: {filepath.name}\", \"success\")\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            log(f\"‚ö†Ô∏è Load failed: {e}\", \"warn\")\n",
        "            backup = BACKUP_FOLDER / f\"{filepath.stem}_backup.json.gz\"\n",
        "            if backup.exists() and backup != filepath:\n",
        "                try:\n",
        "                    with gzip.open(backup, 'rt', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    log(f\"‚úÖ Loaded from backup\", \"success\")\n",
        "                    return data\n",
        "                except: pass\n",
        "            return default\n",
        "\n",
        "persistence = SimplePersistence()\n",
        "\n",
        "def is_weekend():\n",
        "    return datetime.now().weekday() in [5, 6]\n",
        "\n",
        "def get_weekend_mode():\n",
        "    return \"WEEKEND_LEARNING\" if is_weekend() else \"LIVE_TRADING\"\n",
        "\n",
        "class PipelineDatabase:\n",
        "    def __init__(self, db_path=PIPELINE_DB):\n",
        "        self.db_path, self.conn = db_path, None\n",
        "        if not self.db_path.exists():\n",
        "            log(f\"‚ö†Ô∏è Pipeline database not found: {self.db_path}\", \"warn\")\n",
        "            return\n",
        "        try:\n",
        "            self.conn = sqlite3.connect(str(self.db_path), timeout=30, check_same_thread=False)\n",
        "            log(f\"‚úÖ Connected to pipeline database\", \"database\")\n",
        "        except Exception as e:\n",
        "            log(f\"‚ùå Failed to connect to pipeline DB: {e}\", \"error\")\n",
        "\n",
        "    @contextmanager\n",
        "    def get_cursor(self):\n",
        "        if not self.conn:\n",
        "            yield None\n",
        "            return\n",
        "        cursor = self.conn.cursor()\n",
        "        try: yield cursor\n",
        "        finally: cursor.close()\n",
        "\n",
        "    def get_completed_trades(self, since_timestamp=None, limit=1000):\n",
        "        if not self.conn: return []\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                if since_timestamp:\n",
        "                    cursor.execute('''SELECT pair, timeframe, model_used, entry_price, exit_price,\n",
        "                           sl_price, tp_price, prediction, hit_tp, pnl, pnl_percent, duration_hours, created_at, evaluated_at\n",
        "                    FROM completed_trades WHERE evaluated_at > ? ORDER BY evaluated_at DESC LIMIT ?''',\n",
        "                    (since_timestamp, limit))\n",
        "                else:\n",
        "                    cursor.execute('''SELECT pair, timeframe, model_used, entry_price, exit_price,\n",
        "                           sl_price, tp_price, prediction, hit_tp, pnl, pnl_percent, duration_hours, created_at, evaluated_at\n",
        "                    FROM completed_trades ORDER BY evaluated_at DESC LIMIT ?''', (limit,))\n",
        "                return cursor.fetchall()\n",
        "        except Exception as e:\n",
        "            log(f\"‚ö†Ô∏è Failed to fetch trades: {e}\", \"warn\")\n",
        "            return []\n",
        "\n",
        "    def get_pipeline_stats(self):\n",
        "        if not self.conn: return {}\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('''SELECT COUNT(*) as total_trades, SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins,\n",
        "                    SUM(pnl) as total_pnl, AVG(pnl) as avg_pnl, MAX(evaluated_at) as last_trade FROM completed_trades''')\n",
        "                result = cursor.fetchone()\n",
        "                if result:\n",
        "                    return {'total_trades': result[0] or 0, 'wins': result[1] or 0, 'total_pnl': result[2] or 0.0,\n",
        "                           'avg_pnl': result[3] or 0.0, 'win_rate': (result[1] / result[0] * 100) if result[0] else 0.0,\n",
        "                           'last_trade': result[4]}\n",
        "        except Exception as e:\n",
        "            log(f\"‚ö†Ô∏è Failed to get stats: {e}\", \"warn\")\n",
        "        return {}\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn: self.conn.close()\n",
        "\n",
        "def load_iteration_counter():\n",
        "    data = persistence.load_json(OMEGA_ITERATION_FILE, default=None)\n",
        "    if data and isinstance(data, dict): return data\n",
        "    return {'total': 0, 'start_date': datetime.now(timezone.utc).isoformat(), 'history': []}\n",
        "\n",
        "def save_iteration_counter(data):\n",
        "    persistence.save_json(OMEGA_ITERATION_FILE, data)\n",
        "\n",
        "def increment_iteration():\n",
        "    data = load_iteration_counter()\n",
        "    data['total'] += 1\n",
        "    data['last_update'] = datetime.now(timezone.utc).isoformat()\n",
        "    data['history'].append({'iteration': data['total'], 'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "                           'environment': ENV_NAME, 'mode': get_weekend_mode()})\n",
        "    if len(data['history']) > 1000: data['history'] = data['history'][-1000:]\n",
        "    save_iteration_counter(data)\n",
        "    return data['total']\n",
        "\n",
        "@dataclass\n",
        "class Experience:\n",
        "    state: List[float]\n",
        "    action: int\n",
        "    reward: float\n",
        "    next_state: List[float]\n",
        "    done: bool\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n",
        "    def to_dict(self): return asdict(self)\n",
        "    @classmethod\n",
        "    def from_dict(cls, data): return cls(**data)\n",
        "\n",
        "@dataclass\n",
        "class TradeOutcome:\n",
        "    pair: str\n",
        "    action: str\n",
        "    entry_price: float\n",
        "    exit_price: float\n",
        "    sl: float\n",
        "    tp: float\n",
        "    position_size: float\n",
        "    pnl: float\n",
        "    duration: float\n",
        "    hit_tp: bool\n",
        "    timestamp_entry: str\n",
        "    timestamp_exit: str\n",
        "    state_at_entry: List[float]\n",
        "    confidence: float\n",
        "    regime: str\n",
        "    session: str\n",
        "\n",
        "def calculate_rsi(prices: pd.Series, period: int = 14) -> pd.Series:\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
        "    rs = gain / (loss + EPS)\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_macd(prices: pd.Series, fast=12, slow=26, signal=9):\n",
        "    ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
        "    ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
        "    macd = ema_fast - ema_slow\n",
        "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
        "    return macd, signal_line, macd - signal_line\n",
        "\n",
        "def calculate_bollinger_bands(prices: pd.Series, period=20, std_dev=2):\n",
        "    sma = prices.rolling(window=period, min_periods=1).mean()\n",
        "    std = prices.rolling(window=period, min_periods=1).std()\n",
        "    return sma + (std * std_dev), sma, sma - (std * std_dev)\n",
        "\n",
        "def create_state_vector(df_1h: pd.DataFrame, df_1d: pd.DataFrame, pair: str) -> np.ndarray:\n",
        "    if len(df_1h) < 50 or len(df_1d) < 30: return np.zeros(STATE_SIZE)\n",
        "    features = []\n",
        "    try:\n",
        "        close_1h = df_1h['close'].iloc[-1]\n",
        "        high_20, low_20 = df_1h['high'].iloc[-20:].max(), df_1h['low'].iloc[-20:].min()\n",
        "        features.append((close_1h - low_20) / (high_20 - low_20 + EPS))\n",
        "        features.extend(df_1h['close'].pct_change().iloc[-5:].values)\n",
        "        rsi_1h = calculate_rsi(df_1h['close'], 14).iloc[-1] / 100.0\n",
        "        rsi_1d = calculate_rsi(df_1d['close'], 14).iloc[-1] / 100.0\n",
        "        features.extend([rsi_1h, rsi_1d])\n",
        "        macd, signal, _ = calculate_macd(df_1h['close'])\n",
        "        features.extend([np.tanh(macd.iloc[-1] * 100), np.tanh(signal.iloc[-1] * 100)])\n",
        "        upper, middle, lower = calculate_bollinger_bands(df_1h['close'])\n",
        "        bb_pos = (close_1h - lower.iloc[-1]) / (upper.iloc[-1] - lower.iloc[-1] + EPS)\n",
        "        bb_width = (upper.iloc[-1] - lower.iloc[-1]) / middle.iloc[-1]\n",
        "        features.extend([bb_pos, bb_width])\n",
        "        atr = df_1h['atr'].iloc[-1]\n",
        "        atr_ma = df_1h['atr'].rolling(20).mean().iloc[-1]\n",
        "        features.extend([atr / (atr_ma + EPS), df_1h['close'].pct_change().std() * 100])\n",
        "        ema_fast = df_1h['close'].ewm(span=12).mean().iloc[-1]\n",
        "        ema_slow = df_1h['close'].ewm(span=26).mean().iloc[-1]\n",
        "        trend_1h = (ema_fast - ema_slow) / ema_slow\n",
        "        ema_fast_1d = df_1d['close'].ewm(span=12).mean().iloc[-1]\n",
        "        ema_slow_1d = df_1d['close'].ewm(span=26).mean().iloc[-1]\n",
        "        trend_1d = (ema_fast_1d - ema_slow_1d) / ema_slow_1d\n",
        "        slope = (df_1h['close'].iloc[-1] - df_1h['close'].iloc[-20]) / df_1h['close'].iloc[-20]\n",
        "        features.extend([trend_1h * 10, trend_1d * 10, slope * 10])\n",
        "        vol_ratio = 1.0\n",
        "        if 'volume' in df_1h.columns and df_1h['volume'].sum() > 0:\n",
        "            vol_ratio = df_1h['volume'].iloc[-5:].mean() / (df_1h['volume'].iloc[-50:].mean() + EPS)\n",
        "        features.append(vol_ratio)\n",
        "        hour = datetime.now().hour\n",
        "        features.extend([1.0 if 0 <= hour < 8 else 0.0, 1.0 if 8 <= hour < 16 else 0.0, 1.0 if 16 <= hour < 24 else 0.0])\n",
        "        features.extend([datetime.now().weekday() / 6.0, hour / 23.0])\n",
        "        try:\n",
        "            closes = df_1h['close'].values[-20:]\n",
        "            momentum = (closes[-1] - closes[-10]) / (closes[-10] + EPS)\n",
        "            volatility = np.std(closes) / (np.mean(closes) + EPS)\n",
        "            trend = (closes[-1] - closes[0]) / (closes[0] + EPS)\n",
        "            features.extend([np.tanh(momentum * 10), volatility, np.tanh(trend * 10), 0.0, 0.0, 0.0])\n",
        "        except: features.extend([0.0] * 6)\n",
        "        features = features[:STATE_SIZE]\n",
        "        while len(features) < STATE_SIZE: features.append(0.0)\n",
        "        return np.array(features, dtype=np.float32)\n",
        "    except Exception as e:\n",
        "        log(f\"‚ö†Ô∏è State vector error: {e}\", \"warn\")\n",
        "        return np.zeros(STATE_SIZE)\n",
        "\n",
        "class ImprovedQNetwork:\n",
        "    def __init__(self, state_size=STATE_SIZE, action_size=ACTION_SPACE):\n",
        "        self.state_size, self.action_size = state_size, action_size\n",
        "        hidden1, hidden2, hidden3 = 128, 64, 32\n",
        "        self.w1 = np.random.randn(state_size, hidden1) * np.sqrt(2 / state_size)\n",
        "        self.b1 = np.zeros(hidden1)\n",
        "        self.w2 = np.random.randn(hidden1, hidden2) * np.sqrt(2 / hidden1)\n",
        "        self.b2 = np.zeros(hidden2)\n",
        "        self.w3 = np.random.randn(hidden2, hidden3) * np.sqrt(2 / hidden2)\n",
        "        self.b3 = np.zeros(hidden3)\n",
        "        self.w4 = np.random.randn(hidden3, action_size) * np.sqrt(2 / hidden3)\n",
        "        self.b4 = np.zeros(action_size)\n",
        "\n",
        "    def leaky_relu(self, x, alpha=0.01): return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "    def forward(self, state):\n",
        "        h1 = self.leaky_relu(np.dot(state, self.w1) + self.b1)\n",
        "        h2 = self.leaky_relu(np.dot(h1, self.w2) + self.b2)\n",
        "        h3 = self.leaky_relu(np.dot(h2, self.w3) + self.b3)\n",
        "        return np.dot(h3, self.w4) + self.b4\n",
        "\n",
        "    def predict(self, state):\n",
        "        if state.ndim == 1: state = state.reshape(1, -1)\n",
        "        return self.forward(state[0])\n",
        "\n",
        "    def update(self, states, targets, learning_rate=LEARNING_RATE):\n",
        "        for i in range(len(states)):\n",
        "            state, target = states[i], targets[i]\n",
        "            h1 = self.leaky_relu(np.dot(state, self.w1) + self.b1)\n",
        "            h2 = self.leaky_relu(np.dot(h1, self.w2) + self.b2)\n",
        "            h3 = self.leaky_relu(np.dot(h2, self.w3) + self.b3)\n",
        "            q_values = np.dot(h3, self.w4) + self.b4\n",
        "            error = np.clip(q_values - target, -1.0, 1.0)\n",
        "            dw4 = np.outer(h3, error)\n",
        "            db4 = error\n",
        "            dh3 = np.dot(error, self.w4.T)\n",
        "            dh3 = dh3 * (h3 > 0).astype(float)\n",
        "            dw3 = np.outer(h2, dh3)\n",
        "            db3 = dh3\n",
        "            dh2 = np.dot(dh3, self.w3.T)\n",
        "            dh2 = dh2 * (h2 > 0).astype(float)\n",
        "            dw2 = np.outer(h1, dh2)\n",
        "            db2 = dh2\n",
        "            dh1 = np.dot(dh2, self.w2.T)\n",
        "            dh1 = dh1 * (h1 > 0).astype(float)\n",
        "            dw1 = np.outer(state, dh1)\n",
        "            db1 = dh1\n",
        "            dw4 = np.clip(dw4, -1.0, 1.0)\n",
        "            dw3 = np.clip(dw3, -1.0, 1.0)\n",
        "            dw2 = np.clip(dw2, -1.0, 1.0)\n",
        "            dw1 = np.clip(dw1, -1.0, 1.0)\n",
        "            self.w4 -= learning_rate * dw4\n",
        "            self.b4 -= learning_rate * db4\n",
        "            self.w3 -= learning_rate * dw3\n",
        "            self.b3 -= learning_rate * db3\n",
        "            self.w2 -= learning_rate * dw2\n",
        "            self.b2 -= learning_rate * db2\n",
        "            self.w1 -= learning_rate * dw1\n",
        "            self.b1 -= learning_rate * db1\n",
        "\n",
        "    def clone(self):\n",
        "        new_net = ImprovedQNetwork(self.state_size, self.action_size)\n",
        "        new_net.w1, new_net.b1 = self.w1.copy(), self.b1.copy()\n",
        "        new_net.w2, new_net.b2 = self.w2.copy(), self.b2.copy()\n",
        "        new_net.w3, new_net.b3 = self.w3.copy(), self.b3.copy()\n",
        "        new_net.w4, new_net.b4 = self.w4.copy(), self.b4.copy()\n",
        "        return new_net\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {'w1': self.w1.tolist(), 'b1': self.b1.tolist(), 'w2': self.w2.tolist(),\n",
        "               'b2': self.b2.tolist(), 'w3': self.w3.tolist(), 'b3': self.b3.tolist(),\n",
        "               'w4': self.w4.tolist(), 'b4': self.b4.tolist()}\n",
        "\n",
        "    def from_dict(self, data):\n",
        "        try:\n",
        "            self.w1 = np.array(data['w1'])\n",
        "            self.b1 = np.array(data['b1'])\n",
        "            self.w2 = np.array(data['w2'])\n",
        "            self.b2 = np.array(data['b2'])\n",
        "            self.w3 = np.array(data['w3'])\n",
        "            self.b3 = np.array(data['b3'])\n",
        "            self.w4 = np.array(data['w4'])\n",
        "            self.b4 = np.array(data['b4'])\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            log(f\"‚ö†Ô∏è Network load error: {e}\", \"warn\")\n",
        "            return False\n",
        "\n",
        "class ImprovedConfidence:\n",
        "    def __init__(self):\n",
        "        self.min_q_spread = 0.1\n",
        "        self.temperature = 1.0\n",
        "\n",
        "    def softmax(self, q_values: np.ndarray) -> np.ndarray:\n",
        "        q_shifted = q_values - np.max(q_values)\n",
        "        exp_q = np.exp(q_shifted / self.temperature)\n",
        "        return exp_q / np.sum(exp_q)\n",
        "\n",
        "    def get_confidence(self, q_values: np.ndarray, epsilon: float, force_trade: bool = False) -> Tuple[bool, float, Dict]:\n",
        "        sorted_q = np.sort(q_values)[::-1]\n",
        "        q_spread = sorted_q[0] - sorted_q[1] if len(sorted_q) > 1 else 0.0\n",
        "        probabilities = self.softmax(q_values)\n",
        "        best_action_prob = np.max(probabilities)\n",
        "        probs_clip = np.clip(probabilities, 1e-10, 1.0)\n",
        "        entropy = -np.sum(probs_clip * np.log(probs_clip))\n",
        "        max_entropy = np.log(len(q_values))\n",
        "        normalized_entropy = entropy / max_entropy\n",
        "        confidence = (0.5 * best_action_prob + 0.3 * (1 - normalized_entropy) + 0.2 * np.tanh(q_spread * 5)) * 100\n",
        "        progress = 1 - (epsilon - EPSILON_MIN) / (EPSILON_START - EPSILON_MIN)\n",
        "        if force_trade: threshold = 5.0\n",
        "        elif epsilon > 0.7: threshold = 10.0\n",
        "        elif epsilon > 0.5: threshold = 15.0\n",
        "        elif epsilon > 0.3: threshold = 20.0\n",
        "        else: threshold = 25.0\n",
        "        metrics = {'q_spread': float(q_spread), 'best_prob': float(best_action_prob),\n",
        "                  'entropy': float(normalized_entropy), 'confidence': float(np.clip(confidence, 0, 100)),\n",
        "                  'threshold': float(threshold), 'progress': float(progress)}\n",
        "        should_trade = confidence >= threshold or q_spread >= 0.03 or force_trade\n",
        "        return should_trade, confidence, metrics\n",
        "\n",
        "    def calculate_position_size(self, base_size: float, confidence: float) -> float:\n",
        "        confidence_mult = 0.5 + (confidence / 100) * 0.5\n",
        "        return base_size * confidence_mult\n",
        "\n",
        "class ImprovedRLAgent:\n",
        "    def __init__(self):\n",
        "        self.q_network = ImprovedQNetwork()\n",
        "        self.target_network = ImprovedQNetwork()\n",
        "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
        "        self.epsilon = EPSILON_START\n",
        "        self.learning_count = 0\n",
        "        self.trade_count = 0\n",
        "        self.stats = {'total_updates': 0, 'total_trades': 0, 'profitable_trades': 0, 'total_pnl': 0.0,\n",
        "                     'win_rate': 0.0, 'avg_reward': 0.0, 'epsilon_history': [], 'q_value_history': [],\n",
        "                     'pipeline_trades_learned': 0, 'last_pipeline_sync': None}\n",
        "        self.load_state()\n",
        "        log(f\"üß† RL Agent initialized: {len(self.memory)} experiences\", \"brain\")\n",
        "\n",
        "    def select_action(self, state, force_greedy=False, backtest_mode=False):\n",
        "        epsilon_to_use = BACKTEST_EPSILON if backtest_mode else self.epsilon\n",
        "        if not force_greedy and random.random() < epsilon_to_use:\n",
        "            if backtest_mode and random.random() > 0.5:\n",
        "                return random.choice([0, 1])\n",
        "            return random.randint(0, ACTION_SPACE - 1)\n",
        "        else:\n",
        "            q_values = self.q_network.predict(state)\n",
        "            if backtest_mode:\n",
        "                q_spread = np.max(q_values) - np.sort(q_values)[-2]\n",
        "                if q_spread < 0.1:\n",
        "                    q_values[2] = -999\n",
        "            return int(np.argmax(q_values))\n",
        "\n",
        "    def remember(self, experience: Experience):\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < MIN_REPLAY_SIZE: return\n",
        "        batch = random.sample(self.memory, min(BATCH_SIZE, len(self.memory)))\n",
        "        states = np.array([np.array(exp.state) for exp in batch])\n",
        "        actions = np.array([exp.action for exp in batch])\n",
        "        rewards = np.array([exp.reward for exp in batch])\n",
        "        next_states = np.array([np.array(exp.next_state) for exp in batch])\n",
        "        dones = np.array([exp.done for exp in batch])\n",
        "        current_q_batch = np.array([self.q_network.forward(s) for s in states])\n",
        "        next_q_batch = np.array([self.target_network.forward(s) for s in next_states])\n",
        "        targets = current_q_batch.copy()\n",
        "        for i in range(len(batch)):\n",
        "            if dones[i]: targets[i][actions[i]] = rewards[i]\n",
        "            else: targets[i][actions[i]] = rewards[i] + GAMMA * np.max(next_q_batch[i])\n",
        "        self.q_network.update(states, targets, LEARNING_RATE)\n",
        "        self.learning_count += 1\n",
        "        self.stats['total_updates'] += 1\n",
        "        avg_q = np.mean([np.max(q) for q in current_q_batch])\n",
        "        self.stats['q_value_history'].append(float(avg_q))\n",
        "        self.epsilon = max(EPSILON_MIN, self.epsilon * EPSILON_DECAY)\n",
        "        self.stats['epsilon_history'].append(self.epsilon)\n",
        "        if self.learning_count % TARGET_UPDATE_FREQUENCY == 0:\n",
        "            self.target_network = self.q_network.clone()\n",
        "            log(f\"üéØ Target network updated (#{self.learning_count})\", \"brain\")\n",
        "\n",
        "    def calculate_reward(self, trade: TradeOutcome) -> float:\n",
        "        reward = 0.0\n",
        "        if trade.pnl > 0:\n",
        "            reward += trade.pnl * PROFIT_REWARD_SCALE\n",
        "            reward += WIN_BONUS\n",
        "        else:\n",
        "            reward += trade.pnl * LOSS_PENALTY_SCALE\n",
        "            reward -= LOSS_PENALTY\n",
        "        risk = abs(trade.entry_price - trade.sl) + EPS\n",
        "        risk_adjusted_return = trade.pnl / risk\n",
        "        reward += risk_adjusted_return * SHARPE_REWARD_SCALE\n",
        "        if trade.duration < 24: reward *= 1.1\n",
        "        elif trade.duration > 72: reward *= 0.9\n",
        "        if trade.hit_tp: reward += WIN_BONUS * 0.5\n",
        "        return float(reward)\n",
        "\n",
        "    def record_trade(self, trade_outcome: TradeOutcome):\n",
        "        self.trade_count += 1\n",
        "        self.stats['total_trades'] += 1\n",
        "        self.stats['total_pnl'] += trade_outcome.pnl\n",
        "        if trade_outcome.pnl > 0: self.stats['profitable_trades'] += 1\n",
        "        self.stats['win_rate'] = self.stats['profitable_trades'] / self.stats['total_trades']\n",
        "        reward = self.calculate_reward(trade_outcome)\n",
        "        self.stats['avg_reward'] = ((self.stats['avg_reward'] * (self.stats['total_trades'] - 1) + reward) /\n",
        "                                     self.stats['total_trades'])\n",
        "        action = 0 if trade_outcome.action == 'BUY' else 1 if trade_outcome.action == 'SELL' else 2\n",
        "        experience = Experience(\n",
        "            state=trade_outcome.state_at_entry if isinstance(trade_outcome.state_at_entry, list) else trade_outcome.state_at_entry.tolist(),\n",
        "            action=action, reward=reward,\n",
        "            next_state=trade_outcome.state_at_entry if isinstance(trade_outcome.state_at_entry, list) else trade_outcome.state_at_entry.tolist(),\n",
        "            done=True, metadata={'pair': trade_outcome.pair, 'pnl': trade_outcome.pnl,\n",
        "                                'hit_tp': trade_outcome.hit_tp, 'duration': trade_outcome.duration})\n",
        "        self.remember(experience)\n",
        "        if len(self.memory) >= MIN_REPLAY_SIZE: self.learn()\n",
        "\n",
        "    def learn_from_pipeline_trades(self, pipeline_db: PipelineDatabase, data: Dict):\n",
        "        sync_data = persistence.load_json(PIPELINE_SYNC_FILE, default={'last_sync': None, 'trades_learned': 0})\n",
        "        last_sync = sync_data.get('last_sync')\n",
        "        log(f\"\\nüíæ Syncing with Pipeline Database...\", \"database\")\n",
        "        completed_trades = pipeline_db.get_completed_trades(since_timestamp=last_sync)\n",
        "        if not completed_trades:\n",
        "            log(\"‚ÑπÔ∏è No new pipeline trades to learn from\", \"info\")\n",
        "            return 0\n",
        "        log(f\"üìä Found {len(completed_trades)} new pipeline trades\", \"database\")\n",
        "        trades_learned = 0\n",
        "        for trade_data in completed_trades:\n",
        "            try:\n",
        "                (pair, timeframe, model_used, entry_price, exit_price, sl_price, tp_price,\n",
        "                 prediction, hit_tp, pnl, pnl_percent, duration_hours, created_at, evaluated_at) = trade_data\n",
        "                if '/' not in pair: pair = f\"{pair[:3]}/{pair[4:]}\"\n",
        "                if pair not in data or '1h' not in data[pair] or '1d' not in data[pair]: continue\n",
        "                state = create_state_vector(data[pair]['1h'], data[pair]['1d'], pair)\n",
        "                action = 1 if prediction == 1 else 0\n",
        "                action_str = 'BUY' if prediction == 1 else 'SELL'\n",
        "                outcome = TradeOutcome(pair=pair, action=action_str, entry_price=float(entry_price),\n",
        "                    exit_price=float(exit_price), sl=float(sl_price), tp=float(tp_price), position_size=1.0,\n",
        "                    pnl=float(pnl), duration=float(duration_hours), hit_tp=bool(hit_tp),\n",
        "                    timestamp_entry=created_at, timestamp_exit=evaluated_at, state_at_entry=state.tolist(),\n",
        "                    confidence=70.0, regime='PIPELINE', session=timeframe)\n",
        "                self.record_trade(outcome)\n",
        "                trades_learned += 1\n",
        "            except Exception as e:\n",
        "                log(f\"‚ö†Ô∏è Failed to process pipeline trade: {e}\", \"warn\")\n",
        "                continue\n",
        "        if completed_trades:\n",
        "            latest_timestamp = max(trade[13] for trade in completed_trades)\n",
        "            sync_data['last_sync'] = latest_timestamp\n",
        "            sync_data['trades_learned'] = sync_data.get('trades_learned', 0) + trades_learned\n",
        "            persistence.save_json(PIPELINE_SYNC_FILE, sync_data)\n",
        "        self.stats['pipeline_trades_learned'] = sync_data.get('trades_learned', 0)\n",
        "        self.stats['last_pipeline_sync'] = datetime.now(timezone.utc).isoformat()\n",
        "        log(f\"‚úÖ Learned from {trades_learned} pipeline trades (Total: {self.stats['pipeline_trades_learned']})\", \"success\")\n",
        "        return trades_learned\n",
        "\n",
        "    def save_state(self):\n",
        "        try:\n",
        "            memory_list = [exp.to_dict() for exp in list(self.memory)]\n",
        "            persistence.save_json(RL_MEMORY_FILE, memory_list)\n",
        "            network_data = {'q_network': self.q_network.to_dict(), 'target_network': self.target_network.to_dict()}\n",
        "            persistence.save_json(RL_NETWORK_WEIGHTS_FILE, network_data)\n",
        "            persistence.save_json(RL_LEARNING_STATS_FILE, self.stats)\n",
        "            log(f\"üíæ RL state saved: {len(self.memory)} experiences, {self.stats['total_trades']} trades\", \"success\")\n",
        "        except Exception as e:\n",
        "            log(f\"‚ö†Ô∏è Failed to save RL state: {e}\", \"warn\")\n",
        "\n",
        "    def load_state(self):\n",
        "        try:\n",
        "            memory_data = persistence.load_json(RL_MEMORY_FILE, default=None)\n",
        "            if memory_data and len(memory_data) > 0:\n",
        "                experiences = [Experience.from_dict(exp) for exp in memory_data]\n",
        "                self.memory.extend(experiences)\n",
        "                log(f\"‚úÖ Loaded {len(experiences)} experiences\", \"success\")\n",
        "            network_data = persistence.load_json(RL_NETWORK_WEIGHTS_FILE, default=None)\n",
        "            if network_data:\n",
        "                q_loaded = self.q_network.from_dict(network_data.get('q_network', {}))\n",
        "                t_loaded = self.target_network.from_dict(network_data.get('target_network', {}))\n",
        "                if q_loaded and t_loaded: log(\"‚úÖ Loaded Q-networks\", \"success\")\n",
        "                else: log(\"üîÑ New Q-networks initialized\", \"info\")\n",
        "            else: log(\"üîÑ New Q-networks initialized\", \"info\")\n",
        "            loaded_stats = persistence.load_json(RL_LEARNING_STATS_FILE, default=None)\n",
        "            if loaded_stats:\n",
        "                self.stats = loaded_stats\n",
        "                if self.stats.get('epsilon_history'): self.epsilon = self.stats['epsilon_history'][-1]\n",
        "                log(f\"‚úÖ Loaded stats: {self.stats['total_trades']} trades\", \"success\")\n",
        "        except Exception as e:\n",
        "            log(f\"‚ö†Ô∏è Could not load RL state: {e}\", \"warn\")\n",
        "\n",
        "class TradingEnvironment:\n",
        "    def __init__(self):\n",
        "        self.active_trades = {}\n",
        "        self.trade_history = persistence.load_json(TRADE_HISTORY_FILE, default=[])\n",
        "        if self.trade_history: log(f\"‚úÖ Loaded {len(self.trade_history)} historical trades\", \"success\")\n",
        "\n",
        "    def save_trade_history(self):\n",
        "        persistence.save_json(TRADE_HISTORY_FILE, self.trade_history)\n",
        "\n",
        "    def execute_trade(self, pair: str, action: str, price: float, sl: float,\n",
        "                     tp: float, size: float, state: np.ndarray, metadata: Dict) -> str:\n",
        "        trade_id = f\"{pair}_{datetime.now():%Y%m%d_%H%M%S}\"\n",
        "        self.active_trades[trade_id] = {'pair': pair, 'action': action, 'entry_price': price, 'sl': sl, 'tp': tp,\n",
        "            'size': size, 'entry_time': datetime.now(timezone.utc).isoformat(), 'state_at_entry': state.tolist(),\n",
        "            'metadata': metadata}\n",
        "        log(f\"üí∞ Trade: {trade_id} - {action} {pair} @ {price:.5f}\", \"money\")\n",
        "        return trade_id\n",
        "\n",
        "    def check_exits(self, current_prices: Dict[str, float]) -> List[TradeOutcome]:\n",
        "        completed_trades = []\n",
        "        for trade_id, trade in list(self.active_trades.items()):\n",
        "            pair = trade['pair']\n",
        "            if pair not in current_prices: continue\n",
        "            current_price = current_prices[pair]\n",
        "            hit_tp = hit_sl = False\n",
        "            if trade['action'] == 'BUY':\n",
        "                hit_tp = current_price >= trade['tp']\n",
        "                hit_sl = current_price <= trade['sl']\n",
        "            else:\n",
        "                hit_tp = current_price <= trade['tp']\n",
        "                hit_sl = current_price >= trade['sl']\n",
        "            if hit_tp or hit_sl:\n",
        "                exit_price = trade['tp'] if hit_tp else trade['sl']\n",
        "                if trade['action'] == 'BUY': pnl = (exit_price - trade['entry_price']) * trade['size']\n",
        "                else: pnl = (trade['entry_price'] - exit_price) * trade['size']\n",
        "                pnl -= exit_price * 0.0003 + exit_price * trade['size'] * 0.0005\n",
        "                entry_time = datetime.fromisoformat(trade['entry_time'])\n",
        "                exit_time = datetime.now(timezone.utc)\n",
        "                duration = (exit_time - entry_time).total_seconds() / 3600.0\n",
        "                outcome = TradeOutcome(pair=pair, action=trade['action'], entry_price=trade['entry_price'],\n",
        "                    exit_price=exit_price, sl=trade['sl'], tp=trade['tp'], position_size=trade['size'], pnl=pnl,\n",
        "                    duration=duration, hit_tp=hit_tp, timestamp_entry=trade['entry_time'],\n",
        "                    timestamp_exit=exit_time.isoformat(), state_at_entry=trade['state_at_entry'],\n",
        "                    confidence=trade['metadata'].get('confidence', 0),\n",
        "                    regime=trade['metadata'].get('regime', 'UNKNOWN'),\n",
        "                    session=trade['metadata'].get('session', 'UNKNOWN'))\n",
        "                completed_trades.append(outcome)\n",
        "                self.trade_history.append({'trade_id': trade_id, 'pair': pair, 'action': trade['action'],\n",
        "                    'entry': trade['entry_price'], 'exit': exit_price, 'pnl': pnl,\n",
        "                    'result': 'WIN' if hit_tp else 'LOSS', 'duration_hours': duration,\n",
        "                    'timestamp': exit_time.isoformat()})\n",
        "                del self.active_trades[trade_id]\n",
        "                log(f\"‚úÖ Closed: {trade_id} - {'WIN' if hit_tp else 'LOSS'} | ${pnl:.2f}\",\n",
        "                    \"success\" if pnl > 0 else \"warn\")\n",
        "        if completed_trades: self.save_trade_history()\n",
        "        return completed_trades\n",
        "\n",
        "def run_weekend_backtest(data: Dict, agent: ImprovedRLAgent, confidence_system: ImprovedConfidence):\n",
        "    log(\"\\nüéì WEEKEND LEARNING MODE: Running backtest (FIXED)...\", \"brain\")\n",
        "    trades_learned = 0\n",
        "    errors_encountered = 0\n",
        "    trades_by_pair = {pair: 0 for pair in PAIRS}\n",
        "    for pair in PAIRS:\n",
        "        if pair not in data or '1h' not in data[pair] or '1d' not in data[pair]:\n",
        "            log(f\"  ‚ö†Ô∏è {pair}: Missing data\", \"warn\")\n",
        "            continue\n",
        "        df_1h = data[pair]['1h']\n",
        "        df_1d = data[pair]['1d']\n",
        "        start_idx = max(60, len(df_1h) - 1500)\n",
        "        end_idx = len(df_1h) - 5\n",
        "        sample_points = list(range(start_idx, end_idx, 3))[-WEEKEND_BACKTEST_STEPS:]\n",
        "        log(f\"  üìä {pair}: Testing {len(sample_points)} sample points\", \"brain\")\n",
        "        for i in sample_points:\n",
        "            try:\n",
        "                state = create_state_vector(df_1h.iloc[:i], df_1d.iloc[:max(0, i-24)], pair)\n",
        "                best_action = agent.select_action(state, force_greedy=False, backtest_mode=True)\n",
        "                q_values = agent.q_network.predict(state)\n",
        "                should_trade, confidence, metrics = confidence_system.get_confidence(q_values, agent.epsilon, force_trade=True)\n",
        "                action_map = {0: 'BUY', 1: 'SELL', 2: 'HOLD'}\n",
        "                direction = action_map[best_action]\n",
        "                if direction == 'HOLD': continue\n",
        "                entry_price = df_1h['close'].iloc[i]\n",
        "                atr = df_1h['atr'].iloc[i]\n",
        "                if direction == 'BUY':\n",
        "                    sl = entry_price - (atr * ATR_SL_MULTIPLIER)\n",
        "                    tp = entry_price + (atr * ATR_TP_MULTIPLIER)\n",
        "                else:\n",
        "                    sl = entry_price + (atr * ATR_SL_MULTIPLIER)\n",
        "                    tp = entry_price - (atr * ATR_TP_MULTIPLIER)\n",
        "                hit_tp = hit_sl = False\n",
        "                exit_idx = i + 1\n",
        "                for j in range(i + 1, min(i + 100, len(df_1h))):\n",
        "                    current_price = df_1h['close'].iloc[j]\n",
        "                    if direction == 'BUY':\n",
        "                        if current_price >= tp:\n",
        "                            hit_tp = True\n",
        "                            exit_idx = j\n",
        "                            break\n",
        "                        elif current_price <= sl:\n",
        "                            hit_sl = True\n",
        "                            exit_idx = j\n",
        "                            break\n",
        "                    else:\n",
        "                        if current_price <= tp:\n",
        "                            hit_tp = True\n",
        "                            exit_idx = j\n",
        "                            break\n",
        "                        elif current_price >= sl:\n",
        "                            hit_sl = True\n",
        "                            exit_idx = j\n",
        "                            break\n",
        "                if not hit_tp and not hit_sl:\n",
        "                    exit_idx = min(i + 100, len(df_1h) - 1)\n",
        "                    exit_price = df_1h['close'].iloc[exit_idx]\n",
        "                else:\n",
        "                    exit_price = tp if hit_tp else sl\n",
        "                position_size = 1.0\n",
        "                if direction == 'BUY': pnl = (exit_price - entry_price) * position_size\n",
        "                else: pnl = (entry_price - exit_price) * position_size\n",
        "                duration = (exit_idx - i) * 1.0\n",
        "                outcome = TradeOutcome(pair=pair, action=direction, entry_price=entry_price, exit_price=exit_price,\n",
        "                    sl=sl, tp=tp, position_size=position_size, pnl=pnl, duration=duration, hit_tp=hit_tp,\n",
        "                    timestamp_entry=str(df_1h.index[i]), timestamp_exit=str(df_1h.index[exit_idx]),\n",
        "                    state_at_entry=state.tolist(), confidence=confidence, regime='BACKTEST', session='WEEKEND')\n",
        "                agent.record_trade(outcome)\n",
        "                trades_learned += 1\n",
        "                trades_by_pair[pair] += 1\n",
        "            except Exception as e:\n",
        "                errors_encountered += 1\n",
        "                if errors_encountered <= 5: log(f\"‚ö†Ô∏è Backtest error at index {i}: {e}\", \"warn\")\n",
        "                continue\n",
        "    log(f\"\\nüìä Weekend Backtest Results by Pair:\", \"brain\")\n",
        "    for pair, count in trades_by_pair.items():\n",
        "        if count > 0: log(f\"  {pair}: {count} trades\", \"brain\")\n",
        "    log(f\"‚úÖ Weekend backtest: {trades_learned} trades learned\", \"success\")\n",
        "    if errors_encountered > 0: log(f\"‚ö†Ô∏è Encountered {errors_encountered} errors during backtest\", \"warn\")\n",
        "    if trades_learned > 0:\n",
        "        recent_exp = list(agent.memory)[-trades_learned:]\n",
        "        wins = sum(1 for exp in recent_exp if exp.metadata.get('pnl', 0) > 0)\n",
        "        win_rate = (wins / trades_learned) * 100\n",
        "        avg_reward = sum(exp.reward for exp in recent_exp) / trades_learned\n",
        "        total_pnl = sum(exp.metadata.get('pnl', 0) for exp in recent_exp)\n",
        "        log(f\"üìä Backtest Performance:\", \"brain\")\n",
        "        log(f\"  Wins: {wins}/{trades_learned} ({win_rate:.1f}%)\", \"brain\")\n",
        "        log(f\"  Avg Reward: {avg_reward:.2f}\", \"brain\")\n",
        "        log(f\"  Total P&L: ${total_pnl:.5f}\", \"brain\")\n",
        "    return trades_learned\n",
        "\n",
        "def fetch_price(pair, timeout=10):\n",
        "    if not BROWSERLESS_TOKEN: return None\n",
        "    try:\n",
        "        fc, tc = pair.split(\"/\")\n",
        "        url = f\"https://production-sfo.browserless.io/content?token={BROWSERLESS_TOKEN}\"\n",
        "        r = requests.post(url, json={\"url\": f\"https://www.x-rates.com/calculator/?from={fc}&to={tc}&amount=1\"}, timeout=timeout)\n",
        "        m = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', r.text)\n",
        "        return float(m.group(1).replace(\",\", \"\")) if m else None\n",
        "    except: return None\n",
        "\n",
        "def ensure_atr(df):\n",
        "    if \"atr\" in df.columns and df[\"atr\"].median() > MIN_ATR:\n",
        "        return df.assign(atr=df[\"atr\"].fillna(MIN_ATR).clip(lower=MIN_ATR))\n",
        "    high, low, close = df[\"high\"].values, df[\"low\"].values, df[\"close\"].values\n",
        "    tr = np.maximum.reduce([high - low, np.abs(high - np.roll(close, 1)), np.abs(low - np.roll(close, 1))])\n",
        "    tr[0] = high[0] - low[0] if len(tr) > 0 else MIN_ATR\n",
        "    df[\"atr\"] = pd.Series(tr, index=df.index).rolling(ATR_PERIOD, min_periods=1).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "    return df\n",
        "\n",
        "def update_pickle_data():\n",
        "    log(\"üîÑ Updating pickle data...\", \"info\")\n",
        "    updated_count = 0\n",
        "    for pair in PAIRS:\n",
        "        latest_price = fetch_price(pair)\n",
        "        if not latest_price or latest_price <= 0: continue\n",
        "        pair_key = pair.replace(\"/\", \"_\")\n",
        "        for pkl_file in PICKLE_FOLDER.glob(f\"{pair_key}*.pkl\"):\n",
        "            if any(x in pkl_file.name for x in ['_model', 'indicator_cache', '.bak']): continue\n",
        "            try:\n",
        "                try: df = pd.read_pickle(pkl_file, compression='gzip')\n",
        "                except: df = pd.read_pickle(pkl_file, compression=None)\n",
        "                if not isinstance(df, pd.DataFrame) or len(df) < 10: continue\n",
        "                if not all(c in df.columns for c in ['open', 'high', 'low', 'close']): continue\n",
        "                last_time = df.index[-1]\n",
        "                new_time = datetime.now().replace(second=0, microsecond=0)\n",
        "                if new_time > last_time:\n",
        "                    new_row = pd.DataFrame({'open': [float(latest_price)], 'high': [float(latest_price)],\n",
        "                        'low': [float(latest_price)], 'close': [float(latest_price)], 'volume': [0]}, index=[new_time])\n",
        "                    df = pd.concat([df, new_row]).tail(5000).ffill().bfill()\n",
        "                    df = ensure_atr(df)\n",
        "                    df.to_pickle(pkl_file, compression='gzip')\n",
        "                    updated_count += 1\n",
        "            except: pass\n",
        "    log(f\"‚úÖ Updated {updated_count} files\", \"success\")\n",
        "    return updated_count\n",
        "\n",
        "def load_data(folder):\n",
        "    log(f\"üìÇ Loading data from: {folder}\", \"info\")\n",
        "    if not folder.exists(): return {}\n",
        "    all_pkl = [p for p in folder.glob(\"*.pkl\") if not any(s in p.name for s in ['_model', 'indicator_cache', '.bak'])]\n",
        "    pair_files = defaultdict(list)\n",
        "    currencies = [\"EUR\", \"GBP\", \"USD\", \"AUD\", \"NZD\", \"CAD\", \"CHF\", \"JPY\"]\n",
        "    for pkl in all_pkl:\n",
        "        parts = pkl.stem.split('_')\n",
        "        if len(parts) >= 2 and parts[0] in currencies and parts[1] in currencies:\n",
        "            pair_files[f\"{parts[0]}_{parts[1]}\"].append(pkl)\n",
        "    combined = {}\n",
        "    for pk, files in pair_files.items():\n",
        "        pair = f\"{pk[:3]}/{pk[4:]}\"\n",
        "        if pair not in PAIRS: continue\n",
        "        pair_data = {}\n",
        "        for pkl in files:\n",
        "            try:\n",
        "                try: df = pd.read_pickle(pkl, compression='gzip')\n",
        "                except: df = pd.read_pickle(pkl, compression=None)\n",
        "                if not isinstance(df, pd.DataFrame) or len(df) < 50: continue\n",
        "                if not all(c in df.columns for c in ['open', 'high', 'low', 'close']): continue\n",
        "                df = df.ffill().bfill().dropna(subset=['open', 'high', 'low', 'close'])\n",
        "                df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "                if df.index.tz: df.index = df.index.tz_localize(None)\n",
        "                df = df[df.index.notna()]\n",
        "                tf = \"1d\" if \"1d\" in pkl.stem or \"daily\" in pkl.stem else \"1h\"\n",
        "                if tf not in [\"1d\", \"1h\"]: continue\n",
        "                df = ensure_atr(df)\n",
        "                pair_data[tf] = df\n",
        "                log(f\"‚úÖ {pair} [{tf}]: {len(df)} rows\", \"success\")\n",
        "            except: pass\n",
        "        if pair_data: combined[pair] = pair_data\n",
        "    log(f\"‚úÖ Loaded {len(combined)} pairs\", \"success\")\n",
        "    return combined\n",
        "\n",
        "def send_email(signals, iteration, rl_stats, mode, pipeline_stats):\n",
        "    if not GMAIL_APP_PASSWORD:\n",
        "        log(\"‚ö†Ô∏è Email skipped: No password\", \"warn\")\n",
        "        return\n",
        "    try:\n",
        "        msg = MIMEMultipart('alternative')\n",
        "        msg['Subject'] = f\"üß† BEACON v18.1 [{mode}] - Iter #{iteration}\"\n",
        "        msg['From'] = GMAIL_USER\n",
        "        msg['To'] = GMAIL_USER\n",
        "        active_signals = sum(1 for s in signals.values() if s.get('direction') != 'HOLD')\n",
        "        epsilon = rl_stats.get('epsilon_history', [EPSILON_START])[-1] if rl_stats.get('epsilon_history') else EPSILON_START\n",
        "        mode_badge = \"WEEKEND LEARNING\" if mode == \"WEEKEND_LEARNING\" else \"LIVE TRADING\"\n",
        "        mode_color = \"#f59e0b\" if mode == \"WEEKEND_LEARNING\" else \"#10b981\"\n",
        "        html = f\"\"\"<!DOCTYPE html><html><head><style>\n",
        "body{{font-family:-apple-system,sans-serif;background:#0f172a;margin:0;padding:20px}}\n",
        ".container{{max-width:1000px;margin:0 auto;background:white;border-radius:12px;box-shadow:0 10px 40px rgba(0,0,0,0.4)}}\n",
        ".header{{background:linear-gradient(135deg,#7c3aed,#4c1d95);color:white;padding:50px;text-align:center}}\n",
        ".header h1{{margin:0;font-size:38px;font-weight:900}}\n",
        ".mode-badge{{background:{mode_color};padding:12px 24px;border-radius:30px;margin-top:18px;font-weight:800}}\n",
        ".stats{{background:#fef3c7;padding:25px;margin:25px;border-radius:10px}}\n",
        ".stat-grid{{display:grid;grid-template-columns:repeat(auto-fit,minmax(150px,1fr));gap:15px}}\n",
        ".stat-item{{background:white;padding:15px;border-radius:8px;text-align:center}}\n",
        ".stat-value{{font-size:28px;font-weight:900;color:#7c3aed}}\n",
        ".stat-label{{font-size:12px;color:#6b7280;margin-top:5px}}\n",
        "</style></head><body><div class=\"container\">\n",
        "<div class=\"header\"><h1>üß† TRADE BEACON v18.1</h1><div class=\"mode-badge\">{mode_badge}</div>\n",
        "<p style=\"margin:20px 0 0\">Iteration #{iteration} | {datetime.now():%Y-%m-%d %H:%M UTC}</p></div>\n",
        "<div class=\"stats\"><div style=\"font-size:20px;font-weight:800;margin-bottom:15px\">üß† RL Agent Stats</div>\n",
        "<div class=\"stat-grid\">\n",
        "<div class=\"stat-item\"><div class=\"stat-value\">{rl_stats.get('total_trades',0)}</div><div class=\"stat-label\">TRADES</div></div>\n",
        "<div class=\"stat-item\"><div class=\"stat-value\">{rl_stats.get('win_rate',0)*100:.1f}%</div><div class=\"stat-label\">WIN RATE</div></div>\n",
        "<div class=\"stat-item\"><div class=\"stat-value\">${rl_stats.get('total_pnl',0):.2f}</div><div class=\"stat-label\">TOTAL P&L</div></div>\n",
        "<div class=\"stat-item\"><div class=\"stat-value\">{epsilon:.3f}</div><div class=\"stat-label\">EPSILON</div></div>\n",
        "<div class=\"stat-item\"><div class=\"stat-value\">{active_signals}</div><div class=\"stat-label\">SIGNALS</div></div>\n",
        "<div class=\"stat-item\"><div class=\"stat-value\">{rl_stats.get('pipeline_trades_learned',0)}</div><div class=\"stat-label\">PIPELINE</div></div>\n",
        "</div></div></div></body></html>\"\"\"\n",
        "        msg.attach(MIMEText(html, 'html'))\n",
        "        with smtplib.SMTP_SSL('smtp.gmail.com', 465, timeout=30) as srv:\n",
        "            srv.login(GMAIL_USER, GMAIL_APP_PASSWORD)\n",
        "            srv.send_message(msg)\n",
        "        log(f\"‚úÖ Email sent\", \"success\")\n",
        "    except Exception as e:\n",
        "        log(f\"‚ùå Email failed: {e}\", \"error\")\n",
        "\n",
        "def push_git(files, msg):\n",
        "    if IN_GHA or not FOREX_PAT: return False\n",
        "    try:\n",
        "        REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "        repo_path = SAVE_FOLDER if (SAVE_FOLDER / \".git\").exists() else BASE_FOLDER\n",
        "        if not (repo_path / \".git\").exists():\n",
        "            subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_path)], capture_output=True, timeout=60, check=True)\n",
        "        os.chdir(repo_path)\n",
        "        for f in files:\n",
        "            if (repo_path / f).exists(): subprocess.run([\"git\", \"add\", str(f)], check=False)\n",
        "        subprocess.run([\"git\", \"commit\", \"-m\", msg], capture_output=True, check=False)\n",
        "        subprocess.run([\"git\", \"pull\", \"--rebase\", \"origin\", \"main\"], capture_output=True, check=False)\n",
        "        result = subprocess.run([\"git\", \"push\", \"origin\", \"main\"], capture_output=True, timeout=30)\n",
        "        return result.returncode == 0\n",
        "    except: return False\n",
        "    finally:\n",
        "        try: os.chdir(SAVE_FOLDER)\n",
        "        except: pass\n",
        "\n",
        "def print_diagnostics(agent: ImprovedRLAgent, trade_history: list):\n",
        "    log(\"\\n\" + \"=\"*70, \"brain\")\n",
        "    log(\"üìä RL AGENT DIAGNOSTICS\", \"brain\")\n",
        "    log(\"=\"*70, \"brain\")\n",
        "    stats = agent.stats\n",
        "    log(f\"\\nüéØ Overall Performance:\", \"brain\")\n",
        "    log(f\"  Total Trades: {stats['total_trades']}\", \"info\")\n",
        "    log(f\"  Win Rate: {stats['win_rate']*100:.1f}%\", \"info\")\n",
        "    log(f\"  Total P&L: ${stats['total_pnl']:.2f}\", \"money\")\n",
        "    log(f\"  Avg Reward: {stats['avg_reward']:.2f}\", \"info\")\n",
        "    log(f\"  Profitable Trades: {stats['profitable_trades']}\", \"success\")\n",
        "    log(f\"\\nüß† Learning Status:\", \"brain\")\n",
        "    log(f\"  Current Epsilon: {agent.epsilon:.4f}\", \"info\")\n",
        "    log(f\"  Exploration Rate: {agent.epsilon*100:.1f}%\", \"info\")\n",
        "    log(f\"  Total Updates: {stats['total_updates']}\", \"info\")\n",
        "    log(f\"  Memory: {len(agent.memory)}/{MEMORY_SIZE} ({len(agent.memory)/MEMORY_SIZE*100:.1f}%)\", \"info\")\n",
        "    if stats.get('q_value_history'):\n",
        "        recent_q = stats['q_value_history'][-100:]\n",
        "        log(f\"  Q-Values: Œº={np.mean(recent_q):.4f}, œÉ={np.std(recent_q):.4f}\", \"info\")\n",
        "    log(f\"\\nüí° Health Check:\", \"brain\")\n",
        "    if stats['win_rate'] < 0.25: log(\"  ‚ö†Ô∏è Win rate < 25% - Continue training\", \"warn\")\n",
        "    elif stats['win_rate'] < 0.35: log(\"  ‚ö° Win rate 25-35% - Getting better\", \"warn\")\n",
        "    else: log(\"  ‚úÖ Win rate > 35% - Ready for live\", \"success\")\n",
        "    if stats['total_pnl'] < -50: log(\"  ‚ö†Ô∏è Significant losses - Review strategy\", \"warn\")\n",
        "    elif stats['total_pnl'] < 0: log(\"  ‚ö° Negative P&L - Continue learning\", \"warn\")\n",
        "    else: log(\"  ‚úÖ Positive P&L - Strategy profitable\", \"success\")\n",
        "    if stats['total_trades'] < 500: log(\"  ‚ö†Ô∏è < 500 trades - Need more data\", \"warn\")\n",
        "    elif stats['total_trades'] < 1000: log(\"  ‚ö° 500-1000 trades - Building foundation\", \"warn\")\n",
        "    else: log(\"  ‚úÖ > 1000 trades - Good experience\", \"success\")\n",
        "    log(\"=\"*70, \"brain\")\n",
        "\n",
        "def main():\n",
        "    log(\"=\" * 70, \"rocket\")\n",
        "    log(\"üß† TRADE BEACON v18.1 - FIXED WEEKEND LEARNING\", \"brain\")\n",
        "    log(\"=\" * 70, \"rocket\")\n",
        "    mode = get_weekend_mode()\n",
        "    log(f\"üìÖ Mode: {mode}\", \"info\")\n",
        "    iteration = increment_iteration()\n",
        "    agent = ImprovedRLAgent()\n",
        "    env = TradingEnvironment()\n",
        "    confidence_system = ImprovedConfidence()\n",
        "    pipeline_db = PipelineDatabase()\n",
        "    try:\n",
        "        log(f\"\\nüìä Iteration #{iteration} | {ENV_NAME} | {mode}\", \"info\")\n",
        "        if mode == \"LIVE_TRADING\": update_pickle_data()\n",
        "        data = load_data(PICKLE_FOLDER)\n",
        "        if not data: raise ValueError(\"‚ùå No data loaded\")\n",
        "        if pipeline_db.conn:\n",
        "            pipeline_trades_learned = agent.learn_from_pipeline_trades(pipeline_db, data)\n",
        "            pipeline_stats = pipeline_db.get_pipeline_stats()\n",
        "            log(f\"\\nüíæ Pipeline Stats:\", \"database\")\n",
        "            log(f\"  Total Trades: {pipeline_stats.get('total_trades', 0)}\", \"database\")\n",
        "            log(f\"  Win Rate: {pipeline_stats.get('win_rate', 0):.1f}%\", \"database\")\n",
        "            log(f\"  Total P&L: ${pipeline_stats.get('total_pnl', 0.0):.5f}\", \"database\")\n",
        "            log(f\"  Trades Learned by RL: {agent.stats.get('pipeline_trades_learned', 0)}\", \"database\")\n",
        "        else:\n",
        "            pipeline_trades_learned = 0\n",
        "            pipeline_stats = {}\n",
        "            log(\"‚ö†Ô∏è Pipeline database not available\", \"warn\")\n",
        "        if mode == \"WEEKEND_LEARNING\":\n",
        "            backtest_trades = run_weekend_backtest(data, agent, confidence_system)\n",
        "            log(f\"üéì Weekend: {backtest_trades} backtest + {pipeline_trades_learned} pipeline = {backtest_trades + pipeline_trades_learned} total\", \"brain\")\n",
        "        log(\"\\nüíπ Fetching prices...\", \"info\")\n",
        "        current_prices = {}\n",
        "        for pair in PAIRS:\n",
        "            if mode == \"WEEKEND_LEARNING\":\n",
        "                if pair in data and '1h' in data[pair]:\n",
        "                    price = data[pair]['1h'].iloc[-1]['close']\n",
        "                    current_prices[pair] = price\n",
        "            else:\n",
        "                price = fetch_price(pair)\n",
        "                if not price and pair in data and '1h' in data[pair]:\n",
        "                    price = data[pair]['1h'].iloc[-1]['close']\n",
        "                if price: current_prices[pair] = price\n",
        "        log(\"\\nüîç Checking trades...\", \"info\")\n",
        "        completed_trades = env.check_exits(current_prices)\n",
        "        if completed_trades:\n",
        "            log(f\"\\nüéì Learning from {len(completed_trades)} completed trades...\", \"brain\")\n",
        "            for trade_outcome in completed_trades: agent.record_trade(trade_outcome)\n",
        "        log(\"\\nüß† Generating signals...\", \"brain\")\n",
        "        signals = {}\n",
        "        for pair in PAIRS:\n",
        "            if pair not in data or '1h' not in data[pair] or '1d' not in data[pair]:\n",
        "                signals[pair] = {'direction': 'HOLD', 'last_price': current_prices.get(pair, 0)}\n",
        "                continue\n",
        "            state = create_state_vector(data[pair]['1h'], data[pair]['1d'], pair)\n",
        "            q_values = agent.q_network.predict(state)\n",
        "            should_trade, confidence, metrics = confidence_system.get_confidence(q_values, agent.epsilon)\n",
        "            best_action = np.argmax(q_values)\n",
        "            action_map = {0: 'BUY', 1: 'SELL', 2: 'HOLD'}\n",
        "            direction = action_map[best_action]\n",
        "            if not should_trade: direction = 'HOLD'\n",
        "            price = current_prices.get(pair, 0)\n",
        "            atr = data[pair]['1h']['atr'].iloc[-1]\n",
        "            if direction == 'BUY':\n",
        "                sl = price - (atr * ATR_SL_MULTIPLIER)\n",
        "                tp = price + (atr * ATR_TP_MULTIPLIER)\n",
        "            elif direction == 'SELL':\n",
        "                sl = price + (atr * ATR_SL_MULTIPLIER)\n",
        "                tp = price - (atr * ATR_TP_MULTIPLIER)\n",
        "            else: sl = tp = price\n",
        "            signals[pair] = {'direction': direction, 'last_price': price, 'SL': float(sl), 'TP': float(tp),\n",
        "                'confidence': confidence, 'threshold': metrics['threshold'],\n",
        "                'timestamp': datetime.now(timezone.utc).isoformat()}\n",
        "            log(f\"  {pair}: Q={q_values[best_action]:.3f}, Conf={confidence:.1f}%, Thresh={metrics['threshold']:.1f}%, {direction}\", \"brain\")\n",
        "            if direction != 'HOLD' and len(env.active_trades) < MAX_POSITIONS and mode == \"LIVE_TRADING\":\n",
        "                base_size = (BASE_CAPITAL * MAX_RISK_PER_TRADE) / (abs(price - sl) + EPS)\n",
        "                position_size = confidence_system.calculate_position_size(base_size, confidence)\n",
        "                position_size = min(position_size, MAX_TRADE_CAP / price)\n",
        "                env.execute_trade(pair, direction, price, sl, tp, position_size, state,\n",
        "                    {'confidence': confidence, 'regime': 'RL', 'session': 'LIVE'})\n",
        "        log(\"\\nüíæ Saving...\", \"info\")\n",
        "        output = {'timestamp': datetime.now(timezone.utc).isoformat(), 'iteration': iteration,\n",
        "            'version': 'v18.1-fixed-weekend-learning', 'mode': mode, 'signals': signals,\n",
        "            'rl_stats': agent.stats, 'active_trades': len(env.active_trades), 'pipeline_stats': pipeline_stats}\n",
        "        persistence.save_json(OMEGA_SIGNALS_FILE, output)\n",
        "        agent.save_state()\n",
        "        print_diagnostics(agent, env.trade_history)\n",
        "        if mode == \"LIVE_TRADING\": send_email(signals, iteration, agent.stats, mode, pipeline_stats)\n",
        "        else: log(\"üìß Email skipped (weekend mode)\", \"info\")\n",
        "        files = [f\"outputs/{OMEGA_SIGNALS_FILE.name}\", f\"omega_state/{OMEGA_ITERATION_FILE.name}\",\n",
        "            f\"rl_memory/{RL_MEMORY_FILE.name}\", f\"rl_memory/{RL_LEARNING_STATS_FILE.name}\",\n",
        "            f\"rl_memory/{TRADE_HISTORY_FILE.name}\", f\"rl_memory/{PIPELINE_SYNC_FILE.name}\",\n",
        "            f\"rl_memory/{RL_NETWORK_WEIGHTS_FILE.name}\"]\n",
        "        commit_msg = f\"üß† v18.1 #{iteration} [{mode}] WR={agent.stats['win_rate']*100:.1f}% P&L=${agent.stats['total_pnl']:.2f}\"\n",
        "        push_git(files, commit_msg)\n",
        "        log(\"\\n\" + \"=\" * 70, \"success\")\n",
        "        log(\"‚úÖ CYCLE COMPLETE\", \"success\")\n",
        "        log(\"=\" * 70, \"success\")\n",
        "        log(f\"Iteration: #{iteration} ({ENV_NAME})\", \"info\")\n",
        "        log(f\"Mode: {mode}\", \"info\")\n",
        "        log(f\"RL Trades: {agent.stats['total_trades']}\", \"brain\")\n",
        "        log(f\"Pipeline Trades: {agent.stats.get('pipeline_trades_learned', 0)}\", \"database\")\n",
        "        log(f\"Win Rate: {agent.stats['win_rate']*100:.1f}%\", \"info\")\n",
        "        log(f\"Total P&L: ${agent.stats['total_pnl']:.2f}\", \"money\")\n",
        "        log(f\"Active Trades: {len(env.active_trades)}\", \"info\")\n",
        "        log(f\"Epsilon: {agent.epsilon:.3f}\", \"info\")\n",
        "        log(f\"Memory: {len(agent.memory)} samples\", \"brain\")\n",
        "        if agent.stats['total_trades'] > 100:\n",
        "            if agent.stats['win_rate'] >= 0.35: log(\"\\n‚úÖ Agent ready for cautious live trading\", \"success\")\n",
        "            elif agent.stats['win_rate'] >= 0.25: log(\"\\n‚ö° Continue weekend training before live\", \"warn\")\n",
        "            else: log(\"\\n‚ö†Ô∏è More training needed\", \"warn\")\n",
        "    except Exception as e:\n",
        "        log(f\"\\n‚ùå Error: {e}\", \"error\")\n",
        "        logging.exception(\"Fatal error\")\n",
        "        raise\n",
        "    finally:\n",
        "        if pipeline_db.conn: pipeline_db.close()\n",
        "        log(f\"\\nüß† Cycle complete (Iteration #{iteration})\", \"brain\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "0DZGRBKkZVWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}