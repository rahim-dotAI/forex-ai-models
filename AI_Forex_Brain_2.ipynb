{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eaVLBC20LX_"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üåç Notebook Initialization ‚Äî Colab + GitHub Actions + Local\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Detect Environment\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "IN_LOCAL = not IN_COLAB and not IN_GHA\n",
        "\n",
        "ENV_NAME = \"Colab\" if IN_COLAB else \"GitHub Actions\" if IN_GHA else \"Local\"\n",
        "print(f\"üîç Detected environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Safe Working Folder (Auto-Switch)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    BASE_DIR = Path(\"/content\")\n",
        "elif IN_GHA:\n",
        "    BASE_DIR = Path(\"/home/runner/work\")\n",
        "else:\n",
        "    BASE_DIR = Path(\".\")\n",
        "\n",
        "REPO_NAME = \"forex-ai-models\"  # Updated repo name\n",
        "SAVE_FOLDER = BASE_DIR / REPO_NAME\n",
        "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(SAVE_FOLDER)\n",
        "print(f\"‚úÖ Working directory set to: {SAVE_FOLDER.resolve()}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Git Configuration (Universal)\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], check=False)\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Tokens & Secrets\n",
        "# ======================================================\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "BROWSERLESS_TOKEN = os.environ.get(\"BROWSERLESS_TOKEN\")\n",
        "\n",
        "# Load Colab secrets if missing\n",
        "if IN_COLAB and not FOREX_PAT:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get('FOREX_PAT')\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è No Colab secret found for FOREX_PAT\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    print(\"‚ö†Ô∏è FOREX_PAT not found ‚Äî GitHub cloning may fail.\")\n",
        "if not BROWSERLESS_TOKEN:\n",
        "    print(\"‚ö†Ô∏è BROWSERLESS_TOKEN not found.\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Output Folders\n",
        "# ======================================================\n",
        "CSV_FOLDER = SAVE_FOLDER / \"csvs\"\n",
        "PICKLE_FOLDER = SAVE_FOLDER / \"pickles\"\n",
        "LOGS_FOLDER = SAVE_FOLDER / \"logs\"\n",
        "\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Output folders ready:\")\n",
        "print(f\"   ‚Ä¢ CSVs:    {CSV_FOLDER}\")\n",
        "print(f\"   ‚Ä¢ Pickles: {PICKLE_FOLDER}\")\n",
        "print(f\"   ‚Ä¢ Logs:    {LOGS_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Environment Debug Info\n",
        "# ======================================================\n",
        "print(f\"Python version: {sys.version.split()[0]}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"Directory contents: {os.listdir('.')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oih6CDfjAjG9"
      },
      "outputs": [],
      "source": [
        "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn river\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JS9qXRF_JXJO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your keys (only for this session)\n",
        "os.environ['ALPHA_VANTAGE_KEY'] = '1W58NPZXOG5SLHZ6'\n",
        "os.environ['BROWSERLESS_TOKEN'] = '2TMVUBAjFwrr7Tb283f0da6602a4cb698b81778bda61967f7'\n",
        "\n",
        "# Test if they work\n",
        "print(\"Alpha Vantage Key:\", os.environ.get('ALPHA_VANTAGE_KEY'))\n",
        "print(\"Browserless Token:\", os.environ.get('BROWSERLESS_TOKEN'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfQDCpi612f3"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# ‚ö° Full Colab-ready GitHub Sync + Remove LFS\n",
        "# ======================================================\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import urllib.parse\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Environment / Paths\n",
        "# -----------------------------\n",
        "REPO_PARENT = Path(\"/content/forex-automation\")\n",
        "REPO_PARENT.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(REPO_PARENT)\n",
        "\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "REPO_FOLDER = REPO_PARENT / GITHUB_REPO\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ GitHub Token\n",
        "# -----------------------------\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "if not FOREX_PAT:\n",
        "    from google.colab import userdata\n",
        "    FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "    if FOREX_PAT:\n",
        "        os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "        print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå Missing FOREX_PAT. Set it in Colab userdata or GitHub secrets.\")\n",
        "\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Clean old repo\n",
        "# -----------------------------\n",
        "if REPO_FOLDER.exists():\n",
        "    print(f\"üóë Removing old repo: {REPO_FOLDER}\")\n",
        "    shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Clone repo safely (skip LFS)\n",
        "# -----------------------------\n",
        "print(\"üîó Cloning repo (skipping LFS)...\")\n",
        "env = os.environ.copy()\n",
        "env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
        "\n",
        "subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_FOLDER)], check=True, env=env)\n",
        "os.chdir(REPO_FOLDER)\n",
        "print(f\"‚úÖ Repo cloned successfully into {REPO_FOLDER}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Uninstall LFS and convert files\n",
        "# -----------------------------\n",
        "print(\"‚öôÔ∏è Removing Git LFS and converting files...\")\n",
        "subprocess.run([\"git\", \"lfs\", \"uninstall\"], check=True)\n",
        "subprocess.run([\"git\", \"lfs\", \"migrate\", \"export\", \"--include=*.csv\"], check=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Configure Git user\n",
        "# -----------------------------\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], check=True)\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Stage, commit, push\n",
        "# -----------------------------\n",
        "subprocess.run([\"git\", \"add\", \"-A\"], check=True)\n",
        "status = subprocess.run([\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True)\n",
        "\n",
        "if status.stdout.strip():\n",
        "    subprocess.run([\"git\", \"commit\", \"-m\", \"Remove LFS and convert files to normal Git\"], check=True)\n",
        "    subprocess.run([\"git\", \"push\", \"origin\", BRANCH], check=True)\n",
        "    print(\"üöÄ Repo updated: LFS removed permanently.\")\n",
        "else:\n",
        "    print(\"‚úÖ No changes detected. LFS already removed.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Create standard output folders\n",
        "# -----------------------------\n",
        "for folder in [\"csvs\", \"pickles\", \"logs\"]:\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "print(\"üìÅ Output folders ready: csvs/, pickles/, logs/\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Summary\n",
        "# -----------------------------\n",
        "print(\"\\nüßæ Summary:\")\n",
        "print(f\"‚Ä¢ Working Directory: {os.getcwd()}\")\n",
        "print(f\"‚Ä¢ Repository: https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}\")\n",
        "print(\"‚úÖ All operations completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wKFLKkgP1aBP"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üöÄ FULLY FIXED ALPHA VANTAGE FX WORKFLOW\n",
        "# - Uses URL-safe PAT\n",
        "# - Loads from Colab secrets\n",
        "# - Cleans stale repo + skips LFS\n",
        "# - GitHub Actions + Colab Safe\n",
        "# ======================================================\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import requests\n",
        "import subprocess\n",
        "import threading\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Detect Environment\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "print(f\"Detected environment: {'Colab' if IN_COLAB else 'GitHub/Local'}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Working directories\n",
        "# ======================================================\n",
        "BASE_FOLDER = Path(\"/content/forex-alpha-models\") if IN_COLAB else Path(\"./forex-alpha-models\")\n",
        "BASE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(BASE_FOLDER)\n",
        "\n",
        "PICKLE_FOLDER = BASE_FOLDER / \"pickles\"\n",
        "CSV_FOLDER = BASE_FOLDER / \"csvs\"\n",
        "LOG_FOLDER = BASE_FOLDER / \"logs\"\n",
        "\n",
        "for folder in [PICKLE_FOLDER, CSV_FOLDER, LOG_FOLDER]:\n",
        "    folder.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Working directory: {BASE_FOLDER.resolve()}\")\n",
        "print(f\"‚úÖ Output folders ready: {PICKLE_FOLDER}, {CSV_FOLDER}, {LOG_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GitHub Configuration\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "REPO_FOLDER = BASE_FOLDER / GITHUB_REPO\n",
        "\n",
        "# Load PAT from env or Colab userdata\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå Missing FOREX_PAT. Set it in Colab userdata or GitHub secrets.\")\n",
        "\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Safe Repo Clone / Sync\n",
        "# ======================================================\n",
        "if REPO_FOLDER.exists():\n",
        "    print(f\"üóë Removing old repo: {REPO_FOLDER}\")\n",
        "    shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "print(\"üîó Cloning repo (skipping LFS)...\")\n",
        "env = os.environ.copy()\n",
        "env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
        "\n",
        "subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)], check=True, env=env)\n",
        "os.chdir(REPO_FOLDER)\n",
        "print(f\"‚úÖ Repo cloned successfully into {REPO_FOLDER}\")\n",
        "\n",
        "# Configure Git identity\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], check=True)\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Alpha Vantage Setup\n",
        "# ======================================================\n",
        "ALPHA_VANTAGE_KEY = os.environ.get(\"ALPHA_VANTAGE_KEY\")\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    raise ValueError(\"‚ùå ALPHA_VANTAGE_KEY missing!\")\n",
        "\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "lock = threading.Lock()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def fetch_alpha_vantage_fx(pair, outputsize='full', max_retries=3, retry_delay=5):\n",
        "    base_url = 'https://www.alphavantage.co/query'\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "    params = {\n",
        "        'function': 'FX_DAILY',\n",
        "        'from_symbol': from_currency,\n",
        "        'to_symbol': to_currency,\n",
        "        'outputsize': outputsize,\n",
        "        'datatype': 'json',\n",
        "        'apikey': ALPHA_VANTAGE_KEY\n",
        "    }\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            r = requests.get(base_url, params=params, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "            if 'Time Series FX (Daily)' not in data:\n",
        "                raise ValueError(f\"Unexpected API response: {data}\")\n",
        "            ts = data['Time Series FX (Daily)']\n",
        "            df = pd.DataFrame(ts).T\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df.sort_index()\n",
        "            df = df.rename(columns={\n",
        "                '1. open': 'open',\n",
        "                '2. high': 'high',\n",
        "                '3. low': 'low',\n",
        "                '4. close': 'close'\n",
        "            }).astype(float)\n",
        "            df = ensure_tz_naive(df)\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed fetching {pair}: {e}\")\n",
        "            time.sleep(retry_delay)\n",
        "    print(f\"‚ùå Failed to fetch {pair} after {max_retries} retries\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Process Pairs for Unified CSV Pipeline\n",
        "# ======================================================\n",
        "def process_pair(pair):\n",
        "    filename = pair.replace(\"/\", \"_\") + \".csv\"\n",
        "    filepath = CSV_FOLDER / filename\n",
        "\n",
        "    if filepath.exists():\n",
        "        existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "    else:\n",
        "        existing_df = pd.DataFrame()\n",
        "\n",
        "    old_hash = file_hash(filepath)\n",
        "    new_df = fetch_alpha_vantage_fx(pair)\n",
        "    if new_df.empty:\n",
        "        return None, f\"No new data for {pair}\"\n",
        "\n",
        "    combined_df = pd.concat([existing_df, new_df]) if not existing_df.empty else new_df\n",
        "    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    with lock:\n",
        "        combined_df.to_csv(filepath)\n",
        "\n",
        "    new_hash = file_hash(filepath)\n",
        "    changed = old_hash != new_hash\n",
        "    print(f\"‚ÑπÔ∏è {pair} total rows: {len(combined_df)}\")\n",
        "    return str(filepath) if changed else None, f\"{pair} {'updated' if changed else 'no changes'}\"\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Execute All Pairs in Parallel\n",
        "# ======================================================\n",
        "changed_files = []\n",
        "tasks = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for pair in FX_PAIRS:\n",
        "        tasks.append(executor.submit(process_pair, pair))\n",
        "    for future in as_completed(tasks):\n",
        "        filepath, msg = future.result()\n",
        "        print(msg)\n",
        "        if filepath:\n",
        "            changed_files.append(filepath)\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Commit & Push Changes\n",
        "# ======================================================\n",
        "if changed_files:\n",
        "    print(f\"üöÄ Committing {len(changed_files)} updated files...\")\n",
        "    subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "    subprocess.run([\"git\", \"commit\", \"-m\", \"Update Alpha Vantage FX data\"], check=False)\n",
        "    subprocess.run([\"git\", \"push\", \"origin\", BRANCH], check=False)\n",
        "else:\n",
        "    print(\"‚úÖ No changes to commit.\")\n",
        "\n",
        "print(\"‚úÖ All FX pairs processed, saved, pushed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By3b3WMVJ-M3"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# FULLY IMPROVED FOREX DATA WORKFLOW - YFINANCE\n",
        "# Colab + GitHub Actions Safe, 403-Proof, Large History\n",
        "# ======================================================\n",
        "\n",
        "import os, time, hashlib, subprocess, shutil, threading\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Detect environment\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "IN_LOCAL = not IN_COLAB and not IN_GHA\n",
        "\n",
        "print(f\"Detected environment: {'Colab' if IN_COLAB else ('GitHub Actions' if IN_GHA else 'Local')}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Working directories\n",
        "# ======================================================\n",
        "BASE_DIR = Path(\"/content/forex-alpha-models\") if IN_COLAB else Path(\"./forex-alpha-models\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(BASE_DIR)\n",
        "\n",
        "PICKLE_FOLDER = BASE_DIR / \"pickles\"; PICKLE_FOLDER.mkdir(exist_ok=True)\n",
        "CSV_FOLDER = BASE_DIR / \"csvs\"; CSV_FOLDER.mkdir(exist_ok=True)\n",
        "LOG_FOLDER = BASE_DIR / \"logs\"; LOG_FOLDER.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Working directory: {BASE_DIR.resolve()}\")\n",
        "print(f\"‚úÖ Output folders ready: {PICKLE_FOLDER}, {CSV_FOLDER}, {LOG_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Git configuration\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT missing!\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
        "\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Clone or update repo safely\n",
        "# ======================================================\n",
        "REPO_FOLDER = BASE_DIR / GITHUB_REPO\n",
        "def ensure_repo_cloned(repo_url, repo_folder, branch=\"main\"):\n",
        "    repo_folder = Path(repo_folder)\n",
        "    tmp_folder = repo_folder.parent / (repo_folder.name + \"_tmp\")\n",
        "    if tmp_folder.exists(): shutil.rmtree(tmp_folder)\n",
        "    if not (repo_folder / \".git\").exists():\n",
        "        print(f\"üì• Cloning repo into {tmp_folder} ...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"-b\", branch, repo_url, str(tmp_folder)], check=True)\n",
        "        if repo_folder.exists(): shutil.rmtree(repo_folder)\n",
        "        tmp_folder.rename(repo_folder)\n",
        "    else:\n",
        "        print(\"üîÑ Repo exists, pulling latest...\")\n",
        "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"fetch\", \"origin\"], check=True)\n",
        "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"checkout\", branch], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"pull\", \"origin\", branch], check=False)\n",
        "    print(f\"‚úÖ Repo ready at {repo_folder.resolve()}\")\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "ensure_repo_cloned(REPO_URL, REPO_FOLDER, BRANCH)\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ FX pairs & timeframes\n",
        "# ======================================================\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "TIMEFRAMES = {\n",
        "    \"1d_5y\": (\"1d\", \"5y\"),\n",
        "    \"1h_2y\": (\"1h\", \"2y\"),\n",
        "    \"15m_60d\": (\"15m\", \"60d\"),\n",
        "    \"5m_1mo\": (\"5m\", \"1mo\"),\n",
        "    \"1m_7d\": (\"1m\", \"7d\")\n",
        "}\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Helper functions\n",
        "# ======================================================\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    if not filepath.exists(): return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"): md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    if df is None or df.empty: return df\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz: df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def merge_data(existing_df, new_df):\n",
        "    existing_df = ensure_tz_naive(existing_df)\n",
        "    new_df = ensure_tz_naive(new_df)\n",
        "    if existing_df.empty: return new_df\n",
        "    if new_df.empty: return existing_df\n",
        "    combined = pd.concat([existing_df, new_df])\n",
        "    combined = combined[~combined.index.duplicated(keep=\"last\")]\n",
        "    combined.sort_index(inplace=True)\n",
        "    return combined\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Worker function for pairs/timeframes\n",
        "# ======================================================\n",
        "def process_pair_tf(pair, tf_name, interval, period, max_retries=3, retry_delay=5):\n",
        "    symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
        "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
        "    filepath = REPO_FOLDER / filename\n",
        "\n",
        "    existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
        "    old_hash = file_hash(filepath)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            df = yf.download(symbol, interval=interval, period=period, progress=False, auto_adjust=False, threads=True)\n",
        "            if df.empty: raise ValueError(\"No data returned\")\n",
        "            df = df[[c for c in ['Open','High','Low','Close','Volume'] if c in df.columns]]\n",
        "            df.rename(columns=lambda x: x.lower(), inplace=True)\n",
        "            df = ensure_tz_naive(df)\n",
        "            combined_df = merge_data(existing_df, df)\n",
        "            combined_df.to_csv(filepath)\n",
        "            if old_hash != file_hash(filepath):\n",
        "                return f\"üìà Updated {pair} {tf_name}\", str(filepath)\n",
        "            return f\"‚úÖ No changes {pair} {tf_name}\", None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt+1}/{max_retries} failed for {pair} {tf_name}: {e}\")\n",
        "            if attempt < max_retries: time.sleep(retry_delay)\n",
        "            else: return f\"‚ùå Failed {pair} {tf_name}\", None\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Parallel execution\n",
        "# ======================================================\n",
        "changed_files = []\n",
        "tasks = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    for pair in FX_PAIRS:\n",
        "        for tf_name, (interval, period) in TIMEFRAMES.items():\n",
        "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period))\n",
        "\n",
        "for future in as_completed(tasks):\n",
        "    msg, filename = future.result()\n",
        "    print(msg)\n",
        "    if filename: changed_files.append(filename)\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Commit & push updates\n",
        "# ======================================================\n",
        "if changed_files:\n",
        "    print(f\"üöÄ Committing {len(changed_files)} updated files...\")\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"add\"] + changed_files, check=False)\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"commit\", \"-m\", \"Update YFinance FX data CSVs\"], check=False)\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"push\", \"origin\", BRANCH], check=False)\n",
        "else:\n",
        "    print(\"‚úÖ No changes detected, nothing to push.\")\n",
        "\n",
        "print(\"üéØ All FX pairs & timeframes processed safely with maximum historical rows!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# FX CSV Combine + Incremental Indicators Pipeline\n",
        "# Fully optimized for YFinance + Alpha Vantage\n",
        "# Thread-safe, timezone-safe, Git-push-safe, large dataset-ready\n",
        "# FIXED: Infinity handling before scaling\n",
        "# ======================================================\n",
        "\n",
        "import os, time, hashlib, subprocess, shutil\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Environment & folders\n",
        "# -----------------------------\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "ROOT_DIR = Path(\"/content/forex-alpha-models\") if IN_COLAB else Path(\".\")\n",
        "ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    levels = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\"}\n",
        "    print(f\"{levels.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Git configuration\n",
        "# -----------------------------\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Abdul Rahim\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT missing!\")\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Ensure repo exists\n",
        "# -----------------------------\n",
        "def ensure_repo():\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        if REPO_FOLDER.exists():\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "        print_status(f\"Cloning repo into {REPO_FOLDER}...\", \"info\")\n",
        "        subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)], check=True)\n",
        "    else:\n",
        "        print_status(\"Repo exists, pulling latest...\", \"info\")\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"fetch\", \"origin\"], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"checkout\", BRANCH], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH], check=False)\n",
        "        print_status(\"Repo synced successfully\", \"success\")\n",
        "ensure_repo()\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Helpers\n",
        "# -----------------------------\n",
        "def ensure_tz_naive(df):\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    df.index = df.index.tz_localize(None)\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath):\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def safe_numeric(df, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['open','high','low','close']\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df.replace([np.inf,-np.inf], np.nan, inplace=True)\n",
        "    df.dropna(subset=columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Incremental CSV combine\n",
        "# -----------------------------\n",
        "def combine_csv(csv_path):\n",
        "    target_file = REPO_FOLDER / csv_path.name\n",
        "    existing_df = ensure_tz_naive(pd.read_csv(target_file, index_col=0, parse_dates=True)) if target_file.exists() else pd.DataFrame()\n",
        "    new_df = ensure_tz_naive(pd.read_csv(csv_path, index_col=0, parse_dates=True))\n",
        "    combined_df = pd.concat([existing_df, new_df])\n",
        "    combined_df = combined_df[~combined_df.index.duplicated(keep=\"last\")]\n",
        "    combined_df.sort_index(inplace=True)\n",
        "    return combined_df, target_file\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Incremental indicators (FIXED)\n",
        "# -----------------------------\n",
        "def add_indicators_incremental(existing_df, combined_df):\n",
        "    new_rows = combined_df.loc[~combined_df.index.isin(existing_df.index)] if not existing_df.empty else combined_df\n",
        "    if new_rows.empty:\n",
        "        return None\n",
        "    new_rows = safe_numeric(new_rows)\n",
        "    new_rows.sort_index(inplace=True)\n",
        "\n",
        "    # Trend indicators\n",
        "    trend = {\n",
        "        'SMA_10': lambda d: ta.trend.sma_indicator(d['close'],10),\n",
        "        'SMA_50': lambda d: ta.trend.sma_indicator(d['close'],50),\n",
        "        'SMA_200': lambda d: ta.trend.sma_indicator(d['close'],200),\n",
        "        'EMA_10': lambda d: ta.trend.ema_indicator(d['close'],10),\n",
        "        'EMA_50': lambda d: ta.trend.ema_indicator(d['close'],50),\n",
        "        'EMA_200': lambda d: ta.trend.ema_indicator(d['close'],200),\n",
        "        'MACD': lambda d: ta.trend.macd(d['close']),\n",
        "        'MACD_signal': lambda d: ta.trend.macd_signal(d['close']),\n",
        "        'ADX': lambda d: ta.trend.adx(d['high'], d['low'], d['close'],14)\n",
        "    }\n",
        "    # Momentum indicators\n",
        "    momentum = {\n",
        "        'RSI_14': lambda d: ta.momentum.rsi(d['close'],14),\n",
        "        'StochRSI': lambda d: ta.momentum.stochrsi(d['close'],14),\n",
        "        'CCI': lambda d: ta.trend.cci(d['high'],d['low'],d['close'],20),\n",
        "        'ROC': lambda d: ta.momentum.roc(d['close'],12),\n",
        "        'Williams_%R': lambda d: WilliamsRIndicator(d['high'],d['low'],d['close'],14).williams_r()\n",
        "    }\n",
        "    # Volatility\n",
        "    volatility = {\n",
        "        'Bollinger_High': lambda d: ta.volatility.bollinger_hband(d['close'],20,2),\n",
        "        'Bollinger_Low': lambda d: ta.volatility.bollinger_lband(d['close'],20,2),\n",
        "        'ATR': lambda d: ta.volatility.average_true_range(d['high'],d['low'],d['close'],14),\n",
        "        'STDDEV_20': lambda d: d['close'].rolling(20).std()\n",
        "    }\n",
        "    # Volume-based\n",
        "    volume = {}\n",
        "    if 'volume' in new_rows.columns:\n",
        "        volume = {\n",
        "            'OBV': lambda d: ta.volume.on_balance_volume(d['close'],d['volume']),\n",
        "            'MFI': lambda d: ta.volume.money_flow_index(d['high'],d['low'],d['close'],d['volume'],14)\n",
        "        }\n",
        "\n",
        "    indicators = {**trend, **momentum, **volatility, **volume}\n",
        "    for name, func in indicators.items():\n",
        "        try:\n",
        "            new_rows[name] = func(new_rows)\n",
        "        except Exception:\n",
        "            new_rows[name] = np.nan\n",
        "\n",
        "    # Cross signals\n",
        "    if 'EMA_10' in new_rows.columns and 'EMA_50' in new_rows.columns:\n",
        "        new_rows['EMA_10_cross_EMA_50'] = (new_rows['EMA_10'] > new_rows['EMA_50']).astype(int)\n",
        "    if 'EMA_50' in new_rows.columns and 'EMA_200' in new_rows.columns:\n",
        "        new_rows['EMA_50_cross_EMA_200'] = (new_rows['EMA_50'] > new_rows['EMA_200']).astype(int)\n",
        "    if 'SMA_10' in new_rows.columns and 'SMA_50' in new_rows.columns:\n",
        "        new_rows['SMA_10_cross_SMA_50'] = (new_rows['SMA_10'] > new_rows['SMA_50']).astype(int)\n",
        "    if 'SMA_50' in new_rows.columns and 'SMA_200' in new_rows.columns:\n",
        "        new_rows['SMA_50_cross_SMA_200'] = (new_rows['SMA_50'] > new_rows['SMA_200']).astype(int)\n",
        "\n",
        "    # üîß CRITICAL FIX: Clean infinity/NaN values before scaling\n",
        "    numeric_cols = new_rows.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0 and not new_rows[numeric_cols].dropna(how='all').empty:\n",
        "        # Replace infinity values with NaN\n",
        "        new_rows[numeric_cols] = new_rows[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Forward fill NaN values, then backward fill, then fill remaining with 0\n",
        "        new_rows[numeric_cols] = new_rows[numeric_cols].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "\n",
        "        # Clip extreme values to a reasonable range (optional but recommended)\n",
        "        for col in numeric_cols:\n",
        "            if new_rows[col].std() > 0:  # Only clip if there's variation\n",
        "                mean_val = new_rows[col].mean()\n",
        "                std_val = new_rows[col].std()\n",
        "                lower_bound = mean_val - (5 * std_val)\n",
        "                upper_bound = mean_val + (5 * std_val)\n",
        "                new_rows[col] = new_rows[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "        # Now scale safely\n",
        "        scaler = MinMaxScaler()\n",
        "        try:\n",
        "            new_rows[numeric_cols] = scaler.fit_transform(new_rows[numeric_cols])\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Scaling warning: {e} - using original values\", \"warn\")\n",
        "            # If scaling still fails, just normalize manually\n",
        "            for col in numeric_cols:\n",
        "                col_min = new_rows[col].min()\n",
        "                col_max = new_rows[col].max()\n",
        "                if col_max > col_min:\n",
        "                    new_rows[col] = (new_rows[col] - col_min) / (col_max - col_min)\n",
        "\n",
        "    return new_rows\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Worker function\n",
        "# -----------------------------\n",
        "def process_csv_file(csv_file):\n",
        "    combined_df, target_file = combine_csv(csv_file)\n",
        "    existing_pickle = PICKLE_FOLDER / f\"{csv_file.stem}_indicators.pkl\"\n",
        "    existing_df = pd.read_pickle(existing_pickle) if existing_pickle.exists() else pd.DataFrame()\n",
        "\n",
        "    new_indicators = add_indicators_incremental(existing_df, combined_df)\n",
        "    if new_indicators is not None:\n",
        "        updated_df = pd.concat([existing_df, new_indicators]).sort_index()\n",
        "        with lock:\n",
        "            updated_df.to_pickle(existing_pickle, protocol=4)\n",
        "            combined_df.to_csv(target_file)\n",
        "        msg = f\"{csv_file.name} updated with {len(new_indicators)} new rows\"\n",
        "    else:\n",
        "        msg = f\"{csv_file.name} no new rows\"\n",
        "\n",
        "    total_rows = len(combined_df)\n",
        "    print_status(f\"{csv_file.name} total rows: {total_rows}\", \"info\")\n",
        "\n",
        "    return str(existing_pickle) if new_indicators is not None else None, msg\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Process all CSVs in parallel\n",
        "# -----------------------------\n",
        "csv_files = list(CSV_FOLDER.glob(\"*.csv\"))\n",
        "if not csv_files:\n",
        "    print_status(\"No CSVs found to process ‚Äî pipeline will skip\", \"warn\")\n",
        "\n",
        "changed_files = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=min(8, len(csv_files) or 1)) as executor:\n",
        "    futures = [executor.submit(process_csv_file, f) for f in csv_files]\n",
        "    for future in as_completed(futures):\n",
        "        file, msg = future.result()\n",
        "        print_status(msg, \"success\" if file else \"info\")\n",
        "        if file:\n",
        "            changed_files.append(file)\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Commit & push updates\n",
        "# -----------------------------\n",
        "if changed_files:\n",
        "    print_status(f\"Committing {len(changed_files)} updated files...\", \"info\")\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"add\"] + changed_files, check=False)\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"commit\", \"-m\", \"üìà Auto update FX CSVs & indicators\"], check=False)\n",
        "    push_cmd = f\"git -C {REPO_FOLDER} push {REPO_URL} {BRANCH}\"\n",
        "    for attempt in range(3):\n",
        "        if subprocess.run(push_cmd, shell=True).returncode == 0:\n",
        "            print_status(\"Push successful\", \"success\")\n",
        "            break\n",
        "        else:\n",
        "            print_status(f\"Push attempt {attempt+1} failed, retrying...\", \"warn\")\n",
        "            time.sleep(5)\n",
        "else:\n",
        "    print_status(\"No files changed ‚Äî skipping push\", \"info\")\n",
        "\n",
        "print_status(\"All CSVs combined, incremental indicators added, and Git updated successfully.\", \"success\")"
      ],
      "metadata": {
        "id": "GhxeIu8yMZ-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "VERSION 3.6 ‚Äì ULTRA-PERSISTENT SELF-LEARNING HYBRID FX PIPELINE (FIXED)\n",
        "========================================================================\n",
        "üöÄ CRITICAL FIXES:\n",
        "- ‚úÖ SQLite INDEX syntax fixed (was causing crash)\n",
        "- ‚úÖ Trades evaluated in NEXT iteration (after price has moved)\n",
        "- ‚úÖ All data persists in Git repo (survives GitHub Actions)\n",
        "- ‚úÖ Real accuracy tracking (not artificial 100%)\n",
        "- ‚úÖ Proper minimum trade age before evaluation (1+ hours)\n",
        "- ‚úÖ Learning system gets real performance data\n",
        "- ‚úÖ Database auto-commits to Git after each run\n",
        "\n",
        "NEW IMPROVEMENTS:\n",
        "- ‚úÖ Separate pending_trades and completed_trades tables\n",
        "- ‚úÖ Minimum age requirement for trade evaluation\n",
        "- ‚úÖ Better model performance comparison\n",
        "- ‚úÖ CSV and pickle files persist properly\n",
        "- ‚úÖ Proper SQLite index creation (separate statements)\n",
        "- ‚úÖ Enhanced error handling and validation\n",
        "\"\"\"\n",
        "\n",
        "import os, time, json, re, shutil, subprocess, pickle, filecmp, sqlite3\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import ta\n",
        "import logging\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from collections import defaultdict\n",
        "\n",
        "# ======================================================\n",
        "# 0Ô∏è‚É£ Logging & Environment\n",
        "# ======================================================\n",
        "ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=LOGS_FOLDER / \"pipeline.log\",\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
        ")\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    icons = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\",\"debug\":\"üêû\",\"error\":\"‚ùå\"}\n",
        "    getattr(logging, level if level != \"warn\" else \"warning\", logging.info)(msg)\n",
        "    print(f\"{icons.get(level,'‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# ======================================================\n",
        "# üÜï FIXED DATABASE - Stores in Git Repo\n",
        "# ======================================================\n",
        "PERSISTENT_DB = REPO_FOLDER / \"ml_persistent_memory.db\"\n",
        "\n",
        "class FixedTradeMemoryDatabase:\n",
        "    \"\"\"\n",
        "    FIXED VERSION - SQLite compatible database\n",
        "\n",
        "    IMPROVEMENTS:\n",
        "    - ‚úÖ Proper SQLite INDEX syntax (created separately)\n",
        "    - ‚úÖ Enhanced error handling\n",
        "    - ‚úÖ Transaction management\n",
        "    - ‚úÖ Data validation\n",
        "    - ‚úÖ Backup and recovery\n",
        "\n",
        "    EVALUATION FLOW:\n",
        "    - Stores trades at end of iteration N\n",
        "    - Evaluates them at start of iteration N+1 (after 1+ hours)\n",
        "    - All data stored in Git repo for persistence\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, db_path=PERSISTENT_DB):\n",
        "        self.db_path = db_path\n",
        "        self.conn = None\n",
        "        self.min_age_hours = 1  # Minimum hours before evaluation\n",
        "        self.initialize_database()\n",
        "\n",
        "    def initialize_database(self):\n",
        "        \"\"\"\n",
        "        Create database in Git repo (persists across runs)\n",
        "        FIXED: Proper SQLite syntax for indexes\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.conn = sqlite3.connect(str(self.db_path), timeout=30)\n",
        "            self.conn.execute(\"PRAGMA journal_mode=WAL\")  # Better concurrency\n",
        "            self.conn.execute(\"PRAGMA synchronous=NORMAL\")  # Faster writes\n",
        "            cursor = self.conn.cursor()\n",
        "\n",
        "            # ===== TABLE 1: Pending trades (waiting to be evaluated) =====\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS pending_trades (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    created_at TEXT NOT NULL,\n",
        "                    iteration INTEGER NOT NULL,\n",
        "                    pair TEXT NOT NULL,\n",
        "                    timeframe TEXT NOT NULL,\n",
        "                    sgd_prediction INTEGER,\n",
        "                    rf_prediction INTEGER,\n",
        "                    ensemble_prediction INTEGER,\n",
        "                    entry_price REAL NOT NULL,\n",
        "                    sl_price REAL NOT NULL,\n",
        "                    tp_price REAL NOT NULL,\n",
        "                    confidence REAL,\n",
        "                    evaluated BOOLEAN DEFAULT 0\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Create indexes separately (SQLite proper syntax)\n",
        "            cursor.execute('''\n",
        "                CREATE INDEX IF NOT EXISTS idx_pending_eval\n",
        "                ON pending_trades(evaluated, created_at)\n",
        "            ''')\n",
        "\n",
        "            cursor.execute('''\n",
        "                CREATE INDEX IF NOT EXISTS idx_pending_pair\n",
        "                ON pending_trades(pair, evaluated)\n",
        "            ''')\n",
        "\n",
        "            # ===== TABLE 2: Completed trades (historical results) =====\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS completed_trades (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    pending_trade_id INTEGER,\n",
        "                    created_at TEXT NOT NULL,\n",
        "                    evaluated_at TEXT NOT NULL,\n",
        "                    iteration_created INTEGER,\n",
        "                    iteration_evaluated INTEGER,\n",
        "                    pair TEXT NOT NULL,\n",
        "                    timeframe TEXT NOT NULL,\n",
        "                    model_used TEXT NOT NULL,\n",
        "                    entry_price REAL NOT NULL,\n",
        "                    exit_price REAL NOT NULL,\n",
        "                    sl_price REAL NOT NULL,\n",
        "                    tp_price REAL NOT NULL,\n",
        "                    prediction INTEGER,\n",
        "                    hit_tp BOOLEAN NOT NULL,\n",
        "                    pnl REAL NOT NULL,\n",
        "                    duration_hours REAL,\n",
        "                    FOREIGN KEY (pending_trade_id) REFERENCES pending_trades(id)\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Create indexes separately\n",
        "            cursor.execute('''\n",
        "                CREATE INDEX IF NOT EXISTS idx_completed_model\n",
        "                ON completed_trades(model_used, evaluated_at)\n",
        "            ''')\n",
        "\n",
        "            cursor.execute('''\n",
        "                CREATE INDEX IF NOT EXISTS idx_completed_pair\n",
        "                ON completed_trades(pair, model_used, evaluated_at)\n",
        "            ''')\n",
        "\n",
        "            cursor.execute('''\n",
        "                CREATE INDEX IF NOT EXISTS idx_completed_timestamp\n",
        "                ON completed_trades(evaluated_at)\n",
        "            ''')\n",
        "\n",
        "            # ===== TABLE 3: Model performance cache =====\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS model_stats_cache (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    updated_at TEXT NOT NULL,\n",
        "                    pair TEXT NOT NULL,\n",
        "                    model_name TEXT NOT NULL,\n",
        "                    days INTEGER NOT NULL,\n",
        "                    total_trades INTEGER DEFAULT 0,\n",
        "                    winning_trades INTEGER DEFAULT 0,\n",
        "                    accuracy_pct REAL DEFAULT 0.0,\n",
        "                    total_pnl REAL DEFAULT 0.0,\n",
        "                    avg_pnl REAL DEFAULT 0.0,\n",
        "                    UNIQUE(pair, model_name, days) ON CONFLICT REPLACE\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            cursor.execute('''\n",
        "                CREATE INDEX IF NOT EXISTS idx_stats_lookup\n",
        "                ON model_stats_cache(pair, model_name, days)\n",
        "            ''')\n",
        "\n",
        "            # ===== TABLE 4: Pipeline execution log =====\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS execution_log (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    timestamp TEXT NOT NULL,\n",
        "                    iteration INTEGER NOT NULL,\n",
        "                    status TEXT NOT NULL,\n",
        "                    trades_stored INTEGER DEFAULT 0,\n",
        "                    trades_evaluated INTEGER DEFAULT 0,\n",
        "                    duration_seconds REAL,\n",
        "                    error_message TEXT\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            self.conn.commit()\n",
        "            print_status(\"‚úÖ Fixed ML Database initialized (persists in Git)\", \"success\")\n",
        "\n",
        "            # Verify database integrity\n",
        "            self._verify_database_integrity()\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Database initialization failed: {e}\", \"error\")\n",
        "            raise\n",
        "\n",
        "    def _verify_database_integrity(self):\n",
        "        \"\"\"Verify database structure is correct\"\"\"\n",
        "        try:\n",
        "            cursor = self.conn.cursor()\n",
        "\n",
        "            # Check if tables exist\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT name FROM sqlite_master\n",
        "                WHERE type='table' AND name IN (\n",
        "                    'pending_trades', 'completed_trades',\n",
        "                    'model_stats_cache', 'execution_log'\n",
        "                )\n",
        "            \"\"\")\n",
        "\n",
        "            tables = [row[0] for row in cursor.fetchall()]\n",
        "            expected_tables = ['pending_trades', 'completed_trades',\n",
        "                             'model_stats_cache', 'execution_log']\n",
        "\n",
        "            for table in expected_tables:\n",
        "                if table in tables:\n",
        "                    print_status(f\"  ‚úì Table '{table}' exists\", \"debug\")\n",
        "                else:\n",
        "                    print_status(f\"  ‚úó Table '{table}' missing!\", \"error\")\n",
        "\n",
        "            # Check indexes\n",
        "            cursor.execute(\"\"\"\n",
        "                SELECT name FROM sqlite_master\n",
        "                WHERE type='index' AND name LIKE 'idx_%'\n",
        "            \"\"\")\n",
        "\n",
        "            indexes = [row[0] for row in cursor.fetchall()]\n",
        "            print_status(f\"  üìä Found {len(indexes)} indexes\", \"debug\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Database verification warning: {e}\", \"warn\")\n",
        "\n",
        "    def store_new_signals(self, aggregated_signals, current_iteration):\n",
        "        \"\"\"\n",
        "        Store signals at END of current iteration.\n",
        "        They will be evaluated in NEXT iteration.\n",
        "\n",
        "        Args:\n",
        "            aggregated_signals: Dict of signals by pair\n",
        "            current_iteration: Current iteration number\n",
        "\n",
        "        Returns:\n",
        "            int: Number of signals stored\n",
        "        \"\"\"\n",
        "        if not aggregated_signals:\n",
        "            print_status(\"‚ö†Ô∏è No signals to store\", \"warn\")\n",
        "            return 0\n",
        "\n",
        "        cursor = self.conn.cursor()\n",
        "        stored_count = 0\n",
        "        failed_count = 0\n",
        "\n",
        "        try:\n",
        "            # Start transaction\n",
        "            cursor.execute(\"BEGIN TRANSACTION\")\n",
        "\n",
        "            for pair, pair_data in aggregated_signals.items():\n",
        "                signals = pair_data.get('signals', {})\n",
        "\n",
        "                for tf_name, signal_data in signals.items():\n",
        "                    if not signal_data:\n",
        "                        continue\n",
        "\n",
        "                    # Validate required fields\n",
        "                    required_fields = ['live', 'SL', 'TP']\n",
        "                    if not all(signal_data.get(f, 0) > 0 for f in required_fields):\n",
        "                        print_status(\n",
        "                            f\"‚ö†Ô∏è Skipping invalid signal for {pair} {tf_name}\",\n",
        "                            \"warn\"\n",
        "                        )\n",
        "                        failed_count += 1\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        cursor.execute('''\n",
        "                            INSERT INTO pending_trades\n",
        "                            (created_at, iteration, pair, timeframe,\n",
        "                             sgd_prediction, rf_prediction, ensemble_prediction,\n",
        "                             entry_price, sl_price, tp_price, confidence)\n",
        "                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                        ''', (\n",
        "                            datetime.now(timezone.utc).isoformat(),\n",
        "                            current_iteration,\n",
        "                            pair,\n",
        "                            tf_name,\n",
        "                            signal_data.get('sgd_pred', 0),\n",
        "                            signal_data.get('rf_pred', 0),\n",
        "                            signal_data.get('signal', 0),\n",
        "                            signal_data.get('live', 0),\n",
        "                            signal_data.get('SL', 0),\n",
        "                            signal_data.get('TP', 0),\n",
        "                            signal_data.get('confidence', 0.5)\n",
        "                        ))\n",
        "                        stored_count += 1\n",
        "                    except sqlite3.Error as e:\n",
        "                        print_status(f\"‚ö†Ô∏è Failed to store {pair} {tf_name}: {e}\", \"warn\")\n",
        "                        failed_count += 1\n",
        "\n",
        "            # Commit transaction\n",
        "            self.conn.commit()\n",
        "\n",
        "            # Log execution\n",
        "            cursor.execute('''\n",
        "                INSERT INTO execution_log\n",
        "                (timestamp, iteration, status, trades_stored)\n",
        "                VALUES (?, ?, 'signals_stored', ?)\n",
        "            ''', (\n",
        "                datetime.now(timezone.utc).isoformat(),\n",
        "                current_iteration,\n",
        "                stored_count\n",
        "            ))\n",
        "            self.conn.commit()\n",
        "\n",
        "            print_status(\n",
        "                f\"üíæ Stored {stored_count} trades for next iteration \"\n",
        "                f\"({failed_count} failed)\",\n",
        "                \"success\"\n",
        "            )\n",
        "            return stored_count\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            self.conn.rollback()\n",
        "            print_status(f\"‚ùå Transaction failed: {e}\", \"error\")\n",
        "            return 0\n",
        "\n",
        "    def evaluate_pending_trades(self, current_prices, current_iteration):\n",
        "        \"\"\"\n",
        "        Evaluate trades from PREVIOUS iterations.\n",
        "        Only evaluates trades older than min_age_hours.\n",
        "\n",
        "        Args:\n",
        "            current_prices: Dict of current prices by pair\n",
        "            current_iteration: Current iteration number\n",
        "\n",
        "        Returns:\n",
        "            dict: Evaluation results by model\n",
        "        \"\"\"\n",
        "        if not current_prices:\n",
        "            print_status(\"‚ö†Ô∏è No current prices provided\", \"warn\")\n",
        "            return {}\n",
        "\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        # Get unevaluated trades that are OLD ENOUGH\n",
        "        min_age = (datetime.now(timezone.utc) - timedelta(hours=self.min_age_hours)).isoformat()\n",
        "\n",
        "        try:\n",
        "            cursor.execute('''\n",
        "                SELECT id, pair, timeframe, sgd_prediction, rf_prediction,\n",
        "                       ensemble_prediction, entry_price, sl_price, tp_price,\n",
        "                       created_at, iteration\n",
        "                FROM pending_trades\n",
        "                WHERE evaluated = 0 AND created_at < ?\n",
        "                ORDER BY created_at ASC\n",
        "            ''', (min_age,))\n",
        "\n",
        "            pending_trades = cursor.fetchall()\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Failed to fetch pending trades: {e}\", \"error\")\n",
        "            return {}\n",
        "\n",
        "        if not pending_trades:\n",
        "            print_status(\n",
        "                f\"‚ÑπÔ∏è No trades old enough to evaluate (need {self.min_age_hours}+ hours)\",\n",
        "                \"info\"\n",
        "            )\n",
        "            return {}\n",
        "\n",
        "        print_status(\n",
        "            f\"üîç Evaluating {len(pending_trades)} trades from previous iteration(s)\",\n",
        "            \"info\"\n",
        "        )\n",
        "\n",
        "        results_by_model = defaultdict(lambda: {\n",
        "            'closed_trades': 0,\n",
        "            'wins': 0,\n",
        "            'losses': 0,\n",
        "            'total_pnl': 0.0,\n",
        "            'trades': []\n",
        "        })\n",
        "\n",
        "        evaluated_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        try:\n",
        "            # Start transaction\n",
        "            cursor.execute(\"BEGIN TRANSACTION\")\n",
        "\n",
        "            for trade in pending_trades:\n",
        "                (trade_id, pair, timeframe, sgd_pred, rf_pred, ensemble_pred,\n",
        "                 entry_price, sl_price, tp_price, created_at, created_iteration) = trade\n",
        "\n",
        "                current_price = current_prices.get(pair, 0)\n",
        "\n",
        "                if current_price <= 0:\n",
        "                    print_status(f\"‚ö†Ô∏è No current price for {pair}, skipping\", \"warn\")\n",
        "                    skipped_count += 1\n",
        "                    continue\n",
        "\n",
        "                # Validate prices\n",
        "                if not self._validate_trade_prices(entry_price, sl_price, tp_price, current_price):\n",
        "                    print_status(\n",
        "                        f\"‚ö†Ô∏è Invalid prices for {pair} {timeframe}, skipping\",\n",
        "                        \"warn\"\n",
        "                    )\n",
        "                    skipped_count += 1\n",
        "                    continue\n",
        "\n",
        "                # Evaluate for each model that made a prediction\n",
        "                for model_name, prediction in [\n",
        "                    ('SGD', sgd_pred),\n",
        "                    ('RandomForest', rf_pred),\n",
        "                    ('Ensemble', ensemble_pred)\n",
        "                ]:\n",
        "                    if prediction is None:\n",
        "                        continue\n",
        "\n",
        "                    # Check if TP or SL was hit\n",
        "                    hit_tp, hit_sl, exit_price = self._evaluate_trade_outcome(\n",
        "                        prediction, current_price, tp_price, sl_price\n",
        "                    )\n",
        "\n",
        "                    # If trade closed, record result\n",
        "                    if exit_price:\n",
        "                        # Calculate P&L\n",
        "                        pnl = self._calculate_pnl(\n",
        "                            prediction, entry_price, exit_price\n",
        "                        )\n",
        "\n",
        "                        # Duration\n",
        "                        duration_hours = self._calculate_duration_hours(created_at)\n",
        "\n",
        "                        # Insert into completed trades\n",
        "                        try:\n",
        "                            cursor.execute('''\n",
        "                                INSERT INTO completed_trades\n",
        "                                (pending_trade_id, created_at, evaluated_at,\n",
        "                                 iteration_created, iteration_evaluated,\n",
        "                                 pair, timeframe, model_used, entry_price, exit_price,\n",
        "                                 sl_price, tp_price, prediction, hit_tp, pnl, duration_hours)\n",
        "                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                            ''', (\n",
        "                                trade_id, created_at, datetime.now(timezone.utc).isoformat(),\n",
        "                                created_iteration, current_iteration,\n",
        "                                pair, timeframe, model_name, entry_price, exit_price,\n",
        "                                sl_price, tp_price, prediction, hit_tp, pnl, duration_hours\n",
        "                            ))\n",
        "                        except sqlite3.Error as e:\n",
        "                            print_status(\n",
        "                                f\"‚ö†Ô∏è Failed to record completed trade: {e}\",\n",
        "                                \"warn\"\n",
        "                            )\n",
        "                            continue\n",
        "\n",
        "                        # Accumulate results\n",
        "                        results_by_model[model_name]['closed_trades'] += 1\n",
        "                        results_by_model[model_name]['total_pnl'] += pnl\n",
        "\n",
        "                        if hit_tp:\n",
        "                            results_by_model[model_name]['wins'] += 1\n",
        "                        else:\n",
        "                            results_by_model[model_name]['losses'] += 1\n",
        "\n",
        "                        results_by_model[model_name]['trades'].append({\n",
        "                            'pair': pair,\n",
        "                            'timeframe': timeframe,\n",
        "                            'pnl': pnl,\n",
        "                            'hit_tp': hit_tp\n",
        "                        })\n",
        "\n",
        "                        status = \"WIN ‚úÖ\" if hit_tp else \"LOSS ‚ùå\"\n",
        "                        print_status(\n",
        "                            f\"{status} {model_name}: {pair} {timeframe} \"\n",
        "                            f\"Entry={entry_price:.5f} Exit={exit_price:.5f} \"\n",
        "                            f\"P&L=${pnl:.5f} ({duration_hours:.1f}h)\",\n",
        "                            \"success\" if hit_tp else \"warn\"\n",
        "                        )\n",
        "\n",
        "                # Mark pending trade as evaluated\n",
        "                try:\n",
        "                    cursor.execute('''\n",
        "                        UPDATE pending_trades\n",
        "                        SET evaluated = 1\n",
        "                        WHERE id = ?\n",
        "                    ''', (trade_id,))\n",
        "                    evaluated_count += 1\n",
        "                except sqlite3.Error as e:\n",
        "                    print_status(f\"‚ö†Ô∏è Failed to mark trade as evaluated: {e}\", \"warn\")\n",
        "\n",
        "            # Commit transaction\n",
        "            self.conn.commit()\n",
        "\n",
        "            # Log execution\n",
        "            cursor.execute('''\n",
        "                INSERT INTO execution_log\n",
        "                (timestamp, iteration, status, trades_evaluated)\n",
        "                VALUES (?, ?, 'trades_evaluated', ?)\n",
        "            ''', (\n",
        "                datetime.now(timezone.utc).isoformat(),\n",
        "                current_iteration,\n",
        "                evaluated_count\n",
        "            ))\n",
        "            self.conn.commit()\n",
        "\n",
        "            print_status(\n",
        "                f\"‚úÖ Evaluated {evaluated_count} trades \"\n",
        "                f\"({skipped_count} skipped)\",\n",
        "                \"success\"\n",
        "            )\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            self.conn.rollback()\n",
        "            print_status(f\"‚ùå Evaluation transaction failed: {e}\", \"error\")\n",
        "            return {}\n",
        "\n",
        "        # Calculate accuracies\n",
        "        for model_name, results in results_by_model.items():\n",
        "            if results['closed_trades'] > 0:\n",
        "                results['accuracy'] = (results['wins'] / results['closed_trades']) * 100\n",
        "            else:\n",
        "                results['accuracy'] = 0.0\n",
        "\n",
        "        # Update model stats cache\n",
        "        self._update_stats_cache()\n",
        "\n",
        "        return dict(results_by_model)\n",
        "\n",
        "    def _validate_trade_prices(self, entry, sl, tp, current):\n",
        "        \"\"\"Validate that trade prices are reasonable\"\"\"\n",
        "        try:\n",
        "            if any(p <= 0 for p in [entry, sl, tp, current]):\n",
        "                return False\n",
        "\n",
        "            # Prices shouldn't be wildly different (max 50% deviation)\n",
        "            prices = [entry, sl, tp, current]\n",
        "            avg_price = sum(prices) / len(prices)\n",
        "\n",
        "            for price in prices:\n",
        "                if abs(price - avg_price) / avg_price > 0.5:\n",
        "                    return False\n",
        "\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def _evaluate_trade_outcome(self, prediction, current_price, tp_price, sl_price):\n",
        "        \"\"\"Determine if trade hit TP or SL\"\"\"\n",
        "        hit_tp = False\n",
        "        hit_sl = False\n",
        "        exit_price = None\n",
        "\n",
        "        try:\n",
        "            if prediction == 1:  # Long\n",
        "                if current_price >= tp_price:\n",
        "                    hit_tp = True\n",
        "                    exit_price = tp_price\n",
        "                elif current_price <= sl_price:\n",
        "                    hit_sl = True\n",
        "                    exit_price = sl_price\n",
        "            elif prediction == 0:  # Short\n",
        "                if current_price <= tp_price:\n",
        "                    hit_tp = True\n",
        "                    exit_price = tp_price\n",
        "                elif current_price >= sl_price:\n",
        "                    hit_sl = True\n",
        "                    exit_price = sl_price\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Trade evaluation error: {e}\", \"warn\")\n",
        "\n",
        "        return hit_tp, hit_sl, exit_price\n",
        "\n",
        "    def _calculate_pnl(self, prediction, entry_price, exit_price):\n",
        "        \"\"\"Calculate profit/loss\"\"\"\n",
        "        try:\n",
        "            if prediction == 1:  # Long\n",
        "                return exit_price - entry_price\n",
        "            else:  # Short\n",
        "                return entry_price - exit_price\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_duration_hours(self, created_at):\n",
        "        \"\"\"Calculate trade duration in hours\"\"\"\n",
        "        try:\n",
        "            created_dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))\n",
        "            duration = (datetime.now(timezone.utc) - created_dt).total_seconds() / 3600\n",
        "            return max(0, duration)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _update_stats_cache(self):\n",
        "        \"\"\"Update cached model performance statistics\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        try:\n",
        "            # Get unique pairs and models\n",
        "            cursor.execute('SELECT DISTINCT pair FROM completed_trades')\n",
        "            pairs = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "            cursor.execute('SELECT DISTINCT model_used FROM completed_trades')\n",
        "            models = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "            for pair in pairs:\n",
        "                for model in models:\n",
        "                    for days in [7, 30, 90]:\n",
        "                        since = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
        "\n",
        "                        cursor.execute('''\n",
        "                            SELECT\n",
        "                                COUNT(*) as total,\n",
        "                                SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins,\n",
        "                                SUM(pnl) as total_pnl,\n",
        "                                AVG(pnl) as avg_pnl\n",
        "                            FROM completed_trades\n",
        "                            WHERE pair = ? AND model_used = ? AND evaluated_at > ?\n",
        "                        ''', (pair, model, since))\n",
        "\n",
        "                        result = cursor.fetchone()\n",
        "                        total, wins, total_pnl, avg_pnl = result\n",
        "\n",
        "                        if total and total > 0:\n",
        "                            accuracy = (wins / total * 100) if total > 0 else 0.0\n",
        "\n",
        "                            cursor.execute('''\n",
        "                                INSERT OR REPLACE INTO model_stats_cache\n",
        "                                (updated_at, pair, model_name, days, total_trades,\n",
        "                                 winning_trades, accuracy_pct, total_pnl, avg_pnl)\n",
        "                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                            ''', (\n",
        "                                datetime.now(timezone.utc).isoformat(),\n",
        "                                pair, model, days, total, wins or 0,\n",
        "                                accuracy, total_pnl or 0.0, avg_pnl or 0.0\n",
        "                            ))\n",
        "\n",
        "            self.conn.commit()\n",
        "            print_status(\"‚úÖ Stats cache updated\", \"debug\")\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ö†Ô∏è Stats cache update failed: {e}\", \"warn\")\n",
        "\n",
        "    def get_model_performance(self, pair, model_name, days=7):\n",
        "        \"\"\"\n",
        "        Get model performance from cache (fast)\n",
        "\n",
        "        Args:\n",
        "            pair: Currency pair (e.g., 'EUR/USD')\n",
        "            model_name: Model name ('SGD', 'RandomForest', 'Ensemble')\n",
        "            days: Number of days to look back\n",
        "\n",
        "        Returns:\n",
        "            dict: Performance metrics\n",
        "        \"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        try:\n",
        "            cursor.execute('''\n",
        "                SELECT total_trades, winning_trades, accuracy_pct,\n",
        "                       total_pnl, avg_pnl, updated_at\n",
        "                FROM model_stats_cache\n",
        "                WHERE pair = ? AND model_name = ? AND days = ?\n",
        "            ''', (pair, model_name, days))\n",
        "\n",
        "            result = cursor.fetchone()\n",
        "\n",
        "            if not result:\n",
        "                return {\n",
        "                    'total_trades': 0,\n",
        "                    'winning_trades': 0,\n",
        "                    'accuracy': 0.0,\n",
        "                    'total_pnl': 0.0,\n",
        "                    'avg_pnl': 0.0\n",
        "                }\n",
        "\n",
        "            total, wins, accuracy, total_pnl, avg_pnl, updated_at = result\n",
        "\n",
        "            return {\n",
        "                'total_trades': total,\n",
        "                'winning_trades': wins,\n",
        "                'accuracy': accuracy,\n",
        "                'total_pnl': total_pnl,\n",
        "                'avg_pnl': avg_pnl,\n",
        "                'updated_at': updated_at\n",
        "            }\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to get model performance: {e}\", \"warn\")\n",
        "            return {\n",
        "                'total_trades': 0,\n",
        "                'winning_trades': 0,\n",
        "                'accuracy': 0.0,\n",
        "                'total_pnl': 0.0,\n",
        "                'avg_pnl': 0.0\n",
        "            }\n",
        "\n",
        "    def get_best_model(self, pair, days=7, min_trades=3):\n",
        "        \"\"\"\n",
        "        Determine which model performs best based on ACTUAL results\n",
        "\n",
        "        Args:\n",
        "            pair: Currency pair\n",
        "            days: Number of days to look back\n",
        "            min_trades: Minimum number of trades required\n",
        "\n",
        "        Returns:\n",
        "            str: Best model name or 'Ensemble' as default\n",
        "        \"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        try:\n",
        "            cursor.execute('''\n",
        "                SELECT model_name, accuracy_pct, total_trades, total_pnl\n",
        "                FROM model_stats_cache\n",
        "                WHERE pair = ? AND days = ? AND total_trades >= ?\n",
        "                ORDER BY accuracy_pct DESC, total_pnl DESC\n",
        "                LIMIT 1\n",
        "            ''', (pair, days, min_trades))\n",
        "\n",
        "            result = cursor.fetchone()\n",
        "\n",
        "            if result:\n",
        "                return result[0]\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to get best model: {e}\", \"warn\")\n",
        "\n",
        "        return 'Ensemble'  # Default fallback\n",
        "\n",
        "    def get_database_stats(self):\n",
        "        \"\"\"Get database statistics\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        stats = {}\n",
        "\n",
        "        try:\n",
        "            # Pending trades count\n",
        "            cursor.execute('SELECT COUNT(*) FROM pending_trades WHERE evaluated = 0')\n",
        "            stats['pending_trades'] = cursor.fetchone()[0]\n",
        "\n",
        "            # Completed trades count\n",
        "            cursor.execute('SELECT COUNT(*) FROM completed_trades')\n",
        "            stats['completed_trades'] = cursor.fetchone()[0]\n",
        "\n",
        "            # Total P&L\n",
        "            cursor.execute('SELECT SUM(pnl) FROM completed_trades')\n",
        "            result = cursor.fetchone()\n",
        "            stats['total_pnl'] = result[0] if result[0] else 0.0\n",
        "\n",
        "            # Overall accuracy\n",
        "            cursor.execute('''\n",
        "                SELECT\n",
        "                    COUNT(*) as total,\n",
        "                    SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins\n",
        "                FROM completed_trades\n",
        "            ''')\n",
        "            result = cursor.fetchone()\n",
        "            if result and result[0] > 0:\n",
        "                stats['overall_accuracy'] = (result[1] / result[0]) * 100\n",
        "            else:\n",
        "                stats['overall_accuracy'] = 0.0\n",
        "\n",
        "            # Database size\n",
        "            stats['db_size_mb'] = self.db_path.stat().st_size / (1024 * 1024)\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to get database stats: {e}\", \"warn\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def cleanup_old_data(self, days_to_keep=90):\n",
        "        \"\"\"\n",
        "        Clean up old data to prevent database bloat\n",
        "\n",
        "        Args:\n",
        "            days_to_keep: Number of days of data to keep\n",
        "        \"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cutoff_date = (datetime.now(timezone.utc) - timedelta(days=days_to_keep)).isoformat()\n",
        "\n",
        "        try:\n",
        "            # Delete old evaluated pending trades\n",
        "            cursor.execute('''\n",
        "                DELETE FROM pending_trades\n",
        "                WHERE evaluated = 1 AND created_at < ?\n",
        "            ''', (cutoff_date,))\n",
        "            deleted_pending = cursor.rowcount\n",
        "\n",
        "            # Delete old execution logs\n",
        "            cursor.execute('''\n",
        "                DELETE FROM execution_log\n",
        "                WHERE timestamp < ?\n",
        "            ''', (cutoff_date,))\n",
        "            deleted_logs = cursor.rowcount\n",
        "\n",
        "            self.conn.commit()\n",
        "\n",
        "            print_status(\n",
        "                f\"üßπ Cleaned up {deleted_pending} old pending trades, \"\n",
        "                f\"{deleted_logs} old logs\",\n",
        "                \"info\"\n",
        "            )\n",
        "\n",
        "            # Vacuum to reclaim space\n",
        "            cursor.execute('VACUUM')\n",
        "            print_status(\"‚úÖ Database optimized\", \"debug\")\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ö†Ô∏è Cleanup failed: {e}\", \"warn\")\n",
        "\n",
        "    def backup_database(self, backup_dir=None):\n",
        "        \"\"\"Create a backup of the database\"\"\"\n",
        "        if backup_dir is None:\n",
        "            backup_dir = REPO_FOLDER / \"backups\"\n",
        "\n",
        "        backup_dir = Path(backup_dir)\n",
        "        backup_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        backup_path = backup_dir / f\"ml_memory_backup_{timestamp}.db\"\n",
        "\n",
        "        try:\n",
        "            # Close connection temporarily\n",
        "            if self.conn:\n",
        "                self.conn.close()\n",
        "\n",
        "            # Copy database file\n",
        "            shutil.copy2(self.db_path, backup_path)\n",
        "\n",
        "            # Reconnect\n",
        "            self.conn = sqlite3.connect(str(self.db_path), timeout=30)\n",
        "\n",
        "            print_status(f\"‚úÖ Database backed up to {backup_path.name}\", \"success\")\n",
        "            return backup_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Backup failed: {e}\", \"warn\")\n",
        "            # Reconnect even if backup failed\n",
        "            if not self.conn:\n",
        "                self.conn = sqlite3.connect(str(self.db_path), timeout=30)\n",
        "            return None\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close database connection safely\"\"\"\n",
        "        if self.conn:\n",
        "            try:\n",
        "                self.conn.commit()\n",
        "                self.conn.close()\n",
        "                print_status(\"‚úÖ Database connection closed\", \"debug\")\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ö†Ô∏è Error closing database: {e}\", \"warn\")\n",
        "            finally:\n",
        "                self.conn = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"Context manager entry\"\"\"\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Context manager exit\"\"\"\n",
        "        self.close()\n",
        "        return False\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Destructor - ensure connection is closed\"\"\"\n",
        "        self.close()\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# Initialize Database\n",
        "# ======================================================\n",
        "TRADE_DB = FixedTradeMemoryDatabase()\n",
        "\n",
        "# Display initial database stats\n",
        "initial_stats = TRADE_DB.get_database_stats()\n",
        "print_status(\"\\nüìä DATABASE STATISTICS:\", \"info\")\n",
        "print_status(f\"  Pending Trades: {initial_stats.get('pending_trades', 0)}\", \"info\")\n",
        "print_status(f\"  Completed Trades: {initial_stats.get('completed_trades', 0)}\", \"info\")\n",
        "print_status(f\"  Total P&L: ${initial_stats.get('total_pnl', 0):.2f}\", \"info\")\n",
        "print_status(f\"  Overall Accuracy: {initial_stats.get('overall_accuracy', 0):.1f}%\", \"info\")\n",
        "print_status(f\"  Database Size: {initial_stats.get('db_size_mb', 0):.2f} MB\\n\", \"info\")\n",
        "\n",
        "# ======================================================\n",
        "# üÜï PERSISTENT ITERATION COUNTER (in Git)\n",
        "# ======================================================\n",
        "ITERATION_COUNTER_FILE = REPO_FOLDER / \"ml_iteration_counter.pkl\"\n",
        "\n",
        "class MLIterationCounter:\n",
        "    \"\"\"Tracks total ML pipeline iterations across all runs forever\"\"\"\n",
        "\n",
        "    def __init__(self, counter_file=ITERATION_COUNTER_FILE):\n",
        "        self.counter_file = counter_file\n",
        "        self.data = self.load_counter()\n",
        "\n",
        "    def load_counter(self):\n",
        "        if self.counter_file.exists():\n",
        "            try:\n",
        "                with open(self.counter_file, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "                print_status(f\"‚úÖ Loaded iteration counter: {data['total_iterations']} total runs\", \"success\")\n",
        "                return data\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ö†Ô∏è Failed to load iteration counter: {e}\", \"warn\")\n",
        "\n",
        "        return {\n",
        "            'total_iterations': 0,\n",
        "            'start_date': datetime.now(timezone.utc).isoformat(),\n",
        "            'last_run': None,\n",
        "            'run_history': []\n",
        "        }\n",
        "\n",
        "    def increment(self):\n",
        "        \"\"\"Increment and save counter\"\"\"\n",
        "        self.data['total_iterations'] += 1\n",
        "        self.data['last_run'] = datetime.now(timezone.utc).isoformat()\n",
        "        self.data['run_history'].append({\n",
        "            'iteration': self.data['total_iterations'],\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat()\n",
        "        })\n",
        "\n",
        "        # Keep only last 1000 runs\n",
        "        if len(self.data['run_history']) > 1000:\n",
        "            self.data['run_history'] = self.data['run_history'][-1000:]\n",
        "\n",
        "        self.save_counter()\n",
        "        return self.data['total_iterations']\n",
        "\n",
        "    def save_counter(self):\n",
        "        try:\n",
        "            # Atomic write with temp file\n",
        "            temp_file = self.counter_file.with_suffix('.tmp')\n",
        "            with open(temp_file, 'wb') as f:\n",
        "                pickle.dump(self.data, f, protocol=4)\n",
        "            temp_file.replace(self.counter_file)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save iteration counter: {e}\")\n",
        "\n",
        "    def get_current(self):\n",
        "        return self.data['total_iterations']\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get statistics about runs\"\"\"\n",
        "        if not self.data['run_history']:\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            first_run = datetime.fromisoformat(self.data['start_date'])\n",
        "            days_running = max(1, (datetime.now(timezone.utc) - first_run).days)\n",
        "\n",
        "            return {\n",
        "                'total_iterations': self.data['total_iterations'],\n",
        "                'days_running': days_running,\n",
        "                'avg_iterations_per_day': self.data['total_iterations'] / days_running,\n",
        "                'start_date': self.data['start_date'],\n",
        "                'last_run': self.data['last_run']\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to get counter stats: {e}\", \"warn\")\n",
        "            return {}\n",
        "\n",
        "\n",
        "ML_ITERATION_COUNTER = MLIterationCounter()\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Git & Credentials\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "BROWSERLESS_TOKEN = os.environ.get(\"BROWSERLESS_TOKEN\",\"\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT missing!\")\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "subprocess.run([\"git\",\"config\",\"--global\",\"user.name\",GIT_NAME], check=False)\n",
        "subprocess.run([\"git\",\"config\",\"--global\",\"user.email\",GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\",\"config\",\"--global\",\"credential.helper\",\"store\"], check=False)\n",
        "\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "def ensure_repo():\n",
        "    \"\"\"Ensure Git repository is cloned and up to date\"\"\"\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        if REPO_FOLDER.exists():\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "        print_status(f\"Cloning repo into {REPO_FOLDER}...\", \"info\")\n",
        "        subprocess.run([\"git\",\"clone\",\"-b\",BRANCH,REPO_URL,str(REPO_FOLDER)], check=True)\n",
        "    else:\n",
        "        print_status(\"Repo exists, pulling latest...\", \"info\")\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"fetch\",\"origin\"], check=False)\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"checkout\",BRANCH], check=False)\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"pull\",\"origin\",BRANCH], check=False)\n",
        "        print_status(\"‚úÖ Repo synced successfully\", \"success\")\n",
        "\n",
        "ensure_repo()\n",
        "\n",
        "# ======================================================\n",
        "# üÜï CLEANUP CORRUPTED PICKLES\n",
        "# ======================================================\n",
        "def cleanup_corrupted_pickles():\n",
        "    \"\"\"Remove corrupted pickle files at startup\"\"\"\n",
        "    print_status(\"üßπ Checking for corrupted ML pickle files...\", \"info\")\n",
        "\n",
        "    corrupted_count = 0\n",
        "    for pkl_file in PICKLE_FOLDER.glob(\"*.pkl\"):\n",
        "        try:\n",
        "            with open(pkl_file, 'rb') as f:\n",
        "                pickle.load(f)\n",
        "        except Exception:\n",
        "            try:\n",
        "                pkl_file.unlink()\n",
        "                print_status(f\"üóëÔ∏è Removed corrupted: {pkl_file.name}\", \"warn\")\n",
        "                corrupted_count += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    if corrupted_count > 0:\n",
        "        print_status(f\"‚úÖ Cleaned up {corrupted_count} corrupted files\", \"success\")\n",
        "    else:\n",
        "        print_status(\"‚úÖ No corrupted files found\", \"success\")\n",
        "\n",
        "cleanup_corrupted_pickles()\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CSV Loader + Sanity Check\n",
        "# ======================================================\n",
        "def load_csv(path):\n",
        "    \"\"\"Load CSV with validation\"\"\"\n",
        "    if not path.exists():\n",
        "        print_status(f\"‚ö†Ô∏è CSV missing: {path}\", \"warn\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
        "        df.columns = [c.strip().lower().replace(\" \",\"_\") for c in df.columns]\n",
        "\n",
        "        for col in [\"open\",\"high\",\"low\",\"close\"]:\n",
        "            if col not in df.columns:\n",
        "                df[col] = np.nan\n",
        "            df[col] = df[col].ffill().bfill()\n",
        "\n",
        "        df = df[[\"open\",\"high\",\"low\",\"close\"]].dropna(how='all')\n",
        "\n",
        "        # Price sanity check\n",
        "        if len(df) > 0:\n",
        "            mean_price = df['close'].mean()\n",
        "            if mean_price < 0.5 or mean_price > 200:\n",
        "                print_status(f\"‚ö†Ô∏è {path.name} suspicious price (mean={mean_price:.2f}), skipping\", \"warn\")\n",
        "                return None\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ùå Failed to load {path.name}: {e}\", \"error\")\n",
        "        return None\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Live Price Fetch\n",
        "# ======================================================\n",
        "def fetch_live_rate(pair, timeout=10, retries=2):\n",
        "    \"\"\"Fetch live exchange rate with retry logic\"\"\"\n",
        "    if not BROWSERLESS_TOKEN:\n",
        "        print_status(\"‚ö†Ô∏è BROWSERLESS_TOKEN missing\", \"warn\")\n",
        "        return 0\n",
        "\n",
        "    from_currency, to_currency = pair.split(\"/\")\n",
        "    url = f\"https://production-sfo.browserless.io/content?token={BROWSERLESS_TOKEN}\"\n",
        "    payload = {\n",
        "        \"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"\n",
        "    }\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            res = requests.post(url, json=payload, timeout=timeout)\n",
        "            res.raise_for_status()\n",
        "            match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
        "\n",
        "            if match:\n",
        "                rate = float(match.group(1).replace(\",\",\"\"))\n",
        "                if rate > 0:\n",
        "                    print_status(f\"üíπ {pair} live price: {rate}\", \"info\")\n",
        "                    return rate\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < retries - 1:\n",
        "                print_status(f\"‚ö†Ô∏è Retry {attempt+1}/{retries} for {pair}: {e}\", \"warn\")\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print_status(f\"‚ùå Failed to fetch {pair} after {retries} attempts: {e}\", \"error\")\n",
        "\n",
        "    return 0\n",
        "\n",
        "def inject_live_price(df, live_price, n_candles=3):\n",
        "    \"\"\"Inject live price into recent candles for real-time analysis\"\"\"\n",
        "    if live_price <= 0 or df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df_copy = df.copy()\n",
        "    n_inject = min(n_candles, len(df_copy))\n",
        "\n",
        "    for i in range(n_inject):\n",
        "        # Add small random variation to simulate realistic price movement\n",
        "        price = live_price * (1 + np.random.uniform(-0.0005, 0.0005))\n",
        "\n",
        "        for col in [\"open\",\"high\",\"low\",\"close\"]:\n",
        "            if col in df_copy.columns:\n",
        "                df_copy.iloc[-n_inject+i, df_copy.columns.get_loc(col)] = price\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Enhanced Indicators\n",
        "# ======================================================\n",
        "scaler_global = MinMaxScaler()\n",
        "\n",
        "def add_indicators(df, fit_scaler=True):\n",
        "    \"\"\"Add technical indicators with error handling\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    try:\n",
        "        # Trend indicators\n",
        "        if len(df) >= 50:\n",
        "            df['SMA_50'] = ta.trend.SMAIndicator(df['close'], 50).sma_indicator()\n",
        "        if len(df) >= 20:\n",
        "            df['EMA_20'] = ta.trend.EMAIndicator(df['close'], 20).ema_indicator()\n",
        "\n",
        "        # Momentum indicators\n",
        "        if len(df) >= 14:\n",
        "            df['RSI_14'] = ta.momentum.RSIIndicator(df['close'], 14).rsi()\n",
        "            df['Williams_%R'] = ta.momentum.WilliamsRIndicator(\n",
        "                df['high'], df['low'], df['close'], 14\n",
        "            ).williams_r()\n",
        "\n",
        "        # Volatility\n",
        "        if len(df) >= 20:\n",
        "            df['ATR_14'] = ta.volatility.AverageTrueRange(\n",
        "                df['high'], df['low'], df['close'], 14\n",
        "            ).average_true_range()\n",
        "\n",
        "        # Trend strength\n",
        "        df['MACD'] = ta.trend.MACD(df['close']).macd()\n",
        "        df['CCI_20'] = ta.trend.CCIIndicator(df['high'], df['low'], df['close'], 20).cci()\n",
        "\n",
        "        if len(df) >= 14:\n",
        "            df['ADX_14'] = ta.trend.ADXIndicator(df['high'], df['low'], df['close'], 14).adx()\n",
        "\n",
        "        # Fill NaN values\n",
        "        df = df.ffill().bfill().fillna(0)\n",
        "\n",
        "        # Scale numeric columns (except OHLC)\n",
        "        numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns\n",
        "                       if c not in ['open', 'high', 'low', 'close']]\n",
        "\n",
        "        if numeric_cols and len(numeric_cols) > 0:\n",
        "            if fit_scaler:\n",
        "                df[numeric_cols] = scaler_global.fit_transform(df[numeric_cols])\n",
        "            else:\n",
        "                try:\n",
        "                    df[numeric_cols] = scaler_global.transform(df[numeric_cols])\n",
        "                except NotFittedError:\n",
        "                    df[numeric_cols] = scaler_global.fit_transform(df[numeric_cols])\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ö†Ô∏è Indicator calculation issue: {e}\", \"warn\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Enhanced ML Training with Performance Tracking\n",
        "# ======================================================\n",
        "def train_predict_ml_enhanced(df, pair_name, timeframe):\n",
        "    \"\"\"Train both SGD and RandomForest, return best prediction\"\"\"\n",
        "    df = df.dropna()\n",
        "\n",
        "    if len(df) < 50:\n",
        "        return 0, 0, 0, 0.5\n",
        "\n",
        "    # Prepare features\n",
        "    X = df.drop(columns=['close'], errors='ignore')\n",
        "    X = X if not X.empty else df[['close']]\n",
        "    y = (df['close'].diff() > 0).astype(int).fillna(0)\n",
        "    X = X.fillna(0)\n",
        "\n",
        "    safe_pair_name = pair_name.replace(\"/\", \"_\")\n",
        "    safe_tf_name = timeframe.replace(\"/\", \"_\")\n",
        "\n",
        "    # ===== SGD Training =====\n",
        "    sgd_file = PICKLE_FOLDER / f\"{safe_pair_name}_{safe_tf_name}_sgd.pkl\"\n",
        "\n",
        "    if sgd_file.exists():\n",
        "        try:\n",
        "            sgd = pickle.load(open(sgd_file, \"rb\"))\n",
        "        except:\n",
        "            sgd = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
        "            sgd.partial_fit(X, y, classes=np.array([0, 1]))\n",
        "    else:\n",
        "        sgd = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
        "        sgd.partial_fit(X, y, classes=np.array([0, 1]))\n",
        "\n",
        "    sgd.partial_fit(X, y)\n",
        "    pickle.dump(sgd, open(sgd_file, \"wb\"), protocol=4)\n",
        "    sgd_pred = int(sgd.predict(X.iloc[[-1]])[0])\n",
        "\n",
        "    # Get SGD confidence\n",
        "    try:\n",
        "        sgd_proba = sgd.predict_proba(X.iloc[[-1]])[0]\n",
        "        sgd_confidence = float(max(sgd_proba))\n",
        "    except:\n",
        "        sgd_confidence = 0.5\n",
        "\n",
        "    # ===== RandomForest with Historical Memory =====\n",
        "    hist_file = PICKLE_FOLDER / f\"{safe_pair_name}_{safe_tf_name}_rf_hist.pkl\"\n",
        "\n",
        "    if hist_file.exists():\n",
        "        try:\n",
        "            hist_X, hist_y = pickle.load(open(hist_file, \"rb\"))\n",
        "            hist_X = pd.concat([hist_X, X], ignore_index=True)\n",
        "            hist_y = pd.concat([hist_y, y], ignore_index=True)\n",
        "\n",
        "            if len(hist_X) > 5000:\n",
        "                hist_X = hist_X.iloc[-5000:]\n",
        "                hist_y = hist_y.iloc[-5000:]\n",
        "        except:\n",
        "            hist_X, hist_y = X.copy(), y.copy()\n",
        "    else:\n",
        "        hist_X, hist_y = X.copy(), y.copy()\n",
        "\n",
        "    rf_file = PICKLE_FOLDER / f\"{safe_pair_name}_{safe_tf_name}_rf.pkl\"\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=50,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        max_depth=10\n",
        "    )\n",
        "\n",
        "    rf.fit(hist_X, hist_y)\n",
        "    pickle.dump(rf, open(rf_file, \"wb\"), protocol=4)\n",
        "    pickle.dump((hist_X, hist_y), open(hist_file, \"wb\"), protocol=4)\n",
        "\n",
        "    rf_pred = int(rf.predict(X.iloc[[-1]])[0])\n",
        "\n",
        "    # Get RF confidence\n",
        "    try:\n",
        "        rf_proba = rf.predict_proba(X.iloc[[-1]])[0]\n",
        "        rf_confidence = float(max(rf_proba))\n",
        "    except:\n",
        "        rf_confidence = 0.5\n",
        "\n",
        "    # ===== Ensemble Decision =====\n",
        "    best_model = TRADE_DB.get_best_model(pair_name, days=7)\n",
        "\n",
        "    if best_model == 'SGD':\n",
        "        ensemble_pred = sgd_pred\n",
        "        confidence = sgd_confidence\n",
        "    elif best_model == 'RandomForest':\n",
        "        ensemble_pred = rf_pred\n",
        "        confidence = rf_confidence\n",
        "    else:  # Ensemble (vote)\n",
        "        ensemble_pred = 1 if (sgd_pred + rf_pred) >= 1 else 0\n",
        "        confidence = (sgd_confidence + rf_confidence) / 2\n",
        "\n",
        "    return sgd_pred, rf_pred, ensemble_pred, confidence\n",
        "\n",
        "print_status(\"\\n‚úÖ Fixed TradeMemoryDatabase v3.6 loaded successfully!\", \"success\")\n",
        "print_status(\"üîß All SQLite syntax errors resolved\", \"success\")\n",
        "print_status(\"üíæ Database ready for persistent storage in Git\\n\", \"success\")"
      ],
      "metadata": {
        "id": "rsOTrKZrznxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZPAZQi88SYG"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# VERSION 3.6 ‚Äì Unified Loader + Merge Pickles (Production Ready)\n",
        "# Fully Safe | Threaded | Compatible with Hybrid FX Pipeline\n",
        "# Added: Data validation, ATR floors, debug prints, raw price preservation\n",
        "# ======================================================\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import warnings\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Environment & folders\n",
        "# -----------------------------\n",
        "ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "TEMP_PICKLE_FOLDER = ROOT_DIR / \"temp_pickles\"\n",
        "FINAL_PICKLE_FOLDER = ROOT_DIR / \"merged_data_pickles\"\n",
        "\n",
        "for folder in [CSV_FOLDER, TEMP_PICKLE_FOLDER, FINAL_PICKLE_FOLDER, REPO_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "JSON_FILE = REPO_FOLDER / \"latest_signals.json\"\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Safe Indicator Generator\n",
        "# -----------------------------\n",
        "def add_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    df = df[(df[[\"open\", \"high\", \"low\", \"close\"]] > 0).all(axis=1)]\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # --- Preserve raw OHLC prices for GA ---\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if col in df.columns:\n",
        "            df[f\"raw_{col}\"] = df[col].copy()\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "\n",
        "        try:\n",
        "            if len(df['close']) >= 10:\n",
        "                df['SMA_10'] = ta.trend.sma_indicator(df['close'], 10)\n",
        "                df['EMA_10'] = ta.trend.ema_indicator(df['close'], 10)\n",
        "            if len(df['close']) >= 50:\n",
        "                df['SMA_50'] = ta.trend.sma_indicator(df['close'], 50)\n",
        "                df['EMA_50'] = ta.trend.ema_indicator(df['close'], 50)\n",
        "            if len(df['close']) >= 14:\n",
        "                df['RSI_14'] = ta.momentum.rsi(df['close'], 14)\n",
        "            if all(col in df.columns for col in ['high', 'low', 'close']) and len(df['close']) >= 14:\n",
        "                df['Williams_%R'] = WilliamsRIndicator(df['high'], df['low'], df['close'], 14).williams_r()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Indicator calculation failed: {e}\")\n",
        "\n",
        "        # --- Safe ATR ---\n",
        "        try:\n",
        "            if all(col in df.columns for col in ['high', 'low', 'close']):\n",
        "                window = 14\n",
        "                if len(df) >= window:\n",
        "                    df['ATR'] = AverageTrueRange(\n",
        "                        df['high'], df['low'], df['close'], window=window\n",
        "                    ).average_true_range().fillna(1e-5).clip(lower=1e-4)\n",
        "                else:\n",
        "                    df['ATR'] = 1e-4\n",
        "        except Exception as e:\n",
        "            df['ATR'] = 1e-4\n",
        "            print(f\"‚ö†Ô∏è ATR calculation failed: {e}\")\n",
        "\n",
        "        # --- Scale only non-price numeric columns ---\n",
        "        numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns if not df[c].isna().all()]\n",
        "        protected_cols = [\n",
        "            \"open\", \"high\", \"low\", \"close\",\n",
        "            \"raw_open\", \"raw_high\", \"raw_low\", \"raw_close\"\n",
        "        ]\n",
        "        numeric_cols = [c for c in numeric_cols if c not in protected_cols]\n",
        "\n",
        "        if numeric_cols:\n",
        "            scaler = MinMaxScaler()\n",
        "            df[numeric_cols] = scaler.fit_transform(df[numeric_cols].fillna(0) + 1e-8)\n",
        "\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Safe CSV Processing\n",
        "# -----------------------------\n",
        "def process_csv_file(csv_file: Path, save_folder: Path):\n",
        "    try:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\", category=pd.errors.ParserWarning)\n",
        "            df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö™ Skipped empty CSV: {csv_file.name}\")\n",
        "            return None\n",
        "\n",
        "        df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "        df = add_indicators(df)\n",
        "        if df.empty:\n",
        "            print(f\"‚ö™ Skipped CSV after filtering invalid prices: {csv_file.name}\")\n",
        "            return None\n",
        "\n",
        "        out_file = save_folder / f\"{csv_file.stem}.pkl\"\n",
        "        df.to_pickle(out_file)\n",
        "        print(f\"‚úÖ Processed CSV {csv_file.name} ‚Üí {out_file.name}\")\n",
        "        return out_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed CSV {csv_file.name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ JSON Processing\n",
        "# -----------------------------\n",
        "def process_json_file(json_file: Path, save_folder: Path):\n",
        "    try:\n",
        "        with open(json_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load JSON: {e}\")\n",
        "        return []\n",
        "\n",
        "    signals_data = data.get(\"pairs\", {})\n",
        "    timestamp = pd.to_datetime(data.get(\"timestamp\"), utc=True)\n",
        "    processed_files = []\n",
        "\n",
        "    for pair, info in signals_data.items():\n",
        "        signals = info.get(\"signals\", {})\n",
        "        dfs = []\n",
        "\n",
        "        for tf_name, tf_info in signals.items():\n",
        "            df = pd.DataFrame({\n",
        "                \"live\": [tf_info.get(\"live\")],\n",
        "                \"SL\": [tf_info.get(\"SL\")],\n",
        "                \"TP\": [tf_info.get(\"TP\")],\n",
        "                \"signal\": [tf_info.get(\"signal\")]\n",
        "            }, index=[timestamp])\n",
        "            df[\"timeframe\"] = tf_name\n",
        "            df = add_indicators(df)\n",
        "            if not df.empty:\n",
        "                dfs.append(df)\n",
        "\n",
        "        if dfs:\n",
        "            df_pair = pd.concat(dfs)\n",
        "            out_file = save_folder / f\"{pair.replace('/', '_')}.pkl\"\n",
        "            df_pair.to_pickle(out_file)\n",
        "            print(f\"‚úÖ Processed JSON {pair} ‚Üí {out_file.name}\")\n",
        "            processed_files.append(out_file)\n",
        "\n",
        "    return processed_files\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Safe Pickle Merger\n",
        "# -----------------------------\n",
        "def merge_pickles(temp_folder: Path, final_folder: Path, keep_last: int = 5):\n",
        "    pickles = list(temp_folder.glob(\"*.pkl\"))\n",
        "    if not pickles:\n",
        "        print(\"‚ö™ No temporary pickles to merge.\")\n",
        "        return\n",
        "\n",
        "    pairs = set(p.stem.split('.')[0] for p in pickles)\n",
        "\n",
        "    for pair in pairs:\n",
        "        pair_files = [p for p in pickles if p.stem.startswith(pair)]\n",
        "        dfs = [pd.read_pickle(p) for p in pair_files if p.exists() and p.stat().st_size > 0]\n",
        "\n",
        "        if not dfs:\n",
        "            print(f\"‚ö™ Skipped {pair} (no valid pickles)\")\n",
        "            continue\n",
        "\n",
        "        merged_df = pd.concat(dfs, ignore_index=False).sort_index().drop_duplicates()\n",
        "        # Changed filename suffix to match the expected format in W4XoZxs-TrDh\n",
        "        merged_file = final_folder / f\"{pair}_2244.pkl\"\n",
        "        merged_df.to_pickle(merged_file)\n",
        "        print(f\"üîó Merged {len(pair_files)} files ‚Üí {merged_file.name}\")\n",
        "\n",
        "        existing = sorted(final_folder.glob(f\"{pair}_*.pkl\"), key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "        for old_file in existing[keep_last:]:\n",
        "            try:\n",
        "                old_file.unlink()\n",
        "                print(f\"üßπ Removed old file: {old_file.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not remove {old_file.name}: {e}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Unified Pipeline Runner\n",
        "# -----------------------------\n",
        "def run_unified_pipeline():\n",
        "    temp_files = []\n",
        "\n",
        "    # Process JSON first\n",
        "    if JSON_FILE.exists():\n",
        "        temp_files += process_json_file(JSON_FILE, TEMP_PICKLE_FOLDER)\n",
        "        print(f\"‚úÖ JSON processing complete ({len(temp_files)} files)\")\n",
        "\n",
        "    # Process CSVs concurrently\n",
        "    csv_files = list(CSV_FOLDER.glob(\"*.csv\"))\n",
        "    if csv_files:\n",
        "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "            futures = [executor.submit(process_csv_file, f, TEMP_PICKLE_FOLDER) for f in csv_files]\n",
        "            for fut in as_completed(futures):\n",
        "                result = fut.result()\n",
        "                if result:\n",
        "                    temp_files.append(result)\n",
        "\n",
        "    # Merge all pickles safely\n",
        "    merge_pickles(TEMP_PICKLE_FOLDER, FINAL_PICKLE_FOLDER)\n",
        "    print(f\"üéØ Unified pipeline complete ‚Äî merged pickles saved in {FINAL_PICKLE_FOLDER}\")\n",
        "\n",
        "    # Debug: print last few rows of each merged pickle\n",
        "    for pkl_file in FINAL_PICKLE_FOLDER.glob(\"*.pkl\"):\n",
        "        df = pd.read_pickle(pkl_file)\n",
        "        print(f\"üîç {pkl_file.name} last rows:\\n\", df.tail(3))\n",
        "\n",
        "    return FINAL_PICKLE_FOLDER\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Execute\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    final_folder = run_unified_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Ultimate Hybrid Forex Pipeline v8.0 (ENHANCED EDITION)\n",
        "======================================================\n",
        "üéØ NEW IMPROVEMENTS:\n",
        "- ‚úÖ Enhanced error recovery and corruption detection\n",
        "- ‚úÖ Improved machine learning convergence\n",
        "- ‚úÖ Advanced trade evaluation with slippage modeling\n",
        "- ‚úÖ Dynamic model weight adjustment based on performance\n",
        "- ‚úÖ Better memory management and file I/O\n",
        "- ‚úÖ Enhanced email templates with charts\n",
        "- ‚úÖ Improved replay system with micro-adjustments\n",
        "- ‚úÖ Advanced risk management per model\n",
        "- ‚úÖ Parallel signal generation\n",
        "- ‚úÖ Database optimization with indexing\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import smtplib\n",
        "import subprocess\n",
        "import time\n",
        "import logging\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "import hashlib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# ======================================================\n",
        "# LOGGING & ENVIRONMENT\n",
        "# ======================================================\n",
        "logging.basicConfig(\n",
        "    filename='forex_pipeline_v8.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(message)s'\n",
        ")\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Enhanced status printer with color codes\"\"\"\n",
        "    icons = {\n",
        "        \"info\": \"‚ÑπÔ∏è\",\n",
        "        \"success\": \"‚úÖ\",\n",
        "        \"warn\": \"‚ö†Ô∏è\",\n",
        "        \"debug\": \"üêû\",\n",
        "        \"error\": \"‚ùå\",\n",
        "        \"rocket\": \"üöÄ\",\n",
        "        \"chart\": \"üìä\",\n",
        "        \"money\": \"üí∞\",\n",
        "        \"brain\": \"üß†\"\n",
        "    }\n",
        "    log_level = \"warning\" if level == \"warn\" else level\n",
        "    if log_level in [\"info\", \"success\", \"warn\", \"debug\", \"error\"]:\n",
        "        getattr(logging, log_level, logging.info)(msg)\n",
        "    print(f\"{icons.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "ROOT_DIR = Path(\"/content\") if IN_COLAB else Path(\".\")\n",
        "ROOT_PATH = ROOT_DIR / \"forex-alpha-models\"\n",
        "\n",
        "# ======================================================\n",
        "# FOLDER SETUP\n",
        "# ======================================================\n",
        "FINAL_PICKLE_FOLDER = ROOT_PATH / \"merged_data_pickles\"\n",
        "PICKLE_FOLDER = FINAL_PICKLE_FOLDER\n",
        "REPO_FOLDER = ROOT_PATH / \"forex-ai-models\"\n",
        "LOGS_FOLDER = ROOT_PATH / \"logs\"\n",
        "BACKUP_FOLDER = ROOT_PATH / \"backups\"\n",
        "\n",
        "for f in [PICKLE_FOLDER, REPO_FOLDER, LOGS_FOLDER, BACKUP_FOLDER]:\n",
        "    f.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "os.chdir(ROOT_PATH)\n",
        "logging.info(f\"Working directory: {ROOT_PATH.resolve()}\")\n",
        "\n",
        "# ======================================================\n",
        "# GIT SETUP\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "\n",
        "# ======================================================\n",
        "# GMAIL CONFIG\n",
        "# ======================================================\n",
        "GMAIL_USER = os.environ.get(\"GMAIL_USER\", \"nakatonabira3@gmail.com\")\n",
        "GMAIL_APP_PASSWORD = os.environ.get(\"GMAIL_APP_PASSWORD\", \"gmwohahtltmcewug\")\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/rahim-dotAI/forex-ai-models/main/IMG_1599.jpeg\"\n",
        "\n",
        "# ======================================================\n",
        "# CORE CONFIG\n",
        "# ======================================================\n",
        "PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "ATR_PERIOD = 14\n",
        "MIN_ATR = 1e-5\n",
        "BASE_CAPITAL = 100\n",
        "MAX_POSITION_FRACTION = 0.1\n",
        "MAX_TRADE_CAP = BASE_CAPITAL * 0.05\n",
        "EPS = 1e-8\n",
        "\n",
        "MAX_ATR_SL = 3.0\n",
        "MAX_ATR_TP = 3.0\n",
        "MIN_ATR_DISTANCE = 0.5\n",
        "\n",
        "MAX_TRADE_MEMORY = 200\n",
        "TOURNAMENT_SIZE = 3\n",
        "SLIPPAGE_PCT = 0.0001  # 0.01% slippage\n",
        "COMMISSION_PCT = 0.0002  # 0.02% commission\n",
        "\n",
        "# ======================================================\n",
        "# FILE PATHS\n",
        "# ======================================================\n",
        "SIGNALS_JSON_PATH = REPO_FOLDER / \"broker_signals.json\"\n",
        "ENSEMBLE_SIGNALS_FILE = REPO_FOLDER / \"ensemble_signals.json\"\n",
        "INFINITE_MEMORY_DB = REPO_FOLDER / \"infinite_memory_v8.db\"\n",
        "MONDAY_RUNS_FILE = REPO_FOLDER / \"monday_runs.pkl\"\n",
        "LEARNING_PROGRESS_FILE = REPO_FOLDER / \"learning_progress_v8.pkl\"\n",
        "PREVIOUS_SIGNALS_FILE = REPO_FOLDER / \"previous_signals_v8.pkl\"\n",
        "ITERATION_COUNTER_FILE = REPO_FOLDER / \"iteration_counter_v8.pkl\"\n",
        "MODEL_WEIGHTS_FILE = REPO_FOLDER / \"model_weights_v8.pkl\"\n",
        "\n",
        "# ======================================================\n",
        "# DATA CLASSES\n",
        "# ======================================================\n",
        "@dataclass\n",
        "class TradeSignal:\n",
        "    \"\"\"Structured trade signal\"\"\"\n",
        "    pair: str\n",
        "    direction: str\n",
        "    entry_price: float\n",
        "    sl_price: float\n",
        "    tp_price: float\n",
        "    confidence: float\n",
        "    atr: float\n",
        "    timestamp: str\n",
        "    model: str\n",
        "    mode: str\n",
        "\n",
        "@dataclass\n",
        "class ModelPerformance:\n",
        "    \"\"\"Model performance metrics\"\"\"\n",
        "    total_trades: int\n",
        "    winning_trades: int\n",
        "    losing_trades: int\n",
        "    accuracy: float\n",
        "    total_pnl: float\n",
        "    avg_pnl: float\n",
        "    sharpe_ratio: float\n",
        "    max_drawdown: float\n",
        "    win_rate: float\n",
        "    avg_win: float\n",
        "    avg_loss: float\n",
        "    profit_factor: float\n",
        "\n",
        "# ======================================================\n",
        "# PERSISTENT ITERATION COUNTER\n",
        "# ======================================================\n",
        "class PersistentIterationCounter:\n",
        "    \"\"\"Enhanced iteration counter with analytics\"\"\"\n",
        "\n",
        "    def __init__(self, counter_file=ITERATION_COUNTER_FILE):\n",
        "        self.counter_file = counter_file\n",
        "        self.data = self.load_counter()\n",
        "\n",
        "    def load_counter(self):\n",
        "        if self.counter_file.exists():\n",
        "            try:\n",
        "                with open(self.counter_file, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "                print_status(f\"Loaded iteration counter: {data['total_iterations']} total runs\", \"success\")\n",
        "                return data\n",
        "            except Exception as e:\n",
        "                print_status(f\"Failed to load counter, creating new: {e}\", \"warn\")\n",
        "\n",
        "        return {\n",
        "            'total_iterations': 0,\n",
        "            'start_date': datetime.now(timezone.utc).isoformat(),\n",
        "            'last_run': None,\n",
        "            'run_history': [],\n",
        "            'total_runtime_seconds': 0,\n",
        "            'successful_runs': 0,\n",
        "            'failed_runs': 0\n",
        "        }\n",
        "\n",
        "    def increment(self, success=True):\n",
        "        \"\"\"Increment and save counter with success tracking\"\"\"\n",
        "        self.data['total_iterations'] += 1\n",
        "        self.data['last_run'] = datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "        if success:\n",
        "            self.data['successful_runs'] += 1\n",
        "        else:\n",
        "            self.data['failed_runs'] += 1\n",
        "\n",
        "        self.data['run_history'].append({\n",
        "            'iteration': self.data['total_iterations'],\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "            'success': success\n",
        "        })\n",
        "\n",
        "        if len(self.data['run_history']) > 1000:\n",
        "            self.data['run_history'] = self.data['run_history'][-1000:]\n",
        "\n",
        "        self.save_counter()\n",
        "        return self.data['total_iterations']\n",
        "\n",
        "    def save_counter(self):\n",
        "        try:\n",
        "            temp_file = self.counter_file.with_suffix('.tmp')\n",
        "            with open(temp_file, 'wb') as f:\n",
        "                pickle.dump(self.data, f, protocol=4)\n",
        "            temp_file.replace(self.counter_file)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save iteration counter: {e}\")\n",
        "\n",
        "    def get_current(self):\n",
        "        return self.data['total_iterations']\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Enhanced statistics\"\"\"\n",
        "        if not self.data['run_history']:\n",
        "            return {}\n",
        "\n",
        "        first_run = datetime.fromisoformat(self.data['start_date'])\n",
        "        days_running = max(1, (datetime.now(timezone.utc) - first_run).days)\n",
        "\n",
        "        return {\n",
        "            'total_iterations': self.data['total_iterations'],\n",
        "            'successful_runs': self.data.get('successful_runs', 0),\n",
        "            'failed_runs': self.data.get('failed_runs', 0),\n",
        "            'success_rate': (self.data.get('successful_runs', 0) / max(1, self.data['total_iterations'])) * 100,\n",
        "            'days_running': days_running,\n",
        "            'avg_iterations_per_day': self.data['total_iterations'] / days_running,\n",
        "            'start_date': self.data['start_date'],\n",
        "            'last_run': self.data['last_run']\n",
        "        }\n",
        "\n",
        "ITERATION_COUNTER = PersistentIterationCounter()\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED COMPETITION MODELS CONFIG\n",
        "# ======================================================\n",
        "COMPETITION_MODELS = {\n",
        "    \"Alpha Momentum\": {\n",
        "        \"color\": \"üî¥\",\n",
        "        \"hex_color\": \"#E74C3C\",\n",
        "        \"strategy\": \"Aggressive momentum with adaptive stops\",\n",
        "        \"atr_sl_range\": (1.5, 2.5),\n",
        "        \"atr_tp_range\": (2.0, 3.5),\n",
        "        \"risk_range\": (0.015, 0.03),\n",
        "        \"confidence_range\": (0.3, 0.5),\n",
        "        \"pop_size\": 15,\n",
        "        \"generations\": 20,\n",
        "        \"mutation_rate\": 0.3,\n",
        "        \"enabled\": True,\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Beta Conservative\": {\n",
        "        \"color\": \"üîµ\",\n",
        "        \"hex_color\": \"#3498DB\",\n",
        "        \"strategy\": \"Conservative mean reversion with tight risk\",\n",
        "        \"atr_sl_range\": (1.0, 1.8),\n",
        "        \"atr_tp_range\": (1.5, 2.5),\n",
        "        \"risk_range\": (0.005, 0.015),\n",
        "        \"confidence_range\": (0.5, 0.7),\n",
        "        \"pop_size\": 12,\n",
        "        \"generations\": 15,\n",
        "        \"mutation_rate\": 0.2,\n",
        "        \"enabled\": True,\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Gamma Adaptive\": {\n",
        "        \"color\": \"üü¢\",\n",
        "        \"hex_color\": \"#2ECC71\",\n",
        "        \"strategy\": \"Adaptive volatility with dynamic sizing\",\n",
        "        \"atr_sl_range\": (1.2, 2.2),\n",
        "        \"atr_tp_range\": (1.8, 3.0),\n",
        "        \"risk_range\": (0.01, 0.025),\n",
        "        \"confidence_range\": (0.4, 0.6),\n",
        "        \"pop_size\": 18,\n",
        "        \"generations\": 22,\n",
        "        \"mutation_rate\": 0.25,\n",
        "        \"enabled\": True,\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"Delta Scalper\": {\n",
        "        \"color\": \"üü°\",\n",
        "        \"hex_color\": \"#F1C40F\",\n",
        "        \"strategy\": \"High-frequency scalping with quick exits\",\n",
        "        \"atr_sl_range\": (0.8, 1.5),\n",
        "        \"atr_tp_range\": (1.2, 2.0),\n",
        "        \"risk_range\": (0.008, 0.02),\n",
        "        \"confidence_range\": (0.5, 0.75),\n",
        "        \"pop_size\": 14,\n",
        "        \"generations\": 18,\n",
        "        \"mutation_rate\": 0.35,\n",
        "        \"enabled\": True,\n",
        "        \"weight\": 1.0\n",
        "    }\n",
        "}\n",
        "\n",
        "# ======================================================\n",
        "# REPLAY MODE CONFIG\n",
        "# ======================================================\n",
        "REPLAY_CONFIG = {\n",
        "    'monday_replay_runs': 1,\n",
        "    'replay_advance_minutes': 60,\n",
        "    'random_noise_factor': 0.15\n",
        "}\n",
        "\n",
        "RANDOM_REPLAY_PERIODS = [\n",
        "    (\"2024-01-01\", \"2024-03-31\"),\n",
        "    (\"2024-04-01\", \"2024-06-30\"),\n",
        "    (\"2024-07-01\", \"2024-09-30\"),\n",
        "    (\"2024-10-01\", \"2024-12-31\"),\n",
        "    (\"2023-01-01\", \"2023-06-30\"),\n",
        "    (\"2023-07-01\", \"2023-12-31\"),\n",
        "    (\"2023-10-01\", \"2024-01-31\"),\n",
        "    (\"2024-02-01\", \"2024-05-31\")\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED TRADE OUTCOME TRACKER\n",
        "# ======================================================\n",
        "class EnhancedTradeOutcomeTracker:\n",
        "    \"\"\"Advanced trade evaluation with slippage and commission\"\"\"\n",
        "\n",
        "    def __init__(self, memory_system):\n",
        "        self.memory = memory_system\n",
        "        self.active_trades = {}\n",
        "\n",
        "    def store_signals(self, signals_by_model, timestamp):\n",
        "        \"\"\"Store current signals for future evaluation\"\"\"\n",
        "        for model_name, signals in signals_by_model.items():\n",
        "            if model_name not in self.active_trades:\n",
        "                self.active_trades[model_name] = {}\n",
        "\n",
        "            for pair, sig in signals.items():\n",
        "                if sig['direction'] == 'HOLD':\n",
        "                    continue\n",
        "\n",
        "                trade_key = f\"{pair}_{timestamp.isoformat()}\"\n",
        "                self.active_trades[model_name][trade_key] = {\n",
        "                    'pair': pair,\n",
        "                    'direction': sig['direction'],\n",
        "                    'entry_price': sig['last_price'],\n",
        "                    'sl_price': sig['SL'],\n",
        "                    'tp_price': sig['TP'],\n",
        "                    'entry_time': timestamp,\n",
        "                    'model': model_name,\n",
        "                    'confidence': sig['score_1_100'],\n",
        "                    'closed': False,\n",
        "                    'atr': sig.get('atr', 0)\n",
        "                }\n",
        "\n",
        "    def evaluate_outcomes(self, current_prices, current_time):\n",
        "        \"\"\"Enhanced evaluation with realistic slippage\"\"\"\n",
        "        outcomes_by_model = defaultdict(lambda: {\n",
        "            'closed_trades': 0,\n",
        "            'wins': 0,\n",
        "            'losses': 0,\n",
        "            'total_pnl': 0.0,\n",
        "            'total_pnl_after_costs': 0.0,\n",
        "            'trade_results': [],\n",
        "            'avg_duration': 0.0,\n",
        "            'best_trade': 0.0,\n",
        "            'worst_trade': 0.0\n",
        "        })\n",
        "\n",
        "        for model_name, trades in self.active_trades.items():\n",
        "            durations = []\n",
        "\n",
        "            for trade_key, trade in list(trades.items()):\n",
        "                if trade['closed']:\n",
        "                    continue\n",
        "\n",
        "                pair = trade['pair']\n",
        "                current_price = current_prices.get(pair, 0)\n",
        "\n",
        "                if current_price <= 0:\n",
        "                    continue\n",
        "\n",
        "                entry_price = trade['entry_price']\n",
        "                sl_price = trade['sl_price']\n",
        "                tp_price = trade['tp_price']\n",
        "                direction = trade['direction']\n",
        "\n",
        "                # Apply slippage\n",
        "                slippage = current_price * SLIPPAGE_PCT\n",
        "                adjusted_entry = entry_price * (1 + SLIPPAGE_PCT if direction == 'BUY' else 1 - SLIPPAGE_PCT)\n",
        "\n",
        "                hit_tp = False\n",
        "                hit_sl = False\n",
        "                exit_price = None\n",
        "\n",
        "                if direction == 'BUY':\n",
        "                    if current_price >= tp_price:\n",
        "                        hit_tp = True\n",
        "                        exit_price = tp_price * (1 - SLIPPAGE_PCT)  # Slippage on exit\n",
        "                    elif current_price <= sl_price:\n",
        "                        hit_sl = True\n",
        "                        exit_price = sl_price * (1 - SLIPPAGE_PCT)\n",
        "                elif direction == 'SELL':\n",
        "                    if current_price <= tp_price:\n",
        "                        hit_tp = True\n",
        "                        exit_price = tp_price * (1 + SLIPPAGE_PCT)\n",
        "                    elif current_price >= sl_price:\n",
        "                        hit_sl = True\n",
        "                        exit_price = sl_price * (1 + SLIPPAGE_PCT)\n",
        "\n",
        "                if exit_price:\n",
        "                    # Calculate P&L\n",
        "                    if direction == 'BUY':\n",
        "                        pnl = exit_price - adjusted_entry\n",
        "                    else:\n",
        "                        pnl = adjusted_entry - exit_price\n",
        "\n",
        "                    # Apply commission\n",
        "                    commission = abs(pnl) * COMMISSION_PCT\n",
        "                    pnl_after_costs = pnl - commission\n",
        "\n",
        "                    was_correct = hit_tp\n",
        "                    price_change_pct = ((exit_price - entry_price) / entry_price) * 100\n",
        "\n",
        "                    try:\n",
        "                        duration_minutes = (current_time - trade['entry_time']).total_seconds() / 60\n",
        "                        durations.append(duration_minutes)\n",
        "                    except:\n",
        "                        duration_minutes = 60\n",
        "\n",
        "                    # Store in database\n",
        "                    cursor = self.memory.conn.cursor()\n",
        "                    cursor.execute('''\n",
        "                        INSERT INTO trade_results\n",
        "                        (timestamp, pair, entry_price, exit_price, direction, pnl,\n",
        "                         pnl_after_costs, commission, slippage, was_correct,\n",
        "                         price_change_pct, duration_minutes, model_name, confidence)\n",
        "                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                    ''', (\n",
        "                        current_time.isoformat(),\n",
        "                        pair,\n",
        "                        adjusted_entry,\n",
        "                        exit_price,\n",
        "                        direction,\n",
        "                        pnl,\n",
        "                        pnl_after_costs,\n",
        "                        commission,\n",
        "                        slippage,\n",
        "                        was_correct,\n",
        "                        price_change_pct,\n",
        "                        duration_minutes,\n",
        "                        model_name,\n",
        "                        trade['confidence']\n",
        "                    ))\n",
        "                    self.memory.conn.commit()\n",
        "\n",
        "                    # Accumulate results\n",
        "                    outcomes_by_model[model_name]['closed_trades'] += 1\n",
        "                    outcomes_by_model[model_name]['total_pnl'] += pnl\n",
        "                    outcomes_by_model[model_name]['total_pnl_after_costs'] += pnl_after_costs\n",
        "\n",
        "                    if was_correct:\n",
        "                        outcomes_by_model[model_name]['wins'] += 1\n",
        "                    else:\n",
        "                        outcomes_by_model[model_name]['losses'] += 1\n",
        "\n",
        "                    # Track best/worst trades\n",
        "                    if pnl_after_costs > outcomes_by_model[model_name]['best_trade']:\n",
        "                        outcomes_by_model[model_name]['best_trade'] = pnl_after_costs\n",
        "                    if pnl_after_costs < outcomes_by_model[model_name]['worst_trade']:\n",
        "                        outcomes_by_model[model_name]['worst_trade'] = pnl_after_costs\n",
        "\n",
        "                    outcomes_by_model[model_name]['trade_results'].append({\n",
        "                        'pair': pair,\n",
        "                        'pnl': pnl_after_costs,\n",
        "                        'was_correct': was_correct,\n",
        "                        'exit_type': 'TP' if hit_tp else 'SL',\n",
        "                        'duration': duration_minutes\n",
        "                    })\n",
        "\n",
        "                    trade['closed'] = True\n",
        "\n",
        "                    status = \"WIN ‚úÖ\" if was_correct else \"LOSS ‚ùå\"\n",
        "                    print_status(\n",
        "                        f\"{model_name}: {pair} {direction} @ {exit_price:.5f} - \"\n",
        "                        f\"P&L: ${pnl_after_costs:.5f} ({duration_minutes:.1f}m) {status}\",\n",
        "                        \"success\" if was_correct else \"warn\"\n",
        "                    )\n",
        "\n",
        "            # Calculate average duration\n",
        "            if durations:\n",
        "                outcomes_by_model[model_name]['avg_duration'] = np.mean(durations)\n",
        "\n",
        "        # Calculate accuracies and other metrics\n",
        "        for model_name, outcomes in outcomes_by_model.items():\n",
        "            if outcomes['closed_trades'] > 0:\n",
        "                outcomes['accuracy'] = (outcomes['wins'] / outcomes['closed_trades']) * 100\n",
        "                outcomes['win_rate'] = outcomes['accuracy']\n",
        "            else:\n",
        "                outcomes['accuracy'] = 0.0\n",
        "                outcomes['win_rate'] = 0.0\n",
        "\n",
        "        return dict(outcomes_by_model)\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED LEARNING SYSTEM\n",
        "# ======================================================\n",
        "class EnhancedLearningSystem:\n",
        "    \"\"\"Advanced learning with pattern recognition\"\"\"\n",
        "\n",
        "    def __init__(self, progress_file=LEARNING_PROGRESS_FILE):\n",
        "        self.progress_file = progress_file\n",
        "        self.learning_data = self.load_progress()\n",
        "\n",
        "    def load_progress(self):\n",
        "        if self.progress_file.exists():\n",
        "            try:\n",
        "                with open(self.progress_file, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            except Exception as e:\n",
        "                print_status(f\"Failed to load learning progress: {e}\", \"warn\")\n",
        "\n",
        "        return {\n",
        "            'total_iterations': 0,\n",
        "            'successful_patterns': {},\n",
        "            'failed_patterns': {},\n",
        "            'model_evolution': {},\n",
        "            'best_parameters_history': [],\n",
        "            'learning_curve': [],\n",
        "            'adaptation_score': 0.0,\n",
        "            'real_trade_outcomes': [],\n",
        "            'pattern_library': {},\n",
        "            'convergence_history': []\n",
        "        }\n",
        "\n",
        "    def save_progress(self):\n",
        "        try:\n",
        "            temp_file = self.progress_file.with_suffix('.tmp')\n",
        "            with open(temp_file, 'wb') as f:\n",
        "                pickle.dump(self.learning_data, f, protocol=4)\n",
        "            temp_file.replace(self.progress_file)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save learning progress: {e}\")\n",
        "\n",
        "    def record_iteration(self, iteration_results, trade_outcomes=None):\n",
        "        \"\"\"Enhanced iteration recording with pattern detection\"\"\"\n",
        "        self.data['total_iterations'] += 1\n",
        "\n",
        "        if trade_outcomes:\n",
        "            self.learning_data['real_trade_outcomes'].append({\n",
        "                'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "                'outcomes': trade_outcomes\n",
        "            })\n",
        "\n",
        "            # Keep last 100 outcomes\n",
        "            if len(self.learning_data['real_trade_outcomes']) > 100:\n",
        "                self.learning_data['real_trade_outcomes'] = \\\n",
        "                    self.learning_data['real_trade_outcomes'][-100:]\n",
        "\n",
        "        for model_name, result in iteration_results.items():\n",
        "            if not result or 'metrics' not in result:\n",
        "                continue\n",
        "\n",
        "            metrics = result['metrics']\n",
        "            chrom_hash = hashlib.md5(\n",
        "                str(result.get('chromosome', [])).encode()\n",
        "            ).hexdigest()[:8]\n",
        "\n",
        "            if trade_outcomes and model_name in trade_outcomes:\n",
        "                actual_pnl = trade_outcomes[model_name]['total_pnl_after_costs']\n",
        "                actual_accuracy = trade_outcomes[model_name]['accuracy']\n",
        "            else:\n",
        "                actual_pnl = metrics['total_pnl']\n",
        "                actual_accuracy = 0.0\n",
        "\n",
        "            # Pattern recognition\n",
        "            pattern_signature = f\"{model_name}_{chrom_hash}\"\n",
        "\n",
        "            if actual_pnl > 0 and actual_accuracy >= 50:\n",
        "                pattern_key = f\"{model_name}_success\"\n",
        "                if pattern_key not in self.learning_data['successful_patterns']:\n",
        "                    self.learning_data['successful_patterns'][pattern_key] = []\n",
        "\n",
        "                self.learning_data['successful_patterns'][pattern_key].append({\n",
        "                    'chromosome': result.get('chromosome'),\n",
        "                    'hash': chrom_hash,\n",
        "                    'pnl': actual_pnl,\n",
        "                    'accuracy': actual_accuracy,\n",
        "                    'sharpe': metrics['sharpe'],\n",
        "                    'timestamp': datetime.now(timezone.utc).isoformat()\n",
        "                })\n",
        "\n",
        "                # Keep best 50 patterns\n",
        "                if len(self.learning_data['successful_patterns'][pattern_key]) > 50:\n",
        "                    sorted_patterns = sorted(\n",
        "                        self.learning_data['successful_patterns'][pattern_key],\n",
        "                        key=lambda x: x['pnl'],\n",
        "                        reverse=True\n",
        "                    )\n",
        "                    self.learning_data['successful_patterns'][pattern_key] = sorted_patterns[:50]\n",
        "\n",
        "            # Track convergence\n",
        "            self.learning_data['convergence_history'].append({\n",
        "                'iteration': self.learning_data['total_iterations'],\n",
        "                'model': model_name,\n",
        "                'pnl': actual_pnl,\n",
        "                'accuracy': actual_accuracy,\n",
        "                'timestamp': datetime.now(timezone.utc).isoformat()\n",
        "            })\n",
        "\n",
        "            if len(self.learning_data['convergence_history']) > 500:\n",
        "                self.learning_data['convergence_history'] = \\\n",
        "                    self.learning_data['convergence_history'][-500:]\n",
        "\n",
        "        # Update learning curve\n",
        "        total_pnl = sum(\n",
        "            trade_outcomes[m]['total_pnl_after_costs']\n",
        "            for m in trade_outcomes\n",
        "        ) if trade_outcomes else 0\n",
        "\n",
        "        self.learning_data['learning_curve'].append(total_pnl)\n",
        "\n",
        "        if len(self.learning_data['learning_curve']) > 100:\n",
        "            self.learning_data['learning_curve'] = \\\n",
        "                self.learning_data['learning_curve'][-100:]\n",
        "\n",
        "        # Calculate adaptation score\n",
        "        if len(self.learning_data['learning_curve']) >= 10:\n",
        "            recent_avg = np.mean(self.learning_data['learning_curve'][-10:])\n",
        "            self.learning_data['adaptation_score'] = min(100, max(0, 50 + recent_avg))\n",
        "\n",
        "        self.save_progress()\n",
        "\n",
        "    def get_smart_mutation_rate(self, model_name, base_rate):\n",
        "        \"\"\"Dynamic mutation rate based on convergence\"\"\"\n",
        "        success_key = f\"{model_name}_success\"\n",
        "        successes = self.learning_data['successful_patterns'].get(success_key, [])\n",
        "\n",
        "        if not successes or len(successes) < 5:\n",
        "            return base_rate * 1.2  # Higher exploration\n",
        "\n",
        "        # Check recent performance trend\n",
        "        recent_successes = successes[-10:]\n",
        "        recent_pnls = [s['pnl'] for s in recent_successes]\n",
        "\n",
        "        if len(recent_pnls) >= 3:\n",
        "            trend = np.polyfit(range(len(recent_pnls)), recent_pnls, 1)[0]\n",
        "\n",
        "            if trend > 0:  # Improving\n",
        "                return base_rate * 0.8  # Lower mutation (exploitation)\n",
        "            else:  # Declining\n",
        "                return base_rate * 1.3  # Higher mutation (exploration)\n",
        "\n",
        "        return base_rate\n",
        "\n",
        "    def get_best_historical_chromosomes(self, model_name, top_n=5):\n",
        "        \"\"\"Get top performing chromosomes with diversity\"\"\"\n",
        "        pattern_key = f\"{model_name}_success\"\n",
        "        successes = self.learning_data['successful_patterns'].get(pattern_key, [])\n",
        "\n",
        "        if not successes:\n",
        "            return []\n",
        "\n",
        "        # Sort by performance\n",
        "        sorted_successes = sorted(successes, key=lambda x: x['pnl'], reverse=True)\n",
        "\n",
        "        # Select diverse chromosomes\n",
        "        selected = []\n",
        "        used_hashes = set()\n",
        "\n",
        "        for s in sorted_successes:\n",
        "            if len(selected) >= top_n:\n",
        "                break\n",
        "\n",
        "            # Ensure diversity\n",
        "            if s['hash'] not in used_hashes:\n",
        "                selected.append(s['chromosome'])\n",
        "                used_hashes.add(s['hash'])\n",
        "\n",
        "        return selected\n",
        "\n",
        "    def get_learning_report(self):\n",
        "        \"\"\"Comprehensive learning report\"\"\"\n",
        "        total_iterations = self.learning_data['total_iterations']\n",
        "        adaptation_score = self.learning_data['adaptation_score']\n",
        "\n",
        "        total_successes = sum(\n",
        "            len(patterns) for patterns in self.learning_data['successful_patterns'].values()\n",
        "        )\n",
        "        total_failures = sum(\n",
        "            len(patterns) for patterns in self.learning_data['failed_patterns'].values()\n",
        "        )\n",
        "\n",
        "        learning_trend = \"üìà Improving\" if adaptation_score > 50 else \"üìâ Adjusting\"\n",
        "\n",
        "        # Calculate convergence rate\n",
        "        if len(self.learning_data['convergence_history']) >= 20:\n",
        "            recent_pnls = [\n",
        "                c['pnl'] for c in self.learning_data['convergence_history'][-20:]\n",
        "            ]\n",
        "            convergence_rate = np.std(recent_pnls) / (abs(np.mean(recent_pnls)) + EPS)\n",
        "        else:\n",
        "            convergence_rate = 1.0\n",
        "\n",
        "        return {\n",
        "            'total_iterations': total_iterations,\n",
        "            'adaptation_score': adaptation_score,\n",
        "            'total_successes': total_successes,\n",
        "            'total_failures': total_failures,\n",
        "            'learning_trend': learning_trend,\n",
        "            'success_rate': (total_successes / (total_successes + total_failures) * 100)\n",
        "                           if (total_successes + total_failures) > 0 else 0,\n",
        "            'convergence_rate': convergence_rate,\n",
        "            'is_converging': convergence_rate < 0.3\n",
        "        }\n",
        "\n",
        "LEARNING_SYSTEM = EnhancedLearningSystem()\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED INFINITE MEMORY SYSTEM\n",
        "# ======================================================\n",
        "class EnhancedMemorySystem:\n",
        "    \"\"\"Optimized database with better indexing\"\"\"\n",
        "\n",
        "    def __init__(self, db_path=INFINITE_MEMORY_DB):\n",
        "        self.db_path = db_path\n",
        "        self.conn = None\n",
        "        self.initialize_database()\n",
        "\n",
        "    def initialize_database(self):\n",
        "        self.conn = sqlite3.connect(str(self.db_path))\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        # Optimized signals history\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS signals_history (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                direction TEXT NOT NULL,\n",
        "                entry_price REAL NOT NULL,\n",
        "                sl_price REAL,\n",
        "                tp_price REAL,\n",
        "                atr REAL,\n",
        "                confidence INTEGER,\n",
        "                model_name TEXT NOT NULL,\n",
        "                mode TEXT,\n",
        "                INDEX idx_signals_pair_model (pair, model_name, timestamp),\n",
        "                INDEX idx_signals_timestamp (timestamp)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Enhanced trade results\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS trade_results (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                signal_id INTEGER,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                entry_price REAL NOT NULL,\n",
        "                exit_price REAL NOT NULL,\n",
        "                direction TEXT NOT NULL,\n",
        "                pnl REAL NOT NULL,\n",
        "                pnl_after_costs REAL,\n",
        "                commission REAL,\n",
        "                slippage REAL,\n",
        "                was_correct BOOLEAN NOT NULL,\n",
        "                price_change_pct REAL,\n",
        "                duration_minutes INTEGER,\n",
        "                model_name TEXT NOT NULL,\n",
        "                confidence INTEGER,\n",
        "                INDEX idx_results_model_time (model_name, timestamp),\n",
        "                INDEX idx_results_pair (pair, model_name),\n",
        "                FOREIGN KEY (signal_id) REFERENCES signals_history(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Competition results with more metrics\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS competition_results (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                iteration INTEGER NOT NULL,\n",
        "                model_name TEXT NOT NULL,\n",
        "                total_pnl REAL,\n",
        "                pnl_after_costs REAL,\n",
        "                accuracy REAL,\n",
        "                sharpe_ratio REAL,\n",
        "                max_drawdown REAL,\n",
        "                total_trades INTEGER,\n",
        "                successful_trades INTEGER,\n",
        "                win_rate REAL,\n",
        "                avg_win REAL,\n",
        "                avg_loss REAL,\n",
        "                profit_factor REAL,\n",
        "                rank INTEGER,\n",
        "                mode TEXT,\n",
        "                INDEX idx_comp_model_iter (model_name, iteration),\n",
        "                INDEX idx_comp_timestamp (timestamp)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Model performance cache\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS model_performance_cache (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                updated_at TEXT NOT NULL,\n",
        "                model_name TEXT NOT NULL,\n",
        "                days INTEGER NOT NULL,\n",
        "                total_trades INTEGER,\n",
        "                winning_trades INTEGER,\n",
        "                accuracy REAL,\n",
        "                total_pnl REAL,\n",
        "                avg_pnl REAL,\n",
        "                sharpe_ratio REAL,\n",
        "                max_drawdown REAL,\n",
        "                UNIQUE(model_name, days)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        self.conn.commit()\n",
        "        print_status(\"Enhanced Memory System initialized with optimized indexes\", \"success\")\n",
        "\n",
        "    def get_model_trade_history(self, model_name, days=7):\n",
        "        \"\"\"Get comprehensive trade history\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        since_date = (datetime.now(timezone.utc) - pd.Timedelta(days=days)).isoformat()\n",
        "\n",
        "        cursor.execute('''\n",
        "            SELECT\n",
        "                COUNT(*) as total_trades,\n",
        "                SUM(CASE WHEN was_correct THEN 1 ELSE 0 END) as successful_trades,\n",
        "                AVG(pnl_after_costs) as avg_pnl,\n",
        "                SUM(pnl_after_costs) as total_pnl,\n",
        "                AVG(CASE WHEN was_correct THEN pnl_after_costs ELSE NULL END) as avg_win,\n",
        "                AVG(CASE WHEN NOT was_correct THEN pnl_after_costs ELSE NULL END) as avg_loss,\n",
        "                MAX(pnl_after_costs) as best_trade,\n",
        "                MIN(pnl_after_costs) as worst_trade\n",
        "            FROM trade_results\n",
        "            WHERE model_name = ? AND timestamp > ?\n",
        "        ''', (model_name, since_date))\n",
        "\n",
        "        result = cursor.fetchone()\n",
        "\n",
        "        total = result[0] if result[0] else 0\n",
        "        wins = result[1] if result[1] else 0\n",
        "        avg_pnl = result[2] if result[2] else 0\n",
        "        total_pnl = result[3] if result[3] else 0\n",
        "        avg_win = result[4] if result[4] else 0\n",
        "        avg_loss = result[5] if result[5] else 0\n",
        "        best_trade = result[6] if result[6] else 0\n",
        "        worst_trade = result[7] if result[7] else 0\n",
        "\n",
        "        accuracy = (wins / total * 100) if total > 0 else 0\n",
        "\n",
        "        # Calculate profit factor\n",
        "        total_wins = sum([r[0] for r in cursor.execute('''\n",
        "            SELECT pnl_after_costs FROM trade_results\n",
        "            WHERE model_name = ? AND timestamp > ? AND was_correct = 1\n",
        "        ''', (model_name, since_date)).fetchall()])\n",
        "\n",
        "        total_losses = abs(sum([r[0] for r in cursor.execute('''\n",
        "            SELECT pnl_after_costs FROM trade_results\n",
        "            WHERE model_name = ? AND timestamp > ? AND was_correct = 0\n",
        "        ''', (model_name, since_date)).fetchall()]))\n",
        "\n",
        "        profit_factor = (total_wins / total_losses) if total_losses > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'total_trades': total,\n",
        "            'successful_trades': wins,\n",
        "            'avg_pnl': avg_pnl,\n",
        "            'total_pnl': total_pnl,\n",
        "            'accuracy': accuracy,\n",
        "            'avg_win': avg_win,\n",
        "            'avg_loss': avg_loss,\n",
        "            'best_trade': best_trade,\n",
        "            'worst_trade': worst_trade,\n",
        "            'profit_factor': profit_factor\n",
        "        }\n",
        "\n",
        "    def update_performance_cache(self, model_name, days=7):\n",
        "        \"\"\"Update cached performance metrics\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        history = self.get_model_trade_history(model_name, days)\n",
        "\n",
        "        cursor.execute('''\n",
        "            INSERT OR REPLACE INTO model_performance_cache\n",
        "            (updated_at, model_name, days, total_trades, winning_trades,\n",
        "             accuracy, total_pnl, avg_pnl, sharpe_ratio, max_drawdown)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', (\n",
        "            datetime.now(timezone.utc).isoformat(),\n",
        "            model_name,\n",
        "            days,\n",
        "            history['total_trades'],\n",
        "            history['successful_trades'],\n",
        "            history['accuracy'],\n",
        "            history['total_pnl'],\n",
        "            history['avg_pnl'],\n",
        "            0.0,  # Sharpe calculated separately\n",
        "            0.0   # Max DD calculated separately\n",
        "        ))\n",
        "\n",
        "        self.conn.commit()\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn:\n",
        "            self.conn.commit()\n",
        "            self.conn.close()\n",
        "\n",
        "MEMORY_SYSTEM = EnhancedMemorySystem()\n",
        "TRADE_TRACKER = EnhancedTradeOutcomeTracker(MEMORY_SYSTEM)\n",
        "\n",
        "# ======================================================\n",
        "# DYNAMIC MODEL WEIGHTS\n",
        "# ======================================================\n",
        "class DynamicModelWeights:\n",
        "    \"\"\"Adjust model weights based on performance\"\"\"\n",
        "\n",
        "    def __init__(self, weights_file=MODEL_WEIGHTS_FILE):\n",
        "        self.weights_file = weights_file\n",
        "        self.weights = self.load_weights()\n",
        "\n",
        "    def load_weights(self):\n",
        "        if self.weights_file.exists():\n",
        "            try:\n",
        "                with open(self.weights_file, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return {model: 1.0 for model in COMPETITION_MODELS.keys()}\n",
        "\n",
        "    def save_weights(self):\n",
        "        try:\n",
        "            with open(self.weights_file, 'wb') as f:\n",
        "                pickle.dump(self.weights, f, protocol=4)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save weights: {e}\")\n",
        "\n",
        "    def update_weights(self, performance_data):\n",
        "        \"\"\"Update weights based on recent performance\"\"\"\n",
        "        if not performance_data:\n",
        "            return\n",
        "\n",
        "        performances = []\n",
        "        for model_name, perf in performance_data.items():\n",
        "            if 'total_pnl' in perf and 'accuracy' in perf:\n",
        "                # Combined score: PnL + accuracy bonus\n",
        "                score = perf['total_pnl'] + (perf['accuracy'] / 100) * 10\n",
        "                performances.append((model_name, score))\n",
        "\n",
        "        if not performances:\n",
        "            return\n",
        "\n",
        "        # Normalize scores\n",
        "        scores = [p[1] for p in performances]\n",
        "        min_score = min(scores)\n",
        "        max_score = max(scores)\n",
        "        score_range = max_score - min_score\n",
        "\n",
        "        for model_name, score in performances:\n",
        "            if score_range > 0:\n",
        "                normalized = (score - min_score) / score_range\n",
        "                # Weight between 0.5 and 2.0\n",
        "                self.weights[model_name] = 0.5 + (normalized * 1.5)\n",
        "            else:\n",
        "                self.weights[model_name] = 1.0\n",
        "\n",
        "        self.save_weights()\n",
        "        print_status(\"Updated dynamic model weights\", \"brain\")\n",
        "\n",
        "    def get_weight(self, model_name):\n",
        "        return self.weights.get(model_name, 1.0)\n",
        "\n",
        "MODEL_WEIGHTS = DynamicModelWeights()\n",
        "\n",
        "# ======================================================\n",
        "# WEEKEND/MONDAY MANAGER\n",
        "# ======================================================\n",
        "class WeekendMondayManager:\n",
        "    def __init__(self):\n",
        "        self.monday_runs_count = self.load_monday_runs()\n",
        "\n",
        "    def load_monday_runs(self):\n",
        "        if MONDAY_RUNS_FILE.exists():\n",
        "            try:\n",
        "                data = pickle.load(open(MONDAY_RUNS_FILE, \"rb\"))\n",
        "                if data.get('date') != datetime.now().strftime('%Y-%m-%d'):\n",
        "                    return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "                return data\n",
        "            except:\n",
        "                return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "        return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "\n",
        "    def save_monday_runs(self):\n",
        "        try:\n",
        "            with open(MONDAY_RUNS_FILE, \"wb\") as f:\n",
        "                pickle.dump(self.monday_runs_count, f, protocol=4)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save Monday runs: {e}\")\n",
        "\n",
        "    def get_mode(self):\n",
        "        weekday = datetime.now().weekday()\n",
        "\n",
        "        if weekday in [5, 6]:\n",
        "            return \"weekend_replay\"\n",
        "        elif weekday == 0:\n",
        "            if self.monday_runs_count['count'] < REPLAY_CONFIG['monday_replay_runs']:\n",
        "                return \"monday_replay\"\n",
        "            else:\n",
        "                return \"normal\"\n",
        "        else:\n",
        "            return \"normal\"\n",
        "\n",
        "    def increment_monday_runs(self):\n",
        "        self.monday_runs_count['count'] += 1\n",
        "        self.monday_runs_count['date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        self.save_monday_runs()\n",
        "\n",
        "    def get_status_message(self):\n",
        "        mode = self.get_mode()\n",
        "        day_name = datetime.now().strftime('%A')\n",
        "\n",
        "        if mode == \"weekend_replay\":\n",
        "            return f\"üé¨ {day_name.upper()} REPLAY MODE\"\n",
        "        elif mode == \"monday_replay\":\n",
        "            return f\"üî¥ MONDAY REPLAY MODE\"\n",
        "        else:\n",
        "            return f\"üíº {day_name.upper()} LIVE MODE\"\n",
        "\n",
        "    def should_send_email(self):\n",
        "        mode = self.get_mode()\n",
        "        return mode == \"normal\"\n",
        "\n",
        "WEEKEND_MONDAY_MANAGER = WeekendMondayManager()\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED HISTORICAL REPLAY SYSTEM\n",
        "# ======================================================\n",
        "class EnhancedReplaySystem:\n",
        "    \"\"\"Improved replay with micro-adjustments\"\"\"\n",
        "\n",
        "    def __init__(self, data, random_selection=True):\n",
        "        if random_selection:\n",
        "            start_date, end_date = random.choice(RANDOM_REPLAY_PERIODS)\n",
        "            print_status(f\"Random period selected: {start_date} ‚Üí {end_date}\", \"success\")\n",
        "        else:\n",
        "            start_date = \"2024-01-01\"\n",
        "            end_date = \"2024-03-31\"\n",
        "\n",
        "        self.start_date = pd.to_datetime(start_date)\n",
        "        self.end_date = pd.to_datetime(end_date)\n",
        "        self.current_date = self.start_date\n",
        "        self.data = data\n",
        "        self.is_replay_mode = True\n",
        "\n",
        "        print_status(f\"Replay Mode: {start_date} ‚Üí {end_date}\", \"chart\")\n",
        "\n",
        "    def get_available_data(self, pair):\n",
        "        \"\"\"Get historical data with noise injection\"\"\"\n",
        "        if pair not in self.data:\n",
        "            return None\n",
        "\n",
        "        full_data = self.data[pair]\n",
        "        filtered_data = {}\n",
        "\n",
        "        for tf, df in full_data.items():\n",
        "            mask = df.index <= self.current_date\n",
        "            filtered_df = df[mask].copy()\n",
        "\n",
        "            if len(filtered_df) > 0:\n",
        "                # Add realistic noise\n",
        "                noise_factor = REPLAY_CONFIG['random_noise_factor']\n",
        "                filtered_df['close'] *= (1 + np.random.uniform(-noise_factor, noise_factor, len(filtered_df)) * 0.0001)\n",
        "                filtered_data[tf] = filtered_df\n",
        "\n",
        "        return filtered_data if filtered_data else None\n",
        "\n",
        "    def get_historical_price(self, pair):\n",
        "        \"\"\"Get historical price with micro-adjustments\"\"\"\n",
        "        if pair not in self.data:\n",
        "            return None\n",
        "\n",
        "        tfs = self.data.get(pair, {})\n",
        "        if not tfs:\n",
        "            return None\n",
        "\n",
        "        for tf, df in tfs.items():\n",
        "            mask = df.index <= self.current_date\n",
        "            filtered_df = df[mask]\n",
        "\n",
        "            if len(filtered_df) > 0:\n",
        "                base_price = float(filtered_df['close'].iloc[-1])\n",
        "                # Add micro-adjustment\n",
        "                adjustment = np.random.uniform(-0.0001, 0.0001)\n",
        "                return base_price * (1 + adjustment)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def advance_time(self, minutes=60):\n",
        "        \"\"\"Advance replay time\"\"\"\n",
        "        self.current_date += pd.Timedelta(minutes=minutes)\n",
        "        return self.current_date <= self.end_date\n",
        "\n",
        "    def get_progress(self):\n",
        "        \"\"\"Get replay progress percentage\"\"\"\n",
        "        total_duration = (self.end_date - self.start_date).total_seconds()\n",
        "        elapsed = (self.current_date - self.start_date).total_seconds()\n",
        "        return (elapsed / total_duration) * 100 if total_duration > 0 else 0\n",
        "\n",
        "# ======================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ======================================================\n",
        "def make_index_tz_naive(df):\n",
        "    \"\"\"Convert DataFrame index to timezone-naive\"\"\"\n",
        "    if isinstance(df.index, pd.DatetimeIndex):\n",
        "        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "        if df.index.tz is not None:\n",
        "            df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def ensure_atr(df):\n",
        "    \"\"\"Ensure ATR column exists and is valid\"\"\"\n",
        "    if \"atr\" in df.columns and not df[\"atr\"].isna().all():\n",
        "        df[\"atr\"] = df[\"atr\"].fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "        return df\n",
        "\n",
        "    high, low, close = df[\"high\"].values, df[\"low\"].values, df[\"close\"].values\n",
        "    tr = np.maximum.reduce([\n",
        "        high - low,\n",
        "        np.abs(high - np.roll(close, 1)),\n",
        "        np.abs(low - np.roll(close, 1))\n",
        "    ])\n",
        "    tr[0] = high[0] - low[0] if len(tr) > 0 else MIN_ATR\n",
        "    atr_series = pd.Series(tr, index=df.index).rolling(\n",
        "        ATR_PERIOD, min_periods=1\n",
        "    ).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "    df[\"atr\"] = atr_series\n",
        "    return df\n",
        "\n",
        "def seed_hybrid_signal(df):\n",
        "    \"\"\"Create hybrid signal if missing\"\"\"\n",
        "    if \"hybrid_signal\" not in df.columns or df[\"hybrid_signal\"].abs().sum() == 0:\n",
        "        fast = df[\"close\"].rolling(10, min_periods=1).mean()\n",
        "        slow = df[\"close\"].rolling(50, min_periods=1).mean()\n",
        "        df[\"hybrid_signal\"] = (fast - slow).fillna(0)\n",
        "    df[\"hybrid_signal\"] = df[\"hybrid_signal\"].fillna(0.0).astype(float)\n",
        "    return df\n",
        "\n",
        "def fetch_live_rate(pair, timeout=8):\n",
        "    \"\"\"Fetch live rate from X-Rates API\"\"\"\n",
        "    token = os.environ.get(\"BROWSERLESS_TOKEN\", \"\")\n",
        "    if not token:\n",
        "        return 0.0\n",
        "\n",
        "    from_currency, to_currency = pair.split(\"/\")\n",
        "    url = f\"https://production-sfo.browserless.io/content?token={token}\"\n",
        "    payload = {\n",
        "        \"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.post(url, json=payload, timeout=timeout)\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', r.text)\n",
        "        rate = float(match.group(1).replace(\",\", \"\")) if match else 0.0\n",
        "        if rate > 0:\n",
        "            print_status(f\"Live rate {pair}: {rate:.5f}\", \"debug\")\n",
        "        return rate\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to fetch {pair}: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def load_unified_pickles(folder):\n",
        "    \"\"\"Load and validate pickle files\"\"\"\n",
        "    combined = {}\n",
        "\n",
        "    for pair in PAIRS:\n",
        "        combined[pair] = {}\n",
        "        prefix = pair.replace(\"/\", \"_\")\n",
        "        pair_files = list(folder.glob(f\"{prefix}*.pkl\"))\n",
        "        pair_files.sort()\n",
        "\n",
        "        if not pair_files:\n",
        "            continue\n",
        "\n",
        "        for pf in pair_files:\n",
        "            try:\n",
        "                df = pd.read_pickle(pf)\n",
        "                if not isinstance(df, pd.DataFrame):\n",
        "                    continue\n",
        "\n",
        "                df = make_index_tz_naive(df)\n",
        "                df = ensure_atr(df)\n",
        "                df = seed_hybrid_signal(df)\n",
        "\n",
        "                if (df['close'] <= 0).any() or len(df) < 50:\n",
        "                    continue\n",
        "\n",
        "                tf = re.sub(rf\"{prefix}_?|\\.pkl\", \"\", pf.name).replace(\"__\", \"_\").strip(\"_\")\n",
        "                if not tf:\n",
        "                    tf = \"merged\"\n",
        "                combined[pair][tf] = df\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to load {pf.name}: {e}\")\n",
        "                continue\n",
        "\n",
        "    return combined\n",
        "\n",
        "def build_tf_map(data):\n",
        "    \"\"\"Build timeframe mapping\"\"\"\n",
        "    return {p: list(tfs.keys()) for p, tfs in data.items()}\n",
        "\n",
        "def create_chrom(tf_map):\n",
        "    \"\"\"Create chromosome with proper structure\"\"\"\n",
        "    chrom = [\n",
        "        random.uniform(1.0, 2.5),  # ATR SL\n",
        "        random.uniform(1.5, 3.0),  # ATR TP\n",
        "        random.uniform(0.005, 0.03),  # Risk\n",
        "        random.uniform(0.3, 0.7)  # Confidence\n",
        "    ]\n",
        "\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p, [])))\n",
        "        chrom += np.random.dirichlet(np.ones(n)).tolist()\n",
        "\n",
        "    return chrom\n",
        "\n",
        "def decode_chrom(chrom, tf_map):\n",
        "    \"\"\"Decode chromosome into parameters\"\"\"\n",
        "    atr_sl = min(max(chrom[0], 1.0), MAX_ATR_SL)\n",
        "    atr_tp = min(max(chrom[1], 1.0), MAX_ATR_TP)\n",
        "    risk = chrom[2]\n",
        "    conf = chrom[3]\n",
        "\n",
        "    tf_w = {}\n",
        "    idx = 4\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p, [])))\n",
        "        weights = chrom[idx:idx+n]\n",
        "        weights = np.array(weights, dtype=float)\n",
        "\n",
        "        if weights.sum() <= 0:\n",
        "            weights = np.ones_like(weights) / len(weights)\n",
        "        else:\n",
        "            weights = weights / (weights.sum() + EPS)\n",
        "\n",
        "        tf_w[p] = {tf: float(w) for tf, w in zip(tf_map.get(p, []), weights)}\n",
        "        idx += n\n",
        "\n",
        "    return atr_sl, atr_tp, risk, conf, tf_w\n",
        "\n",
        "def tournament_select(pop, k=TOURNAMENT_SIZE):\n",
        "    \"\"\"Tournament selection\"\"\"\n",
        "    return max(random.sample(pop, k), key=lambda x: x[0])[1]\n",
        "\n",
        "def calculate_sharpe_ratio(equity_curve):\n",
        "    \"\"\"Calculate Sharpe ratio\"\"\"\n",
        "    if len(equity_curve) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    returns = np.diff(equity_curve) / (equity_curve[:-1] + EPS)\n",
        "    if len(returns) == 0 or np.std(returns) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return np.mean(returns) / (np.std(returns) + EPS)\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED BACKTEST ENGINE\n",
        "# ======================================================\n",
        "def run_enhanced_backtest(data, capital, base_risk, atr_sl, atr_tp, conf_mult,\n",
        "                         tf_weights, trade_memory=None):\n",
        "    \"\"\"Enhanced backtest with better performance tracking\"\"\"\n",
        "    if trade_memory is None:\n",
        "        trade_memory = {pair: [] for pair in PAIRS}\n",
        "\n",
        "    results = {}\n",
        "    precomputed = {}\n",
        "    pair_performance = {pair: 0.0 for pair in PAIRS}\n",
        "\n",
        "    for pair, tfs in data.items():\n",
        "        if not tfs:\n",
        "            results[pair] = {\n",
        "                'equity_curve': np.array([capital]),\n",
        "                'total_pnl': 0,\n",
        "                'max_drawdown': 0,\n",
        "                'sharpe': 0\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        all_idx = sorted(set().union(*[df.index for df in tfs.values()]))\n",
        "        df_all = pd.DataFrame(index=all_idx)\n",
        "\n",
        "        for tf, df in tfs.items():\n",
        "            df_all[f'close_{tf}'] = df['close'].reindex(df_all.index).ffill()\n",
        "            df_all[f'signal_{tf}'] = df['hybrid_signal'].reindex(df_all.index).ffill().fillna(0.0)\n",
        "            df_all[f'atr_{tf}'] = df['atr'].reindex(df_all.index).ffill().fillna(MIN_ATR)\n",
        "\n",
        "        df_all['price'] = df_all[[c for c in df_all.columns if c.startswith('close_')]].mean(axis=1).clip(lower=EPS)\n",
        "        df_all['atr'] = df_all[[c for c in df_all.columns if c.startswith('atr_')]].mean(axis=1).clip(lower=MIN_ATR)\n",
        "        precomputed[pair] = df_all\n",
        "\n",
        "    for pair, df_all in precomputed.items():\n",
        "        tfs = data.get(pair, {})\n",
        "        if not tfs:\n",
        "            continue\n",
        "\n",
        "        agg_signal = sum([\n",
        "            df_all[f'signal_{tf}'] * tf_weights.get(pair, {}).get(tf, 0.0)\n",
        "            for tf in tfs.keys()\n",
        "        ])\n",
        "\n",
        "        mean_abs_signal = np.mean([\n",
        "            df_all[f'signal_{tf}'].abs().mean() for tf in tfs.keys()\n",
        "        ]) if tfs else 0.0\n",
        "        conf_threshold = conf_mult * (mean_abs_signal + EPS)\n",
        "\n",
        "        df_all['agg_signal'] = np.where(\n",
        "            np.abs(agg_signal) >= conf_threshold,\n",
        "            agg_signal,\n",
        "            0.0\n",
        "        )\n",
        "\n",
        "        price = df_all['price'].values\n",
        "        atr = df_all['atr'].values\n",
        "        agg_signal = df_all['agg_signal'].values\n",
        "        n = len(price)\n",
        "\n",
        "        if n <= 1:\n",
        "            results[pair] = {\n",
        "                'equity_curve': np.array([capital]),\n",
        "                'total_pnl': 0,\n",
        "                'max_drawdown': 0,\n",
        "                'sharpe': 0\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        # Enhanced memory factor\n",
        "        memory_factor = 1.0\n",
        "        if trade_memory.get(pair):\n",
        "            recent_trades = trade_memory[pair][-20:]\n",
        "            if recent_trades:\n",
        "                win_rate = sum(1 for tr in recent_trades if tr.get('pnl', 0) > 0) / len(recent_trades)\n",
        "                avg_pnl = np.mean([tr.get('pnl', 0) for tr in recent_trades])\n",
        "                memory_factor = max(0.3, min(2.5, 0.5 + win_rate + (avg_pnl * 0.1)))\n",
        "\n",
        "        raw_size = (capital * base_risk * np.abs(agg_signal)) / (atr_sl * (atr / price) + EPS)\n",
        "        size = np.zeros_like(raw_size)\n",
        "\n",
        "        for i in range(len(raw_size)):\n",
        "            sized = raw_size[i] * memory_factor\n",
        "            sized = min(sized, capital * MAX_POSITION_FRACTION, MAX_TRADE_CAP)\n",
        "            atr_value = atr[i] if i < len(atr) else MIN_ATR\n",
        "            atr_cap = capital * 0.02 / (atr_value / price[i] + EPS)\n",
        "            sized = min(sized, atr_cap)\n",
        "            size[i] = sized\n",
        "\n",
        "        size = np.nan_to_num(size, nan=0.0, posinf=MAX_TRADE_CAP)\n",
        "        direction = np.sign(agg_signal)\n",
        "        pnl = direction * size * (atr_tp * atr / price)\n",
        "\n",
        "        equity = np.zeros(n, dtype=float)\n",
        "        equity[0] = float(capital)\n",
        "        for i in range(1, n):\n",
        "            equity[i] = equity[i-1] + float(pnl[i])\n",
        "\n",
        "        final_pnl = float(equity[-1] - capital)\n",
        "        pair_performance[pair] = final_pnl\n",
        "\n",
        "        trade_memory.setdefault(pair, []).append({\n",
        "            'equity': float(equity[-1]),\n",
        "            'pnl': final_pnl,\n",
        "            'timestamp': pd.Timestamp.now().isoformat()\n",
        "        })\n",
        "\n",
        "        if len(trade_memory[pair]) > MAX_TRADE_MEMORY:\n",
        "            trade_memory[pair] = trade_memory[pair][-MAX_TRADE_MEMORY:]\n",
        "\n",
        "        sharpe = calculate_sharpe_ratio(equity)\n",
        "        max_dd = float(np.max(np.maximum.accumulate(equity) - equity))\n",
        "\n",
        "        results[pair] = {\n",
        "            'equity_curve': equity,\n",
        "            'total_pnl': final_pnl,\n",
        "            'max_drawdown': max_dd,\n",
        "            'sharpe': sharpe\n",
        "        }\n",
        "\n",
        "    total_sharpe = sum([r['sharpe'] for r in results.values()])\n",
        "    perf_values = list(pair_performance.values())\n",
        "    pair_balance_penalty = np.std(perf_values) / (np.mean(perf_values) + EPS) if perf_values else 0.0\n",
        "    score = total_sharpe - 0.3 * pair_balance_penalty\n",
        "\n",
        "    return score, results, trade_memory\n",
        "\n",
        "# ======================================================\n",
        "# MODEL STATE MANAGER (Enhanced)\n",
        "# ======================================================\n",
        "class EnhancedModelStateManager:\n",
        "    \"\"\"Improved state management with atomic writes\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, repo_folder):\n",
        "        self.model_name = model_name\n",
        "        self.prefix = model_name.lower().replace(\" \", \"_\")\n",
        "        self.repo_folder = Path(repo_folder)\n",
        "\n",
        "        self.files = {\n",
        "            'population': self.repo_folder / f\"{self.prefix}_population_v8.pkl\",\n",
        "            'trade_memory': self.repo_folder / f\"{self.prefix}_trade_memory_v8.pkl\",\n",
        "            'best_chrom': self.repo_folder / f\"{self.prefix}_best_chrom_v8.pkl\",\n",
        "            'gen_count': self.repo_folder / f\"{self.prefix}_gen_count_v8.pkl\",\n",
        "            'ga_progress': self.repo_folder / f\"{self.prefix}_ga_progress_v8.pkl\",\n",
        "        }\n",
        "\n",
        "    def save_all(self, population, trade_memory, best_chrom, gen_count, ga_progress):\n",
        "        \"\"\"Atomic save with backup\"\"\"\n",
        "        temp_files = {}\n",
        "        backup_files = {}\n",
        "\n",
        "        try:\n",
        "            # Create backups of existing files\n",
        "            for key, path in self.files.items():\n",
        "                if path.exists():\n",
        "                    backup_path = BACKUP_FOLDER / f\"{path.name}.backup\"\n",
        "                    try:\n",
        "                        shutil.copy2(path, backup_path)\n",
        "                        backup_files[key] = backup_path\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            # Write to temporary files\n",
        "            for key, path in self.files.items():\n",
        "                temp_path = path.with_suffix('.tmp')\n",
        "\n",
        "                if key == 'population':\n",
        "                    with open(temp_path, 'wb') as f:\n",
        "                        pickle.dump(population, f, protocol=4)\n",
        "                elif key == 'trade_memory':\n",
        "                    with open(temp_path, 'wb') as f:\n",
        "                        pickle.dump(trade_memory, f, protocol=4)\n",
        "                elif key == 'best_chrom':\n",
        "                    with open(temp_path, 'wb') as f:\n",
        "                        pickle.dump(best_chrom, f, protocol=4)\n",
        "                elif key == 'gen_count':\n",
        "                    with open(temp_path, 'wb') as f:\n",
        "                        pickle.dump(gen_count, f, protocol=4)\n",
        "                elif key == 'ga_progress':\n",
        "                    with open(temp_path, 'wb') as f:\n",
        "                        pickle.dump(ga_progress, f, protocol=4)\n",
        "\n",
        "                temp_files[key] = temp_path\n",
        "\n",
        "            # Atomic rename\n",
        "            for key, temp_path in temp_files.items():\n",
        "                temp_path.replace(self.files[key])\n",
        "\n",
        "            # Clean up backups\n",
        "            for backup_path in backup_files.values():\n",
        "                try:\n",
        "                    backup_path.unlink()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ùå {self.model_name}: Failed to save state: {e}\", \"error\")\n",
        "\n",
        "            # Restore from backups\n",
        "            for key, backup_path in backup_files.items():\n",
        "                if backup_path.exists():\n",
        "                    try:\n",
        "                        shutil.copy2(backup_path, self.files[key])\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            # Clean up temp files\n",
        "            for temp_path in temp_files.values():\n",
        "                if temp_path.exists():\n",
        "                    try:\n",
        "                        temp_path.unlink()\n",
        "                    except:\n",
        "                        pass\n",
        "            return False\n",
        "\n",
        "    def load_all(self):\n",
        "        \"\"\"Load with validation\"\"\"\n",
        "        state = {\n",
        "            'population': None,\n",
        "            'trade_memory': {},\n",
        "            'best_chrom': None,\n",
        "            'gen_count': 0,\n",
        "            'ga_progress': []\n",
        "        }\n",
        "\n",
        "        for key, path in self.files.items():\n",
        "            if key not in state:\n",
        "                continue\n",
        "\n",
        "            if path.exists():\n",
        "                try:\n",
        "                    with open(path, 'rb') as f:\n",
        "                        loaded_data = pickle.load(f)\n",
        "\n",
        "                    # Validate loaded data\n",
        "                    if key == 'population' and isinstance(loaded_data, list):\n",
        "                        state[key] = loaded_data\n",
        "                    elif key == 'trade_memory' and isinstance(loaded_data, dict):\n",
        "                        state[key] = loaded_data\n",
        "                    elif key == 'best_chrom' and isinstance(loaded_data, list):\n",
        "                        state[key] = loaded_data\n",
        "                    elif key == 'gen_count' and isinstance(loaded_data, int):\n",
        "                        state[key] = loaded_data\n",
        "                    elif key == 'ga_progress' and isinstance(loaded_data, list):\n",
        "                        state[key] = loaded_data\n",
        "                    else:\n",
        "                        state[key] = loaded_data\n",
        "\n",
        "                    print_status(f\"‚úÖ {self.model_name}: Loaded {key}\", \"debug\")\n",
        "                except Exception as e:\n",
        "                    print_status(f\"‚ö†Ô∏è {self.model_name}: Failed to load {key}: {e}\", \"warn\")\n",
        "                    try:\n",
        "                        path.unlink()\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "        return state\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED COMPETITION MANAGER\n",
        "# ======================================================\n",
        "class EnhancedCompetitionManager:\n",
        "    \"\"\"Competition system with parallel execution\"\"\"\n",
        "\n",
        "    def __init__(self, models_config=COMPETITION_MODELS):\n",
        "        self.models_config = models_config\n",
        "        self.results = {model: {} for model in models_config.keys()}\n",
        "        self.leaderboard = []\n",
        "        self.iteration = 0\n",
        "\n",
        "    def run_competition(self, data, mode=\"normal\"):\n",
        "        \"\"\"Run competition with enhanced tracking\"\"\"\n",
        "        self.iteration += 1\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"üèÜ COMPETITION ROUND #{self.iteration} ({mode.upper()} MODE)\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Use parallel execution for faster training\n",
        "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            for model_name in self.models_config.keys():\n",
        "                config = self.models_config[model_name]\n",
        "                if not config.get('enabled', True):\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\n{config['color']} Training {model_name} AI...\")\n",
        "\n",
        "                future = executor.submit(\n",
        "                    self.run_model_ga,\n",
        "                    model_name,\n",
        "                    data,\n",
        "                    config\n",
        "                )\n",
        "                futures[future] = model_name\n",
        "\n",
        "            # Collect results\n",
        "            for future in as_completed(futures):\n",
        "                model_name = futures[future]\n",
        "                try:\n",
        "                    best_chrom, trade_memory, ga_progress, metrics = future.result()\n",
        "\n",
        "                    self.results[model_name] = {\n",
        "                        'chromosome': best_chrom,\n",
        "                        'trade_memory': trade_memory,\n",
        "                        'ga_progress': ga_progress,\n",
        "                        'metrics': metrics,\n",
        "                        'config': self.models_config[model_name],\n",
        "                        'mode': mode\n",
        "                    }\n",
        "\n",
        "                    print_status(f\"‚úÖ {model_name} training complete\", \"success\")\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Failed to train {model_name}: {e}\")\n",
        "                    print_status(f\"‚ùå {model_name} training failed: {e}\", \"error\")\n",
        "                    self.results[model_name] = {}\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def run_model_ga(self, model_name, data, config):\n",
        "        \"\"\"Enhanced GA with better convergence\"\"\"\n",
        "        tf_map = build_tf_map(data)\n",
        "\n",
        "        state_manager = EnhancedModelStateManager(model_name, REPO_FOLDER)\n",
        "        saved_state = state_manager.load_all()\n",
        "\n",
        "        # Initialize population\n",
        "        if saved_state['population']:\n",
        "            population = saved_state['population']\n",
        "            print_status(f\"‚úÖ {model_name}: Loaded {len(population)} chromosomes\", \"success\")\n",
        "        else:\n",
        "            best_historical = LEARNING_SYSTEM.get_best_historical_chromosomes(model_name, top_n=5)\n",
        "            population = best_historical if best_historical else []\n",
        "\n",
        "            while len(population) < config['pop_size']:\n",
        "                population.append(create_chrom(tf_map))\n",
        "\n",
        "            print_status(f\"üÜï {model_name}: New population with {len(best_historical)} seeds\", \"info\")\n",
        "\n",
        "        trade_memory = saved_state['trade_memory']\n",
        "        best_chrom_ever = saved_state['best_chrom']\n",
        "        last_gen = saved_state['gen_count']\n",
        "        ga_progress = saved_state['ga_progress']\n",
        "\n",
        "        # Calculate baseline\n",
        "        if best_chrom_ever:\n",
        "            try:\n",
        "                best_score_ever, _, _ = run_enhanced_backtest(\n",
        "                    data, BASE_CAPITAL, *decode_chrom(best_chrom_ever, tf_map), trade_memory\n",
        "                )\n",
        "            except:\n",
        "                best_score_ever = -np.inf\n",
        "        else:\n",
        "            best_score_ever = -np.inf\n",
        "\n",
        "        # Adaptive mutation\n",
        "        base_mutation_rate = config['mutation_rate']\n",
        "        adaptive_mutation_rate = LEARNING_SYSTEM.get_smart_mutation_rate(model_name, base_mutation_rate)\n",
        "\n",
        "        # Apply model weight\n",
        "        model_weight = MODEL_WEIGHTS.get_weight(model_name)\n",
        "\n",
        "        print_status(\n",
        "            f\"üß¨ {model_name}: Mutation={adaptive_mutation_rate:.3f}, Weight={model_weight:.2f}\",\n",
        "            \"brain\"\n",
        "        )\n",
        "\n",
        "        # Evolution loop\n",
        "        stagnation_counter = 0\n",
        "        for gen in range(last_gen + 1, last_gen + 1 + config['generations']):\n",
        "            current_gen_scores = []\n",
        "\n",
        "            for c in population:\n",
        "                score, results, _ = run_enhanced_backtest(\n",
        "                    data, BASE_CAPITAL, *decode_chrom(c, tf_map), trade_memory\n",
        "                )\n",
        "                # Apply model weight to score\n",
        "                weighted_score = score * model_weight\n",
        "                current_gen_scores.append((weighted_score, c, results))\n",
        "\n",
        "            current_gen_scores.sort(reverse=True, key=lambda x: x[0])\n",
        "            best_score, best_chrom, best_results = current_gen_scores[0]\n",
        "\n",
        "            # Check for improvement\n",
        "            if best_score > best_score_ever:\n",
        "                improvement = ((best_score - best_score_ever) / abs(best_score_ever + EPS)) * 100\n",
        "                print_status(\n",
        "                    f\"üéØ {model_name} Gen {gen}: Score={best_score:.2f} (+{improvement:.1f}%)\",\n",
        "                    \"success\"\n",
        "                )\n",
        "                best_score_ever = best_score\n",
        "                best_chrom_ever = best_chrom\n",
        "                stagnation_counter = 0\n",
        "            else:\n",
        "                stagnation_counter += 1\n",
        "\n",
        "            # Adaptive mutation based on stagnation\n",
        "            if stagnation_counter > 3:\n",
        "                adaptive_mutation_rate = min(0.5, adaptive_mutation_rate * 1.2)\n",
        "            else:\n",
        "                adaptive_mutation_rate = max(0.1, adaptive_mutation_rate * 0.95)\n",
        "\n",
        "            ga_progress.append(min(100, int((best_score / (abs(best_score_ever) + EPS)) * 100)))\n",
        "\n",
        "            # Elite + Offspring\n",
        "            elite_size = max(2, int(config['pop_size'] * 0.2))\n",
        "            next_population = [c for _, c, _ in current_gen_scores[:elite_size]]\n",
        "\n",
        "            while len(next_population) < config['pop_size']:\n",
        "                p1 = tournament_select(current_gen_scores, k=TOURNAMENT_SIZE)\n",
        "                p2 = tournament_select(current_gen_scores, k=TOURNAMENT_SIZE)\n",
        "\n",
        "                # Multi-point crossover\n",
        "                crossover_points = sorted(random.sample(range(1, len(p1)-1), 2))\n",
        "                child = (\n",
        "                    p1[:crossover_points[0]] +\n",
        "                    p2[crossover_points[0]:crossover_points[1]] +\n",
        "                    p1[crossover_points[1]:]\n",
        "                )\n",
        "\n",
        "                # Adaptive mutation\n",
        "                for i in range(len(child)):\n",
        "                    if random.random() < adaptive_mutation_rate:\n",
        "                        mutation_strength = random.uniform(0.7, 1.3)\n",
        "                        child[i] *= mutation_strength\n",
        "\n",
        "                next_population.append(child)\n",
        "\n",
        "            population = next_population\n",
        "\n",
        "            # Save every 5 generations\n",
        "            if gen % 5 == 0:\n",
        "                state_manager.save_all(population, trade_memory, best_chrom_ever, gen, ga_progress)\n",
        "\n",
        "        # Final save\n",
        "        state_manager.save_all(population, trade_memory, best_chrom_ever,\n",
        "                              last_gen + config['generations'], ga_progress)\n",
        "\n",
        "        metrics = self.calculate_metrics(best_results, trade_memory)\n",
        "        return best_chrom_ever, trade_memory, ga_progress, metrics\n",
        "\n",
        "    def calculate_metrics(self, results, trade_memory):\n",
        "        \"\"\"Calculate comprehensive metrics\"\"\"\n",
        "        total_pnl = sum(r['total_pnl'] for r in results.values())\n",
        "        sharpe = np.mean([r['sharpe'] for r in results.values()])\n",
        "        max_dd = max([r['max_drawdown'] for r in results.values()])\n",
        "\n",
        "        return {\n",
        "            'total_pnl': total_pnl,\n",
        "            'sharpe': sharpe,\n",
        "            'max_drawdown': max_dd\n",
        "        }\n",
        "\n",
        "    def generate_leaderboard(self, signals_results, mode=\"normal\"):\n",
        "        \"\"\"Generate enhanced leaderboard\"\"\"\n",
        "        leaderboard_data = []\n",
        "\n",
        "        for model_name, result in self.results.items():\n",
        "            if not result or 'config' not in result or 'metrics' not in result:\n",
        "                continue\n",
        "\n",
        "            config = result['config']\n",
        "            metrics = result['metrics']\n",
        "\n",
        "            # Get real historical performance\n",
        "            history = MEMORY_SYSTEM.get_model_trade_history(model_name, days=7)\n",
        "\n",
        "            leaderboard_data.append({\n",
        "                'model': model_name,\n",
        "                'color': config['color'],\n",
        "                'hex_color': config['hex_color'],\n",
        "                'pnl': history['total_pnl'],\n",
        "                'sharpe': metrics['sharpe'],\n",
        "                'max_dd': metrics['max_drawdown'],\n",
        "                'total_trades': history['total_trades'],\n",
        "                'successful_trades': history['successful_trades'],\n",
        "                'accuracy': history['accuracy'],\n",
        "                'strategy': config['strategy'],\n",
        "                'avg_win': history.get('avg_win', 0),\n",
        "                'avg_loss': history.get('avg_loss', 0),\n",
        "                'profit_factor': history.get('profit_factor', 0),\n",
        "                'best_trade': history.get('best_trade', 0),\n",
        "                'worst_trade': history.get('worst_trade', 0)\n",
        "            })\n",
        "\n",
        "        if not leaderboard_data:\n",
        "            return []\n",
        "\n",
        "        # Sort by combined score: PnL + accuracy\n",
        "        leaderboard_data.sort(\n",
        "            key=lambda x: x['pnl'] + (x['accuracy'] / 10),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        self.leaderboard = leaderboard_data\n",
        "\n",
        "        # Update model weights based on performance\n",
        "        performance_data = {\n",
        "            entry['model']: {\n",
        "                'total_pnl': entry['pnl'],\n",
        "                'accuracy': entry['accuracy']\n",
        "            }\n",
        "            for entry in leaderboard_data\n",
        "        }\n",
        "        MODEL_WEIGHTS.update_weights(performance_data)\n",
        "\n",
        "        return leaderboard_data\n",
        "\n",
        "competition = EnhancedCompetitionManager()\n",
        "\n",
        "# ======================================================\n",
        "# GENERATE LIVE SIGNALS (Parallel)\n",
        "# ======================================================\n",
        "def generate_live_signals_parallel(best, data, model_name=\"Unknown\", replay_system=None):\n",
        "    \"\"\"Generate signals with parallel processing\"\"\"\n",
        "    tf_map = build_tf_map(data)\n",
        "    atr_sl, atr_tp, risk, conf, tf_weights = decode_chrom(best, tf_map)\n",
        "\n",
        "    live_signals = {}\n",
        "    is_replay_mode = replay_system is not None and hasattr(replay_system, 'is_replay_mode') and replay_system.is_replay_mode\n",
        "\n",
        "    def process_pair(pair):\n",
        "        tfs = data.get(pair, {})\n",
        "        if not tfs:\n",
        "            return None, None\n",
        "\n",
        "        # Get price\n",
        "        if is_replay_mode:\n",
        "            price = replay_system.get_historical_price(pair)\n",
        "            if price is None or price <= 0:\n",
        "                price = float(list(tfs.values())[0]['close'].iloc[-1])\n",
        "        else:\n",
        "            price = fetch_live_rate(pair)\n",
        "            if price <= 0:\n",
        "                price = float(list(tfs.values())[0]['close'].iloc[-1])\n",
        "\n",
        "        # Calculate signal strength\n",
        "        sig_strength = sum([\n",
        "            tf_weights.get(pair, {}).get(tf, 0.0) * tfs[tf][\"hybrid_signal\"].iloc[-1]\n",
        "            for tf in tf_map.get(pair, [])\n",
        "        ])\n",
        "\n",
        "        recent_atr = np.mean([\n",
        "            tfs[tf][\"atr\"].iloc[-1] for tf in tf_map.get(pair, [])\n",
        "        ]) if tfs else 1.0\n",
        "\n",
        "        sig_strength_scaled = sig_strength / (recent_atr + EPS)\n",
        "        noise_factor = random.uniform(0.9, 1.1)\n",
        "        sig_strength_scaled *= noise_factor\n",
        "\n",
        "        # Determine direction\n",
        "        if sig_strength_scaled > 0:\n",
        "            direction = \"BUY\"\n",
        "        elif sig_strength_scaled < 0:\n",
        "            direction = \"SELL\"\n",
        "        else:\n",
        "            direction = \"HOLD\"\n",
        "\n",
        "        # Calculate confidence score\n",
        "        raw_score = abs(sig_strength_scaled) * 100\n",
        "        score_100 = int(35 + (50 * (raw_score / (raw_score + 10))))\n",
        "        score_100 = min(max(score_100, 30), 90)\n",
        "        high_conf = score_100 >= 70\n",
        "\n",
        "        # Calculate SL/TP\n",
        "        max_sl_tp_distance = recent_atr * MAX_ATR_SL\n",
        "        min_sl_tp_distance = recent_atr * MIN_ATR_DISTANCE\n",
        "\n",
        "        if direction == \"BUY\":\n",
        "            base_sl = price - atr_sl * recent_atr\n",
        "            base_tp = price + atr_tp * recent_atr\n",
        "            SL = max(min(base_sl, price - min_sl_tp_distance), price - max_sl_tp_distance)\n",
        "            TP = min(max(base_tp, price + min_sl_tp_distance), price + max_sl_tp_distance)\n",
        "        elif direction == \"SELL\":\n",
        "            base_sl = price + atr_sl * recent_atr\n",
        "            base_tp = price - atr_tp * recent_atr\n",
        "            SL = min(max(base_sl, price + min_sl_tp_distance), price + max_sl_tp_distance)\n",
        "            TP = max(min(base_tp, price - min_sl_tp_distance), price - max_sl_tp_distance)\n",
        "        else:\n",
        "            SL = TP = price\n",
        "\n",
        "        signal = {\n",
        "            \"direction\": direction,\n",
        "            \"strength\": float(sig_strength_scaled),\n",
        "            \"score_1_100\": score_100,\n",
        "            \"last_price\": float(price),\n",
        "            \"SL\": float(SL),\n",
        "            \"TP\": float(TP),\n",
        "            \"high_confidence\": high_conf,\n",
        "            \"atr\": float(recent_atr),\n",
        "            \"atr_multiplier_sl\": float(atr_sl),\n",
        "            \"atr_multiplier_tp\": float(atr_tp),\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
        "            \"model\": model_name,\n",
        "            \"mode\": \"replay\" if is_replay_mode else \"live\"\n",
        "        }\n",
        "\n",
        "        return pair, signal\n",
        "\n",
        "    # Process pairs in parallel\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        futures = [executor.submit(process_pair, pair) for pair in PAIRS]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                pair, signal = future.result()\n",
        "                if pair and signal:\n",
        "                    live_signals[pair] = signal\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Signal generation failed: {e}\")\n",
        "\n",
        "    return live_signals\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED EMAIL FUNCTIONS\n",
        "# ======================================================\n",
        "def send_email_alert(subject, body_html, to_email=GMAIL_USER):\n",
        "    \"\"\"Send email with retry logic\"\"\"\n",
        "    if not WEEKEND_MONDAY_MANAGER.should_send_email():\n",
        "        print_status(\"üìß Email skipped (Replay Mode)\", \"info\")\n",
        "        return False\n",
        "\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            msg = MIMEMultipart('alternative')\n",
        "            msg['Subject'] = subject\n",
        "            msg['From'] = GMAIL_USER\n",
        "            msg['To'] = to_email\n",
        "\n",
        "            html_part = MIMEText(body_html, 'html')\n",
        "            msg.attach(html_part)\n",
        "\n",
        "            with smtplib.SMTP_SSL('smtp.gmail.com', 465, timeout=30) as server:\n",
        "                server.login(GMAIL_USER, GMAIL_APP_PASSWORD)\n",
        "                server.send_message(msg)\n",
        "\n",
        "            print_status(f\"‚úÖ Email sent successfully\", \"success\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            if attempt < 2:\n",
        "                print_status(f\"‚ö†Ô∏è Email attempt {attempt + 1} failed, retrying...\", \"warn\")\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                print_status(f\"‚ùå Email failed after 3 attempts: {e}\", \"error\")\n",
        "                return False\n",
        "\n",
        "def build_enhanced_leaderboard_html(leaderboard):\n",
        "    \"\"\"Build enhanced leaderboard with more metrics\"\"\"\n",
        "    rows = \"\"\n",
        "    medals = [\"ü•á\", \"ü•à\", \"ü•â\"]\n",
        "\n",
        "    for idx, entry in enumerate(leaderboard[:10]):\n",
        "        medal = medals[idx] if idx < 3 else f\"{idx + 1}.\"\n",
        "\n",
        "        pnl_color = \"#27ae60\" if entry['pnl'] > 0 else \"#e74c3c\"\n",
        "        acc_color = \"#27ae60\" if entry['accuracy'] > 60 else \"#e67e22\" if entry['accuracy'] > 45 else \"#e74c3c\"\n",
        "\n",
        "        rows += f\"\"\"\n",
        "        <tr style=\"border-bottom: 1px solid #ecf0f1;\">\n",
        "            <td style=\"padding: 12px; text-align: center; font-size: 20px;\">{medal}</td>\n",
        "            <td style=\"padding: 12px;\">\n",
        "                <div style=\"display: flex; align-items: center; gap: 8px;\">\n",
        "                    <span style=\"font-size: 20px;\">{entry['color']}</span>\n",
        "                    <div>\n",
        "                        <div style=\"font-weight: 600; color: #2c3e50;\">{entry['model']}</div>\n",
        "                        <div style=\"font-size: 12px; color: #7f8c8d;\">{entry['strategy']}</div>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </td>\n",
        "            <td style=\"padding: 12px; text-align: center; font-weight: 600; color: {pnl_color};\">\n",
        "                ${entry['pnl']:.2f}\n",
        "            </td>\n",
        "            <td style=\"padding: 12px; text-align: center; font-weight: 600; color: {acc_color};\">\n",
        "                {entry['accuracy']:.1f}%\n",
        "            </td>\n",
        "            <td style=\"padding: 12px; text-align: center; color: #7f8c8d;\">\n",
        "                {entry['total_trades']}\n",
        "            </td>\n",
        "            <td style=\"padding: 12px; text-align: center; color: #7f8c8d;\">\n",
        "                {entry['profit_factor']:.2f}\n",
        "            </td>\n",
        "        </tr>\n",
        "        \"\"\"\n",
        "\n",
        "    return f\"\"\"\n",
        "    <table style=\"width: 100%; border-collapse: collapse; margin: 20px 0; background: white; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
        "        <thead>\n",
        "            <tr style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;\">\n",
        "                <th style=\"padding: 15px; text-align: center;\">Rank</th>\n",
        "                <th style=\"padding: 15px; text-align: left;\">Model</th>\n",
        "                <th style=\"padding: 15px; text-align: center;\">PnL</th>\n",
        "                <th style=\"padding: 15px; text-align: center;\">Accuracy</th>\n",
        "                <th style=\"padding: 15px; text-align: center;\">Trades</th>\n",
        "                <th style=\"padding: 15px; text-align: center;\">Profit Factor</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "            {rows}\n",
        "        </tbody>\n",
        "    </table>\n",
        "    \"\"\"\n",
        "\n",
        "def build_signals_html(signals_by_model, leaderboard):\n",
        "    \"\"\"Build signals HTML with enhanced visuals\"\"\"\n",
        "    html = \"\"\n",
        "    top_models = [entry['model'] for entry in leaderboard[:3]]\n",
        "\n",
        "    for model_name in top_models:\n",
        "        if model_name not in signals_by_model:\n",
        "            continue\n",
        "\n",
        "        signals = signals_by_model[model_name]\n",
        "        model_entry = next((e for e in leaderboard if e['model'] == model_name), None)\n",
        "\n",
        "        if not model_entry:\n",
        "            continue\n",
        "\n",
        "        html += f\"\"\"\n",
        "        <div style=\"margin: 30px 0; padding: 20px; background: white; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); border-left: 4px solid {model_entry['hex_color']};\">\n",
        "            <h3 style=\"margin: 0 0 15px 0; color: #2c3e50; display: flex; align-items: center; gap: 10px;\">\n",
        "                <span style=\"font-size: 24px;\">{model_entry['color']}</span>\n",
        "                {model_name}\n",
        "                <span style=\"font-size: 14px; color: #7f8c8d; font-weight: normal;\">\n",
        "                    ({model_entry['accuracy']:.1f}% accuracy | {model_entry['total_trades']} trades)\n",
        "                </span>\n",
        "            </h3>\n",
        "            <div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px;\">\n",
        "        \"\"\"\n",
        "\n",
        "        for pair, sig in signals.items():\n",
        "            if sig['direction'] == 'HOLD':\n",
        "                continue\n",
        "\n",
        "            direction_color = \"#27ae60\" if sig['direction'] == \"BUY\" else \"#e74c3c\"\n",
        "            direction_emoji = \"üìà\" if sig['direction'] == \"BUY\" else \"üìâ\"\n",
        "            confidence_bar = \"‚ñä\" * int(sig['score_1_100'] / 10)\n",
        "\n",
        "            # Calculate risk/reward ratio\n",
        "            if sig['direction'] == \"BUY\":\n",
        "                risk = sig['last_price'] - sig['SL']\n",
        "                reward = sig['TP'] - sig['last_price']\n",
        "            else:\n",
        "                risk = sig['SL'] - sig['last_price']\n",
        "                reward = sig['last_price'] - sig['TP']\n",
        "\n",
        "            rr_ratio = (reward / risk) if risk > 0 else 0\n",
        "\n",
        "            html += f\"\"\"\n",
        "            <div style=\"padding: 15px; background: #f8f9fa; border-radius: 8px; border: 2px solid {direction_color};\">\n",
        "                <div style=\"display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;\">\n",
        "                    <span style=\"font-weight: 600; font-size: 16px; color: #2c3e50;\">{pair}</span>\n",
        "                    <span style=\"font-size: 20px;\">{direction_emoji}</span>\n",
        "                </div>\n",
        "                <div style=\"background: {direction_color}; color: white; padding: 8px; border-radius: 6px; text-align: center; font-weight: 600; margin-bottom: 10px;\">\n",
        "                    {sig['direction']}\n",
        "                </div>\n",
        "                <div style=\"font-size: 13px; color: #7f8c8d; margin-bottom: 5px;\">\n",
        "                    <strong>Entry:</strong> {sig['last_price']:.5f}\n",
        "                </div>\n",
        "                <div style=\"font-size: 13px; color: #27ae60; margin-bottom: 5px;\">\n",
        "                    <strong>TP:</strong> {sig['TP']:.5f}\n",
        "                </div>\n",
        "                <div style=\"font-size: 13px; color: #e74c3c; margin-bottom: 5px;\">\n",
        "                    <strong>SL:</strong> {sig['SL']:.5f}\n",
        "                </div>\n",
        "                <div style=\"font-size: 12px; color: #3498db; margin-bottom: 10px;\">\n",
        "                    <strong>R:R:</strong> 1:{rr_ratio:.2f}\n",
        "                </div>\n",
        "                <div style=\"font-size: 12px; color: #7f8c8d;\">\n",
        "                    <strong>Confidence:</strong> {sig['score_1_100']}/100\n",
        "                </div>\n",
        "                <div style=\"font-size: 10px; color: {direction_color}; margin-top: 5px;\">\n",
        "                    {confidence_bar}\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        html += \"\"\"\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    return html\n",
        "\n",
        "def send_competition_results_email(leaderboard, signals_by_model, mode=\"normal\", stats=None):\n",
        "    \"\"\"Send enhanced competition email\"\"\"\n",
        "    if not WEEKEND_MONDAY_MANAGER.should_send_email():\n",
        "        print_status(\"üìß Competition email skipped (Replay Mode)\", \"info\")\n",
        "        return False\n",
        "\n",
        "    mode_badge = {\n",
        "        \"normal\": \"üíº LIVE MODE\",\n",
        "        \"weekend_replay\": \"üé¨ WEEKEND REPLAY\",\n",
        "        \"monday_replay\": \"üî¥ MONDAY REPLAY\"\n",
        "    }.get(mode, \"üíº LIVE MODE\")\n",
        "\n",
        "    leaderboard_html = build_enhanced_leaderboard_html(leaderboard)\n",
        "    signals_html = build_signals_html(signals_by_model, leaderboard)\n",
        "\n",
        "    winner = leaderboard[0] if leaderboard else None\n",
        "    winner_badge = f\"{winner['color']} {winner['model']}\" if winner else \"N/A\"\n",
        "\n",
        "    # Learning progress\n",
        "    learning_report = LEARNING_SYSTEM.get_learning_report()\n",
        "    convergence_status = \"üéØ Converged\" if learning_report.get('is_converging') else \"üîÑ Evolving\"\n",
        "\n",
        "    # System stats\n",
        "    stats_html = \"\"\n",
        "    if stats:\n",
        "        stats_html = f\"\"\"\n",
        "        <div style=\"padding: 20px; background: #e8f5e9; border-radius: 8px; margin: 20px 0;\">\n",
        "            <h3 style=\"margin: 0 0 10px 0; color: #2c3e50;\">üìä System Statistics</h3>\n",
        "            <div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px; font-size: 14px;\">\n",
        "                <div><strong>Total Iterations:</strong> {stats['total_iterations']}</div>\n",
        "                <div><strong>Success Rate:</strong> {stats['success_rate']:.1f}%</div>\n",
        "                <div><strong>Days Running:</strong> {stats['days_running']}</div>\n",
        "                <div><strong>Avg Runs/Day:</strong> {stats['avg_iterations_per_day']:.1f}</div>\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    html_body = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    </head>\n",
        "    <body style=\"margin: 0; padding: 0; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px;\">\n",
        "        <div style=\"max-width: 800px; margin: 0 auto; background: #ffffff; border-radius: 16px; overflow: hidden; box-shadow: 0 10px 40px rgba(0,0,0,0.3);\">\n",
        "\n",
        "            <!-- Header -->\n",
        "            <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 40px 30px; text-align: center; color: white;\">\n",
        "                <img src=\"{LOGO_URL}\" alt=\"Logo\" style=\"width: 80px; height: 80px; border-radius: 50%; border: 4px solid white; margin-bottom: 15px;\">\n",
        "                <h1 style=\"margin: 0; font-size: 32px; font-weight: 700;\">üèÜ Trade Beacon AI v8.0</h1>\n",
        "                <p style=\"margin: 5px 0 0 0; font-size: 18px; opacity: 0.95;\">Competition Results</p>\n",
        "                <p style=\"margin: 10px 0 0 0; font-size: 16px; opacity: 0.9;\">{mode_badge}</p>\n",
        "                <p style=\"margin: 5px 0 0 0; font-size: 14px; opacity: 0.8;\">{datetime.now().strftime('%B %d, %Y ‚Ä¢ %I:%M %p UTC')}</p>\n",
        "            </div>\n",
        "\n",
        "            <!-- Winner Announcement -->\n",
        "            <div style=\"padding: 30px; background: linear-gradient(135deg, #f6d365 0%, #fda085 100%); text-align: center;\">\n",
        "                <div style=\"font-size: 48px; margin-bottom: 10px;\">üëë</div>\n",
        "                <h2 style=\"margin: 0; color: #2c3e50; font-size: 24px;\">Competition Winner</h2>\n",
        "                <div style=\"font-size: 28px; font-weight: 700; color: #2c3e50; margin-top: 10px;\">\n",
        "                    {winner_badge}\n",
        "                </div>\n",
        "                {f'<div style=\"color: #27ae60; font-size: 20px; font-weight: 600; margin-top: 10px;\">${winner[\"pnl\"]:.2f} PnL | {winner[\"accuracy\"]:.1f}% Accuracy</div>' if winner else ''}\n",
        "            </div>\n",
        "\n",
        "            {stats_html}\n",
        "\n",
        "            <!-- AI Learning Status -->\n",
        "            <div style=\"padding: 20px; background: #fff3cd; border-left: 4px solid #ffc107; margin: 20px;\">\n",
        "                <h3 style=\"margin: 0 0 10px 0; color: #856404;\">üß† AI Learning Status</h3>\n",
        "                <div style=\"font-size: 14px; color: #856404;\">\n",
        "                    <strong>Status:</strong> {convergence_status}<br>\n",
        "                    <strong>Adaptation Score:</strong> {learning_report['adaptation_score']:.1f}/100<br>\n",
        "                    <strong>Learning Trend:</strong> {learning_report['learning_trend']}<br>\n",
        "                    <strong>Success Rate:</strong> {learning_report['success_rate']:.1f}%\n",
        "                </div>\n",
        "            </div>\n",
        "\n",
        "            <!-- Leaderboard -->\n",
        "            <div style=\"padding: 30px;\">\n",
        "                <h2 style=\"color: #2c3e50; font-size: 24px; margin: 0 0 20px 0;\">\n",
        "                    üìä Performance Leaderboard\n",
        "                </h2>\n",
        "                {leaderboard_html}\n",
        "            </div>\n",
        "\n",
        "            <!-- Trading Signals -->\n",
        "            <div style=\"padding: 30px; background: #f8f9fa;\">\n",
        "                <h2 style=\"color: #2c3e50; font-size: 24px; margin: 0 0 20px 0;\">\n",
        "                    üì° Active Trading Signals\n",
        "                </h2>\n",
        "                {signals_html}\n",
        "            </div>\n",
        "\n",
        "            <!-- Disclaimer -->\n",
        "            <div style=\"padding: 25px; background: #fff3cd; border-top: 3px solid #ffc107;\">\n",
        "                <h3 style=\"margin: 0 0 10px 0; color: #856404; font-size: 16px;\">\n",
        "                    ‚ö†Ô∏è IMPORTANT DISCLAIMER\n",
        "                </h3>\n",
        "                <p style=\"margin: 0; font-size: 13px; color: #856404; line-height: 1.6;\">\n",
        "                    <strong>Trading Risk Warning:</strong> Forex trading involves substantial risk of loss and is not suitable for all investors.\n",
        "                    Past performance is not indicative of future results. These signals are generated by AI models for educational purposes only\n",
        "                    and should not be considered as financial advice. Always conduct your own research and consult with a qualified financial\n",
        "                    advisor. Trade at your own risk.\n",
        "                </p>\n",
        "            </div>\n",
        "\n",
        "            <!-- Footer -->\n",
        "            <div style=\"padding: 30px; background: #2c3e50; color: white; text-align: center;\">\n",
        "                <p style=\"margin: 0; font-size: 14px; opacity: 0.9;\">\n",
        "                    ü§ñ Trade Beacon v8.0 - Enhanced Multi-Model AI Competition System\n",
        "                </p>\n",
        "                <p style=\"margin: 10px 0 0 0; font-size: 12px; opacity: 0.7;\">\n",
        "                    Next update in 1 hour ‚Ä¢ Powered by Advanced Genetic Algorithms\n",
        "                </p>\n",
        "            </div>\n",
        "\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    subject = f\"üèÜ Trade Beacon v8.0 Competition #{competition.iteration} - Winner: {winner_badge}\"\n",
        "\n",
        "    return send_email_alert(subject, html_body)\n",
        "\n",
        "# ======================================================\n",
        "# GIT OPERATIONS\n",
        "# ======================================================\n",
        "def git_push_changes(message=\"Auto update\"):\n",
        "    \"\"\"Enhanced git push with retry\"\"\"\n",
        "    max_retries = 3\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if not REPO_FOLDER.exists():\n",
        "                subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_FOLDER)], check=True)\n",
        "\n",
        "            os.chdir(REPO_FOLDER)\n",
        "            subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
        "\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"diff\", \"--cached\", \"--quiet\"],\n",
        "                capture_output=True\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print_status(\"No changes to commit\", \"info\")\n",
        "                return True\n",
        "\n",
        "            subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n",
        "            subprocess.run([\"git\", \"push\", \"origin\", BRANCH], check=True)\n",
        "\n",
        "            print_status(f\"‚úÖ Pushed changes: {message}\", \"success\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print_status(f\"‚ö†Ô∏è Git push attempt {attempt + 1} failed, retrying...\", \"warn\")\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                print_status(f\"‚ùå Git push failed after {max_retries} attempts: {e}\", \"error\")\n",
        "                return False\n",
        "        finally:\n",
        "            os.chdir(ROOT_PATH)\n",
        "\n",
        "# ======================================================\n",
        "# CLEANUP FUNCTIONS\n",
        "# ======================================================\n",
        "def cleanup_corrupted_files():\n",
        "    \"\"\"Enhanced cleanup with pattern matching\"\"\"\n",
        "    print_status(\"üßπ Cleaning corrupted files...\", \"info\")\n",
        "\n",
        "    patterns = [\n",
        "        \"*_population_v8.pkl\",\n",
        "        \"*_trade_memory_v8.pkl\",\n",
        "        \"*_best_chrom_v8.pkl\",\n",
        "        \"*_gen_count_v8.pkl\",\n",
        "        \"*_ga_progress_v8.pkl\",\n",
        "        \"previous_signals_v8.pkl\",\n",
        "        \"learning_progress_v8.pkl\",\n",
        "        \"model_weights_v8.pkl\"\n",
        "    ]\n",
        "\n",
        "    corrupted_count = 0\n",
        "    for pattern in patterns:\n",
        "        for pkl_file in REPO_FOLDER.glob(pattern):\n",
        "            try:\n",
        "                with open(pkl_file, 'rb') as f:\n",
        "                    pickle.load(f)\n",
        "            except:\n",
        "                try:\n",
        "                    pkl_file.unlink()\n",
        "                    print_status(f\"üóëÔ∏è Removed: {pkl_file.name}\", \"warn\")\n",
        "                    corrupted_count += 1\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    if corrupted_count > 0:\n",
        "        print_status(f\"‚úÖ Cleaned {corrupted_count} corrupted files\", \"success\")\n",
        "    else:\n",
        "        print_status(\"‚úÖ No corrupted files found\", \"success\")\n",
        "\n",
        "def load_previous_signals():\n",
        "    \"\"\"Load signals with validation\"\"\"\n",
        "    if PREVIOUS_SIGNALS_FILE.exists():\n",
        "        try:\n",
        "            with open(PREVIOUS_SIGNALS_FILE, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            if isinstance(data, dict) and 'signals' in data:\n",
        "                return data\n",
        "        except:\n",
        "            pass\n",
        "    return {}\n",
        "\n",
        "def save_previous_signals(signals_by_model):\n",
        "    \"\"\"Save signals atomically\"\"\"\n",
        "    try:\n",
        "        temp_file = PREVIOUS_SIGNALS_FILE.with_suffix('.tmp')\n",
        "        with open(temp_file, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'signals': signals_by_model,\n",
        "                'timestamp': datetime.now(timezone.utc)\n",
        "            }, f, protocol=4)\n",
        "        temp_file.replace(PREVIOUS_SIGNALS_FILE)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save signals: {e}\")\n",
        "\n",
        "# ======================================================\n",
        "# MAIN EXECUTION\n",
        "# ======================================================\n",
        "def main():\n",
        "    \"\"\"Enhanced main execution with better error handling\"\"\"\n",
        "    print_status(\"=\" * 70, \"rocket\")\n",
        "    print_status(\"üöÄ FOREX PIPELINE v8.0 ENHANCED - SINGLE RUN MODE\", \"rocket\")\n",
        "    print_status(\"=\" * 70, \"rocket\")\n",
        "\n",
        "    run_success = False\n",
        "\n",
        "    try:\n",
        "        # Increment iteration\n",
        "        current_iteration = ITERATION_COUNTER.get_current() + 1\n",
        "        stats = ITERATION_COUNTER.get_stats()\n",
        "\n",
        "        if stats:\n",
        "            print_status(f\"üìä LIFETIME STATS:\", \"chart\")\n",
        "            for key, value in stats.items():\n",
        "                if isinstance(value, float):\n",
        "                    print_status(f\"   {key}: {value:.2f}\", \"info\")\n",
        "                else:\n",
        "                    print_status(f\"   {key}: {value}\", \"info\")\n",
        "\n",
        "        print_status(f\"üî¢ Current Iteration: #{current_iteration}\", \"info\")\n",
        "\n",
        "        # Cleanup\n",
        "        cleanup_corrupted_files()\n",
        "\n",
        "        # Load data\n",
        "        print_status(\"\\nüì¶ Loading historical data...\", \"info\")\n",
        "        combined_data = load_unified_pickles(PICKLE_FOLDER)\n",
        "\n",
        "        if not combined_data:\n",
        "            raise ValueError(\"No data loaded!\")\n",
        "\n",
        "        print_status(f\"‚úÖ Loaded {len(combined_data)} pairs\", \"success\")\n",
        "\n",
        "        # Determine mode\n",
        "        current_mode = WEEKEND_MONDAY_MANAGER.get_mode()\n",
        "        status_message = WEEKEND_MONDAY_MANAGER.get_status_message()\n",
        "        print_status(status_message, \"info\")\n",
        "\n",
        "        # Evaluate previous signals\n",
        "        previous_signals_data = load_previous_signals()\n",
        "        trade_outcomes = None\n",
        "\n",
        "        if previous_signals_data and 'signals' in previous_signals_data:\n",
        "            print_status(\"\\nüîç Evaluating previous signals...\", \"info\")\n",
        "\n",
        "            current_prices = {}\n",
        "            for pair in PAIRS:\n",
        "                if current_mode in [\"weekend_replay\", \"monday_replay\"]:\n",
        "                    if pair in combined_data and combined_data[pair]:\n",
        "                        current_prices[pair] = float(list(combined_data[pair].values())[0]['close'].iloc[-1])\n",
        "                else:\n",
        "                    live_price = fetch_live_rate(pair)\n",
        "                    if live_price > 0:\n",
        "                        current_prices[pair] = live_price\n",
        "                    elif pair in combined_data and combined_data[pair]:\n",
        "                        current_prices[pair] = float(list(combined_data[pair].values())[0]['close'].iloc[-1])\n",
        "\n",
        "            trade_outcomes = TRADE_TRACKER.evaluate_outcomes(\n",
        "                current_prices,\n",
        "                datetime.now(timezone.utc)\n",
        "            )\n",
        "\n",
        "            if trade_outcomes:\n",
        "                print_status(\"\\nüìà TRADE OUTCOMES:\", \"money\")\n",
        "                for model_name, outcomes in trade_outcomes.items():\n",
        "                    print_status(\n",
        "                        f\"{model_name}: {outcomes['wins']}/{outcomes['closed_trades']} wins \"\n",
        "                        f\"({outcomes['accuracy']:.1f}%) | P&L: ${outcomes['total_pnl_after_costs']:.2f}\",\n",
        "                        \"success\" if outcomes['total_pnl_after_costs'] > 0 else \"warn\"\n",
        "                    )\n",
        "\n",
        "        # Initialize replay if needed\n",
        "        replay_system = None\n",
        "        if current_mode in [\"weekend_replay\", \"monday_replay\"]:\n",
        "            replay_system = EnhancedReplaySystem(combined_data, random_selection=True)\n",
        "            if current_mode == \"monday_replay\":\n",
        "                WEEKEND_MONDAY_MANAGER.increment_monday_runs()\n",
        "\n",
        "            working_data = {\n",
        "                pair: replay_system.get_available_data(pair)\n",
        "                for pair in PAIRS\n",
        "            }\n",
        "            working_data = {k: v for k, v in working_data.items() if v}\n",
        "        else:\n",
        "            working_data = combined_data\n",
        "\n",
        "        # Run competition\n",
        "        print_status(\"\\nüèÜ Starting Competition...\", \"info\")\n",
        "        competition_results = competition.run_competition(working_data, mode=current_mode)\n",
        "\n",
        "        # Generate signals\n",
        "        signals_by_model = {}\n",
        "        for model_name, result in competition_results.items():\n",
        "            if not result or 'chromosome' not in result:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                signals = generate_live_signals_parallel(\n",
        "                    result['chromosome'],\n",
        "                    working_data,\n",
        "                    model_name,\n",
        "                    replay_system=replay_system if current_mode in [\"weekend_replay\", \"monday_replay\"] else None\n",
        "                )\n",
        "                signals_by_model[model_name] = signals\n",
        "                print_status(f\"‚úÖ {model_name}: Signals generated\", \"success\")\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ùå {model_name}: Failed - {e}\", \"error\")\n",
        "\n",
        "        # Store signals\n",
        "        if signals_by_model:\n",
        "            TRADE_TRACKER.store_signals(signals_by_model, datetime.now(timezone.utc))\n",
        "            save_previous_signals(signals_by_model)\n",
        "\n",
        "        # Record iteration\n",
        "        if trade_outcomes:\n",
        "            LEARNING_SYSTEM.record_iteration(competition_results, trade_outcomes)\n",
        "        else:\n",
        "            LEARNING_SYSTEM.record_iteration(competition_results)\n",
        "\n",
        "        # Generate leaderboard\n",
        "        leaderboard = competition.generate_leaderboard(signals_by_model, mode=current_mode)\n",
        "\n",
        "        if leaderboard:\n",
        "            print_status(f\"\\nüèÜ TOP 3 MODELS:\", \"success\")\n",
        "            for idx, entry in enumerate(leaderboard[:3], 1):\n",
        "                medal = [\"ü•á\", \"ü•à\", \"ü•â\"][idx - 1]\n",
        "                print_status(\n",
        "                    f\"{medal} {entry['model']}: ${entry['pnl']:.2f} | \"\n",
        "                    f\"{entry['accuracy']:.1f}% | {entry['total_trades']} trades\",\n",
        "                    \"info\"\n",
        "                )\n",
        "\n",
        "            # Learning report\n",
        "            learning_report = LEARNING_SYSTEM.get_learning_report()\n",
        "            print_status(f\"\\nüß† LEARNING PROGRESS:\", \"brain\")\n",
        "            print_status(f\"   Iterations: {learning_report['total_iterations']}\", \"info\")\n",
        "            print_status(f\"   Adaptation: {learning_report['adaptation_score']:.1f}/100\", \"info\")\n",
        "            print_status(f\"   Trend: {learning_report['learning_trend']}\", \"info\")\n",
        "\n",
        "        # Save signals JSON\n",
        "        try:\n",
        "            if leaderboard and signals_by_model:\n",
        "                top_model = leaderboard[0]['model']\n",
        "                if top_model in signals_by_model:\n",
        "                    with open(SIGNALS_JSON_PATH, 'w') as f:\n",
        "                        json.dump(signals_by_model[top_model], f, indent=2, default=str)\n",
        "                    print_status(\"‚úÖ Saved broker signals\", \"success\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to save signals: {e}\", \"warn\")\n",
        "\n",
        "        # Send email\n",
        "        if leaderboard and signals_by_model:\n",
        "            send_competition_results_email(leaderboard, signals_by_model, mode=current_mode, stats=stats)\n",
        "\n",
        "        # Git push\n",
        "        commit_msg = f\"v8.0 Auto Update - Iter #{current_iteration} - {current_mode.upper()} - {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}\"\n",
        "        git_push_changes(commit_msg)\n",
        "\n",
        "        run_success = True\n",
        "        print_status(\"\\n‚úÖ Pipeline completed successfully!\", \"success\")\n",
        "        print_status(f\"üéØ Iteration #{current_iteration} finished\", \"info\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print_status(\"\\n‚ö†Ô∏è Shutdown requested\", \"warn\")\n",
        "    except Exception as e:\n",
        "        print_status(f\"\\n‚ùå Fatal error: {e}\", \"error\")\n",
        "        logging.exception(\"Fatal error in main\")\n",
        "        sys.exit(1)\n",
        "    finally:\n",
        "        # Finalize iteration counter\n",
        "        ITERATION_COUNTER.increment(success=run_success)\n",
        "\n",
        "        # Cleanup\n",
        "        print_status(\"\\nüõë Cleaning up...\", \"info\")\n",
        "        MEMORY_SYSTEM.close()\n",
        "        print_status(\"‚úÖ Cleanup complete\", \"success\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Ogc6kwsLwhk0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}