{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr_DWDx4-LLJ"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üîë API Keys Configuration\n",
        "# ======================================================\n",
        "import os\n",
        "\n",
        "# Set API keys from environment variables or defaults\n",
        "ALPHA_VANTAGE_KEY = os.environ.get('ALPHA_VANTAGE_KEY', '1W58NPZXOG5SLHZ6')\n",
        "BROWSERLESS_TOKEN = os.environ.get('BROWSERLESS_TOKEN', '2TMVUBAjFwrr7Tb283f0da6602a4cb698b81778bda61967f7')\n",
        "\n",
        "# Set environment variables for downstream code\n",
        "os.environ['ALPHA_VANTAGE_KEY'] = ALPHA_VANTAGE_KEY\n",
        "os.environ['BROWSERLESS_TOKEN'] = BROWSERLESS_TOKEN\n",
        "\n",
        "# Validate\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    print(\"‚ö†Ô∏è Warning: ALPHA_VANTAGE_KEY not set!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Alpha Vantage Key: {ALPHA_VANTAGE_KEY[:4]}...{ALPHA_VANTAGE_KEY[-4:]}\")\n",
        "\n",
        "if not BROWSERLESS_TOKEN:\n",
        "    print(\"‚ö†Ô∏è Warning: BROWSERLESS_TOKEN not set!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Browserless Token: {BROWSERLESS_TOKEN[:4]}...{BROWSERLESS_TOKEN[-4:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H52F2WkfvWOc"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üåç Environment Detection & Setup (MUST RUN FIRST!)\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "# Override ENV_NAME if in GitHub Actions\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "# Set base paths based on environment\n",
        "if IN_COLAB:\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "elif IN_GHA:\n",
        "    # GitHub Actions already checks out the repo\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    # Local development\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "\n",
        "# Create necessary directories with organized structure\n",
        "DIRECTORIES = {\n",
        "    \"data_raw\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "}\n",
        "\n",
        "# Create all directories\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Display environment info\n",
        "print(\"=\" * 60)\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üîß Python: {sys.version.split()[0]}\")\n",
        "print(f\"üìç Working Dir: {os.getcwd()}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Validate critical environment variables for GitHub Actions\n",
        "if IN_GHA:\n",
        "    required_vars = [\"FOREX_PAT\", \"GIT_USER_NAME\", \"GIT_USER_EMAIL\"]\n",
        "    missing = [v for v in required_vars if not os.environ.get(v)]\n",
        "    if missing:\n",
        "        print(f\"‚ö†Ô∏è  Warning: Missing environment variables: {', '.join(missing)}\")\n",
        "        sys.exit(1)  # Fail fast in CI if critical vars missing\n",
        "    else:\n",
        "        print(\"‚úÖ All required environment variables present\")\n",
        "\n",
        "# Export commonly used paths as globals\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw\"]\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "DB_PATH = DIRECTORIES[\"database\"] / \"memory_v85.db\"\n",
        "LOG_PATH = DIRECTORIES[\"logs\"] / \"pipeline.log\"\n",
        "OUTPUT_PATH = DIRECTORIES[\"outputs\"] / \"signals.json\"\n",
        "\n",
        "print(f\"\\nüìÅ Key Paths:\")\n",
        "print(f\"   CSV: {CSV_FOLDER}\")\n",
        "print(f\"   Pickles: {PICKLE_FOLDER}\")\n",
        "print(f\"   Database: {DB_PATH}\")\n",
        "print(f\"   Logs: {LOG_PATH}\")\n",
        "print(f\"   Signals: {OUTPUT_PATH}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMHCk7ldwo3p"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# üìÑ GitHub Sync (Environment-Aware) - ALIGNED VERSION\n",
        "# ======================================================\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import urllib.parse\n",
        "import sys\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Environment Detection (MUST MATCH YOUR FIRST CELL!)\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "# Override ENV_NAME if in GitHub Actions\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CRITICAL FIX: Use SAME paths as environment detection\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    # ‚úÖ MATCHES YOUR ENVIRONMENT DETECTION\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"  # Same as env detection!\n",
        "    REPO_FOLDER = SAVE_FOLDER  # Repo IS the save folder\n",
        "    print(\"‚òÅÔ∏è Colab Mode: Cloning directly to /content/forex-ai-models\")\n",
        "\n",
        "elif IN_GHA:\n",
        "    # ‚úÖ GitHub Actions: Use current directory (already in repo)\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER  # We're already in the repo!\n",
        "    print(\"ü§ñ GitHub Actions Mode: Using current directory\")\n",
        "\n",
        "else:\n",
        "    # ‚úÖ Local: Use current directory\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "    print(\"üíª Local Mode: Using current directory\")\n",
        "\n",
        "# Create necessary directories WITH your organized structure\n",
        "DIRECTORIES = {\n",
        "    \"data_raw\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"üîß Running in: {ENV_NAME}\")\n",
        "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
        "print(f\"üíæ Save folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo folder: {REPO_FOLDER}\")\n",
        "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GitHub Configuration\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GitHub Token (Multi-Source)\n",
        "# ======================================================\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "# Try Colab secrets if in Colab and PAT not found\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except ImportError:\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load Colab secret: {e}\")\n",
        "\n",
        "# Validate PAT\n",
        "if not FOREX_PAT:\n",
        "    print(\"‚ö†Ô∏è Warning: FOREX_PAT not found. Git operations may fail.\")\n",
        "    print(\"   Set FOREX_PAT in:\")\n",
        "    print(\"   - GitHub Secrets (for Actions)\")\n",
        "    print(\"   - Colab Secrets (for Colab)\")\n",
        "    print(\"   - Environment variable (for local)\")\n",
        "    REPO_URL = None\n",
        "else:\n",
        "    SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "    REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "    print(\"‚úÖ GitHub token configured\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Handle Repository Based on Environment\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    # ===== GitHub Actions =====\n",
        "    print(\"\\nü§ñ GitHub Actions Mode\")\n",
        "    print(\"‚úÖ Repository already checked out by actions/checkout\")\n",
        "    print(f\"üìÇ Current directory: {Path.cwd()}\")\n",
        "\n",
        "    # Verify .git exists\n",
        "    if not (Path.cwd() / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Warning: .git directory not found!\")\n",
        "        print(\"   Make sure actions/checkout@v4 is in your workflow\")\n",
        "    else:\n",
        "        print(\"‚úÖ Git repository confirmed\")\n",
        "\n",
        "elif IN_COLAB:\n",
        "    # ===== Google Colab =====\n",
        "    print(\"\\n‚òÅÔ∏è Google Colab Mode\")\n",
        "\n",
        "    if not REPO_URL:\n",
        "        print(\"‚ùå Cannot clone repository: FOREX_PAT not available\")\n",
        "    elif not (REPO_FOLDER / \".git\").exists():\n",
        "        # Check if directory exists but isn't a git repo\n",
        "        if REPO_FOLDER.exists():\n",
        "            print(f\"‚ö†Ô∏è Directory exists but is not a git repo. Removing...\")\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "            print(\"‚úÖ Cleaned up non-git directory\")\n",
        "\n",
        "        # Clone repository\n",
        "        print(f\"üì• Cloning repository to {REPO_FOLDER}...\")\n",
        "        env = os.environ.copy()\n",
        "        env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"  # Skip LFS files\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                check=True,\n",
        "                env=env,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            print(\"‚úÖ Repository cloned successfully\")\n",
        "\n",
        "            # Change to repo directory\n",
        "            os.chdir(REPO_FOLDER)\n",
        "            print(f\"üìÇ Changed directory to: {os.getcwd()}\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Clone failed: {e.stderr}\")\n",
        "            print(\"Creating directory structure manually...\")\n",
        "            REPO_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ùå Clone timed out after 60 seconds\")\n",
        "            REPO_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "    else:\n",
        "        # Repository exists, pull latest\n",
        "        print(\"‚úÖ Repository already exists, pulling latest changes...\")\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"pull\", \"origin\", BRANCH],\n",
        "                check=True,\n",
        "                cwd=REPO_FOLDER,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            print(\"‚úÖ Successfully pulled latest changes\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Pull failed: {e.stderr}\")\n",
        "            print(\"Continuing with existing files...\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ö†Ô∏è Pull timed out, continuing anyway...\")\n",
        "\n",
        "    # Configure Git LFS (disable for Colab)\n",
        "    print(\"‚öôÔ∏è Configuring Git LFS...\")\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\"git\", \"lfs\", \"uninstall\"],\n",
        "            check=False,\n",
        "            cwd=REPO_FOLDER,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"‚úÖ LFS disabled for Colab\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è LFS setup warning: {e}\")\n",
        "\n",
        "else:\n",
        "    # ===== Local Environment =====\n",
        "    print(\"\\nüíª Local Development Mode\")\n",
        "    print(f\"üìÇ Working in: {SAVE_FOLDER}\")\n",
        "\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Not a git repository\")\n",
        "        print(\"   Run: git clone https://github.com/rahim-dotAI/forex-ai-models.git\")\n",
        "    else:\n",
        "        print(\"‚úÖ Git repository found\")\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Create Organized Directory Structure\n",
        "# ======================================================\n",
        "print(\"\\nüìÅ Creating organized directory structure...\")\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"   ‚úÖ {dir_name}: {dir_path}\")\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Git Global Configuration\n",
        "# ======================================================\n",
        "print(\"\\nüîß Configuring Git...\")\n",
        "\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "# Set git config\n",
        "git_configs = [\n",
        "    ([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], \"User name\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], \"User email\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], \"Detached HEAD warning\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"init.defaultBranch\", \"main\"], \"Default branch\")\n",
        "]\n",
        "\n",
        "for cmd, description in git_configs:\n",
        "    try:\n",
        "        subprocess.run(cmd, check=False, capture_output=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not set {description}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Export Path Constants (MATCH YOUR ENVIRONMENT DETECTION!)\n",
        "# ======================================================\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw\"]\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "DB_PATH = DIRECTORIES[\"database\"] / \"memory_v85.db\"\n",
        "LOG_PATH = DIRECTORIES[\"logs\"] / \"pipeline.log\"\n",
        "OUTPUT_PATH = DIRECTORIES[\"outputs\"] / \"signals.json\"\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Environment Summary & Validation\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üßæ ENVIRONMENT SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment:      {ENV_NAME}\")\n",
        "print(f\"Working Dir:      {os.getcwd()}\")\n",
        "print(f\"Save Folder:      {SAVE_FOLDER}\")\n",
        "print(f\"Repo Folder:      {REPO_FOLDER}\")\n",
        "print(f\"Repository:       https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}\")\n",
        "print(f\"Branch:           {BRANCH}\")\n",
        "print(f\"Git Repo Exists:  {(REPO_FOLDER / '.git').exists()}\")\n",
        "print(f\"FOREX_PAT Set:    {'‚úÖ Yes' if FOREX_PAT else '‚ùå No'}\")\n",
        "\n",
        "# Check critical paths\n",
        "print(\"\\nüìã Critical Paths:\")\n",
        "print(f\"   CSV Folder:    {CSV_FOLDER}\")\n",
        "print(f\"   Pickle Folder: {PICKLE_FOLDER}\")\n",
        "print(f\"   Database:      {DB_PATH}\")\n",
        "print(f\"   Logs:          {LOG_PATH}\")\n",
        "print(f\"   Signals:       {OUTPUT_PATH}\")\n",
        "\n",
        "print(\"\\nüìÇ Directory Status:\")\n",
        "critical_paths = {\n",
        "    \"Repo .git\": REPO_FOLDER / \".git\",\n",
        "    \"Data Raw\": CSV_FOLDER,\n",
        "    \"Data Processed\": PICKLE_FOLDER,\n",
        "    \"Database\": DIRECTORIES[\"database\"],\n",
        "    \"Logs\": DIRECTORIES[\"logs\"],\n",
        "    \"Outputs\": DIRECTORIES[\"outputs\"]\n",
        "}\n",
        "\n",
        "for name, path in critical_paths.items():\n",
        "    exists = path.exists()\n",
        "    icon = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"  {icon} {name}: {path}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ Setup completed successfully!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# üîü Export Variables for Downstream Cells\n",
        "# ======================================================\n",
        "# These variables are now available in subsequent cells:\n",
        "# - ENV_NAME: Environment name\n",
        "# - IN_COLAB: Boolean for Colab detection\n",
        "# - IN_GHA: Boolean for GitHub Actions detection\n",
        "# - SAVE_FOLDER: Path to save files (same as REPO_FOLDER in Colab)\n",
        "# - REPO_FOLDER: Path to git repository\n",
        "# - CSV_FOLDER, PICKLE_FOLDER, DB_PATH, LOG_PATH, OUTPUT_PATH: Organized paths\n",
        "# - GITHUB_USERNAME, GITHUB_REPO, BRANCH: Git config\n",
        "# - FOREX_PAT: GitHub token (if available)\n",
        "\n",
        "print(\"\\n‚úÖ All environment variables exported for downstream cells\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oih6CDfjAjG9"
      },
      "outputs": [],
      "source": [
        "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn river\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPLbcZC578EJ"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ALPHA VANTAGE FX DATA FETCHER - ALIGNED WITH CLEAN STRUCTURE\n",
        "============================================================\n",
        "‚úÖ Uses NEW clean repo structure (data/raw/alpha_vantage)\n",
        "‚úÖ Data quality validation BEFORE saving\n",
        "‚úÖ Works in GitHub Actions, Google Colab, and Local\n",
        "‚úÖ Unified with YFinance folder structure\n",
        "‚úÖ Thread-safe operations\n",
        "‚úÖ API rate limit handling\n",
        "‚úÖ Automatic retry logic\n",
        "‚úÖ Clear naming: pair_daily_av.csv (av = Alpha Vantage)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import requests\n",
        "import subprocess\n",
        "import threading\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION (MATCHES YOUR SETUP!)\n",
        "# ======================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ Alpha Vantage FX Data Fetcher - Clean Structure Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üìç Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ UNIFIED PATH CONFIGURATION (MATCHES YOUR CLEAN STRUCTURE!)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    print(\"‚òÅÔ∏è Google Colab detected - using clean structure\")\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"  # ‚úÖ MATCHES YOUR SETUP!\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    print(\"ü§ñ GitHub Actions detected - using repository root\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    print(\"üíª Local environment detected - using clean structure\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "# ‚úÖ NEW: Use organized directory structure\n",
        "DIRECTORIES = {\n",
        "    \"data_raw_alpha\": SAVE_FOLDER / \"data\" / \"raw\" / \"alpha_vantage\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "    \"quarantine\": SAVE_FOLDER / \"data\" / \"quarantine\" / \"alpha_vantage\",\n",
        "}\n",
        "\n",
        "# Create all directories\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Export key paths\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw_alpha\"]  # ‚úÖ Alpha Vantage CSVs here\n",
        "QUARANTINE_FOLDER = DIRECTORIES[\"quarantine\"]\n",
        "LOG_FOLDER = DIRECTORIES[\"logs\"]\n",
        "\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo Folder: {REPO_FOLDER}\")\n",
        "print(f\"üìä Alpha Vantage CSV: {CSV_FOLDER}\")\n",
        "print(f\"üóëÔ∏è Quarantine: {QUARANTINE_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ DATA QUALITY VALIDATOR\n",
        "# ======================================================\n",
        "class DataQualityValidator:\n",
        "    \"\"\"Validate data quality before saving\"\"\"\n",
        "\n",
        "    MIN_ROWS = 50  # Alpha Vantage should give us lots of data\n",
        "    MIN_PRICE_CV = 0.01  # 0.01% minimum variation\n",
        "    MIN_UNIQUE_RATIO = 0.01  # 1% unique prices\n",
        "    MIN_TRUE_RANGE = 1e-10\n",
        "    MIN_QUALITY_SCORE = 40.0\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df, pair):\n",
        "        \"\"\"\n",
        "        Validate DataFrame quality\n",
        "        Returns: (is_valid, quality_score, metrics, issues)\n",
        "        \"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return False, 0.0, {}, [\"Empty DataFrame\"]\n",
        "\n",
        "        issues = []\n",
        "        metrics = {}\n",
        "\n",
        "        # Check row count\n",
        "        metrics['row_count'] = len(df)\n",
        "        if len(df) < DataQualityValidator.MIN_ROWS:\n",
        "            issues.append(f\"Too few rows: {len(df)}\")\n",
        "\n",
        "        # Check required columns\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            issues.append(f\"Missing columns: {missing_cols}\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        # Get valid OHLC data\n",
        "        ohlc_data = df[required_cols].dropna()\n",
        "        if len(ohlc_data) == 0:\n",
        "            issues.append(\"No valid OHLC data\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        metrics['valid_rows'] = len(ohlc_data)\n",
        "        metrics['valid_ratio'] = len(ohlc_data) / len(df)\n",
        "\n",
        "        # Price statistics\n",
        "        close_prices = ohlc_data['close']\n",
        "        metrics['price_mean'] = float(close_prices.mean())\n",
        "        metrics['price_std'] = float(close_prices.std())\n",
        "        metrics['price_min'] = float(close_prices.min())\n",
        "        metrics['price_max'] = float(close_prices.max())\n",
        "\n",
        "        # Coefficient of variation\n",
        "        if metrics['price_mean'] > 0:\n",
        "            metrics['price_cv'] = (metrics['price_std'] / metrics['price_mean']) * 100\n",
        "        else:\n",
        "            metrics['price_cv'] = 0.0\n",
        "            issues.append(\"Zero mean price\")\n",
        "\n",
        "        # Unique price ratio\n",
        "        metrics['unique_prices'] = close_prices.nunique()\n",
        "        metrics['unique_ratio'] = metrics['unique_prices'] / len(close_prices)\n",
        "\n",
        "        # Calculate true range\n",
        "        high = ohlc_data['high'].values\n",
        "        low = ohlc_data['low'].values\n",
        "        close = ohlc_data['close'].values\n",
        "\n",
        "        tr = np.maximum.reduce([\n",
        "            high - low,\n",
        "            np.abs(high - np.roll(close, 1)),\n",
        "            np.abs(low - np.roll(close, 1))\n",
        "        ])\n",
        "        tr[0] = high[0] - low[0]\n",
        "\n",
        "        metrics['true_range_median'] = float(np.median(tr))\n",
        "        metrics['true_range_mean'] = float(np.mean(tr))\n",
        "\n",
        "        # Calculate quality score (0-100)\n",
        "        quality_score = 0.0\n",
        "\n",
        "        # Valid data ratio (30 points)\n",
        "        quality_score += metrics['valid_ratio'] * 30\n",
        "\n",
        "        # Price variation (30 points)\n",
        "        if metrics['price_cv'] >= 1.0:\n",
        "            quality_score += 30\n",
        "        elif metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV:\n",
        "            quality_score += (metrics['price_cv'] / 1.0) * 30\n",
        "\n",
        "        # Unique price ratio (20 points)\n",
        "        quality_score += min(metrics['unique_ratio'] * 20, 20)\n",
        "\n",
        "        # True range adequacy (20 points)\n",
        "        if metrics['true_range_median'] >= 1e-5:\n",
        "            quality_score += 20\n",
        "        elif metrics['true_range_median'] >= DataQualityValidator.MIN_TRUE_RANGE:\n",
        "            quality_score += (metrics['true_range_median'] / 1e-5) * 20\n",
        "\n",
        "        metrics['quality_score'] = quality_score\n",
        "\n",
        "        # Determine if valid\n",
        "        is_valid = (quality_score >= DataQualityValidator.MIN_QUALITY_SCORE)\n",
        "\n",
        "        return is_valid, quality_score, metrics, issues\n",
        "\n",
        "validator = DataQualityValidator()\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GITHUB CONFIGURATION\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not access Colab secrets: {e}\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"FOREX_PAT is required\")\n",
        "\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "print(\"‚úÖ GitHub credentials configured\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ REPOSITORY MANAGEMENT (SIMPLIFIED FOR CLEAN STRUCTURE)\n",
        "# ======================================================\n",
        "def ensure_repository():\n",
        "    \"\"\"Ensure repository is available and up-to-date\"\"\"\n",
        "    if IN_GHA:\n",
        "        print(\"\\nü§ñ GitHub Actions: Repository already available\")\n",
        "        if not (REPO_FOLDER / \".git\").exists():\n",
        "            print(\"‚ö†Ô∏è Warning: .git directory not found\")\n",
        "        else:\n",
        "            print(\"‚úÖ Git repository verified\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüì• Managing repository...\")\n",
        "\n",
        "    if REPO_FOLDER.exists() and not (REPO_FOLDER / \".git\").exists():\n",
        "        # Directory exists but isn't a git repo - this shouldn't happen with new structure\n",
        "        print(\"‚ö†Ô∏è Directory exists but is not a git repository\")\n",
        "        return\n",
        "\n",
        "    if (REPO_FOLDER / \".git\").exists():\n",
        "        print(f\"üîÑ Pulling latest changes...\")\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Repository updated successfully\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Pull had issues, continuing anyway\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Update failed: {e} - continuing with existing repo\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Repository not found. This script expects the repo to be set up first.\")\n",
        "        print(\"   Please run the GitHub Sync script first!\")\n",
        "\n",
        "ensure_repository()\n",
        "\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "               capture_output=True, check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "               capture_output=True, check=False)\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ ALPHA VANTAGE CONFIGURATION\n",
        "# ======================================================\n",
        "ALPHA_VANTAGE_KEY = os.environ.get(\"ALPHA_VANTAGE_KEY\")\n",
        "\n",
        "if not ALPHA_VANTAGE_KEY and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        ALPHA_VANTAGE_KEY = userdata.get(\"ALPHA_VANTAGE_KEY\")\n",
        "        if ALPHA_VANTAGE_KEY:\n",
        "            os.environ[\"ALPHA_VANTAGE_KEY\"] = ALPHA_VANTAGE_KEY\n",
        "            print(\"üîê Loaded ALPHA_VANTAGE_KEY from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not access Colab secrets for API key: {e}\")\n",
        "\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    raise ValueError(\"‚ùå ALPHA_VANTAGE_KEY is required\")\n",
        "\n",
        "print(f\"‚úÖ Alpha Vantage API key: {ALPHA_VANTAGE_KEY[:4]}...{ALPHA_VANTAGE_KEY[-4:]}\")\n",
        "\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    \"\"\"Calculate MD5 hash of file to detect changes\"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def fetch_alpha_vantage_fx(pair, outputsize='full', max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Fetch FX data from Alpha Vantage API with retry logic\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with OHLC data or empty DataFrame on failure\n",
        "    \"\"\"\n",
        "    base_url = 'https://www.alphavantage.co/query'\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "\n",
        "    params = {\n",
        "        'function': 'FX_DAILY',\n",
        "        'from_symbol': from_currency,\n",
        "        'to_symbol': to_currency,\n",
        "        'outputsize': outputsize,\n",
        "        'datatype': 'json',\n",
        "        'apikey': ALPHA_VANTAGE_KEY\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  üîΩ Fetching {pair} (attempt {attempt + 1}/{max_retries})...\")\n",
        "\n",
        "            r = requests.get(base_url, params=params, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "\n",
        "            # Check for API errors\n",
        "            if 'Error Message' in data:\n",
        "                raise ValueError(f\"API Error: {data['Error Message']}\")\n",
        "\n",
        "            if 'Note' in data:\n",
        "                print(f\"  ‚ö†Ô∏è API rate limit reached for {pair}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay * 2)\n",
        "                    continue\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            if 'Time Series FX (Daily)' not in data:\n",
        "                raise ValueError(f\"Unexpected response format: {list(data.keys())}\")\n",
        "\n",
        "            # Parse time series data\n",
        "            ts = data['Time Series FX (Daily)']\n",
        "            df = pd.DataFrame(ts).T\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df.sort_index()\n",
        "\n",
        "            # Rename columns\n",
        "            df = df.rename(columns={\n",
        "                '1. open': 'open',\n",
        "                '2. high': 'high',\n",
        "                '3. low': 'low',\n",
        "                '4. close': 'close'\n",
        "            })\n",
        "\n",
        "            # Convert to float\n",
        "            df = df.astype(float)\n",
        "\n",
        "            # Remove timezone\n",
        "            df = ensure_tz_naive(df)\n",
        "\n",
        "            print(f\"  ‚úÖ Fetched {len(df)} rows for {pair}\")\n",
        "            return df\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"  ‚ö†Ô∏è Network error: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Error: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ PAIR PROCESSING WITH QUALITY VALIDATION\n",
        "# ======================================================\n",
        "def process_pair(pair):\n",
        "    \"\"\"\n",
        "    Process single FX pair: fetch, validate quality, merge, save\n",
        "\n",
        "    ‚úÖ Saves to data/raw/alpha_vantage/ with clear naming\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (filepath if changed, status message, quality_score)\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîÑ Processing {pair}...\")\n",
        "\n",
        "    # ‚úÖ Save to Alpha Vantage folder\n",
        "    filename = pair.replace(\"/\", \"_\") + \"_daily_av.csv\"\n",
        "    file_path = CSV_FOLDER / filename\n",
        "\n",
        "    # Load existing data\n",
        "    existing_df = pd.DataFrame()\n",
        "    if file_path.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
        "            existing_df = ensure_tz_naive(existing_df)\n",
        "            print(f\"  üìä Loaded {len(existing_df)} existing rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not load existing data: {e}\")\n",
        "\n",
        "    old_hash = file_hash(file_path)\n",
        "\n",
        "    # Fetch new data\n",
        "    new_df = fetch_alpha_vantage_fx(pair)\n",
        "\n",
        "    if new_df.empty:\n",
        "        return None, f\"‚ùå {pair}: No data fetched\", 0.0\n",
        "\n",
        "    # Merge with existing data\n",
        "    if not existing_df.empty:\n",
        "        combined_df = pd.concat([existing_df, new_df])\n",
        "        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
        "    else:\n",
        "        combined_df = new_df\n",
        "\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    # ‚úÖ VALIDATE QUALITY BEFORE SAVING\n",
        "    is_valid, quality_score, metrics, issues = validator.validate_dataframe(\n",
        "        combined_df, pair\n",
        "    )\n",
        "\n",
        "    print(f\"  üìä Quality score: {quality_score:.1f}/100\")\n",
        "\n",
        "    if not is_valid:\n",
        "        print(f\"  ‚ö†Ô∏è Quality issues: {'; '.join(issues[:2])}\")\n",
        "        print(f\"     CV: {metrics.get('price_cv', 0):.4f}%, Unique: {metrics.get('unique_ratio', 0):.1%}\")\n",
        "\n",
        "        # Quarantine if quality too low\n",
        "        if quality_score < DataQualityValidator.MIN_QUALITY_SCORE:\n",
        "            print(f\"  ‚ùå Data quality too low - quarantining\")\n",
        "\n",
        "            quarantine_file = QUARANTINE_FOLDER / f\"{filename}.bad\"\n",
        "            with lock:\n",
        "                combined_df.to_csv(quarantine_file)\n",
        "\n",
        "                # Save quality report\n",
        "                report_file = QUARANTINE_FOLDER / f\"{filename}.quality.txt\"\n",
        "                with open(report_file, 'w') as f:\n",
        "                    f.write(f\"Quality Report for {pair} (Alpha Vantage)\\n\")\n",
        "                    f.write(f\"{'='*50}\\n\")\n",
        "                    f.write(f\"Quality Score: {quality_score:.1f}/100\\n\")\n",
        "                    f.write(f\"Issues: {'; '.join(issues)}\\n\")\n",
        "                    f.write(f\"\\nMetrics:\\n\")\n",
        "                    for k, v in metrics.items():\n",
        "                        f.write(f\"  {k}: {v}\\n\")\n",
        "\n",
        "            return None, f\"‚ùå {pair}: Quality too low ({quality_score:.1f}/100)\", quality_score\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è Low quality but acceptable - saving with warning\")\n",
        "\n",
        "    # ‚úÖ Quality good, save the file\n",
        "    with lock:\n",
        "        combined_df.to_csv(file_path)\n",
        "\n",
        "    new_hash = file_hash(file_path)\n",
        "    changed = (old_hash != new_hash)\n",
        "\n",
        "    status = \"‚úÖ Updated\" if changed else \"‚ÑπÔ∏è No changes\"\n",
        "    print(f\"  {status} - {len(combined_df)} rows, quality: {quality_score:.1f}/100\")\n",
        "\n",
        "    return (str(file_path) if changed else None), f\"{status} {pair} ({len(combined_df)} rows, Q:{quality_score:.0f})\", quality_score\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ PARALLEL EXECUTION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Fetching FX data with quality validation...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "changed_files = []\n",
        "results = []\n",
        "quality_scores = {}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = {executor.submit(process_pair, pair): pair for pair in FX_PAIRS}\n",
        "\n",
        "    for future in as_completed(futures):\n",
        "        pair = futures[future]\n",
        "        try:\n",
        "            filepath, message, quality = future.result()\n",
        "            results.append(message)\n",
        "            if filepath:\n",
        "                changed_files.append(filepath)\n",
        "                quality_scores[filepath] = quality\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {pair} processing failed: {e}\")\n",
        "            results.append(f\"‚ùå {pair}: Failed\")\n",
        "\n",
        "# ======================================================\n",
        "# üîü RESULTS SUMMARY WITH QUALITY REPORT\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PROCESSING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "print(f\"\\nTotal pairs processed: {len(FX_PAIRS)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "\n",
        "# Quality report\n",
        "if quality_scores:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä QUALITY REPORT\")\n",
        "    print(\"=\" * 70)\n",
        "    avg_quality = sum(quality_scores.values()) / len(quality_scores)\n",
        "    print(f\"Average quality score: {avg_quality:.1f}/100\")\n",
        "\n",
        "    if quality_scores:\n",
        "        print(f\"\\nFiles by quality:\")\n",
        "        for fname, score in sorted(quality_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {'‚úÖ' if score >= 60 else '‚ö†Ô∏è'} {Path(fname).name}: {score:.1f}/100\")\n",
        "\n",
        "# Check quarantine\n",
        "quarantined = list(QUARANTINE_FOLDER.glob(\"*.bad\"))\n",
        "if quarantined:\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚ö†Ô∏è  QUARANTINED FILES: {len(quarantined)}\")\n",
        "    print(\"=\" * 70)\n",
        "    for qfile in quarantined:\n",
        "        print(f\"  ‚ùå {qfile.stem}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ GIT COMMIT & PUSH\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        commit_msg = f\"Update Alpha Vantage data - {len(changed_files)} files\"\n",
        "        if quality_scores:\n",
        "            commit_msg += f\" (Avg Q:{avg_quality:.0f})\"\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Changes committed\")\n",
        "\n",
        "            for attempt in range(3):\n",
        "                print(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/3)...\")\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(\"‚úÖ Successfully pushed to GitHub\")\n",
        "                    break\n",
        "                elif attempt < 2:\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Git error: {e}\")\n",
        "    finally:\n",
        "        os.chdir(SAVE_FOLDER)\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è No changes to commit\")\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALPHA VANTAGE WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Quality validated: ‚úÖ\")\n",
        "if quality_scores:\n",
        "    print(f\"Average quality: {avg_quality:.1f}/100\")\n",
        "print(f\"Status: {'‚úÖ Success' if len(results) == len(FX_PAIRS) else '‚ö†Ô∏è Partial'}\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüìÅ Clean File Structure:\")\n",
        "print(f\"   Alpha Vantage: {CSV_FOLDER}\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ EUR_USD_daily_av.csv\")\n",
        "print(f\"   YFinance: {SAVE_FOLDER / 'data' / 'raw' / 'yfinance'}\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ EUR_USD_1d_5y.csv\")\n",
        "print(\"\\nüéØ Both sources in organized folders!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBW31rh39aMb"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "YFINANCE FX DATA FETCHER - CLEAN STRUCTURE EDITION\n",
        "===================================================\n",
        "‚úÖ Aligned with clean repo structure (data/raw/yfinance)\n",
        "‚úÖ Relaxed quality thresholds for more data acceptance\n",
        "‚úÖ Automatic OHLC logic fixing\n",
        "‚úÖ Enhanced fallback options\n",
        "‚úÖ Smart data cleaning before validation\n",
        "‚úÖ Better symbol format handling\n",
        "‚úÖ Multi-environment support (Colab, GHA, Local)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import subprocess\n",
        "import shutil\n",
        "import threading\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ YFinance FX Data Fetcher - Clean Structure Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION (MATCHES YOUR SETUP!)\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ UNIFIED PATH CONFIGURATION (MATCHES CLEAN STRUCTURE!)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    print(\"‚òÅÔ∏è Google Colab detected - using clean structure\")\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"  # ‚úÖ MATCHES!\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    print(\"ü§ñ GitHub Actions detected - using repository root\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    print(\"üíª Local environment detected - using clean structure\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "# ‚úÖ CREATE ORGANIZED DIRECTORY STRUCTURE\n",
        "DIRECTORIES = {\n",
        "    \"data_raw_yfinance\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "    \"quarantine\": SAVE_FOLDER / \"data\" / \"quarantine\" / \"yfinance\",\n",
        "}\n",
        "\n",
        "# Create all directories\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Export key paths\n",
        "CSV_FOLDER = DIRECTORIES[\"data_raw_yfinance\"]  # ‚úÖ YFinance CSVs here\n",
        "QUARANTINE_FOLDER = DIRECTORIES[\"quarantine\"]\n",
        "LOG_FOLDER = DIRECTORIES[\"logs\"]\n",
        "\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo Folder: {REPO_FOLDER}\")\n",
        "print(f\"üìä YFinance CSV: {CSV_FOLDER}\")\n",
        "print(f\"üóëÔ∏è Quarantine: {QUARANTINE_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GIT CONFIGURATION\n",
        "# ======================================================\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "# Try Colab secrets if in Colab and PAT not found\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not access Colab secrets: {e}\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT is required!\")\n",
        "\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# Configure git\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "               capture_output=True, check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "               capture_output=True, check=False)\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ REPOSITORY MANAGEMENT (SIMPLIFIED)\n",
        "# ======================================================\n",
        "def ensure_repository():\n",
        "    \"\"\"Ensure repository is available and up-to-date\"\"\"\n",
        "    if IN_GHA:\n",
        "        print(\"\\nü§ñ GitHub Actions: Repository already available\")\n",
        "        if not (REPO_FOLDER / \".git\").exists():\n",
        "            print(\"‚ö†Ô∏è Warning: .git directory not found\")\n",
        "        else:\n",
        "            print(\"‚úÖ Git repository verified\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüì• Managing repository...\")\n",
        "\n",
        "    if REPO_FOLDER.exists() and not (REPO_FOLDER / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Directory exists but is not a git repository\")\n",
        "        return\n",
        "\n",
        "    if (REPO_FOLDER / \".git\").exists():\n",
        "        print(f\"üîÑ Pulling latest changes...\")\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Repository updated successfully\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Pull had issues, continuing anyway\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Update failed: {e} - continuing with existing repo\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Repository not found. This script expects the repo to be set up first.\")\n",
        "        print(\"   Please run the GitHub Sync script first!\")\n",
        "\n",
        "ensure_repository()\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ RATE LIMITER\n",
        "# ======================================================\n",
        "class RateLimiter:\n",
        "    \"\"\"Rate limiter for API calls\"\"\"\n",
        "    def __init__(self, requests_per_minute=10, requests_per_hour=350):\n",
        "        self.rpm = requests_per_minute\n",
        "        self.rph = requests_per_hour\n",
        "        self.request_times = []\n",
        "        self.hourly_request_times = []\n",
        "        self.lock = threading.Lock()\n",
        "        self.total_requests = 0\n",
        "\n",
        "    def wait_if_needed(self):\n",
        "        with self.lock:\n",
        "            now = time.time()\n",
        "            self.request_times = [t for t in self.request_times if now - t < 60]\n",
        "            self.hourly_request_times = [t for t in self.hourly_request_times if now - t < 3600]\n",
        "\n",
        "            if len(self.request_times) >= self.rpm:\n",
        "                wait_time = 60 - (now - self.request_times[0])\n",
        "                if wait_time > 0:\n",
        "                    time.sleep(wait_time + 1)\n",
        "                    self.request_times = []\n",
        "\n",
        "            if len(self.hourly_request_times) >= self.rph:\n",
        "                wait_time = 3600 - (now - self.hourly_request_times[0])\n",
        "                if wait_time > 0:\n",
        "                    time.sleep(wait_time + 1)\n",
        "                    self.hourly_request_times = []\n",
        "\n",
        "            self.request_times.append(now)\n",
        "            self.hourly_request_times.append(now)\n",
        "            self.total_requests += 1\n",
        "            time.sleep(1.0 + (hash(str(now)) % 20) / 10)\n",
        "\n",
        "    def get_stats(self):\n",
        "        with self.lock:\n",
        "            return {'total_requests': self.total_requests}\n",
        "\n",
        "rate_limiter = RateLimiter()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ DATA CLEANING & VALIDATION\n",
        "# ======================================================\n",
        "def fix_ohlc_logic(df):\n",
        "    \"\"\"Fix impossible OHLC relationships\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df = df.copy()\n",
        "    required_cols = ['open', 'high', 'low', 'close']\n",
        "\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        return df\n",
        "\n",
        "    # Fix High: should be maximum of OHLC\n",
        "    df['high'] = df[required_cols].max(axis=1)\n",
        "\n",
        "    # Fix Low: should be minimum of OHLC\n",
        "    df['low'] = df[required_cols].min(axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "class DataQualityValidator:\n",
        "    \"\"\"RELAXED validation for more data acceptance\"\"\"\n",
        "\n",
        "    # ‚úÖ RELAXED THRESHOLDS\n",
        "    MIN_ROWS = 5  # Down from 10\n",
        "    MIN_PRICE_CV = 0.01  # Down from 0.1 (1% instead of 10%)\n",
        "    MIN_UNIQUE_RATIO = 0.005  # Down from 0.05 (0.5% instead of 5%)\n",
        "    MIN_TRUE_RANGE = 1e-12  # More lenient\n",
        "    MIN_QUALITY_SCORE = 20.0  # Down from 40.0\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df, pair, tf_name):\n",
        "        \"\"\"Validate with relaxed criteria\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return False, 0.0, {}, [\"Empty DataFrame\"]\n",
        "\n",
        "        issues = []\n",
        "        metrics = {}\n",
        "\n",
        "        metrics['row_count'] = len(df)\n",
        "        if len(df) < DataQualityValidator.MIN_ROWS:\n",
        "            return False, 0.0, metrics, [f\"Too few rows: {len(df)}\"]\n",
        "\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        if not all(col in df.columns for col in required_cols):\n",
        "            return False, 0.0, metrics, [\"Missing OHLC columns\"]\n",
        "\n",
        "        ohlc_data = df[required_cols].dropna()\n",
        "        if len(ohlc_data) == 0:\n",
        "            return False, 0.0, metrics, [\"No valid OHLC data\"]\n",
        "\n",
        "        metrics['valid_rows'] = len(ohlc_data)\n",
        "        metrics['valid_ratio'] = len(ohlc_data) / len(df)\n",
        "\n",
        "        close_prices = ohlc_data['close']\n",
        "        metrics['price_mean'] = float(close_prices.mean())\n",
        "        metrics['price_std'] = float(close_prices.std())\n",
        "        metrics['price_cv'] = (metrics['price_std'] / metrics['price_mean']) * 100 if metrics['price_mean'] > 0 else 0.0\n",
        "\n",
        "        metrics['unique_prices'] = close_prices.nunique()\n",
        "        metrics['unique_ratio'] = metrics['unique_prices'] / len(close_prices)\n",
        "\n",
        "        # Calculate true range\n",
        "        high = ohlc_data['high'].values\n",
        "        low = ohlc_data['low'].values\n",
        "        close = ohlc_data['close'].values\n",
        "\n",
        "        tr = np.maximum.reduce([\n",
        "            high - low,\n",
        "            np.abs(high - np.roll(close, 1)),\n",
        "            np.abs(low - np.roll(close, 1))\n",
        "        ])\n",
        "        tr[0] = high[0] - low[0]\n",
        "\n",
        "        metrics['true_range_median'] = float(np.median(tr))\n",
        "\n",
        "        # Quality score calculation (more lenient)\n",
        "        quality_score = metrics['valid_ratio'] * 30\n",
        "\n",
        "        if metrics['price_cv'] >= 0.5:\n",
        "            quality_score += 40\n",
        "        elif metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV:\n",
        "            quality_score += (metrics['price_cv'] / 0.5) * 40\n",
        "\n",
        "        if metrics['unique_ratio'] >= 0.1:\n",
        "            quality_score += 30\n",
        "        elif metrics['unique_ratio'] >= DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "            quality_score += (metrics['unique_ratio'] / 0.1) * 30\n",
        "\n",
        "        metrics['quality_score'] = quality_score\n",
        "\n",
        "        # Relaxed validation - accept if meets minimum thresholds\n",
        "        is_valid = (\n",
        "            quality_score >= DataQualityValidator.MIN_QUALITY_SCORE and\n",
        "            metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV and\n",
        "            metrics['unique_ratio'] >= DataQualityValidator.MIN_UNIQUE_RATIO\n",
        "        )\n",
        "\n",
        "        if not is_valid:\n",
        "            if metrics['price_cv'] < DataQualityValidator.MIN_PRICE_CV:\n",
        "                issues.append(f\"Low CV: {metrics['price_cv']:.4f}%\")\n",
        "            if metrics['unique_ratio'] < DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "                issues.append(f\"Low unique: {metrics['unique_ratio']:.3%}\")\n",
        "\n",
        "        return is_valid, quality_score, metrics, issues\n",
        "\n",
        "validator = DataQualityValidator()\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ CONFIGURATION\n",
        "# ======================================================\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "# ‚úÖ ENHANCED with more fallback options\n",
        "TIMEFRAMES = {\n",
        "    \"1d_5y\": [\n",
        "        (\"1d\", \"5y\"),\n",
        "        (\"1d\", \"max\"),  # Try max available\n",
        "        (\"1d\", \"3y\"),\n",
        "        (\"1d\", \"2y\"),\n",
        "    ],\n",
        "    \"1h_2y\": [\n",
        "        (\"1h\", \"2y\"),\n",
        "        (\"1h\", \"1y\"),\n",
        "        (\"1h\", \"730d\"),  # Exactly 2 years in days\n",
        "        (\"1h\", \"6mo\")\n",
        "    ],\n",
        "    \"15m_60d\": [\n",
        "        (\"15m\", \"60d\"),\n",
        "        (\"15m\", \"2mo\"),\n",
        "        (\"15m\", \"30d\"),\n",
        "    ],\n",
        "    \"5m_1mo\": [\n",
        "        (\"5m\", \"1mo\"),\n",
        "        (\"5m\", \"30d\"),\n",
        "        (\"5m\", \"14d\"),\n",
        "    ],\n",
        "    \"1m_7d\": [\n",
        "        (\"1m\", \"7d\"),\n",
        "        (\"1m\", \"5d\"),\n",
        "        (\"1m\", \"3d\"),\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"\\nüìä Configuration:\")\n",
        "print(f\"   Pairs: {len(FX_PAIRS)}\")\n",
        "print(f\"   Timeframes: {len(TIMEFRAMES)}\")\n",
        "print(f\"   Total tasks: {len(FX_PAIRS) * len(TIMEFRAMES)}\")\n",
        "print(f\"   Quality threshold: {validator.MIN_QUALITY_SCORE}/100 (RELAXED)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def file_hash(filepath):\n",
        "    \"\"\"Calculate MD5 hash of file\"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def merge_data(existing_df, new_df):\n",
        "    \"\"\"Merge existing and new data, removing duplicates\"\"\"\n",
        "    existing_df = ensure_tz_naive(existing_df)\n",
        "    new_df = ensure_tz_naive(new_df)\n",
        "    if existing_df.empty:\n",
        "        return new_df\n",
        "    if new_df.empty:\n",
        "        return existing_df\n",
        "    combined = pd.concat([existing_df, new_df])\n",
        "    combined = combined[~combined.index.duplicated(keep=\"last\")]\n",
        "    combined.sort_index(inplace=True)\n",
        "    return combined\n",
        "\n",
        "def get_symbol_variants(pair, interval):\n",
        "    \"\"\"Get multiple symbol format variations\"\"\"\n",
        "    base_symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
        "    variants = [base_symbol]\n",
        "\n",
        "    # Additional formats\n",
        "    if interval in [\"1d\", \"1h\"]:\n",
        "        from_curr, to_curr = pair.split(\"/\")\n",
        "        variants.append(f\"{from_curr}{to_curr}=X\")  # No separator\n",
        "        variants.append(f\"{from_curr}=X\")  # Just base currency\n",
        "\n",
        "    return variants\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ WORKER FUNCTION\n",
        "# ======================================================\n",
        "def process_pair_tf(pair, tf_name, interval_period_options, max_retries=3):\n",
        "    \"\"\"\n",
        "    Download YFinance data with OHLC fixing and validation\n",
        "\n",
        "    ‚úÖ Saves to data/raw/yfinance/ with clear naming\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (message, filepath if changed, quality_score)\n",
        "    \"\"\"\n",
        "    # ‚úÖ Save to YFinance folder\n",
        "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
        "    filepath = CSV_FOLDER / filename\n",
        "\n",
        "    existing_df = pd.DataFrame()\n",
        "    if filepath.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "            existing_df = ensure_tz_naive(existing_df)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not load existing data: {e}\")\n",
        "\n",
        "    old_hash = file_hash(filepath)\n",
        "\n",
        "    for option_idx, (interval, period) in enumerate(interval_period_options):\n",
        "        symbol_variants = get_symbol_variants(pair, interval)\n",
        "\n",
        "        for symbol in symbol_variants:\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    rate_limiter.wait_if_needed()\n",
        "\n",
        "                    ticker = yf.Ticker(symbol)\n",
        "                    df = ticker.history(\n",
        "                        period=period,\n",
        "                        interval=interval,\n",
        "                        auto_adjust=False,\n",
        "                        prepost=False,\n",
        "                        actions=False,\n",
        "                        raise_errors=False\n",
        "                    )\n",
        "\n",
        "                    if df.empty:\n",
        "                        raise ValueError(\"Empty data\")\n",
        "\n",
        "                    available_cols = [c for c in ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "                                     if c in df.columns]\n",
        "                    df = df[available_cols]\n",
        "                    df.rename(columns=lambda x: x.lower(), inplace=True)\n",
        "                    df = ensure_tz_naive(df)\n",
        "\n",
        "                    combined_df = merge_data(existing_df, df)\n",
        "\n",
        "                    # ‚úÖ FIX OHLC LOGIC BEFORE VALIDATION\n",
        "                    combined_df = fix_ohlc_logic(combined_df)\n",
        "\n",
        "                    is_valid, quality_score, metrics, issues = validator.validate_dataframe(\n",
        "                        combined_df, pair, tf_name\n",
        "                    )\n",
        "\n",
        "                    if not is_valid:\n",
        "                        if attempt < max_retries - 1:\n",
        "                            time.sleep(3 * (2 ** attempt))\n",
        "                            continue\n",
        "                        elif option_idx < len(interval_period_options) - 1:\n",
        "                            break  # Try next option\n",
        "                        else:\n",
        "                            # Save anyway but mark as low quality\n",
        "                            print(f\"  ‚ö†Ô∏è Low quality ({quality_score:.1f}) but saving: {pair} {tf_name}\")\n",
        "\n",
        "                    # Save the file\n",
        "                    with lock:\n",
        "                        combined_df.to_csv(filepath)\n",
        "\n",
        "                    new_hash = file_hash(filepath)\n",
        "                    changed = (old_hash != new_hash)\n",
        "\n",
        "                    status = \"‚úÖ\" if quality_score >= 50 else \"‚ö†Ô∏è\"\n",
        "                    msg = f\"{status} {pair} {tf_name} - {len(combined_df)} rows, Q:{quality_score:.0f}\"\n",
        "                    print(f\"  {msg}\")\n",
        "                    return msg, str(filepath) if changed else None, quality_score\n",
        "\n",
        "                except Exception as e:\n",
        "                    if attempt < max_retries - 1:\n",
        "                        time.sleep(3 * (2 ** attempt))\n",
        "                    else:\n",
        "                        if option_idx < len(interval_period_options) - 1:\n",
        "                            break  # Try next option\n",
        "\n",
        "    return f\"‚ùå Failed {pair} {tf_name}\", None, 0.0\n",
        "\n",
        "# ======================================================\n",
        "# üîü PARALLEL EXECUTION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Starting YFinance data download...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "changed_files = []\n",
        "results = []\n",
        "quality_scores = {}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    tasks = []\n",
        "    for pair in FX_PAIRS:\n",
        "        for tf_name, options in TIMEFRAMES.items():\n",
        "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, options))\n",
        "\n",
        "    for future in as_completed(tasks):\n",
        "        try:\n",
        "            msg, filename, quality = future.result()\n",
        "            results.append(msg)\n",
        "            if filename:\n",
        "                changed_files.append(filename)\n",
        "                quality_scores[filename] = quality\n",
        "        except Exception as e:\n",
        "            results.append(f\"‚ùå Error: {e}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ SUMMARY\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PROCESSING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "success_count = len([r for r in results if \"‚úÖ\" in r or \"‚ö†Ô∏è\" in r])\n",
        "print(f\"\\nTotal tasks: {len(results)}\")\n",
        "print(f\"Successful: {success_count}/{len(results)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Time: {elapsed_time/60:.1f} min\")\n",
        "\n",
        "if quality_scores:\n",
        "    avg_q = sum(quality_scores.values()) / len(quality_scores)\n",
        "    print(f\"Average quality: {avg_q:.1f}/100\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä QUALITY REPORT\")\n",
        "    print(\"=\" * 70)\n",
        "    for fname, score in sorted(quality_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "        status = \"‚úÖ\" if score >= 50 else \"‚ö†Ô∏è\"\n",
        "        print(f\"  {status} {Path(fname).name}: {score:.1f}/100\")\n",
        "\n",
        "# Check quarantine\n",
        "quarantined = list(QUARANTINE_FOLDER.glob(\"*.bad\"))\n",
        "if quarantined:\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚ö†Ô∏è  QUARANTINED FILES: {len(quarantined)}\")\n",
        "    print(\"=\" * 70)\n",
        "    for qfile in quarantined:\n",
        "        print(f\"  ‚ùå {qfile.stem}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ GIT COMMIT & PUSH\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        commit_msg = f\"Update YFinance data - {len(changed_files)} files\"\n",
        "        if quality_scores:\n",
        "            commit_msg += f\" (Avg Q:{avg_q:.0f})\"\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Changes committed\")\n",
        "\n",
        "            for attempt in range(3):\n",
        "                print(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/3)...\")\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(\"‚úÖ Successfully pushed to GitHub\")\n",
        "                    break\n",
        "                elif attempt < 2:\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  No changes to commit\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Git error: {e}\")\n",
        "    finally:\n",
        "        os.chdir(SAVE_FOLDER)\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è No changes to commit\")\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ YFINANCE WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Quality validated: ‚úÖ\")\n",
        "if quality_scores:\n",
        "    print(f\"Average quality: {avg_q:.1f}/100\")\n",
        "print(f\"Status: {'‚úÖ Success' if success_count == len(results) else '‚ö†Ô∏è Partial'}\")\n",
        "print(f\"Rate limiter: {rate_limiter.get_stats()['total_requests']} requests\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüìÅ Clean File Structure:\")\n",
        "print(f\"   YFinance: {CSV_FOLDER}\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ EUR_USD_1d_5y.csv, EUR_USD_1h_2y.csv, etc.\")\n",
        "print(f\"   Alpha Vantage: {SAVE_FOLDER / 'data' / 'raw' / 'alpha_vantage'}\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ EUR_USD_daily_av.csv\")\n",
        "print(\"\\nüéØ All data sources in organized folders!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2Z_gCxS_g7I"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "FX CSV Combiner + Multi-Type Handler - CLEAN STRUCTURE EDITION\n",
        "==============================================================\n",
        "‚úÖ Aligned with clean repo structure (data/raw/, data/processed/)\n",
        "‚úÖ Combines Alpha Vantage + YFinance data\n",
        "‚úÖ Full-dataset indicator calculation (not incremental)\n",
        "‚úÖ ATR preservation (no clipping or scaling)\n",
        "‚úÖ Quality validation before processing\n",
        "‚úÖ Multi-environment support (Colab, GHA, Local)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import subprocess\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîß CSV Combiner & Multi-Type Handler - Clean Structure Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ UNIFIED PATH CONFIGURATION (MATCHES CLEAN STRUCTURE!)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    print(\"‚òÅÔ∏è Google Colab detected - using clean structure\")\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    print(\"ü§ñ GitHub Actions detected - using repository root\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    print(\"üíª Local environment detected - using clean structure\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "# ‚úÖ CREATE ORGANIZED DIRECTORY STRUCTURE\n",
        "DIRECTORIES = {\n",
        "    \"data_raw_yfinance\": SAVE_FOLDER / \"data\" / \"raw\" / \"yfinance\",\n",
        "    \"data_raw_alpha\": SAVE_FOLDER / \"data\" / \"raw\" / \"alpha_vantage\",\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "    \"quarantine\": SAVE_FOLDER / \"data\" / \"quarantine\" / \"combiner\",\n",
        "}\n",
        "\n",
        "# Create all directories\n",
        "for dir_name, dir_path in DIRECTORIES.items():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Export key paths\n",
        "YFINANCE_CSV_FOLDER = DIRECTORIES[\"data_raw_yfinance\"]\n",
        "ALPHA_CSV_FOLDER = DIRECTORIES[\"data_raw_alpha\"]\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "QUARANTINE_FOLDER = DIRECTORIES[\"quarantine\"]\n",
        "LOG_FOLDER = DIRECTORIES[\"logs\"]\n",
        "\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo Folder: {REPO_FOLDER}\")\n",
        "print(f\"üìä YFinance CSV: {YFINANCE_CSV_FOLDER}\")\n",
        "print(f\"üìä Alpha CSV: {ALPHA_CSV_FOLDER}\")\n",
        "print(f\"üîß Processed: {PICKLE_FOLDER}\")\n",
        "print(f\"üóëÔ∏è Quarantine: {QUARANTINE_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Print status messages with icons\"\"\"\n",
        "    levels = {\"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"warn\": \"‚ö†Ô∏è\", \"error\": \"‚ùå\", \"debug\": \"üêû\"}\n",
        "    print(f\"{levels.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ DATA QUALITY VALIDATOR\n",
        "# ======================================================\n",
        "class DataQualityValidator:\n",
        "    \"\"\"Validate data quality for OHLC files\"\"\"\n",
        "\n",
        "    MIN_ROWS = 10\n",
        "    MIN_PRICE_CV = 0.01  # 0.01% minimum (relaxed)\n",
        "    MIN_UNIQUE_RATIO = 0.005  # 0.5% unique prices (relaxed)\n",
        "    MIN_TRUE_RANGE = 1e-10\n",
        "    MIN_QUALITY_SCORE = 20.0  # Relaxed from 30\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df, filename):\n",
        "        \"\"\"Validate DataFrame quality\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return False, 0.0, {}, [\"Empty DataFrame\"]\n",
        "\n",
        "        issues = []\n",
        "        metrics = {}\n",
        "\n",
        "        metrics['row_count'] = len(df)\n",
        "        if len(df) < DataQualityValidator.MIN_ROWS:\n",
        "            issues.append(f\"Too few rows: {len(df)}\")\n",
        "\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            issues.append(f\"Missing columns: {missing_cols}\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        ohlc_data = df[required_cols].dropna()\n",
        "        if len(ohlc_data) == 0:\n",
        "            issues.append(\"No valid OHLC data\")\n",
        "            return False, 0.0, metrics, issues\n",
        "\n",
        "        metrics['valid_rows'] = len(ohlc_data)\n",
        "        metrics['valid_ratio'] = len(ohlc_data) / len(df)\n",
        "\n",
        "        close_prices = ohlc_data['close']\n",
        "        metrics['price_mean'] = float(close_prices.mean())\n",
        "        metrics['price_std'] = float(close_prices.std())\n",
        "        metrics['price_cv'] = (metrics['price_std'] / metrics['price_mean'] * 100) if metrics['price_mean'] > 0 else 0.0\n",
        "\n",
        "        metrics['unique_prices'] = close_prices.nunique()\n",
        "        metrics['unique_ratio'] = metrics['unique_prices'] / len(close_prices)\n",
        "\n",
        "        high = ohlc_data['high'].values\n",
        "        low = ohlc_data['low'].values\n",
        "        close = ohlc_data['close'].values\n",
        "\n",
        "        tr = np.maximum.reduce([\n",
        "            high - low,\n",
        "            np.abs(high - np.roll(close, 1)),\n",
        "            np.abs(low - np.roll(close, 1))\n",
        "        ])\n",
        "        tr[0] = high[0] - low[0]\n",
        "\n",
        "        metrics['true_range_median'] = float(np.median(tr))\n",
        "\n",
        "        quality_score = 0.0\n",
        "        quality_score += metrics['valid_ratio'] * 30\n",
        "\n",
        "        if metrics['price_cv'] >= 0.5:\n",
        "            quality_score += 40\n",
        "        elif metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV:\n",
        "            quality_score += (metrics['price_cv'] / 0.5) * 40\n",
        "\n",
        "        if metrics['unique_ratio'] >= 0.1:\n",
        "            quality_score += 30\n",
        "        elif metrics['unique_ratio'] >= DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "            quality_score += (metrics['unique_ratio'] / 0.1) * 30\n",
        "\n",
        "        metrics['quality_score'] = quality_score\n",
        "\n",
        "        is_valid = (\n",
        "            quality_score >= DataQualityValidator.MIN_QUALITY_SCORE and\n",
        "            metrics['price_cv'] >= DataQualityValidator.MIN_PRICE_CV\n",
        "        )\n",
        "\n",
        "        if not is_valid:\n",
        "            if metrics['price_cv'] < DataQualityValidator.MIN_PRICE_CV:\n",
        "                issues.append(f\"Low CV: {metrics['price_cv']:.4f}%\")\n",
        "            if metrics['unique_ratio'] < DataQualityValidator.MIN_UNIQUE_RATIO:\n",
        "                issues.append(f\"Low unique: {metrics['unique_ratio']:.3%}\")\n",
        "\n",
        "        return is_valid, quality_score, metrics, issues\n",
        "\n",
        "validator = DataQualityValidator()\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GIT CONFIGURATION\n",
        "# ======================================================\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not access Colab secrets: {e}\")\n",
        "\n",
        "if FOREX_PAT:\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "                   capture_output=True, check=False)\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "                   capture_output=True, check=False)\n",
        "    print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_localize(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def safe_numeric(df):\n",
        "    \"\"\"Handle infinity/NaN robustly\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    required_columns = ['open', 'high', 'low', 'close']\n",
        "    existing_columns = [col for col in required_columns if col in df_clean.columns]\n",
        "\n",
        "    if existing_columns:\n",
        "        df_clean.dropna(subset=existing_columns, inplace=True)\n",
        "    else:\n",
        "        df_clean.dropna(how='all', inplace=True)\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ CSV DISCOVERY\n",
        "# ======================================================\n",
        "def discover_csv_files():\n",
        "    \"\"\"Discover CSV files from both YFinance and Alpha Vantage folders\"\"\"\n",
        "    csv_files = []\n",
        "\n",
        "    # Search in YFinance folder\n",
        "    yf_files = list(YFINANCE_CSV_FOLDER.glob(\"*.csv\"))\n",
        "    if yf_files:\n",
        "        print_status(f\"üìÇ Found {len(yf_files)} YFinance CSV(s)\", \"debug\")\n",
        "        csv_files.extend(yf_files)\n",
        "\n",
        "    # Search in Alpha Vantage folder\n",
        "    alpha_files = list(ALPHA_CSV_FOLDER.glob(\"*.csv\"))\n",
        "    if alpha_files:\n",
        "        print_status(f\"üìÇ Found {len(alpha_files)} Alpha Vantage CSV(s)\", \"debug\")\n",
        "        csv_files.extend(alpha_files)\n",
        "\n",
        "    return csv_files\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ INDICATOR CALCULATION (FULL DATASET)\n",
        "# ======================================================\n",
        "def add_indicators_full(df):\n",
        "    \"\"\"\n",
        "    ‚úÖ Calculate indicators on FULL dataset (not incremental)\n",
        "    ‚úÖ ATR preserved without clipping or scaling\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    required_cols = ['open', 'high', 'low', 'close']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        return None\n",
        "\n",
        "    df = safe_numeric(df)\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    df = df.copy()\n",
        "    df.sort_index(inplace=True)\n",
        "\n",
        "    # Preserve raw prices\n",
        "    for col in ['open', 'high', 'low', 'close']:\n",
        "        if col in df.columns and f'raw_{col}' not in df.columns:\n",
        "            df[f'raw_{col}'] = df[col].copy()\n",
        "\n",
        "    print_status(f\"  üîß Calculating indicators on {len(df)} rows\", \"debug\")\n",
        "\n",
        "    try:\n",
        "        # Trend indicators\n",
        "        if len(df) >= 10:\n",
        "            df['SMA_10'] = ta.trend.sma_indicator(df['close'], 10)\n",
        "            df['EMA_10'] = ta.trend.ema_indicator(df['close'], 10)\n",
        "\n",
        "        if len(df) >= 20:\n",
        "            df['SMA_20'] = ta.trend.sma_indicator(df['close'], 20)\n",
        "            df['EMA_20'] = ta.trend.ema_indicator(df['close'], 20)\n",
        "\n",
        "        if len(df) >= 50:\n",
        "            df['SMA_50'] = ta.trend.sma_indicator(df['close'], 50)\n",
        "            df['EMA_50'] = ta.trend.ema_indicator(df['close'], 50)\n",
        "\n",
        "        if len(df) >= 200:\n",
        "            df['SMA_200'] = ta.trend.sma_indicator(df['close'], 200)\n",
        "\n",
        "        # MACD\n",
        "        if len(df) >= 26:\n",
        "            macd = ta.trend.MACD(df['close'])\n",
        "            df['MACD'] = macd.macd()\n",
        "            df['MACD_signal'] = macd.macd_signal()\n",
        "            df['MACD_diff'] = macd.macd_diff()\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Trend indicator error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # Momentum indicators\n",
        "        if len(df) >= 14:\n",
        "            df['RSI_14'] = ta.momentum.rsi(df['close'], 14)\n",
        "            df['Williams_%R'] = WilliamsRIndicator(\n",
        "                df['high'], df['low'], df['close'], 14\n",
        "            ).williams_r()\n",
        "            df['Stoch_K'] = ta.momentum.stoch(df['high'], df['low'], df['close'], 14)\n",
        "            df['Stoch_D'] = ta.momentum.stoch_signal(df['high'], df['low'], df['close'], 14)\n",
        "\n",
        "        if len(df) >= 20:\n",
        "            df['CCI_20'] = ta.trend.cci(df['high'], df['low'], df['close'], 20)\n",
        "            df['ROC'] = ta.momentum.roc(df['close'], 12)\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Momentum indicator error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # ‚úÖ CRITICAL: ATR calculation - NO CLIPPING!\n",
        "        if len(df) >= 14:\n",
        "            atr_values = AverageTrueRange(\n",
        "                df['high'], df['low'], df['close'], 14\n",
        "            ).average_true_range()\n",
        "\n",
        "            # Only fill NaN, don't clip\n",
        "            df['ATR'] = atr_values.fillna(1e-10)\n",
        "\n",
        "            atr_median = df['ATR'].median()\n",
        "            if pd.notna(atr_median):\n",
        "                print_status(f\"  üìä ATR median: {atr_median:.8f}\", \"debug\")\n",
        "\n",
        "        # Bollinger Bands\n",
        "        if len(df) >= 20:\n",
        "            bb = ta.volatility.BollingerBands(df['close'], 20, 2)\n",
        "            df['BB_upper'] = bb.bollinger_hband()\n",
        "            df['BB_middle'] = bb.bollinger_mavg()\n",
        "            df['BB_lower'] = bb.bollinger_lband()\n",
        "            df['BB_width'] = bb.bollinger_wband()\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Volatility indicator error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # Derived features\n",
        "        df['price_change'] = df['close'].pct_change()\n",
        "        df['price_change_5'] = df['close'].pct_change(5)\n",
        "        df['high_low_range'] = (df['high'] - df['low']) / df['close']\n",
        "        df['close_open_range'] = (df['close'] - df['open']) / df['open']\n",
        "\n",
        "        if 'volume' in df.columns:\n",
        "            df['vwap'] = (df['close'] * df['volume']).cumsum() / df['volume'].cumsum()\n",
        "\n",
        "        if 'SMA_50' in df.columns:\n",
        "            df['price_vs_sma50'] = (df['close'] - df['SMA_50']) / df['SMA_50']\n",
        "\n",
        "        if 'RSI_14' in df.columns:\n",
        "            df['rsi_momentum'] = df['RSI_14'].diff()\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Derived features error: {e}\", \"warn\")\n",
        "\n",
        "    try:\n",
        "        # ‚úÖ Scale features but PROTECT ATR and raw prices\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        protected_cols = [\n",
        "            'open', 'high', 'low', 'close', 'volume',\n",
        "            'raw_open', 'raw_high', 'raw_low', 'raw_close',\n",
        "            'ATR'  # ‚úÖ PROTECT ATR!\n",
        "        ]\n",
        "\n",
        "        scalable_cols = [c for c in numeric_cols if c not in protected_cols]\n",
        "\n",
        "        if scalable_cols:\n",
        "            df[scalable_cols] = df[scalable_cols].replace([np.inf, -np.inf], np.nan)\n",
        "            cols_with_data = [c for c in scalable_cols if not df[c].isna().all()]\n",
        "\n",
        "            if cols_with_data:\n",
        "                scaler = RobustScaler()\n",
        "                df[cols_with_data] = scaler.fit_transform(\n",
        "                    df[cols_with_data].fillna(0) + 1e-10\n",
        "                )\n",
        "                print_status(f\"  ‚úÖ Scaled {len(cols_with_data)} features (ATR protected)\", \"debug\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ö†Ô∏è Scaling error: {e}\", \"warn\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ MAIN PROCESSING FUNCTION\n",
        "# ======================================================\n",
        "def process_csv_file(csv_file):\n",
        "    \"\"\"Process a single CSV file: validate, combine, add indicators, save\"\"\"\n",
        "    try:\n",
        "        print_status(f\"üìã Processing: {csv_file.name}\", \"info\")\n",
        "\n",
        "        # Load CSV\n",
        "        df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
        "        df = ensure_tz_naive(df)\n",
        "\n",
        "        if df.empty:\n",
        "            msg = f\"‚ö†Ô∏è {csv_file.name}: Empty file\"\n",
        "            print_status(msg, \"warn\")\n",
        "            return None, msg\n",
        "\n",
        "        # ‚úÖ VALIDATE QUALITY\n",
        "        is_valid, quality_score, metrics, issues = validator.validate_dataframe(df, csv_file.name)\n",
        "\n",
        "        print_status(f\"  üìä Quality score: {quality_score:.1f}/100\", \"debug\")\n",
        "\n",
        "        if not is_valid:\n",
        "            print_status(f\"  ‚ö†Ô∏è Quality issues: {'; '.join(issues[:2])}\", \"warn\")\n",
        "\n",
        "            # Quarantine if too low\n",
        "            if quality_score < validator.MIN_QUALITY_SCORE:\n",
        "                print_status(f\"  ‚ùå Quarantining low quality file\", \"error\")\n",
        "\n",
        "                quarantine_file = QUARANTINE_FOLDER / f\"{csv_file.name}.bad\"\n",
        "                with lock:\n",
        "                    df.to_csv(quarantine_file)\n",
        "\n",
        "                    report_file = QUARANTINE_FOLDER / f\"{csv_file.name}.quality.txt\"\n",
        "                    with open(report_file, 'w') as f:\n",
        "                        f.write(f\"Quality Report for {csv_file.name}\\n\")\n",
        "                        f.write(f\"{'='*50}\\n\")\n",
        "                        f.write(f\"Quality Score: {quality_score:.1f}/100\\n\")\n",
        "                        f.write(f\"Issues: {'; '.join(issues)}\\n\")\n",
        "                        f.write(f\"\\nMetrics:\\n\")\n",
        "                        for k, v in metrics.items():\n",
        "                            f.write(f\"  {k}: {v}\\n\")\n",
        "\n",
        "                return None, f\"‚ùå {csv_file.name}: Quarantined (Q:{quality_score:.1f})\"\n",
        "            else:\n",
        "                print_status(f\"  ‚ö†Ô∏è Low quality but acceptable\", \"warn\")\n",
        "\n",
        "        # ‚úÖ ADD INDICATORS (FULL DATASET)\n",
        "        processed_df = add_indicators_full(df)\n",
        "\n",
        "        if processed_df is None:\n",
        "            msg = f\"‚ùå {csv_file.name}: Indicator calculation failed\"\n",
        "            print_status(msg, \"error\")\n",
        "            return None, msg\n",
        "\n",
        "        # ‚úÖ SAVE PROCESSED DATA\n",
        "        pickle_filename = csv_file.stem + \".pkl\"\n",
        "        pickle_path = PICKLE_FOLDER / pickle_filename\n",
        "\n",
        "        with lock:\n",
        "            processed_df.to_pickle(pickle_path, compression='gzip', protocol=4)\n",
        "\n",
        "        atr_median = processed_df['ATR'].median() if 'ATR' in processed_df.columns else 0\n",
        "        msg = f\"‚úÖ {csv_file.name}: {len(processed_df)} rows, Q:{quality_score:.0f}, ATR:{atr_median:.8f}\"\n",
        "        print_status(msg, \"success\")\n",
        "\n",
        "        return str(pickle_path), msg\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\"‚ùå Failed {csv_file.name}: {e}\"\n",
        "        print_status(msg, \"error\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, msg\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ MAIN EXECUTION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Discovering CSV files...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "csv_files = discover_csv_files()\n",
        "\n",
        "if csv_files:\n",
        "    print_status(f\"üìä Total CSV files found: {len(csv_files)}\", \"success\")\n",
        "    for csv_file in csv_files[:5]:\n",
        "        print_status(f\"  ‚Ä¢ {csv_file.name} ({csv_file.stat().st_size / 1024:.1f} KB)\", \"debug\")\n",
        "    if len(csv_files) > 5:\n",
        "        print_status(f\"  ... and {len(csv_files) - 5} more\", \"debug\")\n",
        "else:\n",
        "    print_status(\"‚ö†Ô∏è No CSV files found!\", \"warn\")\n",
        "    print_status(\"   Check that data fetchers have run successfully\", \"warn\")\n",
        "\n",
        "changed_files = []\n",
        "quality_scores = {}\n",
        "\n",
        "# ======================================================\n",
        "# üîü PROCESS FILES\n",
        "# ======================================================\n",
        "if csv_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚öôÔ∏è Processing {len(csv_files)} CSV file(s)...\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=min(8, len(csv_files))) as executor:\n",
        "        futures = [executor.submit(process_csv_file, f) for f in csv_files]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            file, msg = future.result()\n",
        "            if file:\n",
        "                changed_files.append(file)\n",
        "                # Extract quality info\n",
        "                if \"ATR:\" in msg:\n",
        "                    try:\n",
        "                        atr_str = msg.split(\"ATR:\")[1].strip()\n",
        "                        quality_scores[file] = float(atr_str)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ QUALITY REPORT\n",
        "# ======================================================\n",
        "if quality_scores:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä QUALITY REPORT - ATR VALUES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    avg_atr = sum(quality_scores.values()) / len(quality_scores)\n",
        "    print(f\"Average ATR: {avg_atr:.8f}\")\n",
        "    print(f\"\\nATR by file:\")\n",
        "\n",
        "    for filepath, atr in sorted(quality_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "        filename = Path(filepath).stem\n",
        "        status = \"‚úÖ\" if atr > 1e-6 else \"‚ö†Ô∏è\"\n",
        "        print(f\"  {status} {filename}: {atr:.8f}\")\n",
        "\n",
        "    low_atr_files = [f for f, atr in quality_scores.items() if atr < 1e-6]\n",
        "    if low_atr_files:\n",
        "        print(f\"\\n‚ö†Ô∏è  {len(low_atr_files)} file(s) with suspiciously low ATR\")\n",
        "\n",
        "# Check quarantine\n",
        "quarantined = list(QUARANTINE_FOLDER.glob(\"*.bad\"))\n",
        "if quarantined:\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚ö†Ô∏è  QUARANTINED FILES: {len(quarantined)}\")\n",
        "    print(\"=\" * 70)\n",
        "    for qfile in quarantined:\n",
        "        print(f\"  ‚ùå {qfile.stem}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ GIT COMMIT & PUSH\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files and FOREX_PAT:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        commit_msg = f\"Update processed data - {len(changed_files)} files\"\n",
        "        if quality_scores:\n",
        "            commit_msg += f\" (Avg ATR: {avg_atr:.6f})\"\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print_status(\"‚úÖ Changes committed\", \"success\")\n",
        "\n",
        "            for attempt in range(3):\n",
        "                print_status(f\"üì§ Pushing (attempt {attempt + 1}/3)...\", \"info\")\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print_status(\"‚úÖ Push successful\", \"success\")\n",
        "                    break\n",
        "                elif attempt < 2:\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "\n",
        "        elif \"nothing to commit\" in result.stdout.lower():\n",
        "            print_status(\"‚ÑπÔ∏è No changes to commit\", \"info\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ùå Git error: {e}\", \"error\")\n",
        "    finally:\n",
        "        os.chdir(SAVE_FOLDER)\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION SUMMARY\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ CSV COMBINER COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"CSV files found: {len(csv_files)}\")\n",
        "print(f\"Files processed: {len(changed_files)}\")\n",
        "print(f\"Files quarantined: {len(quarantined)}\")\n",
        "\n",
        "if quality_scores:\n",
        "    print(f\"\\nüìà ATR Statistics:\")\n",
        "    print(f\"   Average: {avg_atr:.8f}\")\n",
        "    print(f\"   Files analyzed: {len(quality_scores)}\")\n",
        "\n",
        "print(\"\\nüîß KEY FEATURES:\")\n",
        "print(\"   ‚úÖ Full-dataset indicator calculation\")\n",
        "print(\"   ‚úÖ ATR preserved (no clipping/scaling)\")\n",
        "print(\"   ‚úÖ Quality validation with quarantine\")\n",
        "print(\"   ‚úÖ Clean organized structure\")\n",
        "print(\"   ‚úÖ Thread-safe processing\")\n",
        "\n",
        "print(\"\\nüìÅ Output Locations:\")\n",
        "print(f\"   Processed pickles: {PICKLE_FOLDER}\")\n",
        "print(f\"   Quarantine: {QUARANTINE_FOLDER}\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Eo_WVVzzOsh"
      },
      "outputs": [],
      "source": [
        "# TAG: pipeline_main\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ULTRA-PERSISTENT SELF-LEARNING HYBRID FX PIPELINE v4.0\n",
        "======================================================\n",
        "‚úÖ Aligned with clean repo structure\n",
        "‚úÖ Uses processed pickle files from combiner\n",
        "‚úÖ Database-driven ML with memory\n",
        "‚úÖ Multi-timeframe analysis\n",
        "‚úÖ Automated signal generation\n",
        "‚úÖ Performance tracking\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import sqlite3\n",
        "import threading\n",
        "import subprocess\n",
        "import pickle\n",
        "import filecmp\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from contextlib import contextmanager\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ Ultra-Persistent FX Pipeline v4.0 - Clean Structure Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ UNIFIED PATH CONFIGURATION\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    REPO_FOLDER = SAVE_FOLDER\n",
        "elif IN_GHA:\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER\n",
        "\n",
        "# ‚úÖ ORGANIZED DIRECTORIES\n",
        "DIRECTORIES = {\n",
        "    \"data_processed\": SAVE_FOLDER / \"data\" / \"processed\",\n",
        "    \"database\": SAVE_FOLDER / \"database\",\n",
        "    \"logs\": SAVE_FOLDER / \"logs\",\n",
        "    \"outputs\": SAVE_FOLDER / \"outputs\",\n",
        "}\n",
        "\n",
        "for dir_path in DIRECTORIES.values():\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PICKLE_FOLDER = DIRECTORIES[\"data_processed\"]\n",
        "DB_FOLDER = DIRECTORIES[\"database\"]\n",
        "LOGS_FOLDER = DIRECTORIES[\"logs\"]\n",
        "OUTPUTS_FOLDER = DIRECTORIES[\"outputs\"]\n",
        "\n",
        "PERSISTENT_DB = DB_FOLDER / \"memory_v85.db\"\n",
        "\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo Folder: {REPO_FOLDER}\")\n",
        "print(f\"üîß Processed: {PICKLE_FOLDER}\")\n",
        "print(f\"üíø Database: {PERSISTENT_DB}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Enhanced status printing\"\"\"\n",
        "    icons = {\n",
        "        \"info\": \"‚ÑπÔ∏è\",\n",
        "        \"success\": \"‚úÖ\",\n",
        "        \"warn\": \"‚ö†Ô∏è\",\n",
        "        \"debug\": \"üêû\",\n",
        "        \"error\": \"‚ùå\",\n",
        "        \"performance\": \"‚ö°\",\n",
        "        \"data\": \"üìä\"\n",
        "    }\n",
        "    icon = icons.get(level, '‚ÑπÔ∏è')\n",
        "    print(f\"{icon} {msg}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GIT CONFIGURATION\n",
        "# ======================================================\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print_status(\"üîê Loaded FOREX_PAT from Colab secrets\", \"success\")\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ö†Ô∏è Could not access Colab secrets: {e}\", \"warn\")\n",
        "\n",
        "if FOREX_PAT:\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "                   capture_output=True, check=False)\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "                   capture_output=True, check=False)\n",
        "    print_status(f\"‚úÖ Git configured: {GIT_USER_NAME}\", \"success\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ ML IMPORTS\n",
        "# ======================================================\n",
        "try:\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    from sklearn.linear_model import SGDClassifier\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.exceptions import NotFittedError\n",
        "    print_status(\"‚úÖ ML libraries loaded\", \"success\")\n",
        "except ImportError as e:\n",
        "    print_status(f\"‚ùå ML libraries missing: {e}\", \"error\")\n",
        "    raise\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ ENHANCED DATABASE CLASS\n",
        "# ======================================================\n",
        "class EnhancedTradeMemoryDatabase:\n",
        "    \"\"\"Enhanced FX Trading Database v4.0\"\"\"\n",
        "\n",
        "    def __init__(self, db_path=PERSISTENT_DB, min_age_hours=1):\n",
        "        self.db_path = db_path\n",
        "        self.conn = None\n",
        "        self.lock = threading.RLock()\n",
        "        self.min_age_hours = min_age_hours\n",
        "\n",
        "        print_status(f\"üìÅ Database: {self.db_path}\", \"info\")\n",
        "        print_status(f\"‚è±Ô∏è  Min trade age: {self.min_age_hours}h\", \"info\")\n",
        "        self.initialize_database()\n",
        "\n",
        "    @contextmanager\n",
        "    def get_cursor(self):\n",
        "        \"\"\"Context manager for database cursor\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        try:\n",
        "            yield cursor\n",
        "            self.conn.commit()\n",
        "        except Exception as e:\n",
        "            self.conn.rollback()\n",
        "            raise e\n",
        "        finally:\n",
        "            cursor.close()\n",
        "\n",
        "    def initialize_database(self):\n",
        "        \"\"\"Create database with optimized settings\"\"\"\n",
        "        try:\n",
        "            db_exists = self.db_path.exists()\n",
        "\n",
        "            self.conn = sqlite3.connect(\n",
        "                str(self.db_path),\n",
        "                timeout=30,\n",
        "                check_same_thread=False\n",
        "            )\n",
        "\n",
        "            pragmas = [\n",
        "                \"PRAGMA journal_mode=WAL\",\n",
        "                \"PRAGMA synchronous=NORMAL\",\n",
        "                \"PRAGMA cache_size=-64000\",\n",
        "            ]\n",
        "\n",
        "            for pragma in pragmas:\n",
        "                self.conn.execute(pragma)\n",
        "\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS pending_trades (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        created_at TEXT NOT NULL,\n",
        "                        iteration INTEGER NOT NULL,\n",
        "                        pair TEXT NOT NULL,\n",
        "                        timeframe TEXT NOT NULL,\n",
        "                        sgd_prediction INTEGER,\n",
        "                        rf_prediction INTEGER,\n",
        "                        ensemble_prediction INTEGER,\n",
        "                        entry_price REAL NOT NULL,\n",
        "                        sl_price REAL NOT NULL,\n",
        "                        tp_price REAL NOT NULL,\n",
        "                        confidence REAL,\n",
        "                        evaluated BOOLEAN DEFAULT 0\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS completed_trades (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        pending_trade_id INTEGER,\n",
        "                        created_at TEXT NOT NULL,\n",
        "                        evaluated_at TEXT NOT NULL,\n",
        "                        iteration_created INTEGER,\n",
        "                        iteration_evaluated INTEGER,\n",
        "                        pair TEXT NOT NULL,\n",
        "                        timeframe TEXT NOT NULL,\n",
        "                        model_used TEXT NOT NULL,\n",
        "                        entry_price REAL NOT NULL,\n",
        "                        exit_price REAL NOT NULL,\n",
        "                        sl_price REAL NOT NULL,\n",
        "                        tp_price REAL NOT NULL,\n",
        "                        prediction INTEGER,\n",
        "                        hit_tp BOOLEAN NOT NULL,\n",
        "                        pnl REAL NOT NULL,\n",
        "                        pnl_percent REAL,\n",
        "                        duration_hours REAL\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS model_stats_cache (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        updated_at TEXT NOT NULL,\n",
        "                        pair TEXT NOT NULL,\n",
        "                        model_name TEXT NOT NULL,\n",
        "                        days INTEGER NOT NULL,\n",
        "                        total_trades INTEGER DEFAULT 0,\n",
        "                        winning_trades INTEGER DEFAULT 0,\n",
        "                        losing_trades INTEGER DEFAULT 0,\n",
        "                        accuracy_pct REAL DEFAULT 0.0,\n",
        "                        total_pnl REAL DEFAULT 0.0,\n",
        "                        avg_pnl REAL DEFAULT 0.0,\n",
        "                        sharpe_ratio REAL DEFAULT 0.0,\n",
        "                        UNIQUE(pair, model_name, days) ON CONFLICT REPLACE\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "            if db_exists:\n",
        "                print_status(f\"‚úÖ Connected to: {self.db_path.name}\", \"success\")\n",
        "            else:\n",
        "                print_status(f\"‚úÖ Created: {self.db_path.name}\", \"success\")\n",
        "\n",
        "            self._verify_database_integrity()\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Database init failed: {e}\", \"error\")\n",
        "            raise\n",
        "\n",
        "    def _verify_database_integrity(self):\n",
        "        \"\"\"Verify database structure\"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
        "                tables = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "                expected = ['pending_trades', 'completed_trades', 'model_stats_cache']\n",
        "\n",
        "                print_status(\"üìä Database Tables:\", \"data\")\n",
        "                for table in expected:\n",
        "                    if table in tables:\n",
        "                        cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
        "                        count = cursor.fetchone()[0]\n",
        "                        print_status(f\"  ‚úì {table}: {count} rows\", \"data\")\n",
        "                    else:\n",
        "                        print_status(f\"  ‚úó {table}: MISSING!\", \"error\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Verification warning: {e}\", \"warn\")\n",
        "\n",
        "    def store_new_signals(self, signals, current_iteration):\n",
        "        \"\"\"Store signals with batch insert\"\"\"\n",
        "        if not signals:\n",
        "            print_status(\"‚ö†Ô∏è No signals to store\", \"warn\")\n",
        "            return 0\n",
        "\n",
        "        batch_data = []\n",
        "\n",
        "        for pair, pair_data in signals.items():\n",
        "            pair_signals = pair_data.get('signals', {})\n",
        "\n",
        "            for tf_name, signal_data in pair_signals.items():\n",
        "                if not signal_data:\n",
        "                    continue\n",
        "\n",
        "                required = ['live', 'SL', 'TP']\n",
        "                if not all(signal_data.get(f, 0) > 0 for f in required):\n",
        "                    continue\n",
        "\n",
        "                batch_data.append((\n",
        "                    datetime.now(timezone.utc).isoformat(),\n",
        "                    current_iteration,\n",
        "                    pair,\n",
        "                    tf_name,\n",
        "                    signal_data.get('sgd_pred'),\n",
        "                    signal_data.get('rf_pred'),\n",
        "                    signal_data.get('signal'),\n",
        "                    signal_data.get('live', 0),\n",
        "                    signal_data.get('SL', 0),\n",
        "                    signal_data.get('TP', 0),\n",
        "                    signal_data.get('confidence', 0.5)\n",
        "                ))\n",
        "\n",
        "        if not batch_data:\n",
        "            print_status(\"‚ö†Ô∏è No valid signals\", \"warn\")\n",
        "            return 0\n",
        "\n",
        "        try:\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                cursor.executemany('''\n",
        "                    INSERT INTO pending_trades\n",
        "                    (created_at, iteration, pair, timeframe,\n",
        "                     sgd_prediction, rf_prediction, ensemble_prediction,\n",
        "                     entry_price, sl_price, tp_price, confidence)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                ''', batch_data)\n",
        "\n",
        "                stored = len(batch_data)\n",
        "\n",
        "            print_status(f\"üíæ Stored {stored} trades\", \"success\")\n",
        "            return stored\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Batch insert failed: {e}\", \"error\")\n",
        "            return 0\n",
        "\n",
        "    def evaluate_pending_trades(self, current_prices, current_iteration):\n",
        "        \"\"\"Evaluate pending trades\"\"\"\n",
        "        if not current_prices:\n",
        "            print_status(\"‚ö†Ô∏è No current prices\", \"warn\")\n",
        "            return {}\n",
        "\n",
        "        min_age = (datetime.now(timezone.utc) - timedelta(hours=self.min_age_hours)).isoformat()\n",
        "\n",
        "        try:\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    SELECT id, pair, timeframe, sgd_prediction, rf_prediction,\n",
        "                           ensemble_prediction, entry_price, sl_price, tp_price,\n",
        "                           created_at, iteration\n",
        "                    FROM pending_trades\n",
        "                    WHERE evaluated = 0 AND created_at < ?\n",
        "                    ORDER BY created_at ASC\n",
        "                    LIMIT 1000\n",
        "                ''', (min_age,))\n",
        "\n",
        "                pending = cursor.fetchall()\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Failed to fetch: {e}\", \"error\")\n",
        "            return {}\n",
        "\n",
        "        if not pending:\n",
        "            print_status(f\"‚ÑπÔ∏è No trades old enough (need {self.min_age_hours}h+)\", \"info\")\n",
        "            return {}\n",
        "\n",
        "        print_status(f\"üîç Evaluating {len(pending)} trades\", \"info\")\n",
        "\n",
        "        results = defaultdict(lambda: {\n",
        "            'closed_trades': 0,\n",
        "            'wins': 0,\n",
        "            'losses': 0,\n",
        "            'total_pnl': 0.0\n",
        "        })\n",
        "\n",
        "        completed_batch = []\n",
        "        evaluated_ids = []\n",
        "\n",
        "        for trade in pending:\n",
        "            (trade_id, pair, tf, sgd, rf, ens, entry, sl, tp, created, iter_created) = trade\n",
        "\n",
        "            current = current_prices.get(pair, 0)\n",
        "            if current <= 0:\n",
        "                continue\n",
        "\n",
        "            for model_name, pred in [('SGD', sgd), ('RandomForest', rf), ('Ensemble', ens)]:\n",
        "                if pred is None:\n",
        "                    continue\n",
        "\n",
        "                hit_tp, hit_sl, exit_price = self._evaluate_outcome(pred, current, tp, sl)\n",
        "\n",
        "                if exit_price:\n",
        "                    pnl = self._calc_pnl(pred, entry, exit_price)\n",
        "                    pnl_pct = (pnl / entry) * 100\n",
        "                    duration = self._calc_duration(created)\n",
        "\n",
        "                    completed_batch.append((\n",
        "                        trade_id, created, datetime.now(timezone.utc).isoformat(),\n",
        "                        iter_created, current_iteration,\n",
        "                        pair, tf, model_name, entry, exit_price,\n",
        "                        sl, tp, pred, hit_tp, pnl, pnl_pct, duration\n",
        "                    ))\n",
        "\n",
        "                    results[model_name]['closed_trades'] += 1\n",
        "                    results[model_name]['total_pnl'] += pnl\n",
        "\n",
        "                    if hit_tp:\n",
        "                        results[model_name]['wins'] += 1\n",
        "                        status = \"WIN ‚úÖ\"\n",
        "                    else:\n",
        "                        results[model_name]['losses'] += 1\n",
        "                        status = \"LOSS ‚ùå\"\n",
        "\n",
        "                    print_status(\n",
        "                        f\"{status} {model_name}: {pair} {tf} \"\n",
        "                        f\"P&L=${pnl:.5f} ({pnl_pct:+.2f}%) [{duration:.1f}h]\",\n",
        "                        \"success\" if hit_tp else \"warn\"\n",
        "                    )\n",
        "\n",
        "            evaluated_ids.append(trade_id)\n",
        "\n",
        "        if completed_batch:\n",
        "            try:\n",
        "                with self.lock, self.get_cursor() as cursor:\n",
        "                    cursor.executemany('''\n",
        "                        INSERT INTO completed_trades\n",
        "                        (pending_trade_id, created_at, evaluated_at,\n",
        "                         iteration_created, iteration_evaluated,\n",
        "                         pair, timeframe, model_used, entry_price, exit_price,\n",
        "                         sl_price, tp_price, prediction, hit_tp, pnl, pnl_percent,\n",
        "                         duration_hours)\n",
        "                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                    ''', completed_batch)\n",
        "\n",
        "                    if evaluated_ids:\n",
        "                        placeholders = ','.join('?' * len(evaluated_ids))\n",
        "                        cursor.execute(f'''\n",
        "                            UPDATE pending_trades\n",
        "                            SET evaluated = 1\n",
        "                            WHERE id IN ({placeholders})\n",
        "                        ''', evaluated_ids)\n",
        "\n",
        "                print_status(f\"‚úÖ Evaluated {len(evaluated_ids)} trades\", \"success\")\n",
        "\n",
        "            except sqlite3.Error as e:\n",
        "                print_status(f\"‚ùå Evaluation failed: {e}\", \"error\")\n",
        "                return {}\n",
        "\n",
        "        for model, data in results.items():\n",
        "            if data['closed_trades'] > 0:\n",
        "                data['accuracy'] = (data['wins'] / data['closed_trades']) * 100\n",
        "\n",
        "        self._update_stats_cache()\n",
        "\n",
        "        return dict(results)\n",
        "\n",
        "    def _evaluate_outcome(self, pred, current, tp, sl):\n",
        "        \"\"\"Determine if trade hit TP or SL\"\"\"\n",
        "        hit_tp, hit_sl, exit_price = False, False, None\n",
        "\n",
        "        try:\n",
        "            if pred == 1:  # Long\n",
        "                if current >= tp:\n",
        "                    hit_tp, exit_price = True, tp\n",
        "                elif current <= sl:\n",
        "                    hit_sl, exit_price = True, sl\n",
        "            elif pred == 0:  # Short\n",
        "                if current <= tp:\n",
        "                    hit_tp, exit_price = True, tp\n",
        "                elif current >= sl:\n",
        "                    hit_sl, exit_price = True, sl\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return hit_tp, hit_sl, exit_price\n",
        "\n",
        "    def _calc_pnl(self, pred, entry, exit):\n",
        "        \"\"\"Calculate P&L\"\"\"\n",
        "        try:\n",
        "            return exit - entry if pred == 1 else entry - exit\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calc_duration(self, created_at):\n",
        "        \"\"\"Calculate duration in hours\"\"\"\n",
        "        try:\n",
        "            created = datetime.fromisoformat(created_at.replace('Z', '+00:00'))\n",
        "            return max(0, (datetime.now(timezone.utc) - created).total_seconds() / 3600)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _update_stats_cache(self):\n",
        "        \"\"\"Update cached statistics\"\"\"\n",
        "        try:\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                cursor.execute('SELECT DISTINCT pair FROM completed_trades')\n",
        "                pairs = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "                cursor.execute('SELECT DISTINCT model_used FROM completed_trades')\n",
        "                models = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "                for pair in pairs:\n",
        "                    for model in models:\n",
        "                        for days in [7, 30]:\n",
        "                            since = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
        "\n",
        "                            cursor.execute('''\n",
        "                                SELECT\n",
        "                                    COUNT(*) as total,\n",
        "                                    SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins,\n",
        "                                    SUM(pnl) as total_pnl,\n",
        "                                    AVG(pnl) as avg_pnl\n",
        "                                FROM completed_trades\n",
        "                                WHERE pair = ? AND model_used = ? AND evaluated_at > ?\n",
        "                            ''', (pair, model, since))\n",
        "\n",
        "                            result = cursor.fetchone()\n",
        "                            if not result or not result[0]:\n",
        "                                continue\n",
        "\n",
        "                            total, wins, total_pnl, avg_pnl = result\n",
        "                            accuracy = (wins / total * 100) if total > 0 else 0.0\n",
        "\n",
        "                            cursor.execute('''\n",
        "                                SELECT pnl FROM completed_trades\n",
        "                                WHERE pair = ? AND model_used = ? AND evaluated_at > ?\n",
        "                            ''', (pair, model, since))\n",
        "\n",
        "                            pnls = [row[0] for row in cursor.fetchall()]\n",
        "                            sharpe = 0.0\n",
        "                            if len(pnls) > 1:\n",
        "                                pnl_std = np.std(pnls)\n",
        "                                if pnl_std > 0:\n",
        "                                    sharpe = (avg_pnl or 0) / pnl_std\n",
        "\n",
        "                            cursor.execute('''\n",
        "                                INSERT OR REPLACE INTO model_stats_cache\n",
        "                                (updated_at, pair, model_name, days, total_trades,\n",
        "                                 winning_trades, losing_trades, accuracy_pct,\n",
        "                                 total_pnl, avg_pnl, sharpe_ratio)\n",
        "                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                            ''', (\n",
        "                                datetime.now(timezone.utc).isoformat(),\n",
        "                                pair, model, days, total, wins or 0, (total - wins) or 0,\n",
        "                                accuracy, total_pnl or 0.0, avg_pnl or 0.0, sharpe\n",
        "                            ))\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Stats update failed: {e}\", \"warn\")\n",
        "\n",
        "    def get_database_stats(self):\n",
        "        \"\"\"Get database statistics\"\"\"\n",
        "        stats = {}\n",
        "\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('SELECT COUNT(*) FROM pending_trades WHERE evaluated = 0')\n",
        "                stats['pending_trades'] = cursor.fetchone()[0]\n",
        "\n",
        "                cursor.execute('SELECT COUNT(*) FROM completed_trades')\n",
        "                stats['completed_trades'] = cursor.fetchone()[0]\n",
        "\n",
        "                cursor.execute('SELECT SUM(pnl) FROM completed_trades')\n",
        "                result = cursor.fetchone()\n",
        "                stats['total_pnl'] = result[0] if result[0] else 0.0\n",
        "\n",
        "                cursor.execute('''\n",
        "                    SELECT COUNT(*), SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END)\n",
        "                    FROM completed_trades\n",
        "                ''')\n",
        "                result = cursor.fetchone()\n",
        "                if result and result[0] > 0:\n",
        "                    stats['overall_accuracy'] = (result[1] / result[0]) * 100\n",
        "                else:\n",
        "                    stats['overall_accuracy'] = 0.0\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Stats retrieval failed: {e}\", \"warn\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close database connection\"\"\"\n",
        "        try:\n",
        "            if self.conn:\n",
        "                self.conn.close()\n",
        "                print_status(\"‚úÖ Database closed\", \"success\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Close error: {e}\", \"warn\")\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ ML PREDICTION FROM PROCESSED PICKLES\n",
        "# ======================================================\n",
        "def train_predict_from_pickle(pickle_path, pair_name):\n",
        "    \"\"\"Train and predict using processed pickle file with robust feature handling\"\"\"\n",
        "    try:\n",
        "        # Load processed pickle (already has indicators!)\n",
        "        df = pd.read_pickle(pickle_path, compression='gzip')\n",
        "\n",
        "        if df.empty or len(df) < 50:\n",
        "            return None, None, 0.5\n",
        "\n",
        "        # ‚úÖ ROBUST FEATURE SELECTION: Exclude all non-indicator columns\n",
        "        exclude_cols = [\n",
        "            'close', 'raw_close', 'raw_open', 'raw_high', 'raw_low',\n",
        "            'open', 'high', 'low',  # Also exclude OHLC\n",
        "            'volume', 'vwap'  # Optional columns that may not exist everywhere\n",
        "        ]\n",
        "\n",
        "        feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "\n",
        "        if not feature_cols:\n",
        "            print_status(f\"‚ö†Ô∏è No features in {pickle_path.name}\", \"warn\")\n",
        "            return None, None, 0.5\n",
        "\n",
        "        # Use only available features\n",
        "        X = df[feature_cols].fillna(0)\n",
        "        y = (df['close'].diff() > 0).astype(int).fillna(0)\n",
        "\n",
        "        safe_pair = pair_name.replace(\"/\", \"_\")\n",
        "\n",
        "        # ‚úÖ SEPARATE MODELS PER TIMEFRAME to avoid feature mismatch\n",
        "        timeframe = \"unknown\"\n",
        "        if \"1d\" in pickle_path.name or \"daily\" in pickle_path.name:\n",
        "            timeframe = \"1d\"\n",
        "        elif \"1h\" in pickle_path.name:\n",
        "            timeframe = \"1h\"\n",
        "        elif \"15m\" in pickle_path.name:\n",
        "            timeframe = \"15m\"\n",
        "        elif \"5m\" in pickle_path.name:\n",
        "            timeframe = \"5m\"\n",
        "        elif \"1m\" in pickle_path.name:\n",
        "            timeframe = \"1m\"\n",
        "\n",
        "        # ‚úÖ UNIQUE MODEL FILES PER PAIR+TIMEFRAME\n",
        "        model_id = f\"{safe_pair}_{timeframe}\"\n",
        "\n",
        "        # SGD Model (incremental learning)\n",
        "        sgd_file = PICKLE_FOLDER / f\"{model_id}_sgd_model.pkl\"\n",
        "\n",
        "        try:\n",
        "            if sgd_file.exists():\n",
        "                sgd = pickle.load(open(sgd_file, \"rb\"))\n",
        "                # Try to use existing model\n",
        "                try:\n",
        "                    sgd.partial_fit(X, y)\n",
        "                except ValueError:\n",
        "                    # Feature mismatch - retrain from scratch\n",
        "                    sgd = SGDClassifier(max_iter=1000, tol=1e-3)\n",
        "                    sgd.partial_fit(X, y, classes=np.array([0, 1]))\n",
        "            else:\n",
        "                sgd = SGDClassifier(max_iter=1000, tol=1e-3)\n",
        "                sgd.partial_fit(X, y, classes=np.array([0, 1]))\n",
        "\n",
        "            pickle.dump(sgd, open(sgd_file, \"wb\"))\n",
        "            sgd_pred = int(sgd.predict(X.iloc[[-1]])[0])\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è SGD error for {model_id}: {e}\", \"debug\")\n",
        "            sgd_pred = 1  # Default to buy\n",
        "\n",
        "        # RandomForest (full retrain each time - no feature mismatch issues)\n",
        "        try:\n",
        "            rf = RandomForestClassifier(n_estimators=50, class_weight='balanced', random_state=42)\n",
        "            rf.fit(X, y)\n",
        "\n",
        "            rf_file = PICKLE_FOLDER / f\"{model_id}_rf_model.pkl\"\n",
        "            pickle.dump(rf, open(rf_file, \"wb\"))\n",
        "\n",
        "            rf_pred = int(rf.predict(X.iloc[[-1]])[0])\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è RF error for {model_id}: {e}\", \"debug\")\n",
        "            rf_pred = 1  # Default to buy\n",
        "\n",
        "        # Ensemble\n",
        "        confidence = (sgd_pred + rf_pred) / 2.0\n",
        "\n",
        "        return sgd_pred, rf_pred, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ö†Ô∏è ML error for {pickle_path.name}: {e}\", \"debug\")\n",
        "        return None, None, 0.5\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ PROCESS SINGLE PICKLE FILE\n",
        "# ======================================================\n",
        "def process_pickle_file(pickle_path):\n",
        "    \"\"\"Process single processed pickle file\"\"\"\n",
        "    # Extract pair from filename\n",
        "    filename = pickle_path.stem\n",
        "\n",
        "    # Extract pair (e.g., \"EUR_USD_1d_5y\" -> \"EUR/USD\")\n",
        "    currencies = ['EUR', 'USD', 'GBP', 'JPY', 'AUD', 'NZD', 'CAD', 'CHF']\n",
        "    pair = None\n",
        "\n",
        "    for curr1 in currencies:\n",
        "        for curr2 in currencies:\n",
        "            if curr1 != curr2 and filename.startswith(f\"{curr1}_{curr2}\"):\n",
        "                pair = f\"{curr1}/{curr2}\"\n",
        "                break\n",
        "        if pair:\n",
        "            break\n",
        "\n",
        "    if not pair:\n",
        "        print_status(f\"‚ö†Ô∏è Could not extract pair from {filename}\", \"warn\")\n",
        "        return None, {}, \"HOLD\"\n",
        "\n",
        "    # Determine timeframe from filename\n",
        "    timeframe = \"unknown\"\n",
        "    if \"1d\" in filename or \"daily\" in filename:\n",
        "        timeframe = \"1d\"\n",
        "    elif \"1h\" in filename:\n",
        "        timeframe = \"1h\"\n",
        "    elif \"15m\" in filename:\n",
        "        timeframe = \"15m\"\n",
        "    elif \"5m\" in filename:\n",
        "        timeframe = \"5m\"\n",
        "    elif \"1m\" in filename:\n",
        "        timeframe = \"1m\"\n",
        "\n",
        "    try:\n",
        "        # Load pickle\n",
        "        df = pd.read_pickle(pickle_path, compression='gzip')\n",
        "\n",
        "        if df.empty:\n",
        "            return pair, {}, \"HOLD\"\n",
        "\n",
        "        # Get current price (use close if no raw_close)\n",
        "        current_price = df['raw_close'].iloc[-1] if 'raw_close' in df.columns else df['close'].iloc[-1]\n",
        "\n",
        "        # Calculate SL/TP using ATR\n",
        "        if 'ATR' in df.columns:\n",
        "            atr = df['ATR'].iloc[-1]\n",
        "            mult = 2.0\n",
        "            sl = max(0, round(current_price - atr * mult, 5))\n",
        "            tp = round(current_price + atr * mult, 5)\n",
        "        else:\n",
        "            # Fallback if ATR missing\n",
        "            atr_fallback = current_price * 0.01  # 1% of price\n",
        "            sl = max(0, round(current_price - atr_fallback * 2, 5))\n",
        "            tp = round(current_price + atr_fallback * 2, 5)\n",
        "\n",
        "        # ML Predictions\n",
        "        sgd_pred, rf_pred, confidence = train_predict_from_pickle(pickle_path, pair)\n",
        "\n",
        "        if sgd_pred is None:\n",
        "            return pair, {}, \"HOLD\"\n",
        "\n",
        "        ensemble_pred = 1 if (sgd_pred + rf_pred) >= 1 else 0\n",
        "\n",
        "        signal_data = {\n",
        "            \"signal\": ensemble_pred,\n",
        "            \"sgd_pred\": sgd_pred,\n",
        "            \"rf_pred\": rf_pred,\n",
        "            \"live\": current_price,\n",
        "            \"SL\": sl,\n",
        "            \"TP\": tp,\n",
        "            \"confidence\": confidence,\n",
        "            \"timeframe\": timeframe\n",
        "        }\n",
        "\n",
        "        print_status(\n",
        "            f\"{pair} | {timeframe} | Ens:{ensemble_pred} (SGD:{sgd_pred} RF:{rf_pred}) | \"\n",
        "            f\"Price:{current_price:.5f} | SL:{sl:.5f} | TP:{tp:.5f}\",\n",
        "            \"info\"\n",
        "        )\n",
        "\n",
        "        return pair, {timeframe: signal_data}, \"LONG\" if ensemble_pred == 1 else \"SHORT\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ùå Error processing {pickle_path.name}: {e}\", \"error\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return pair, {}, \"HOLD\"\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ MAIN PIPELINE\n",
        "# ======================================================\n",
        "def run_pipeline(current_iteration=1):\n",
        "    \"\"\"Run complete pipeline\"\"\"\n",
        "    print_status(\"=\"*70, \"info\")\n",
        "    print_status(\"üöÄ STARTING ULTRA-PERSISTENT PIPELINE v4.0\", \"success\")\n",
        "    print_status(\"=\"*70, \"info\")\n",
        "\n",
        "    # Initialize database\n",
        "    db = EnhancedTradeMemoryDatabase()\n",
        "\n",
        "    # Get stats\n",
        "    print_status(\"\\nüìä CURRENT DATABASE STATS\", \"data\")\n",
        "    stats = db.get_database_stats()\n",
        "    print_status(f\"  Pending: {stats.get('pending_trades', 0)}\", \"data\")\n",
        "    print_status(f\"  Completed: {stats.get('completed_trades', 0)}\", \"data\")\n",
        "    print_status(f\"  Total P&L: ${stats.get('total_pnl', 0.0):.5f}\", \"data\")\n",
        "    print_status(f\"  Accuracy: {stats.get('overall_accuracy', 0.0):.1f}%\", \"data\")\n",
        "\n",
        "    # Load processed pickles\n",
        "    print_status(\"\\nüîÑ LOADING PROCESSED PICKLES\", \"info\")\n",
        "    print_status(f\"üìÇ Looking in: {PICKLE_FOLDER}\", \"info\")\n",
        "\n",
        "    pickle_files = list(PICKLE_FOLDER.glob(\"*.pkl\"))\n",
        "\n",
        "    # Filter out model files\n",
        "    pickle_files = [f for f in pickle_files if not any(\n",
        "        suffix in f.name for suffix in ['_sgd_model', '_rf_model', 'indicator_cache']\n",
        "    )]\n",
        "\n",
        "    if not pickle_files:\n",
        "        print_status(\"‚ö†Ô∏è No processed pickles found!\", \"warn\")\n",
        "        print_status(\"‚ÑπÔ∏è  Run CSV combiner first to generate processed pickles\", \"info\")\n",
        "        return {}\n",
        "\n",
        "    print_status(f\"‚úÖ Found {len(pickle_files)} processed pickle files\", \"success\")\n",
        "\n",
        "    aggregated_signals = {}\n",
        "    current_prices = {}\n",
        "\n",
        "    # Process each pickle\n",
        "    for pickle_file in pickle_files:\n",
        "        pair, signals, agg_signal = process_pickle_file(pickle_file)\n",
        "\n",
        "        if pair:\n",
        "            if pair not in aggregated_signals:\n",
        "                aggregated_signals[pair] = {\"signals\": {}, \"aggregated\": \"HOLD\"}\n",
        "\n",
        "            # Merge signals for same pair\n",
        "            aggregated_signals[pair][\"signals\"].update(signals)\n",
        "\n",
        "            # Update aggregated signal (use last non-HOLD)\n",
        "            if agg_signal != \"HOLD\":\n",
        "                aggregated_signals[pair][\"aggregated\"] = agg_signal\n",
        "\n",
        "            # Collect current prices\n",
        "            for tf, signal_data in signals.items():\n",
        "                if signal_data.get('live', 0) > 0:\n",
        "                    current_prices[pair] = signal_data['live']\n",
        "                    break\n",
        "\n",
        "    # Store new signals\n",
        "    print_status(\"\\nüíæ STORING SIGNALS\", \"info\")\n",
        "    stored = db.store_new_signals(aggregated_signals, current_iteration)\n",
        "    print_status(f\"‚úÖ Stored {stored} signals\", \"success\")\n",
        "\n",
        "    # Evaluate pending trades\n",
        "    print_status(\"\\nüîç EVALUATING PENDING TRADES\", \"info\")\n",
        "    if current_prices:\n",
        "        results = db.evaluate_pending_trades(current_prices, current_iteration)\n",
        "\n",
        "        if results:\n",
        "            print_status(\"\\nüìà EVALUATION RESULTS\", \"data\")\n",
        "            for model, data in results.items():\n",
        "                print_status(f\"  {model}:\", \"data\")\n",
        "                print_status(f\"    Closed: {data['closed_trades']}\", \"data\")\n",
        "                print_status(f\"    Wins: {data['wins']}\", \"data\")\n",
        "                print_status(f\"    Losses: {data['losses']}\", \"data\")\n",
        "                print_status(f\"    Accuracy: {data.get('accuracy', 0):.1f}%\", \"data\")\n",
        "                print_status(f\"    Total P&L: ${data['total_pnl']:.5f}\", \"data\")\n",
        "    else:\n",
        "        print_status(\"‚ö†Ô∏è No current prices for evaluation\", \"warn\")\n",
        "\n",
        "    # Export to JSON\n",
        "    print_status(\"\\nüìù EXPORTING TO JSON\", \"info\")\n",
        "    json_file = OUTPUTS_FOLDER / \"latest_signals.json\"\n",
        "    tmp_file = OUTPUTS_FOLDER / \"latest_signals_tmp.json\"\n",
        "\n",
        "    export_data = {\n",
        "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"iteration\": current_iteration,\n",
        "        \"pairs\": aggregated_signals,\n",
        "        \"database_stats\": stats\n",
        "    }\n",
        "\n",
        "    with open(tmp_file, \"w\") as f:\n",
        "        json.dump(export_data, f, indent=2)\n",
        "\n",
        "    # Push to GitHub if changes\n",
        "    if FOREX_PAT and (not json_file.exists() or not filecmp.cmp(tmp_file, json_file)):\n",
        "        tmp_file.replace(json_file)\n",
        "        print_status(\"üì§ Pushing to GitHub...\", \"info\")\n",
        "\n",
        "        try:\n",
        "            os.chdir(REPO_FOLDER)\n",
        "\n",
        "            subprocess.run([\"git\", \"add\", str(json_file)], check=False)\n",
        "            subprocess.run(\n",
        "                [\"git\", \"commit\", \"-m\", f\"üìà Update signals - Iteration {current_iteration}\"],\n",
        "                check=False\n",
        "            )\n",
        "\n",
        "            for attempt in range(3):\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "                if result.returncode == 0:\n",
        "                    print_status(\"‚úÖ Pushed to GitHub\", \"success\")\n",
        "                    break\n",
        "                elif attempt < 2:\n",
        "                    time.sleep(5)\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "\n",
        "            os.chdir(SAVE_FOLDER)\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Git push error: {e}\", \"warn\")\n",
        "    else:\n",
        "        print_status(\"‚ÑπÔ∏è No changes - skipping push\", \"info\")\n",
        "        if tmp_file.exists():\n",
        "            tmp_file.unlink()\n",
        "\n",
        "    # Final stats\n",
        "    print_status(\"\\nüìä FINAL DATABASE STATS\", \"data\")\n",
        "    final_stats = db.get_database_stats()\n",
        "    print_status(f\"  Pending: {final_stats.get('pending_trades', 0)}\", \"data\")\n",
        "    print_status(f\"  Completed: {final_stats.get('completed_trades', 0)}\", \"data\")\n",
        "    print_status(f\"  Total P&L: ${final_stats.get('total_pnl', 0.0):.5f}\", \"data\")\n",
        "    print_status(f\"  Accuracy: {final_stats.get('overall_accuracy', 0.0):.1f}%\", \"data\")\n",
        "\n",
        "    db.close()\n",
        "\n",
        "    print_status(\"\\n‚úÖ PIPELINE COMPLETED!\", \"success\")\n",
        "    print_status(\"=\"*70, \"info\")\n",
        "\n",
        "    return aggregated_signals\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ MAIN EXECUTION\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        print_status(\"üöÄ Initializing Ultra-Persistent Pipeline...\", \"info\")\n",
        "\n",
        "        # Run pipeline\n",
        "        signals = run_pipeline(current_iteration=1)\n",
        "\n",
        "        if signals:\n",
        "            print_status(f\"\\nüéâ Generated signals for {len(signals)} pairs!\", \"success\")\n",
        "            for pair, data in signals.items():\n",
        "                print_status(f\"  {pair}: {data['aggregated']}\", \"success\")\n",
        "        else:\n",
        "            print_status(\"\\n‚ö†Ô∏è No signals generated\", \"warn\")\n",
        "\n",
        "        print_status(\"\\n‚úÖ ALL OPERATIONS COMPLETED!\", \"success\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"\\n‚ùå PIPELINE FAILED: {e}\", \"error\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#TAG: pipeline_main\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ULTIMATE FOREX PIPELINE v11.2 - OMEGA ULTIMATE (BEST OF BOTH)\n",
        "==============================================================\n",
        "‚ö° Dynamic seeds + Live updates + Full logging + Documentation\n",
        "üõ°Ô∏è All anti-overfit fixes + Enhanced error handling + Progress tracking\n",
        "‚úÖ Combines strengths of v11.0 and v11.1\n",
        "\"\"\"\n",
        "\n",
        "import os, sys, json, pickle, random, re, smtplib, subprocess, time, logging, warnings\n",
        "from pathlib import Path\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from scipy import stats\n",
        "\n",
        "print(\"=\"*70, \"\\nüî¥ OMEGA v11.2 - ULTIMATE EDITION\\n\", \"=\"*70, sep='')\n",
        "\n",
        "# ======================================================\n",
        "# ENVIRONMENT SETUP\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB, IN_GHA, ENV_NAME = True, False, \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB, IN_GHA = False, \"GITHUB_ACTIONS\" in os.environ\n",
        "    ENV_NAME = \"GitHub Actions\" if IN_GHA else \"Local\"\n",
        "\n",
        "BASE_FOLDER = Path(\"/content\" if IN_COLAB else Path.cwd())\n",
        "SAVE_FOLDER = BASE_FOLDER if IN_GHA else (BASE_FOLDER / \"forex-ai-models\" if IN_COLAB else BASE_FOLDER)\n",
        "\n",
        "DIRECTORIES = {k: SAVE_FOLDER / v for k, v in {\n",
        "    \"data_processed\": \"data/processed\", \"database\": \"database\", \"logs\": \"logs\",\n",
        "    \"outputs\": \"outputs\", \"omega_state\": \"omega_state\"\n",
        "}.items()}\n",
        "\n",
        "for d in DIRECTORIES.values(): d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PICKLE_FOLDER, LOGS_FOLDER = DIRECTORIES[\"data_processed\"], DIRECTORIES[\"logs\"]\n",
        "OUTPUTS_FOLDER, OMEGA_STATE_FOLDER = DIRECTORIES[\"outputs\"], DIRECTORIES[\"omega_state\"]\n",
        "\n",
        "print(f\"üåç {ENV_NAME} | üìÇ {BASE_FOLDER} | ‚ö° {cpu_count()} cores\\n{'='*70}\")\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=str(LOGS_FOLDER / f\"omega_{datetime.now():%Y%m%d_%H%M%S}.log\"),\n",
        "    level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def log(msg, lvl=\"info\"):\n",
        "    icons = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\",\"error\":\"‚ùå\",\"rocket\":\"üöÄ\",\"chart\":\"üìä\",\"brain\":\"üß†\",\"omega\":\"üî¥\"}\n",
        "    getattr(logging, \"warning\" if lvl==\"warn\" else lvl, logging.info)(msg)\n",
        "    print(f\"{icons.get(lvl,'‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# ======================================================\n",
        "# CONFIG\n",
        "# ======================================================\n",
        "GIT_USER_NAME = os.getenv(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.getenv(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME, GITHUB_REPO = \"rahim-dotAI\", \"forex-ai-models\"\n",
        "FOREX_PAT = os.getenv(\"FOREX_PAT\", \"\").strip()\n",
        "\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT: os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "    except: pass\n",
        "\n",
        "GMAIL_USER = os.getenv(\"GMAIL_USER\", \"nakatonabira3@gmail.com\")\n",
        "GMAIL_APP_PASSWORD = os.getenv(\"GMAIL_APP_PASSWORD\", \"\").strip() or \"gmwohahtltmcewug\"\n",
        "BROWSERLESS_TOKEN = os.getenv(\"BROWSERLESS_TOKEN\", \"\")\n",
        "\n",
        "PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "TRAINING_TIMEFRAMES = {p: [\"1d\", \"1h\"] for p in PAIRS}\n",
        "ATR_PERIOD, MIN_ATR, EPS = 14, 1e-5, 1e-8\n",
        "BASE_CAPITAL, MAX_TRADE_CAP = 100, 5.0\n",
        "MAX_ATR_SL, MAX_ATR_TP = 3.0, 3.0\n",
        "WEEKEND_LEARNING_MODE = True\n",
        "\n",
        "OMEGA_SIGNALS_FILE = OUTPUTS_FOLDER / \"omega_signals.json\"\n",
        "OMEGA_LEARNING_FILE = OMEGA_STATE_FOLDER / \"omega_learning.pkl\"\n",
        "OMEGA_ITERATION_FILE = OMEGA_STATE_FOLDER / \"omega_iteration.pkl\"\n",
        "OMEGA_MEMORY_FILE = OMEGA_STATE_FOLDER / \"omega_memory.pkl\"\n",
        "\n",
        "# üõ°Ô∏è FIXED ANTI-OVERFIT CONFIG\n",
        "COMPETITION_MODELS = {\n",
        "    \"Alpha Momentum\": {\n",
        "        \"color\": \"üî¥\", \"strategy\": \"Aggressive momentum\",\n",
        "        \"atr_sl_range\": (1.5, 2.5), \"atr_tp_range\": (2.0, 3.5),\n",
        "        \"risk_range\": (0.010, 0.020), \"confidence_range\": (0.55, 0.70),\n",
        "        \"pop_size\": 16, \"generations\": 8, \"mutation_rate\": 0.12,\n",
        "        \"crossover_rate\": 0.75, \"elite_ratio\": 0.15, \"multi_start\": 3,\n",
        "        \"adaptive_mutation\": True, \"sample_rate\": 0.30, \"use_parallel\": True\n",
        "    },\n",
        "    \"Beta Conservative\": {\n",
        "        \"color\": \"üîµ\", \"strategy\": \"Conservative trend\",\n",
        "        \"atr_sl_range\": (1.2, 2.0), \"atr_tp_range\": (1.8, 2.8),\n",
        "        \"risk_range\": (0.006, 0.012), \"confidence_range\": (0.65, 0.80),\n",
        "        \"pop_size\": 14, \"generations\": 6, \"mutation_rate\": 0.10,\n",
        "        \"crossover_rate\": 0.70, \"elite_ratio\": 0.15, \"multi_start\": 3,\n",
        "        \"adaptive_mutation\": True, \"sample_rate\": 0.30, \"use_parallel\": True\n",
        "    },\n",
        "    \"Gamma Adaptive\": {\n",
        "        \"color\": \"üü¢\", \"strategy\": \"Adaptive momentum\",\n",
        "        \"atr_sl_range\": (1.3, 2.3), \"atr_tp_range\": (2.0, 3.2),\n",
        "        \"risk_range\": (0.008, 0.018), \"confidence_range\": (0.60, 0.75),\n",
        "        \"pop_size\": 18, \"generations\": 10, \"mutation_rate\": 0.15,\n",
        "        \"crossover_rate\": 0.80, \"elite_ratio\": 0.15, \"multi_start\": 3,\n",
        "        \"adaptive_mutation\": True, \"sample_rate\": 0.30, \"use_parallel\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "@dataclass\n",
        "class MarketRegime:\n",
        "    volatility: str\n",
        "    trend: str\n",
        "    strength: float\n",
        "    timestamp: datetime\n",
        "\n",
        "@dataclass\n",
        "class AdvancedMetrics:\n",
        "    sharpe: float\n",
        "    sortino: float\n",
        "    calmar: float\n",
        "    max_drawdown: float\n",
        "    win_rate: float\n",
        "    profit_factor: float\n",
        "    avg_win: float\n",
        "    avg_loss: float\n",
        "    expectancy: float\n",
        "\n",
        "class BalancedAntiOverfitAI:\n",
        "    \"\"\"\n",
        "    üõ°Ô∏è FIXED AI-Powered Anti-Overfitting System\n",
        "    - Realistic thresholds (75% min ratio)\n",
        "    - Balanced penalties (max 60% severity)\n",
        "    - Proper trend detection\n",
        "    \"\"\"\n",
        "    def __init__(self, min_val_ratio=0.75, patience=4):\n",
        "        self.min_val_ratio = min_val_ratio\n",
        "        self.patience = patience\n",
        "        self.history = []\n",
        "        self.best_generalization_score = -float('inf')\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def calculate_generalization_score(self, train_metrics, val_metrics):\n",
        "        \"\"\"FIXED: More realistic scoring\"\"\"\n",
        "        if not train_metrics or not val_metrics:\n",
        "            return 0.0\n",
        "\n",
        "        train_acc = train_metrics.get('accuracy', 0)\n",
        "        val_acc = val_metrics.get('accuracy', 0)\n",
        "\n",
        "        if train_acc < 1:\n",
        "            return 0.0\n",
        "\n",
        "        acc_ratio = val_acc / train_acc\n",
        "\n",
        "        # Scoring: Aim for 80-95% consistency\n",
        "        if acc_ratio >= 0.90:\n",
        "            score = 1.0\n",
        "        elif acc_ratio >= 0.75:\n",
        "            score = 0.5 + (acc_ratio - 0.75) / 0.15 * 0.5\n",
        "        else:\n",
        "            score = max(0.0, acc_ratio / 0.75 * 0.5)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def detect_overfitting(self, train_metrics, val_metrics):\n",
        "        \"\"\"FIXED: Balanced detection with realistic thresholds\"\"\"\n",
        "        if not train_metrics or not val_metrics:\n",
        "            return False, 0.0, []\n",
        "\n",
        "        train_acc = train_metrics.get('accuracy', 0)\n",
        "        val_acc = val_metrics.get('accuracy', 0)\n",
        "        train_pnl = train_metrics.get('total_pnl', 0)\n",
        "        val_pnl = val_metrics.get('total_pnl', 0)\n",
        "\n",
        "        if train_acc < 1:\n",
        "            return False, 0.0, []\n",
        "\n",
        "        acc_ratio = val_acc / train_acc\n",
        "        signals, severity = [], 0.0\n",
        "\n",
        "        # Signal 1: Significant gap (realistic)\n",
        "        if acc_ratio < self.min_val_ratio:\n",
        "            gap = self.min_val_ratio - acc_ratio\n",
        "            signals.append(f\"Val/Train ratio: {acc_ratio:.2f} < {self.min_val_ratio}\")\n",
        "            severity += min(0.4, gap * 2.0)  # Cap at 0.4\n",
        "\n",
        "        # Signal 2: Large absolute difference\n",
        "        if train_acc > val_acc + 20:\n",
        "            signals.append(f\"Large gap: {train_acc:.1f}% vs {val_acc:.1f}%\")\n",
        "            severity += 0.2\n",
        "\n",
        "        # Signal 3: PnL divergence (relaxed)\n",
        "        if train_pnl > 10 and val_pnl < -5:\n",
        "            signals.append(f\"PnL divergence: ${train_pnl:.2f} vs ${val_pnl:.2f}\")\n",
        "            severity += 0.15\n",
        "\n",
        "        # Signal 5: CRITICAL - Zero validation accuracy\n",
        "        if val_acc < 1.0:\n",
        "            signals.append(f\"CRITICAL: Zero validation accuracy\")\n",
        "            severity = 0.6  # Maximum severity\n",
        "\n",
        "        # Signal 4: Declining trend\n",
        "        gen_score = self.calculate_generalization_score(train_metrics, val_metrics)\n",
        "        self.history.append(gen_score)\n",
        "\n",
        "        if len(self.history) >= 4:\n",
        "            recent = np.mean(self.history[-2:])\n",
        "            older = np.mean(self.history[-4:-2])\n",
        "            trend = recent - older\n",
        "\n",
        "            if trend < -0.15:\n",
        "                signals.append(f\"Declining trend: {trend:.3f}\")\n",
        "                severity += 0.15\n",
        "\n",
        "        # FIXED: Cap total severity at 60%\n",
        "        severity = min(0.6, severity)\n",
        "        is_overfitting = severity > 0.20\n",
        "\n",
        "        # Recommendations\n",
        "        recommendations = []\n",
        "        if severity > 0.4:\n",
        "            recommendations.append(\"Moderate regularization needed\")\n",
        "        if severity > 0.25:\n",
        "            recommendations.append(\"Increase mutation rate slightly\")\n",
        "        if acc_ratio < 0.65:\n",
        "            recommendations.append(\"Consider simpler features\")\n",
        "\n",
        "        return is_overfitting, severity, recommendations\n",
        "\n",
        "    def should_stop_early(self, train_metrics, val_metrics):\n",
        "        \"\"\"Determine if training should stop early\"\"\"\n",
        "        gen_score = self.calculate_generalization_score(train_metrics, val_metrics)\n",
        "\n",
        "        if gen_score > self.best_generalization_score * 1.01:\n",
        "            self.best_generalization_score = gen_score\n",
        "            self.patience_counter = 0\n",
        "            return False, \"Improving\"\n",
        "        else:\n",
        "            self.patience_counter += 1\n",
        "            if self.patience_counter >= self.patience:\n",
        "                return True, f\"No improvement for {self.patience} checks\"\n",
        "            return False, f\"Patience: {self.patience_counter}/{self.patience}\"\n",
        "\n",
        "    def apply_adaptive_regularization(self, config, severity):\n",
        "        \"\"\"FIXED: Gradual regularization increases\"\"\"\n",
        "        adjusted_config = config.copy()\n",
        "\n",
        "        if severity > 0.4:  # Moderate\n",
        "            adjusted_config['mutation_rate'] = min(0.25, config['mutation_rate'] * 1.5)\n",
        "            adjusted_config['confidence_range'] = (\n",
        "                config['confidence_range'][0] + 0.08,\n",
        "                min(0.90, config['confidence_range'][1] + 0.08)\n",
        "            )\n",
        "            log(f\"  üõ°Ô∏è Moderate overfitting: Applied regularization\", \"warn\")\n",
        "\n",
        "        elif severity > 0.25:  # Mild\n",
        "            adjusted_config['mutation_rate'] = min(0.20, config['mutation_rate'] * 1.2)\n",
        "            adjusted_config['confidence_range'] = (\n",
        "                config['confidence_range'][0] + 0.05,\n",
        "                min(0.90, config['confidence_range'][1] + 0.05)\n",
        "            )\n",
        "            log(f\"  üõ°Ô∏è Mild overfitting: Applied light regularization\", \"info\")\n",
        "\n",
        "        return adjusted_config\n",
        "\n",
        "    def get_report(self):\n",
        "        \"\"\"Generate overfitting report\"\"\"\n",
        "        if len(self.history) < 2:\n",
        "            return {\n",
        "                'generalization_score': 0,\n",
        "                'best_score': 0,\n",
        "                'trend': 'üìä Initializing',\n",
        "                'patience_counter': 0,\n",
        "                'history': []\n",
        "            }\n",
        "\n",
        "        recent_gen = np.mean(self.history[-3:]) if len(self.history) >= 3 else self.history[-1]\n",
        "        trend = \"üìà Improving\" if recent_gen > self.best_generalization_score * 0.90 else \"üìâ Adjusting\"\n",
        "\n",
        "        return {\n",
        "            'generalization_score': recent_gen,\n",
        "            'best_score': self.best_generalization_score,\n",
        "            'trend': trend,\n",
        "            'patience_counter': self.patience_counter,\n",
        "            'history': self.history[-10:]\n",
        "        }\n",
        "\n",
        "class CircuitBreaker:\n",
        "    def __init__(self, max_errors=5, time_window=300):\n",
        "        self.max_errors, self.time_window = max_errors, time_window\n",
        "        self.errors, self.is_open = [], False\n",
        "\n",
        "    def record_error(self, error_type: str, severity: str):\n",
        "        self.errors.append({'type': error_type, 'severity': severity, 'timestamp': datetime.now()})\n",
        "        cutoff = datetime.now() - timedelta(seconds=self.time_window)\n",
        "        self.errors = [e for e in self.errors if e['timestamp'] > cutoff]\n",
        "        if len(self.errors) >= self.max_errors:\n",
        "            self.is_open = True\n",
        "            log(\"üö® CIRCUIT BREAKER ACTIVATED\", \"error\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "CIRCUIT_BREAKER = CircuitBreaker()\n",
        "\n",
        "def detect_regime(df: pd.DataFrame) -> MarketRegime:\n",
        "    returns = df['close'].pct_change().dropna()\n",
        "    cv, hv = returns.tail(20).std(), returns.std()\n",
        "    vol = 'high' if cv > hv*1.5 else 'low' if cv < hv*0.6 else 'normal'\n",
        "\n",
        "    high, low, close = df['high'].values, df['low'].values, df['close'].values\n",
        "    plus_dm = np.where((high[1:]-high[:-1])>(low[:-1]-low[1:]), np.maximum(high[1:]-high[:-1],0), 0)\n",
        "    minus_dm = np.where((low[:-1]-low[1:])>(high[1:]-high[:-1]), np.maximum(low[:-1]-low[1:],0), 0)\n",
        "    tr = np.maximum.reduce([high[1:]-low[1:], np.abs(high[1:]-close[:-1]), np.abs(low[1:]-close[:-1])])\n",
        "\n",
        "    atr = pd.Series(tr).rolling(14).mean().values\n",
        "    plus_di = 100 * pd.Series(plus_dm).rolling(14).mean().values / (atr + EPS)\n",
        "    minus_di = 100 * pd.Series(minus_dm).rolling(14).mean().values / (atr + EPS)\n",
        "    dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + EPS)\n",
        "    adx = pd.Series(dx).rolling(14).mean().iloc[-1]\n",
        "\n",
        "    return MarketRegime(\n",
        "        volatility=vol,\n",
        "        trend='trending' if adx>25 else 'ranging',\n",
        "        strength=min(100, adx*2) if adx>25 else max(0, 100-adx*3),\n",
        "        timestamp=datetime.now(timezone.utc)\n",
        "    )\n",
        "\n",
        "def calc_kelly(wr: float, aw: float, al: float) -> float:\n",
        "    if al==0 or wr==0: return 0.0\n",
        "    kelly = (wr * aw/abs(al) - (1-wr)) / (aw/abs(al))\n",
        "    return max(0, min(0.25, kelly*0.25))\n",
        "\n",
        "def calc_position_size(equity, risk_pct, atr, atr_mult, wr=0.5, aw=1.0, al=1.0, positions=0):\n",
        "    kelly = calc_kelly(wr, aw, al)\n",
        "    base = equity * kelly\n",
        "    risk = min(equity*risk_pct, MAX_TRADE_CAP) / (atr*atr_mult + EPS)\n",
        "    final = min(base, risk)\n",
        "    return max(0, final*0.5 if positions>=3 else final)\n",
        "\n",
        "def calc_metrics(trades: List[Dict], eq_curve: List[float]) -> AdvancedMetrics:\n",
        "    if not trades or len(eq_curve)<2:\n",
        "        return AdvancedMetrics(0,0,0,0,0,0,0,0,0)\n",
        "\n",
        "    pnls = [t['pnl'] for t in trades]\n",
        "    wins = [p for p in pnls if p>0]\n",
        "    losses = [p for p in pnls if p<0]\n",
        "\n",
        "    wr = len(wins)/len(pnls) if pnls else 0\n",
        "    aw = np.mean(wins) if wins else 0\n",
        "    al = np.mean(losses) if losses else 0\n",
        "    pf = sum(wins)/abs(sum(losses)) if losses else float('inf')\n",
        "    exp = (wr*aw) + ((1-wr)*al)\n",
        "\n",
        "    returns = np.diff(eq_curve) / (np.array(eq_curve[:-1]) + EPS)\n",
        "    sharpe = (np.mean(returns)/np.std(returns)*np.sqrt(252)) if len(returns)>1 and np.std(returns)>0 else 0\n",
        "\n",
        "    down_ret = returns[returns<0]\n",
        "    sortino = (np.mean(returns)/np.std(down_ret)*np.sqrt(252)) if len(down_ret)>1 else sharpe\n",
        "\n",
        "    peak, mdd = eq_curve[0], 0\n",
        "    for e in eq_curve:\n",
        "        peak = max(peak, e)\n",
        "        mdd = max(mdd, (peak-e)/peak)\n",
        "\n",
        "    total_ret = (eq_curve[-1]-eq_curve[0])/eq_curve[0]\n",
        "    calmar = total_ret/mdd if mdd>0 else 0\n",
        "\n",
        "    return AdvancedMetrics(sharpe, sortino, calmar, mdd*100, wr*100, pf, aw, al, exp)\n",
        "\n",
        "def is_weekend() -> bool:\n",
        "    return datetime.now().weekday() >= 5\n",
        "\n",
        "def fetch_price(pair, timeout=10):\n",
        "    if not BROWSERLESS_TOKEN: return None\n",
        "    try:\n",
        "        fc, tc = pair.split(\"/\")\n",
        "        url = f\"https://production-sfo.browserless.io/content?token={BROWSERLESS_TOKEN}\"\n",
        "        r = requests.post(url, json={\"url\": f\"https://www.x-rates.com/calculator/?from={fc}&to={tc}&amount=1\"}, timeout=timeout)\n",
        "        m = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', r.text)\n",
        "        return float(m.group(1).replace(\",\", \"\")) if m else None\n",
        "    except Exception as e:\n",
        "        CIRCUIT_BREAKER.record_error(\"price_fetch\", \"medium\")\n",
        "    return None\n",
        "\n",
        "def ensure_atr(df):\n",
        "    if \"atr\" in df.columns and df[\"atr\"].median() > MIN_ATR:\n",
        "        return df.assign(atr=df[\"atr\"].fillna(MIN_ATR).clip(lower=MIN_ATR))\n",
        "    high, low, close = df[\"high\"].values, df[\"low\"].values, df[\"close\"].values\n",
        "    tr = np.maximum.reduce([high-low, np.abs(high-np.roll(close,1)), np.abs(low-np.roll(close,1))])\n",
        "    tr[0] = high[0]-low[0] if len(tr)>0 else MIN_ATR\n",
        "    df[\"atr\"] = pd.Series(tr, index=df.index).rolling(ATR_PERIOD, min_periods=1).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "    return df\n",
        "\n",
        "def seed_signal(df):\n",
        "    \"\"\"Calculate hybrid signal\"\"\"\n",
        "    if \"hybrid_signal\" in df.columns and df[\"hybrid_signal\"].abs().sum() > 0:\n",
        "        return df\n",
        "\n",
        "    fast = df[\"close\"].rolling(10, min_periods=1).mean()\n",
        "    slow = df[\"close\"].rolling(50, min_periods=1).mean()\n",
        "\n",
        "    ema12 = df[\"close\"].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df[\"close\"].ewm(span=26, adjust=False).mean()\n",
        "    macd_line = ema12 - ema26\n",
        "    macd_signal = macd_line.ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    delta = df[\"close\"].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
        "    rsi = 100 - (100 / (1 + gain / (loss + EPS)))\n",
        "\n",
        "    roc = df[\"close\"].pct_change(10) * 100\n",
        "\n",
        "    raw = (\n",
        "        (fast - slow) * 0.3 +\n",
        "        (macd_line - macd_signal) * 0.3 +\n",
        "        ((rsi - 50) / 50) * 0.2 +\n",
        "        roc.fillna(0) * 0.2\n",
        "    ).fillna(0)\n",
        "\n",
        "    df[\"hybrid_signal\"] = raw.ewm(span=3, adjust=False).mean()\n",
        "\n",
        "    return df\n",
        "\n",
        "def update_pickle_data():\n",
        "    \"\"\"\n",
        "    üîÑ NEW v11.1: Fetch latest prices and update pickle files with live data\n",
        "    - Creates backups before modification\n",
        "    - Validates data integrity\n",
        "    - Handles compression properly\n",
        "    \"\"\"\n",
        "    log(\"üîÑ Updating pickle data with live prices...\", \"info\")\n",
        "    updated_count = 0\n",
        "\n",
        "    for pair in PAIRS:\n",
        "        latest_price = fetch_price(pair)\n",
        "        if not latest_price or latest_price <= 0:\n",
        "            continue\n",
        "\n",
        "        pair_key = pair.replace(\"/\", \"_\")\n",
        "        for pkl_file in PICKLE_FOLDER.glob(f\"{pair_key}*.pkl\"):\n",
        "            # Skip model files - only update data files\n",
        "            if any(x in pkl_file.name for x in ['_model', '_sgd', '_rf', 'indicator_cache']):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Create backup before modifying\n",
        "                backup_file = pkl_file.with_suffix('.pkl.bak')\n",
        "\n",
        "                # Try gzipped first, then regular pickle\n",
        "                try:\n",
        "                    df = pd.read_pickle(pkl_file, compression='gzip')\n",
        "                except:\n",
        "                    try:\n",
        "                        df = pd.read_pickle(pkl_file, compression=None)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                # Verify it's a DataFrame with price data\n",
        "                if not isinstance(df, pd.DataFrame) or len(df) < 10:\n",
        "                    continue\n",
        "                if not all(c in df.columns for c in ['open', 'high', 'low', 'close']):\n",
        "                    continue\n",
        "\n",
        "                # Make backup\n",
        "                import shutil\n",
        "                shutil.copy2(pkl_file, backup_file)\n",
        "\n",
        "                last_time = df.index[-1]\n",
        "                new_time = datetime.now().replace(second=0, microsecond=0)\n",
        "\n",
        "                # Only add if time is newer\n",
        "                if new_time > last_time:\n",
        "                    new_row = pd.DataFrame({\n",
        "                        'open': [float(latest_price)],\n",
        "                        'high': [float(latest_price)],\n",
        "                        'low': [float(latest_price)],\n",
        "                        'close': [float(latest_price)],\n",
        "                        'volume': [0]\n",
        "                    }, index=[new_time])\n",
        "\n",
        "                    df = pd.concat([df, new_row]).tail(5000)\n",
        "\n",
        "                    # Clean data before saving\n",
        "                    df = df.ffill().bfill()  # Fill any NaN values\n",
        "                    df = ensure_atr(seed_signal(df))\n",
        "\n",
        "                    # Verify no NaN in critical columns\n",
        "                    if df[['open', 'high', 'low', 'close', 'atr']].isna().any().any():\n",
        "                        log(f\"  ‚ö†Ô∏è Skipped {pair} [{pkl_file.stem}] - NaN detected\", \"warn\")\n",
        "                        shutil.copy2(backup_file, pkl_file)  # Restore backup\n",
        "                        continue\n",
        "\n",
        "                    # Save with compression\n",
        "                    df.to_pickle(pkl_file, compression='gzip')\n",
        "                    updated_count += 1\n",
        "                    log(f\"  ‚úÖ Updated {pair} [{pkl_file.stem}] @ {latest_price}\", \"success\")\n",
        "\n",
        "                    # Remove backup on success\n",
        "                    if backup_file.exists():\n",
        "                        backup_file.unlink()\n",
        "\n",
        "            except Exception as e:\n",
        "                # Restore from backup if exists\n",
        "                if backup_file.exists():\n",
        "                    try:\n",
        "                        shutil.copy2(backup_file, pkl_file)\n",
        "                        backup_file.unlink()\n",
        "                    except:\n",
        "                        pass\n",
        "                log(f\"  ‚ùå Update failed {pair}: {e}\", \"error\")\n",
        "\n",
        "    log(f\"‚úÖ Updated {updated_count} data files with live prices\", \"success\" if updated_count > 0 else \"info\")\n",
        "    return updated_count\n",
        "\n",
        "def load_data(folder):\n",
        "    log(f\"üìÇ Loading from: {folder}\", \"info\")\n",
        "    if not folder.exists(): return {}\n",
        "\n",
        "    # v11.1 enhancement: Better file filtering\n",
        "    all_pkl = [p for p in folder.glob(\"*.pkl\") if not any(s in p.name for s in\n",
        "               ['_sgd_model','_rf_model','indicator_cache','ultra_','alpha_','_model.pkl','.bak'])]\n",
        "    if not all_pkl: return {}\n",
        "\n",
        "    pair_files = defaultdict(list)\n",
        "    currencies = [\"EUR\",\"GBP\",\"USD\",\"AUD\",\"NZD\",\"CAD\",\"CHF\",\"JPY\"]\n",
        "    for pkl in all_pkl:\n",
        "        parts = pkl.stem.split('_')\n",
        "        if len(parts)>=2 and parts[0] in currencies and parts[1] in currencies:\n",
        "            pair_files[f\"{parts[0]}_{parts[1]}\"].append(pkl)\n",
        "\n",
        "    combined = {}\n",
        "    for pk, files in pair_files.items():\n",
        "        pair = f\"{pk[:3]}/{pk[4:]}\"\n",
        "        if pair not in PAIRS: continue\n",
        "        pair_data = {}\n",
        "        for pkl in files:\n",
        "            try:\n",
        "                # v11.1 enhancement: Better compression handling\n",
        "                try:\n",
        "                    df = pd.read_pickle(pkl, compression='gzip')\n",
        "                except:\n",
        "                    try:\n",
        "                        df = pd.read_pickle(pkl, compression=None)\n",
        "                    except:\n",
        "                        log(f\"‚ùå Failed {pkl.name}: Cannot read file\", \"error\")\n",
        "                        continue\n",
        "\n",
        "                if not isinstance(df, pd.DataFrame) or len(df)<50 or not all(c in df.columns for c in ['open','high','low','close']):\n",
        "                    continue\n",
        "\n",
        "                # v11.1 enhancement: Enhanced data cleaning\n",
        "                df = df.ffill().bfill()\n",
        "                df = df.dropna(subset=['open', 'high', 'low', 'close'])\n",
        "\n",
        "                if len(df) < 50:\n",
        "                    continue\n",
        "\n",
        "                df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "                if df.index.tz: df.index = df.index.tz_localize(None)\n",
        "                df = df[df.index.notna()]\n",
        "\n",
        "                tf = \"1d\" if \"1d\" in pkl.stem or \"daily\" in pkl.stem else \"1h\" if \"1h\" in pkl.stem else \"15m\" if \"15m\" in pkl.stem else \"5m\" if \"5m\" in pkl.stem else \"1m\" if \"1m\" in pkl.stem else \"unified\"\n",
        "                if tf not in TRAINING_TIMEFRAMES.get(pair, [\"1d\",\"1h\"]): continue\n",
        "\n",
        "                df = ensure_atr(seed_signal(df))\n",
        "\n",
        "                # v11.1 enhancement: Final NaN check\n",
        "                if df[['open', 'high', 'low', 'close', 'atr']].isna().any().any():\n",
        "                    log(f\"‚ö†Ô∏è Skipped {pkl.name}: Contains NaN after processing\", \"warn\")\n",
        "                    continue\n",
        "\n",
        "                pair_data[tf] = df\n",
        "                log(f\"‚úÖ {pair} [{tf}]: {len(df)} rows\", \"success\")\n",
        "            except Exception as e:\n",
        "                log(f\"‚ùå Failed {pkl.name}: {e}\", \"error\")\n",
        "                if CIRCUIT_BREAKER.record_error(\"data_load\", \"low\"): raise\n",
        "        if pair_data: combined[pair] = pair_data\n",
        "\n",
        "    log(f\"‚úÖ Loaded {len(combined)} pairs, {sum(len(df) for tfs in combined.values() for df in tfs.values())} rows\", \"success\")\n",
        "    return combined\n",
        "\n",
        "def split_data(data: Dict, train=0.45, gap=0.05, val=0.30):\n",
        "    \"\"\"FIXED: Walk-forward split with temporal gap to prevent leakage\"\"\"\n",
        "    train_data, val_data, test_data = {}, {}, {}\n",
        "\n",
        "    log(f\"üìä Walk-Forward Split: Train={train*100:.0f}% | Gap={gap*100:.0f}% | Val={val*100:.0f}% | Test={100-train*100-gap*100-val*100:.0f}%\", \"info\")\n",
        "\n",
        "    for pair, tfs in data.items():\n",
        "        train_data[pair], val_data[pair], test_data[pair] = {}, {}, {}\n",
        "        for tf, df in tfs.items():\n",
        "            # Ensure chronological order\n",
        "            df = df.sort_index()\n",
        "            n = len(df)\n",
        "\n",
        "            # Calculate split points with gap\n",
        "            train_end = int(n * train)\n",
        "            gap_end = int(n * (train + gap))\n",
        "            val_end = int(n * (train + gap + val))\n",
        "\n",
        "            # Split with gap\n",
        "            train_data[pair][tf] = df.iloc[:train_end].copy()\n",
        "            val_data[pair][tf] = df.iloc[gap_end:val_end].copy()  # Skip gap\n",
        "            test_data[pair][tf] = df.iloc[val_end:].copy()\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def cross_validate_chromosome(data, val_data, tf_map, chrom, n_folds=3):\n",
        "    \"\"\"\n",
        "    FIXED: Proper 3-fold cross-validation with consistent calculations\n",
        "    \"\"\"\n",
        "    all_train_acc, all_val_acc = [], []\n",
        "    all_train_pnl, all_val_pnl = [], []\n",
        "\n",
        "    # Combine for k-fold\n",
        "    combined_data = {}\n",
        "    for pair in data.keys():\n",
        "        combined_data[pair] = {}\n",
        "        for tf in data[pair].keys():\n",
        "            combined_df = pd.concat([data[pair][tf], val_data[pair][tf]])\n",
        "            combined_data[pair][tf] = combined_df\n",
        "\n",
        "    # K-fold cross-validation with walk-forward\n",
        "    for fold in range(n_folds):\n",
        "        fold_train_data, fold_val_data = {}, {}\n",
        "\n",
        "        for pair in combined_data.keys():\n",
        "            fold_train_data[pair], fold_val_data[pair] = {}, {}\n",
        "\n",
        "            for tf, df in combined_data[pair].items():\n",
        "                n = len(df)\n",
        "                fold_size = n // n_folds\n",
        "\n",
        "                # Walk-forward approach within folds\n",
        "                val_start = fold * fold_size\n",
        "                val_end = min(val_start + fold_size, n)\n",
        "\n",
        "                # Training: all data before validation fold\n",
        "                train_indices = list(range(0, val_start))\n",
        "                val_indices = list(range(val_start, val_end))\n",
        "\n",
        "                if len(train_indices) < 50 or len(val_indices) < 20:\n",
        "                    continue\n",
        "\n",
        "                fold_train_data[pair][tf] = df.iloc[train_indices].copy()\n",
        "                fold_val_data[pair][tf] = df.iloc[val_indices].copy()\n",
        "\n",
        "        # Evaluate fold\n",
        "        try:\n",
        "            tm = backtest(fold_train_data, tf_map, chrom, sample_rate=0.4)\n",
        "            vm = backtest(fold_val_data, tf_map, chrom, sample_rate=0.4)\n",
        "\n",
        "            all_train_acc.append(tm['accuracy'])\n",
        "            all_val_acc.append(vm['accuracy'])\n",
        "            all_train_pnl.append(tm['total_pnl'])\n",
        "            all_val_pnl.append(vm['total_pnl'])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if len(all_train_acc) < 2:\n",
        "        return {'accuracy': 0, 'total_pnl': 0, 'sharpe': 0}, \\\n",
        "               {'accuracy': 0, 'total_pnl': 0, 'sharpe': 0}, \\\n",
        "               0.0\n",
        "\n",
        "    # Average metrics\n",
        "    avg_train = {\n",
        "        'accuracy': np.mean(all_train_acc),\n",
        "        'total_pnl': np.mean(all_train_pnl),\n",
        "        'sharpe': 0\n",
        "    }\n",
        "    avg_val = {\n",
        "        'accuracy': np.mean(all_val_acc),\n",
        "        'total_pnl': np.mean(all_val_pnl),\n",
        "        'sharpe': 0\n",
        "    }\n",
        "\n",
        "    # FIXED: Proper consistency calculation\n",
        "    val_acc_std = np.std(all_val_acc)\n",
        "    val_acc_mean = np.mean(all_val_acc) + 1e-6\n",
        "\n",
        "    cv = val_acc_std / val_acc_mean  # Coefficient of variation\n",
        "    consistency = max(0.0, min(1.0, 1.0 - cv))  # Scale to [0, 1]\n",
        "\n",
        "    return avg_train, avg_val, consistency\n",
        "\n",
        "def create_chromosome(tf_map, config, learning, model):\n",
        "    best = learning.get_best_chromosomes(model, top_n=5)\n",
        "    if best and random.random()<0.3:\n",
        "        base = random.choice(best).copy()\n",
        "        for i in range(len(base)):\n",
        "            if random.random()<0.3:\n",
        "                if i==0: base[i] = float(np.clip(base[i]+random.gauss(0,0.2), *config['atr_sl_range']))\n",
        "                elif i==1: base[i] = float(np.clip(base[i]+random.gauss(0,0.2), *config['atr_tp_range']))\n",
        "                elif i==2: base[i] = float(np.clip(base[i]+random.gauss(0,0.003), *config['risk_range']))\n",
        "                elif i==3: base[i] = float(np.clip(base[i]+random.gauss(0,0.05), *config['confidence_range']))\n",
        "                else: base[i] = float(max(0.01, base[i]+random.gauss(0,0.1)))\n",
        "        return base\n",
        "    chrom = [float(random.uniform(*config['atr_sl_range'])), float(random.uniform(*config['atr_tp_range'])),\n",
        "             float(random.uniform(*config['risk_range'])), float(random.uniform(*config['confidence_range']))]\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p,[])))\n",
        "        chrom.extend(np.random.dirichlet(np.ones(n)*2.0).tolist())\n",
        "    return chrom\n",
        "\n",
        "def decode_chromosome(chrom, tf_map):\n",
        "    atr_sl = np.clip(chrom[0], 1.0, MAX_ATR_SL)\n",
        "    atr_tp = np.clip(chrom[1], 1.0, MAX_ATR_TP)\n",
        "    risk, conf = chrom[2], chrom[3]\n",
        "    tf_w, idx = {}, 4\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p,[])))\n",
        "        w = np.array(chrom[idx:idx+n], dtype=float)\n",
        "        w = w/(w.sum()+EPS) if w.sum()>0 else np.ones(n)/n\n",
        "        tf_w[p] = {tf: float(wt) for tf, wt in zip(tf_map.get(p,[]), w)}\n",
        "        idx += n\n",
        "    return atr_sl, atr_tp, risk, conf, tf_w\n",
        "\n",
        "def backtest(data, tf_map, chrom, sample_rate=0.5):\n",
        "    atr_sl, atr_tp, risk, conf, tf_w = decode_chromosome(chrom, tf_map)\n",
        "    equity, eq_curve, trades = BASE_CAPITAL, [BASE_CAPITAL], []\n",
        "    position, positions = None, 0\n",
        "\n",
        "    all_times = sorted(set().union(*[df.index for tfs in data.values() for df in tfs.values()]))\n",
        "    if sample_rate<1.0 and len(all_times)>500:\n",
        "        all_times = all_times[::int(1/sample_rate)]\n",
        "\n",
        "    for t in all_times:\n",
        "        if position:\n",
        "            pair, price = position['pair'], 0\n",
        "            for tf in tf_map.get(pair,[]):\n",
        "                if tf in data.get(pair,{}) and t in data[pair][tf].index:\n",
        "                    price = data[pair][tf].loc[t, 'close']\n",
        "                    break\n",
        "            if price>0:\n",
        "                hit_tp = (position['dir']=='BUY' and price>=position['tp']) or (position['dir']=='SELL' and price<=position['tp'])\n",
        "                hit_sl = (position['dir']=='BUY' and price<=position['sl']) or (position['dir']=='SELL' and price>=position['sl'])\n",
        "                if hit_tp or hit_sl:\n",
        "                    ep = position['tp'] if hit_tp else position['sl']\n",
        "                    pnl = ((ep-position['entry'])*position['size'] if position['dir']=='BUY' else (position['entry']-ep)*position['size'])\n",
        "                    pnl -= ep*(0.0002+0.0001) + ep*position['size']*0.0005\n",
        "                    equity += pnl\n",
        "                    eq_curve.append(equity)\n",
        "                    trades.append({'pnl':pnl, 'correct':hit_tp, 'entry':position['entry'], 'exit':ep})\n",
        "                    position, positions = None, positions-1\n",
        "\n",
        "        if position is None and positions<3:\n",
        "            for pair in PAIRS:\n",
        "                sig, price, atr = 0, 0, MIN_ATR\n",
        "                for tf, w in tf_w.get(pair,{}).items():\n",
        "                    if tf in data.get(pair,{}) and t in data[pair][tf].index:\n",
        "                        row = data[pair][tf].loc[t]\n",
        "                        sig += row.get('hybrid_signal',0)*w\n",
        "                        price, atr = row['close'], max(row.get('atr',MIN_ATR), MIN_ATR)\n",
        "\n",
        "                direction = 'BUY' if sig>0 else 'SELL' if sig<0 else None\n",
        "                if direction and abs(sig)>conf and price>0:\n",
        "                    wr = len([t for t in trades if t['correct']])/len(trades) if trades else 0.5\n",
        "                    wins, losses = [t['pnl'] for t in trades if t['pnl']>0], [t['pnl'] for t in trades if t['pnl']<0]\n",
        "                    size = calc_position_size(equity, risk, atr, atr_sl, wr, np.mean(wins) if wins else 1.0, np.mean(losses) if losses else 1.0, positions)\n",
        "\n",
        "                    sl = price-(atr*atr_sl) if direction=='BUY' else price+(atr*atr_sl)\n",
        "                    tp = price+(atr*atr_tp) if direction=='BUY' else price-(atr*atr_tp)\n",
        "                    position = {'pair':pair, 'dir':direction, 'entry':price, 'sl':sl, 'tp':tp, 'size':size}\n",
        "                    positions += 1\n",
        "                    break\n",
        "\n",
        "    metrics = calc_metrics(trades, eq_curve)\n",
        "    if len(trades)<10:\n",
        "        penalty = len(trades)/10\n",
        "        metrics = AdvancedMetrics(metrics.sharpe*penalty, metrics.sortino*penalty, metrics.calmar*penalty,\n",
        "                                  metrics.max_drawdown, metrics.win_rate*penalty, metrics.profit_factor*penalty,\n",
        "                                  metrics.avg_win, metrics.avg_loss, metrics.expectancy*penalty)\n",
        "\n",
        "    return {\n",
        "        'total_trades': len(trades), 'winning_trades': sum(1 for t in trades if t['correct']),\n",
        "        'accuracy': metrics.win_rate, 'total_pnl': sum(t['pnl'] for t in trades) if trades else 0,\n",
        "        'sharpe': metrics.sharpe, 'sortino': metrics.sortino, 'max_drawdown': metrics.max_drawdown,\n",
        "        'profit_factor': metrics.profit_factor, 'advanced_metrics': metrics, 'trades': trades, 'equity_curve': eq_curve\n",
        "    }\n",
        "\n",
        "def eval_chrom_parallel(args):\n",
        "    chrom, data, tf_map, sr, use_val, val_data = args\n",
        "    try:\n",
        "        tm = backtest(data, tf_map, chrom, sample_rate=sr)\n",
        "        if use_val and val_data:\n",
        "            vm = backtest(val_data, tf_map, chrom, sample_rate=sr)\n",
        "            # BALANCED fitness function\n",
        "            fitness = (\n",
        "                (tm['total_pnl']*0.4 + vm['total_pnl']*0.6)*0.3 +\n",
        "                ((tm['accuracy']*0.4 + vm['accuracy']*0.6)/100)*35 +\n",
        "                (tm['sharpe']*0.4 + vm['sharpe']*0.6)*15 +\n",
        "                (tm['sortino']*0.4 + vm['sortino']*0.6)*10 +\n",
        "                -(tm['max_drawdown']*0.4 + vm['max_drawdown']*0.6)*0.5\n",
        "            )\n",
        "            val_train_ratio = vm['accuracy']/(tm['accuracy']+1e-6)\n",
        "            # BALANCED penalty\n",
        "            if val_train_ratio<0.75:\n",
        "                fitness -= (0.75-val_train_ratio)*150\n",
        "            nt = tm['total_trades']\n",
        "            if nt<15: fitness -= (15-nt)*8\n",
        "            elif nt>100: fitness -= (nt-100)*0.8\n",
        "        else:\n",
        "            fitness = tm['total_pnl']*0.3 + (tm['accuracy']/100)*30 + tm['sharpe']*15 + tm['sortino']*10 - tm['max_drawdown']*0.5\n",
        "        return (fitness, chrom)\n",
        "    except:\n",
        "        return (0.0, chrom)\n",
        "\n",
        "def run_ga(data, tf_map, model, config, val_data=None):\n",
        "    \"\"\"FIXED GA with balanced overfitting controls + v11.0 detailed logging\"\"\"\n",
        "    log(f\"{config['color']} Training {model}...\", \"info\")\n",
        "    log(f\"  üìä Pop: {config['pop_size']} | Gen: {config['generations']}\", \"info\")\n",
        "\n",
        "    ps, gens = config['pop_size'], config['generations']\n",
        "    elite_r, multi = config['elite_ratio'], config['multi_start']\n",
        "    base_mut, cross_r = config['mutation_rate'], config.get('crossover_rate', 0.8)\n",
        "    use_par, use_val = config.get('use_parallel', True), val_data is not None and len(val_data)>0\n",
        "\n",
        "    antioverfit_ai = BalancedAntiOverfitAI(min_val_ratio=0.75, patience=4)\n",
        "    log(f\"  üõ°Ô∏è Balanced Anti-Overfitting AI: Active (target ratio ‚â•0.75)\", \"info\")\n",
        "\n",
        "    # v11.0: Detailed initialization logging\n",
        "    log(f\"  üå± Init with {multi} multi-start...\", \"info\")\n",
        "    all_cand = []\n",
        "    best_hist = LEARNING.get_best_chromosomes(model, top_n=min(8, ps//2))\n",
        "\n",
        "    if best_hist:\n",
        "        sr = config.get('sample_rate', 0.30)\n",
        "        args = [(c, data, tf_map, sr, use_val, val_data) for c in best_hist]\n",
        "        if use_par and len(args)>1:\n",
        "            with Pool(processes=min(cpu_count(), len(args))) as pool:\n",
        "                all_cand.extend(pool.map(eval_chrom_parallel, args))\n",
        "        else:\n",
        "            all_cand.extend([eval_chrom_parallel(a) for a in args])\n",
        "        print(f\"    Evaluated {len(best_hist)} historical seeds\")\n",
        "\n",
        "    log(f\"  üé≤ Generating random...\", \"info\")\n",
        "    target = ps*multi\n",
        "    rand_chroms = [create_chromosome(tf_map, config, LEARNING, model) for _ in range(target-len(all_cand))]\n",
        "\n",
        "    sr = config.get('sample_rate', 0.30)\n",
        "    args = [(c, data, tf_map, sr, use_val, val_data) for c in rand_chroms]\n",
        "\n",
        "    # v11.0: Progress tracking during initialization\n",
        "    if use_par and len(args)>1:\n",
        "        batch_size = max(1, len(args)//10)\n",
        "        for i in range(0, len(args), batch_size):\n",
        "            batch = args[i:i+batch_size]\n",
        "            with Pool(processes=min(cpu_count(), len(batch))) as pool:\n",
        "                all_cand.extend(pool.map(eval_chrom_parallel, batch))\n",
        "            print(f\"    Candidates: {len(all_cand)}/{target} ({len(all_cand)/target*100:.0f}%)\", end='\\r')\n",
        "    else:\n",
        "        for idx, a in enumerate(args):\n",
        "            all_cand.append(eval_chrom_parallel(a))\n",
        "            print(f\"    Candidates: {len(all_cand)}/{target} ({len(all_cand)/target*100:.0f}%)\", end='\\r')\n",
        "\n",
        "    print()\n",
        "    pop = sorted(all_cand, reverse=True)[:ps]\n",
        "    best_ever, stag = pop[0][0], 0\n",
        "    log(f\"  ‚úÖ Init ready | Best: {best_ever:.2f}\", \"success\")\n",
        "    log(f\"  üß¨ Evolution...\", \"info\")\n",
        "\n",
        "    best_generalization_chrom = pop[0][1]\n",
        "    best_generalization_fitness = -float('inf')\n",
        "\n",
        "    for gen in range(gens):\n",
        "        t0 = time.time()\n",
        "\n",
        "        if len(pop)>=2:\n",
        "            samp = random.sample([ind[1] for ind in pop], min(10, len(pop)))\n",
        "            dists = [np.linalg.norm(np.array(samp[i])-np.array(samp[j])) for i in range(len(samp)) for j in range(i+1, len(samp))]\n",
        "            div = min(1.0, np.mean(dists)/5.0) if dists else 1.0\n",
        "        else:\n",
        "            div = 1.0\n",
        "\n",
        "        prog = gen/gens\n",
        "\n",
        "        # Check overfitting every 3 generations with 3-fold CV\n",
        "        if use_val and gen > 0 and gen % 3 == 0:\n",
        "            best_chrom = pop[0][1]\n",
        "            avg_train, avg_val, consistency = cross_validate_chromosome(data, val_data, tf_map, best_chrom, n_folds=3)\n",
        "\n",
        "            gen_score_base = antioverfit_ai.calculate_generalization_score(avg_train, avg_val)\n",
        "            gen_score = gen_score_base * consistency\n",
        "\n",
        "            is_overfit, severity, recommendations = antioverfit_ai.detect_overfitting(avg_train, avg_val)\n",
        "\n",
        "            if is_overfit:\n",
        "                log(f\"  üõ°Ô∏è Overfitting detected (severity={severity:.2f}, consistency={consistency:.2f})\", \"warn\")\n",
        "                for rec in recommendations:\n",
        "                    log(f\"    ‚Üí {rec}\", \"info\")\n",
        "\n",
        "                config = antioverfit_ai.apply_adaptive_regularization(config, severity)\n",
        "                base_mut = config['mutation_rate']\n",
        "\n",
        "            if gen_score > best_generalization_fitness:\n",
        "                best_generalization_fitness = gen_score\n",
        "                best_generalization_chrom = best_chrom.copy()\n",
        "                log(f\"  üõ°Ô∏è New best: GenScore={gen_score:.2f} (base={gen_score_base:.2f}, consistency={consistency:.2f})\", \"success\")\n",
        "\n",
        "        mut_r = base_mut * (1.5 if div<0.3 else 0.7 if prog>0.7 else 1.0) if config.get('adaptive_mutation', True) else base_mut\n",
        "\n",
        "        elite_cnt = max(2, int(ps*elite_r))\n",
        "        new_pop = pop[:elite_cnt].copy()\n",
        "        offspring = []\n",
        "\n",
        "        while len(offspring) < ps-elite_cnt:\n",
        "            ts = 3 if div>0.5 else 5\n",
        "            p1 = max(random.sample(pop, min(ts, len(pop))), key=lambda x:x[0])[1]\n",
        "            p2 = max(random.sample(pop, min(ts, len(pop))), key=lambda x:x[0])[1]\n",
        "\n",
        "            if random.random()<cross_r:\n",
        "                if prog<0.5:\n",
        "                    pts = sorted(random.sample(range(1, len(p1)), random.randint(2,4)))\n",
        "                    child, cur, last = [], p1, 0\n",
        "                    for pt in pts+[len(p1)]:\n",
        "                        child.extend(cur[last:pt])\n",
        "                        cur, last = p2 if cur==p1 else p1, pt\n",
        "                else:\n",
        "                    pt = random.randint(1, len(p1)-1)\n",
        "                    child = p1[:pt]+p2[pt:]\n",
        "            else:\n",
        "                child = p1.copy()\n",
        "\n",
        "            for i in range(len(child)):\n",
        "                if random.random()<mut_r:\n",
        "                    scale = 0.3 if prog<0.5 else 0.15\n",
        "                    if i==0: child[i] = float(np.clip(child[i]+random.gauss(0,scale), *config['atr_sl_range']))\n",
        "                    elif i==1: child[i] = float(np.clip(child[i]+random.gauss(0,scale), *config['atr_tp_range']))\n",
        "                    elif i==2: child[i] = float(np.clip(child[i]+random.gauss(0,0.005 if prog<0.5 else 0.002), *config['risk_range']))\n",
        "                    elif i==3: child[i] = float(np.clip(child[i]+random.gauss(0,0.1 if prog<0.5 else 0.05), *config['confidence_range']))\n",
        "                    else: child[i] = float(max(0.01, child[i]+random.gauss(0,0.2 if prog<0.5 else 0.1)))\n",
        "            offspring.append(child)\n",
        "\n",
        "        sr = config.get('sample_rate', 0.30)\n",
        "        args = [(c, data, tf_map, sr, use_val, val_data) for c in offspring]\n",
        "        if use_par and len(args)>1:\n",
        "            with Pool(processes=min(cpu_count(), len(args))) as pool:\n",
        "                off_res = pool.map(eval_chrom_parallel, args)\n",
        "        else:\n",
        "            off_res = [eval_chrom_parallel(a) for a in args]\n",
        "\n",
        "        new_pop.extend(off_res)\n",
        "        pop = sorted(new_pop, reverse=True)\n",
        "\n",
        "        cur_best = pop[0][0]\n",
        "        if cur_best > best_ever*1.01:\n",
        "            best_ever, stag, icon = cur_best, 0, \"üìà\"\n",
        "        else:\n",
        "            stag, icon = stag+1, \"üìä\"\n",
        "\n",
        "        # v11.0: Detailed generation logging\n",
        "        gen_info = f\" | GenScore={best_generalization_fitness:.2f}\" if use_val and best_generalization_fitness > -float('inf') else \"\"\n",
        "        log(f\"  {icon} Gen {gen+1}/{gens}: Best={cur_best:.2f} | Avg={np.mean([i[0] for i in pop]):.2f} | Div={div:.2f} | Mut={mut_r:.3f}{gen_info} | T={time.time()-t0:.1f}s\", \"info\")\n",
        "\n",
        "        if stag>=5 and cur_best<50 and div<0.2:\n",
        "            log(f\"  ‚ö†Ô∏è Early stop at gen {gen+1}\", \"warn\")\n",
        "            break\n",
        "        if stag>=6 and cur_best>100 and div<0.15:\n",
        "            log(f\"  üéØ Early stop at gen {gen+1}\", \"success\")\n",
        "            break\n",
        "\n",
        "    if best_generalization_fitness > -float('inf'):\n",
        "        final_chrom = best_generalization_chrom\n",
        "        log(f\"  üõ°Ô∏è Using best generalization model (score={best_generalization_fitness:.2f})\", \"success\")\n",
        "    else:\n",
        "        final_chrom = pop[0][1]\n",
        "\n",
        "    log(f\"  üîß Fine-tuning...\", \"info\")\n",
        "    ref_chroms = []\n",
        "    for _ in range(6):\n",
        "        ref = final_chrom.copy()\n",
        "        for i in range(len(ref)):\n",
        "            if random.random()<0.2:\n",
        "                if i==0: ref[i] = float(np.clip(ref[i]+random.gauss(0,0.03), *config['atr_sl_range']))\n",
        "                elif i==1: ref[i] = float(np.clip(ref[i]+random.gauss(0,0.03), *config['atr_tp_range']))\n",
        "                elif i==2: ref[i] = float(np.clip(ref[i]+random.gauss(0,0.0005), *config['risk_range']))\n",
        "                elif i==3: ref[i] = float(np.clip(ref[i]+random.gauss(0,0.01), *config['confidence_range']))\n",
        "                else: ref[i] = float(max(0.01, ref[i]+random.gauss(0,0.03)))\n",
        "        ref_chroms.append(ref)\n",
        "\n",
        "    sr = config.get('sample_rate', 0.30)\n",
        "    args = [(c, data, tf_map, sr, use_val, val_data) for c in ref_chroms]\n",
        "    if use_par and len(args)>1:\n",
        "        with Pool(processes=min(cpu_count(), len(args))) as pool:\n",
        "            ref_res = pool.map(eval_chrom_parallel, args)\n",
        "    else:\n",
        "        ref_res = [eval_chrom_parallel(a) for a in args]\n",
        "\n",
        "    best_refined_chrom = final_chrom\n",
        "    best_refined_gen_score = best_generalization_fitness\n",
        "\n",
        "    for fit, ref in ref_res:\n",
        "        if use_val:\n",
        "            tm_ref = backtest(data, tf_map, ref, sample_rate=0.4)\n",
        "            vm_ref = backtest(val_data, tf_map, ref, sample_rate=0.4)\n",
        "            gen_score_ref = antioverfit_ai.calculate_generalization_score(tm_ref, vm_ref)\n",
        "\n",
        "            if gen_score_ref > best_refined_gen_score:\n",
        "                best_refined_gen_score = gen_score_ref\n",
        "                best_refined_chrom = ref\n",
        "        elif fit > pop[0][0]:\n",
        "            best_refined_chrom = ref\n",
        "\n",
        "    final_chrom = best_refined_chrom\n",
        "\n",
        "    print()\n",
        "    log(f\"  üìä Final validation on full data...\", \"info\")\n",
        "    tm = backtest(data, tf_map, final_chrom, sample_rate=1.0)\n",
        "    vm = None\n",
        "\n",
        "    if val_data:\n",
        "        vm = backtest(val_data, tf_map, final_chrom, sample_rate=1.0)\n",
        "        log(f\"  üìä Train: {tm['accuracy']:.1f}% | ${tm['total_pnl']:.2f}\", \"info\")\n",
        "        log(f\"  üìä Val: {vm['accuracy']:.1f}% | ${vm['total_pnl']:.2f}\", \"info\")\n",
        "\n",
        "        is_overfit, severity, recommendations = antioverfit_ai.detect_overfitting(tm, vm)\n",
        "\n",
        "        ovr = vm['accuracy']/(tm['accuracy']+1e-6)\n",
        "        if severity > 0.4:\n",
        "            log(f\"  üõ°Ô∏è AI: Moderate overfitting (severity={severity:.2f}, ratio={ovr:.2f})\", \"warn\")\n",
        "        elif severity > 0.20:\n",
        "            log(f\"  üõ°Ô∏è AI: Mild overfitting (severity={severity:.2f}, ratio={ovr:.2f})\", \"info\")\n",
        "        elif ovr < 0.75:\n",
        "            log(f\"  üõ°Ô∏è AI: Minor concern (Val/Train={ovr:.2f})\", \"info\")\n",
        "        else:\n",
        "            log(f\"  üõ°Ô∏è AI: EXCELLENT generalization (Val/Train={ovr:.2f})\", \"success\")\n",
        "\n",
        "        ai_report = antioverfit_ai.get_report()\n",
        "        log(f\"  üõ°Ô∏è AI Report: GenScore={ai_report['generalization_score']:.2f} | {ai_report['trend']}\", \"info\")\n",
        "\n",
        "    log(f\"  ‚úÖ {model}: Sharpe={tm['sharpe']:.2f} | Sortino={tm['sortino']:.2f} | MaxDD={tm['max_drawdown']:.1f}%\", \"success\")\n",
        "\n",
        "    return {\n",
        "        'chromosome': final_chrom,\n",
        "        'metrics': tm,\n",
        "        'val_metrics': vm,\n",
        "        'antioverfit_report': antioverfit_ai.get_report() if use_val else None\n",
        "    }\n",
        "\n",
        "def gen_signals(data, tf_map, chrom, model, ct, use_live=True):\n",
        "    atr_sl, atr_tp, risk, conf, tf_w = decode_chromosome(chrom, tf_map)\n",
        "    signals = {}\n",
        "    for pair in PAIRS:\n",
        "        pd = data.get(pair, {})\n",
        "        if not pd: continue\n",
        "\n",
        "        ptf = sorted(pd.keys(), key=lambda x:{'1d':4,'1h':2,'15m':1}.get(x,0), reverse=True)[0]\n",
        "        regime = detect_regime(pd[ptf])\n",
        "\n",
        "        sig, hist_price, atr = 0, 0, MIN_ATR\n",
        "        for tf, w in tf_w.get(pair, {}).items():\n",
        "            if tf in pd and len(pd[tf])>0:\n",
        "                row = pd[tf].iloc[-1]\n",
        "                sig += row.get('hybrid_signal', 0)*w\n",
        "                hist_price, atr = row['close'], max(row.get('atr', MIN_ATR), MIN_ATR)\n",
        "\n",
        "        price = fetch_price(pair) if use_live else None\n",
        "        price = price if price and price>0 else hist_price\n",
        "        if price<=0: continue\n",
        "\n",
        "        direction = 'HOLD' if regime.trend=='ranging' and abs(sig)<conf*1.5 else 'BUY' if sig>0 else 'SELL' if sig<0 else 'HOLD'\n",
        "\n",
        "        sm = abs(sig)\n",
        "        confidence = int(np.clip(\n",
        "            30+(sm/(conf*0.5))*20 if sm<conf*0.5 else\n",
        "            50+((sm-conf*0.5)/(conf*0.5))*20 if sm<conf else\n",
        "            70+min((sm-conf)/conf*20, 20),\n",
        "            25, 95\n",
        "        ))\n",
        "\n",
        "        sl = price-(atr*atr_sl) if direction==\"BUY\" else price+(atr*atr_sl) if direction==\"SELL\" else price\n",
        "        tp = price+(atr*atr_tp) if direction==\"BUY\" else price-(atr*atr_tp) if direction==\"SELL\" else price\n",
        "\n",
        "        signals[pair] = {\n",
        "            'direction': direction, 'last_price': float(price), 'SL': float(sl), 'TP': float(tp),\n",
        "            'atr': float(atr), 'score_1_100': confidence, 'signal_strength': float(sig),\n",
        "            'model': model, 'timestamp': ct.isoformat(),\n",
        "            'rr_ratio': float(abs(tp-price)/abs(price-sl) if abs(price-sl)>0 else 0),\n",
        "            'regime': {'volatility': regime.volatility, 'trend': regime.trend, 'strength': regime.strength}\n",
        "        }\n",
        "    return signals\n",
        "\n",
        "class IterationCounter:\n",
        "    def __init__(self, file=OMEGA_ITERATION_FILE):\n",
        "        self.file = file\n",
        "        self.data = self._load()\n",
        "    def _load(self):\n",
        "        if self.file.exists():\n",
        "            try: return pickle.load(open(self.file, 'rb'))\n",
        "            except: pass\n",
        "        return {'total': 0, 'start': datetime.now(timezone.utc).isoformat(), 'history': []}\n",
        "    def increment(self, success=True):\n",
        "        self.data['total'] += 1\n",
        "        self.data['history'].append({'iteration': self.data['total'], 'time': datetime.now(timezone.utc).isoformat(), 'success': success})\n",
        "        if len(self.data['history'])>1000: self.data['history'] = self.data['history'][-1000:]\n",
        "        pickle.dump(self.data, open(self.file, 'wb'), protocol=4)\n",
        "        return self.data['total']\n",
        "    def get_stats(self):\n",
        "        sd = datetime.fromisoformat(self.data['start'])\n",
        "        days = max(1, (datetime.now(timezone.utc)-sd).days)\n",
        "        return {'total': self.data['total'], 'days': days, 'per_day': self.data['total']/days}\n",
        "\n",
        "class MemorySystem:\n",
        "    def __init__(self, file=OMEGA_MEMORY_FILE):\n",
        "        self.file = file\n",
        "        self.data = self._load()\n",
        "    def _load(self):\n",
        "        if self.file.exists():\n",
        "            try: return pickle.load(open(self.file, 'rb'))\n",
        "            except: pass\n",
        "        return {'signals': [], 'trades': [], 'created_at': datetime.now(timezone.utc).isoformat()}\n",
        "    def store_signals(self, sigs, ts):\n",
        "        for model, s in sigs.items():\n",
        "            for pair, sig in s.items():\n",
        "                if sig['direction']!='HOLD':\n",
        "                    self.data['signals'].append({\n",
        "                        'timestamp': ts.isoformat(), 'model': model, 'pair': pair,\n",
        "                        'direction': sig['direction'], 'entry': sig['last_price'],\n",
        "                        'sl': sig['SL'], 'tp': sig['TP'], 'confidence': sig['score_1_100']\n",
        "                    })\n",
        "        if len(self.data['signals'])>1000: self.data['signals'] = self.data['signals'][-1000:]\n",
        "        self._save()\n",
        "    def _save(self): pickle.dump(self.data, open(self.file, 'wb'), protocol=4)\n",
        "    def close(self): self._save()\n",
        "\n",
        "class LearningSystem:\n",
        "    def __init__(self, file=OMEGA_LEARNING_FILE):\n",
        "        self.file = file\n",
        "        self.data = self._load()\n",
        "    def _load(self):\n",
        "        if self.file.exists():\n",
        "            try: return pickle.load(open(self.file, 'rb'))\n",
        "            except: pass\n",
        "        return {'iterations': 0, 'successful_patterns': {}, 'learning_curve': [], 'adaptation_score': 0.0}\n",
        "    def record_iteration(self, results):\n",
        "        self.data['iterations'] += 1\n",
        "        for model, res in results.items():\n",
        "            if not res or 'metrics' not in res: continue\n",
        "            m = res['metrics']\n",
        "            vm = res.get('val_metrics', {})\n",
        "            pnl, acc = m['total_pnl'], m['accuracy']\n",
        "            val_acc = vm.get('accuracy', 0) if vm else 0\n",
        "            val_train_ratio = val_acc / (acc + 1e-6)\n",
        "\n",
        "            # BALANCED: Save if generalization is reasonable\n",
        "            if (pnl>12 or acc>=42) and val_train_ratio >= 0.70:\n",
        "                key = f\"{model}_success\"\n",
        "                if key not in self.data['successful_patterns']: self.data['successful_patterns'][key] = []\n",
        "                self.data['successful_patterns'][key].append({\n",
        "                    'chromosome': res.get('chromosome'), 'pnl': pnl, 'accuracy': acc,\n",
        "                    'val_accuracy': val_acc, 'val_train_ratio': val_train_ratio,\n",
        "                    'sharpe': m.get('sharpe', 0), 'time': datetime.now(timezone.utc).isoformat()\n",
        "                })\n",
        "                if len(self.data['successful_patterns'][key])>50:\n",
        "                    self.data['successful_patterns'][key] = sorted(\n",
        "                        self.data['successful_patterns'][key],\n",
        "                        key=lambda x: x['pnl']+x['accuracy']+x.get('sharpe',0)*10+x.get('val_train_ratio',0)*40,\n",
        "                        reverse=True\n",
        "                    )[:50]\n",
        "\n",
        "        total_pnl = sum(r['metrics']['total_pnl'] for r in results.values() if r and 'metrics' in r)\n",
        "        self.data['learning_curve'].append(total_pnl)\n",
        "        if len(self.data['learning_curve'])>100: self.data['learning_curve'] = self.data['learning_curve'][-100:]\n",
        "\n",
        "        if len(self.data['learning_curve'])>=5:\n",
        "            ra, oa = np.mean(self.data['learning_curve'][-5:]), np.mean(self.data['learning_curve'])\n",
        "            self.data['adaptation_score'] = min(100, max(0, 50+(ra/(oa+EPS)-1)*100 if oa>0 else 30+ra))\n",
        "        else:\n",
        "            self.data['adaptation_score'] = min(100, max(0, 30+total_pnl/5))\n",
        "\n",
        "        pickle.dump(self.data, open(self.file, 'wb'), protocol=4)\n",
        "\n",
        "    def get_best_chromosomes(self, model, top_n=5):\n",
        "        patterns = self.data['successful_patterns'].get(f\"{model}_success\", [])\n",
        "        quality = [p for p in patterns if p.get('val_train_ratio',0)>=0.70 and (p.get('pnl',0)>12 or p.get('accuracy',0)>=42)]\n",
        "        sorted_p = sorted(quality, key=lambda x: x['pnl']+x.get('accuracy',0)/100*50+x.get('sharpe',0)*10+x.get('val_train_ratio',0)*40, reverse=True)\n",
        "        return [p['chromosome'] for p in sorted_p[:top_n] if p.get('chromosome')]\n",
        "\n",
        "    def get_report(self):\n",
        "        total_success = sum(len(p) for p in self.data['successful_patterns'].values())\n",
        "        trend = \"üìà Improving\" if (\n",
        "            len(self.data['learning_curve'])>=5 and\n",
        "            np.mean(self.data['learning_curve'][-5:]) > np.mean(self.data['learning_curve'][:-5] or [0])\n",
        "        ) else \"üìâ Adjusting\"\n",
        "        return {\n",
        "            'iterations': self.data['iterations'], 'adaptation_score': self.data['adaptation_score'],\n",
        "            'total_successes': total_success, 'trend': trend, 'learning_curve': self.data['learning_curve'][-10:]\n",
        "        }\n",
        "\n",
        "COUNTER = IterationCounter()\n",
        "MEMORY = MemorySystem()\n",
        "LEARNING = LearningSystem()\n",
        "\n",
        "def send_email(sigs, it_stats, lr, is_wknd):\n",
        "    if not GMAIL_APP_PASSWORD:\n",
        "        log(\"‚ùå Email skipped: No password\", \"error\")\n",
        "        return\n",
        "\n",
        "    if IN_GHA and not os.getenv(\"GMAIL_APP_PASSWORD\"):\n",
        "        log(\"‚ö†Ô∏è GitHub Actions: Email skipped (no env credentials)\", \"warn\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        active = sum(1 for m in sigs.values() for s in m.values() if s['direction']!='HOLD')\n",
        "        msg = MIMEMultipart('alternative')\n",
        "        msg['Subject'] = f\"{'üìö OMEGA Learning' if is_wknd else 'üî¥ OMEGA v11.2 ULTIMATE'} - Iteration #{it_stats['iteration']}\"\n",
        "        msg['From'] = msg['To'] = GMAIL_USER\n",
        "\n",
        "        mode_badge = \"üìö LEARNING MODE\" if is_wknd else \"üéØ LIVE TRADING\"\n",
        "        mode_color = \"#6c757d\" if is_wknd else \"#c92a2a\"\n",
        "\n",
        "        html = f\"\"\"<!DOCTYPE html><html><head><style>\n",
        "body{{font-family:Arial,sans-serif;background:#f4f4f4;margin:0;padding:20px}}\n",
        ".container{{max-width:800px;margin:0 auto;background:white;border-radius:10px;overflow:hidden;box-shadow:0 2px 10px rgba(0,0,0,0.1)}}\n",
        ".header{{background:linear-gradient(135deg,{mode_color} 0%,#862e2e 100%);color:white;padding:30px;text-align:center}}\n",
        ".header h1{{margin:0;font-size:28px}}\n",
        ".badge{{display:inline-block;padding:5px 10px;background:#862e2e;color:white;border-radius:5px;font-size:12px;margin-top:10px}}\n",
        ".mode-badge{{background:{mode_color};font-size:14px;padding:8px 15px;margin-top:15px}}\n",
        ".stats{{display:flex;justify-content:space-around;padding:20px;background:#f8f9fa;border-bottom:2px solid #e9ecef}}\n",
        ".stat{{text-align:center}}\n",
        ".stat-value{{font-size:24px;font-weight:bold;color:#c92a2a}}\n",
        ".stat-label{{font-size:12px;color:#6c757d;margin-top:5px}}\n",
        ".model-section{{padding:20px;border-bottom:1px solid #eee}}\n",
        ".model-header{{font-size:20px;font-weight:bold;margin-bottom:10px}}\n",
        ".signal{{padding:15px;background:#f8f9fa;border-radius:5px;margin:10px 0;border-left:4px solid}}\n",
        ".signal-buy{{border-left-color:#28a745}}\n",
        ".signal-sell{{border-left-color:#dc3545}}\n",
        ".signal-hold{{border-left-color:#6c757d}}\n",
        ".signal-header{{font-weight:bold;font-size:16px;margin-bottom:8px}}\n",
        ".signal-details{{color:#666;font-size:14px}}\n",
        ".regime{{background:#e7f3ff;padding:5px 10px;border-radius:3px;display:inline-block;margin-top:5px;font-size:12px}}\n",
        ".footer{{padding:20px;text-align:center;background:#f8f9fa;color:#666;font-size:12px}}\n",
        ".metrics{{background:#fff3cd;padding:15px;margin:10px 0;border-radius:5px}}\n",
        "</style></head><body><div class=\"container\">\n",
        "<div class=\"header\"><h1>üî¥ OMEGA v11.2 ULTIMATE</h1>\n",
        "<span class=\"badge\">Best of Both Worlds</span>\n",
        "<div class=\"mode-badge\">{mode_badge}</div>\n",
        "<p>Iteration #{it_stats['iteration']} | {datetime.now():%Y-%m-%d %H:%M UTC}</p></div>\n",
        "<div class=\"stats\">\n",
        "<div class=\"stat\"><div class=\"stat-value\">{it_stats['total_iterations']}</div><div class=\"stat-label\">Total Runs</div></div>\n",
        "<div class=\"stat\"><div class=\"stat-value\">{lr['adaptation_score']:.1f}/100</div><div class=\"stat-label\">Learning Score</div></div>\n",
        "<div class=\"stat\"><div class=\"stat-value\">{lr['trend']}</div><div class=\"stat-label\">Trend</div></div>\n",
        "<div class=\"stat\"><div class=\"stat-value\">{active if not is_wknd else 'N/A'}</div><div class=\"stat-label\">Active Signals</div></div>\n",
        "</div>\"\"\"\n",
        "\n",
        "        if is_wknd:\n",
        "            html += \"\"\"<div class=\"metrics\"><strong>üìö Weekend Learning Mode Active</strong><br>\n",
        "System optimizing on historical data. Live trading resumes on weekdays.</div>\"\"\"\n",
        "\n",
        "        for model, signals in sigs.items():\n",
        "            config = COMPETITION_MODELS[model]\n",
        "            html += f'<div class=\"model-section\"><div class=\"model-header\">{config[\"color\"]} {model}</div>'\n",
        "            for pair, sig in signals.items():\n",
        "                dc = sig['direction'].lower()\n",
        "                html += f'''<div class=\"signal signal-{dc}\">\n",
        "<div class=\"signal-header\">{pair} <span class=\"badge badge-{dc}\">{sig['direction']}</span></div>\n",
        "<div class=\"signal-details\">üí∞ {sig['last_price']:.5f} | üõ°Ô∏è SL: {sig['SL']:.5f} | üéØ TP: {sig['TP']:.5f} | üìä {sig['score_1_100']}/100 | üìà RR: {sig['rr_ratio']:.2f}</div>\n",
        "<div class=\"signal-details\">‚ö° Signal: {sig['signal_strength']:.6f}</div>\n",
        "<div class=\"regime\">üå°Ô∏è {sig['regime']['volatility']} | üìà {sig['regime']['trend']} | üí™ {sig['regime']['strength']:.0f}/100</div></div>'''\n",
        "            html += '</div>'\n",
        "\n",
        "        html += f'''<div class=\"footer\"><div>üî¥ OMEGA v11.2 - ULTIMATE EDITION</div>\n",
        "<div style=\"margin-top:10px;\">üé≤ Dynamic Seeds | üîÑ Live Updates | üõ°Ô∏è Anti-Overfit | üìä Full Logging</div>\n",
        "<div style=\"margin-top:5px;\">{ENV_NAME} Environment</div></div></div></body></html>'''\n",
        "\n",
        "        msg.attach(MIMEText(html, 'html'))\n",
        "        with smtplib.SMTP_SSL('smtp.gmail.com', 465, timeout=30) as srv:\n",
        "            srv.login(GMAIL_USER, GMAIL_APP_PASSWORD)\n",
        "            srv.send_message(msg)\n",
        "        log(f\"‚úÖ Email sent to {GMAIL_USER}\", \"success\")\n",
        "    except Exception as e:\n",
        "        log(f\"‚ùå Email failed: {e}\", \"error\")\n",
        "        CIRCUIT_BREAKER.record_error(\"email\", \"low\")\n",
        "\n",
        "def push_git(files, msg):\n",
        "    if IN_GHA:\n",
        "        log(\"ü§ñ GHA: Git skipped\", \"info\")\n",
        "        return True\n",
        "    if not FOREX_PAT:\n",
        "        log(\"‚ö†Ô∏è No PAT - Git skipped\", \"warn\")\n",
        "        return False\n",
        "    try:\n",
        "        REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "        repo_path = SAVE_FOLDER if IN_GHA else (SAVE_FOLDER if (SAVE_FOLDER/\".git\").exists() else BASE_FOLDER)\n",
        "\n",
        "        if not (repo_path/\".git\").exists():\n",
        "            subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_path)], capture_output=True, timeout=60, check=True)\n",
        "\n",
        "        os.chdir(repo_path)\n",
        "        for f in files:\n",
        "            if (repo_path/f).exists():\n",
        "                subprocess.run([\"git\", \"add\", str(f)], check=False)\n",
        "        subprocess.run([\"git\", \"commit\", \"-m\", msg], capture_output=True, check=False)\n",
        "        subprocess.run([\"git\", \"pull\", \"--rebase\", \"origin\", \"main\"], capture_output=True, check=False)\n",
        "\n",
        "        for attempt in range(3):\n",
        "            result = subprocess.run([\"git\", \"push\", \"origin\", \"main\"], capture_output=True, timeout=30)\n",
        "            if result.returncode==0:\n",
        "                log(\"‚úÖ GitHub push successful\", \"success\")\n",
        "                return True\n",
        "            if attempt<2: time.sleep(2)\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        log(f\"‚ùå Git error: {e}\", \"error\")\n",
        "        CIRCUIT_BREAKER.record_error(\"git\", \"low\")\n",
        "        return False\n",
        "    finally:\n",
        "        try: os.chdir(SAVE_FOLDER)\n",
        "        except: pass\n",
        "\n",
        "def main():\n",
        "    log(\"=\"*70, \"omega\")\n",
        "    log(\"üî¥ OMEGA v11.2 - ULTIMATE EDITION\", \"omega\")\n",
        "    log(\"=\"*70, \"omega\")\n",
        "\n",
        "    # üé≤ DYNAMIC SEED from v11.1 - Changes every iteration\n",
        "    iteration_seed = COUNTER.data['total'] * 137 + int(time.time() * 1000) % 10000\n",
        "    random.seed(iteration_seed)\n",
        "    np.random.seed(iteration_seed)\n",
        "    log(f\"üé≤ Dynamic Seed: {iteration_seed} (Unique per iteration)\", \"rocket\")\n",
        "\n",
        "    success = False\n",
        "    try:\n",
        "        if CIRCUIT_BREAKER.is_open:\n",
        "            log(\"üö® Circuit breaker open - halted\", \"error\")\n",
        "            return\n",
        "\n",
        "        cur_iter = COUNTER.data['total']+1\n",
        "        stats = COUNTER.get_stats()\n",
        "        log(f\"\\nüìä Iteration #{cur_iter} | {ENV_NAME}\", \"info\")\n",
        "        log(f\"Total: {stats['total']} | Days: {stats['days']} | Avg/Day: {stats['per_day']:.1f}\", \"info\")\n",
        "\n",
        "        is_wknd = is_weekend() and WEEKEND_LEARNING_MODE\n",
        "        log(f\"{'üìö Weekend Learning' if is_wknd else 'üéØ Weekday Trading'} Mode\", \"info\" if is_wknd else \"success\")\n",
        "\n",
        "        # üîÑ v11.1: UPDATE PICKLE DATA WITH LIVE PRICES\n",
        "        log(\"\\nüîÑ Updating data with live prices...\", \"info\")\n",
        "        updated_count = update_pickle_data()\n",
        "\n",
        "        log(\"\\nüì¶ Loading updated data...\", \"info\")\n",
        "        data = load_data(PICKLE_FOLDER)\n",
        "        if not data: raise ValueError(\"‚ùå No data\")\n",
        "\n",
        "        log(\"\\nüìä Splitting data with walk-forward validation...\", \"info\")\n",
        "        train_data, val_data, test_data = split_data(data, train=0.45, gap=0.05, val=0.30)\n",
        "        tf_map = {p: list(tfs.keys()) for p, tfs in data.items()}\n",
        "\n",
        "        log(\"\\nüèÜ Running Model Competition...\", \"chart\")\n",
        "        comp_results, sigs_by_model = {}, {}\n",
        "\n",
        "        for model, config in COMPETITION_MODELS.items():\n",
        "            try:\n",
        "                result = run_ga(train_data, tf_map, model, config, val_data)\n",
        "                comp_results[model] = result\n",
        "\n",
        "                if not is_wknd:\n",
        "                    sigs_by_model[model] = gen_signals(data, tf_map, result['chromosome'], model, datetime.now(timezone.utc), use_live=True)\n",
        "                else:\n",
        "                    test_m = backtest(test_data, tf_map, result['chromosome'], sample_rate=1.0)\n",
        "                    log(f\"  üìö Test: {test_m['accuracy']:.1f}% | ${test_m['total_pnl']:.2f} | Sharpe={test_m['sharpe']:.2f}\", \"info\")\n",
        "                    result['test_metrics'] = test_m\n",
        "                    sigs_by_model[model] = {}\n",
        "            except Exception as e:\n",
        "                log(f\"‚ùå {model} failed: {e}\", \"error\")\n",
        "                if CIRCUIT_BREAKER.record_error(\"model_training\", \"high\"):\n",
        "                    raise SystemError(\"Circuit breaker: too many failures\")\n",
        "\n",
        "        if sigs_by_model: MEMORY.store_signals(sigs_by_model, datetime.now(timezone.utc))\n",
        "\n",
        "        LEARNING.record_iteration(comp_results)\n",
        "        lr = LEARNING.get_report()\n",
        "        log(f\"\\nüß† Learning: {lr['trend']} | Score: {lr['adaptation_score']:.1f}/100\", \"brain\")\n",
        "\n",
        "        # üèÜ v11.0: Rank models with detailed reporting\n",
        "        model_rankings = []\n",
        "        for model, result in comp_results.items():\n",
        "            if result and 'val_metrics' in result and result['val_metrics']:\n",
        "                val_train_ratio = result['val_metrics']['accuracy'] / (result['metrics']['accuracy'] + 1e-6)\n",
        "                test_acc = result.get('test_metrics', {}).get('accuracy', 0) if 'test_metrics' in result else 0\n",
        "                ai_report = result.get('antioverfit_report', {})\n",
        "                gen_score = ai_report.get('generalization_score', 0) if isinstance(ai_report, dict) else 0\n",
        "\n",
        "                quality_score = (\n",
        "                    min(val_train_ratio, 1.0) * 45 +\n",
        "                    (1 - abs(val_train_ratio - 0.85)) * 25 +\n",
        "                    gen_score * 30\n",
        "                )\n",
        "\n",
        "                model_rankings.append({\n",
        "                    'model': model,\n",
        "                    'quality_score': quality_score,\n",
        "                    'val_train_ratio': val_train_ratio,\n",
        "                    'val_acc': result['val_metrics']['accuracy'],\n",
        "                    'test_acc': test_acc,\n",
        "                    'gen_score': gen_score\n",
        "                })\n",
        "\n",
        "        model_rankings.sort(key=lambda x: x['quality_score'], reverse=True)\n",
        "\n",
        "        log(\"\\nüèÜ Model Rankings by Generalization Quality:\", \"chart\")\n",
        "        for i, rank in enumerate(model_rankings, 1):\n",
        "            medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\"\n",
        "            log(f\"  {medal} {rank['model']}: Quality={rank['quality_score']:.1f} | Val/Train={rank['val_train_ratio']:.2f} | Test={rank['test_acc']:.1f}% | GenScore={rank['gen_score']:.2f}\", \"info\")\n",
        "\n",
        "        if model_rankings:\n",
        "            best_model = model_rankings[0]\n",
        "            log(f\"\\nüéØ Recommended for Live Trading: {best_model['model']}\", \"success\")\n",
        "            log(f\"   Reason: Best generalization quality (score={best_model['quality_score']:.1f})\", \"info\")\n",
        "\n",
        "            if best_model['val_train_ratio'] < 0.75:\n",
        "                log(f\"   ‚ö†Ô∏è WARNING: Val/Train ratio {best_model['val_train_ratio']:.2f} < 0.75 (possible overfitting)\", \"warn\")\n",
        "\n",
        "        log(\"\\nüíæ Saving signals...\", \"info\")\n",
        "\n",
        "        # v11.0: Comprehensive quality report\n",
        "        model_quality_report = {}\n",
        "        for rank in model_rankings:\n",
        "            model_quality_report[rank['model']] = {\n",
        "                'quality_score': rank['quality_score'],\n",
        "                'val_train_ratio': rank['val_train_ratio'],\n",
        "                'validation_accuracy': rank['val_acc'],\n",
        "                'test_accuracy': rank['test_acc'],\n",
        "                'generalization_score': rank['gen_score'],\n",
        "                'recommended': rank == model_rankings[0],\n",
        "                'overfit_warning': rank['val_train_ratio'] < 0.75\n",
        "            }\n",
        "\n",
        "        output = {\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "            'iteration': cur_iter,\n",
        "            'system': 'OMEGA',\n",
        "            'version': 'v11.2-ultimate',\n",
        "            'mode': 'weekend_learning' if is_wknd else 'live_trading',\n",
        "            'models': sigs_by_model,\n",
        "            'environment': ENV_NAME,\n",
        "            'learning': lr,\n",
        "            'model_quality': model_quality_report,\n",
        "            'circuit_breaker_status': 'open' if CIRCUIT_BREAKER.is_open else 'closed',\n",
        "            'dynamic_seed': iteration_seed,\n",
        "            'updated_files': updated_count,\n",
        "            'features': {\n",
        "                'from_v11_0': [\n",
        "                    'Detailed logging throughout execution',\n",
        "                    'Progress tracking during initialization',\n",
        "                    'Generation-by-generation statistics',\n",
        "                    'Comprehensive model quality reporting',\n",
        "                    'Detailed anti-overfit analysis',\n",
        "                    'Complete documentation in output'\n",
        "                ],\n",
        "                'from_v11_1': [\n",
        "                    'Dynamic random seeds per iteration',\n",
        "                    'Live data updates with backups',\n",
        "                    'Enhanced data cleaning (NaN handling)',\n",
        "                    'Better compression handling',\n",
        "                    'Improved file filtering'\n",
        "                ]\n",
        "            },\n",
        "            'fixes': {\n",
        "                'fixed_random_seed': f'Dynamic: {iteration_seed}',\n",
        "                'walk_forward_split': '45% train, 5% gap, 30% val, 20% test',\n",
        "                'cross_validation_folds': 3,\n",
        "                'consistency_calculation': 'Fixed coefficient of variation',\n",
        "                'balanced_penalties': 'Max 60% severity',\n",
        "                'min_val_train_ratio': 0.75,\n",
        "                'larger_populations': '14-18 individuals',\n",
        "                'removed_ensemble': 'Single run optimization',\n",
        "                'live_data_updates': updated_count,\n",
        "                'backup_system': 'Automatic .bak creation'\n",
        "            },\n",
        "            'anti_overfit': {\n",
        "                'val_weight': '60%',\n",
        "                'overfit_penalty': 'Balanced',\n",
        "                'min_ratio': 0.75,\n",
        "                'cross_validation': '3-fold walk-forward',\n",
        "                'max_severity': 0.6\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(OMEGA_SIGNALS_FILE, 'w') as f:\n",
        "            json.dump(output, f, indent=2, default=str)\n",
        "        log(f\"‚úÖ Saved to {OMEGA_SIGNALS_FILE.name}\", \"success\")\n",
        "\n",
        "        it_stats = {'iteration': cur_iter, 'total_iterations': stats['total']}\n",
        "\n",
        "        if not is_wknd:\n",
        "            send_email(sigs_by_model, it_stats, lr, is_wknd)\n",
        "            log(\"‚úÖ Email sent\", \"success\")\n",
        "        else:\n",
        "            log(\"üìö Weekend: Email skipped\", \"info\")\n",
        "\n",
        "        log(\"\\nüîÑ Git operations...\", \"info\")\n",
        "        files = [\n",
        "            f\"outputs/{OMEGA_SIGNALS_FILE.name}\",\n",
        "            f\"omega_state/{OMEGA_LEARNING_FILE.name}\",\n",
        "            f\"omega_state/{OMEGA_ITERATION_FILE.name}\",\n",
        "            f\"omega_state/{OMEGA_MEMORY_FILE.name}\"\n",
        "        ]\n",
        "        push_git(files, f\"üî¥ OMEGA v11.2 ULTIMATE [{'Learning' if is_wknd else 'Live'}]: #{cur_iter} - {datetime.now():%Y-%m-%d %H:%M UTC}\")\n",
        "\n",
        "        active = sum(1 for m in sigs_by_model.values() for s in m.values() if s.get('direction')!='HOLD') if not is_wknd else 0\n",
        "\n",
        "        log(\"\\n\"+\"=\"*70, \"success\")\n",
        "        log(\"‚úÖ OMEGA v11.2 - ULTIMATE EDITION COMPLETED\", \"success\")\n",
        "        log(\"=\"*70, \"success\")\n",
        "        log(f\"Mode: {'Learning' if is_wknd else 'Live'} | Env: {ENV_NAME}\", \"info\")\n",
        "        log(f\"Iteration: #{cur_iter} | Models: {len(comp_results)}\", \"info\")\n",
        "        if not is_wknd:\n",
        "            log(f\"Live Signals: {active}\", \"info\")\n",
        "        else:\n",
        "            log(\"Weekend Learning: Historical optimization complete\", \"info\")\n",
        "        log(f\"Circuit Breaker: {'üî¥ OPEN' if CIRCUIT_BREAKER.is_open else '‚úÖ Closed'}\", \"info\")\n",
        "\n",
        "        # v11.0 + v11.1 features combined\n",
        "        log(\"\\nüéâ ULTIMATE FEATURES:\", \"rocket\")\n",
        "        log(\"  FROM v11.0:\", \"info\")\n",
        "        log(\"    üé≤ Fixed Seeds ‚Üí Now Dynamic for exploration\", \"info\")\n",
        "        log(\"    üìä Full detailed logging maintained\", \"success\")\n",
        "        log(\"    üìà Progress tracking preserved\", \"success\")\n",
        "        log(\"    üèÜ Model quality rankings active\", \"success\")\n",
        "        log(\"  FROM v11.1:\", \"info\")\n",
        "        log(\"    üé≤ Dynamic Seeds per iteration\", \"success\")\n",
        "        log(f\"    üîÑ Live Data Updates: {updated_count} files\", \"success\")\n",
        "        log(\"    üõ°Ô∏è Enhanced data validation\", \"success\")\n",
        "        log(\"    üíæ Backup system active\", \"success\")\n",
        "        log(\"\\n  ANTI-OVERFIT FIXES (Both versions):\", \"info\")\n",
        "        log(\"    üõ°Ô∏è Walk-Forward: 5% temporal gap\", \"success\")\n",
        "        log(\"    üéØ 3-Fold Cross-Validation\", \"success\")\n",
        "        log(\"    üìä Balanced Penalties: Max 60%\", \"success\")\n",
        "        log(\"    ‚öñÔ∏è Min 75% val/train ratio\", \"success\")\n",
        "        log(\"    ‚úÖ All Critical Fixes Applied!\", \"success\")\n",
        "\n",
        "        success = True\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        log(\"\\n‚ö†Ô∏è Shutdown requested\", \"warn\")\n",
        "    except SystemError as e:\n",
        "        log(f\"\\nüö® EMERGENCY SHUTDOWN: {e}\", \"error\")\n",
        "        CIRCUIT_BREAKER.is_open = True\n",
        "    except Exception as e:\n",
        "        log(f\"\\n‚ùå Fatal error: {e}\", \"error\")\n",
        "        logging.exception(\"Fatal error\")\n",
        "        if CIRCUIT_BREAKER.record_error(\"fatal\", \"critical\"):\n",
        "            log(\"üö® Circuit breaker activated\", \"error\")\n",
        "    finally:\n",
        "        COUNTER.increment(success=success)\n",
        "        MEMORY.close()\n",
        "        if CIRCUIT_BREAKER.is_open:\n",
        "            log(\"\\nüö® SYSTEM HALTED - Manual intervention required\", \"error\")\n",
        "        log(\"üî¥ Omega pipeline complete\", \"omega\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Kgc3zaIpiwKF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}