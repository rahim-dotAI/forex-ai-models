{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üîë API Keys Configuration\n",
        "# ======================================================\n",
        "import os\n",
        "\n",
        "# Set API keys from environment variables or defaults\n",
        "ALPHA_VANTAGE_KEY = os.environ.get('ALPHA_VANTAGE_KEY', '1W58NPZXOG5SLHZ6')\n",
        "BROWSERLESS_TOKEN = os.environ.get('BROWSERLESS_TOKEN', '2TMVUBAjFwrr7Tb283f0da6602a4cb698b81778bda61967f7')\n",
        "\n",
        "# Set environment variables for downstream code\n",
        "os.environ['ALPHA_VANTAGE_KEY'] = ALPHA_VANTAGE_KEY\n",
        "os.environ['BROWSERLESS_TOKEN'] = BROWSERLESS_TOKEN\n",
        "\n",
        "# Validate\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    print(\"‚ö†Ô∏è Warning: ALPHA_VANTAGE_KEY not set!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Alpha Vantage Key: {ALPHA_VANTAGE_KEY[:4]}...{ALPHA_VANTAGE_KEY[-4:]}\")\n",
        "\n",
        "if not BROWSERLESS_TOKEN:\n",
        "    print(\"‚ö†Ô∏è Warning: BROWSERLESS_TOKEN not set!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Browserless Token: {BROWSERLESS_TOKEN[:4]}...{BROWSERLESS_TOKEN[-4:]}\")"
      ],
      "metadata": {
        "id": "mr_DWDx4-LLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üåç Environment Detection & Setup (MUST RUN FIRST!)\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "# Override ENV_NAME if in GitHub Actions\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "# Set base paths based on environment\n",
        "if IN_COLAB:\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "elif IN_GHA:\n",
        "    # GitHub Actions already checks out the repo\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "else:\n",
        "    # Local development\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "\n",
        "# Create necessary directories\n",
        "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Display environment info\n",
        "print(\"=\" * 60)\n",
        "print(f\"üåç Environment: {ENV_NAME}\")\n",
        "print(f\"üìÇ Base Folder: {BASE_FOLDER}\")\n",
        "print(f\"üíæ Save Folder: {SAVE_FOLDER}\")\n",
        "print(f\"üîß Python: {sys.version.split()[0]}\")\n",
        "print(f\"üìç Working Dir: {os.getcwd()}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Validate critical environment variables for GitHub Actions\n",
        "if IN_GHA:\n",
        "    required_vars = [\"FOREX_PAT\", \"GIT_USER_NAME\", \"GIT_USER_EMAIL\"]\n",
        "    missing = [v for v in required_vars if not os.environ.get(v)]\n",
        "    if missing:\n",
        "        print(f\"‚ö†Ô∏è Warning: Missing environment variables: {', '.join(missing)}\")\n",
        "    else:\n",
        "        print(\"‚úÖ All required environment variables present\")"
      ],
      "metadata": {
        "id": "8ZTLJKoW9rQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üìÑ GitHub Sync (Environment-Aware) - FULLY FIXED VERSION\n",
        "# ======================================================\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import urllib.parse\n",
        "import sys\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Environment Detection (Self-Contained)\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "# Override ENV_NAME if in GitHub Actions\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CRITICAL FIX: Smart Path Configuration\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    # ‚úÖ GitHub Actions: Use current directory (already in repo)\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER\n",
        "    REPO_FOLDER = BASE_FOLDER  # We're already in the repo!\n",
        "    print(\"ü§ñ GitHub Actions Mode: Using current directory\")\n",
        "\n",
        "elif IN_COLAB:\n",
        "    # ‚úÖ Colab: Use separate workspace folder\n",
        "    BASE_FOLDER = Path(\"/content\")\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"forex_workspace\"  # Different name to avoid confusion\n",
        "    REPO_FOLDER = SAVE_FOLDER / \"forex-ai-models\"  # Repo goes inside workspace\n",
        "    print(\"‚òÅÔ∏è Colab Mode: Using workspace structure\")\n",
        "\n",
        "else:\n",
        "    # ‚úÖ Local: Use current directory or custom path\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    SAVE_FOLDER = BASE_FOLDER / \"workspace\"\n",
        "    REPO_FOLDER = SAVE_FOLDER / \"forex-ai-models\"\n",
        "    print(\"üíª Local Mode: Using workspace structure\")\n",
        "\n",
        "# Create necessary directories\n",
        "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"üîß Running in: {ENV_NAME}\")\n",
        "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
        "print(f\"üíæ Save folder: {SAVE_FOLDER}\")\n",
        "print(f\"üì¶ Repo folder: {REPO_FOLDER}\")\n",
        "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GitHub Configuration\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ GitHub Token (Multi-Source)\n",
        "# ======================================================\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "# Try Colab secrets if in Colab and PAT not found\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except ImportError:\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load Colab secret: {e}\")\n",
        "\n",
        "# Validate PAT\n",
        "if not FOREX_PAT:\n",
        "    print(\"‚ö†Ô∏è Warning: FOREX_PAT not found. Git operations may fail.\")\n",
        "    print(\"   Set FOREX_PAT in:\")\n",
        "    print(\"   - GitHub Secrets (for Actions)\")\n",
        "    print(\"   - Colab Secrets (for Colab)\")\n",
        "    print(\"   - Environment variable (for local)\")\n",
        "    REPO_URL = None\n",
        "else:\n",
        "    SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "    REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "    print(\"‚úÖ GitHub token configured\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Handle Repository Based on Environment\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    # ===== GitHub Actions =====\n",
        "    print(\"\\nü§ñ GitHub Actions Mode\")\n",
        "    print(\"‚úÖ Repository already checked out by actions/checkout\")\n",
        "    print(f\"üìÇ Current directory: {Path.cwd()}\")\n",
        "\n",
        "    # Verify .git exists\n",
        "    if not (Path.cwd() / \".git\").exists():\n",
        "        print(\"‚ö†Ô∏è Warning: .git directory not found!\")\n",
        "        print(\"   Make sure actions/checkout@v4 is in your workflow\")\n",
        "    else:\n",
        "        print(\"‚úÖ Git repository confirmed\")\n",
        "\n",
        "    # No need to clone - we're already in the repo!\n",
        "\n",
        "elif IN_COLAB:\n",
        "    # ===== Google Colab =====\n",
        "    print(\"\\n‚òÅÔ∏è Google Colab Mode\")\n",
        "\n",
        "    if not REPO_URL:\n",
        "        print(\"‚ùå Cannot clone repository: FOREX_PAT not available\")\n",
        "    elif not (REPO_FOLDER / \".git\").exists():\n",
        "        # Clone repository\n",
        "        print(f\"üì• Cloning repository to {REPO_FOLDER}...\")\n",
        "        env = os.environ.copy()\n",
        "        env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"  # Skip LFS files\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                check=True,\n",
        "                env=env,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            print(\"‚úÖ Repository cloned successfully\")\n",
        "\n",
        "            # Change to repo directory\n",
        "            os.chdir(REPO_FOLDER)\n",
        "            print(f\"üìÇ Changed directory to: {os.getcwd()}\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Clone failed: {e.stderr}\")\n",
        "            print(\"Continuing with existing directory...\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ùå Clone timed out after 60 seconds\")\n",
        "    else:\n",
        "        # Repository exists, pull latest\n",
        "        print(\"‚úÖ Repository already exists, pulling latest changes...\")\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"pull\", \"origin\", BRANCH],\n",
        "                check=True,\n",
        "                cwd=REPO_FOLDER,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            print(\"‚úÖ Successfully pulled latest changes\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Pull failed: {e.stderr}\")\n",
        "            print(\"Continuing with existing files...\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ö†Ô∏è Pull timed out, continuing anyway...\")\n",
        "\n",
        "    # Configure Git LFS (disable for Colab)\n",
        "    print(\"‚öôÔ∏è Configuring Git LFS...\")\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\"git\", \"lfs\", \"uninstall\"],\n",
        "            check=False,\n",
        "            cwd=REPO_FOLDER,\n",
        "            capture_output=True\n",
        "        )\n",
        "        subprocess.run(\n",
        "            [\"git\", \"lfs\", \"migrate\", \"export\", \"--include=*.csv\"],\n",
        "            check=False,\n",
        "            cwd=REPO_FOLDER,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"‚úÖ LFS configuration updated\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è LFS setup warning: {e}\")\n",
        "\n",
        "else:\n",
        "    # ===== Local Environment =====\n",
        "    print(\"\\nüíª Local Development Mode\")\n",
        "    print(f\"üìÇ Working in: {SAVE_FOLDER}\")\n",
        "\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        if REPO_URL:\n",
        "            print(f\"üì• Cloning repository to {REPO_FOLDER}...\")\n",
        "            try:\n",
        "                subprocess.run(\n",
        "                    [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                    check=True,\n",
        "                    timeout=60\n",
        "                )\n",
        "                print(\"‚úÖ Repository cloned successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Clone failed: {e}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Not a git repository and no PAT available\")\n",
        "            print(\"   Run: git clone https://github.com/rahim-dotAI/forex-ai-models.git\")\n",
        "    else:\n",
        "        print(\"‚úÖ Git repository found\")\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Git Global Configuration\n",
        "# ======================================================\n",
        "print(\"\\nüîß Configuring Git...\")\n",
        "\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "# Set git config\n",
        "git_configs = [\n",
        "    ([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], \"User name\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], \"User email\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], \"Detached HEAD warning\"),\n",
        "    ([\"git\", \"config\", \"--global\", \"init.defaultBranch\", \"main\"], \"Default branch\")\n",
        "]\n",
        "\n",
        "for cmd, description in git_configs:\n",
        "    try:\n",
        "        subprocess.run(cmd, check=False, capture_output=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not set {description}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Environment Summary & Validation\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üßæ ENVIRONMENT SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment:      {ENV_NAME}\")\n",
        "print(f\"Working Dir:      {os.getcwd()}\")\n",
        "print(f\"Save Folder:      {SAVE_FOLDER}\")\n",
        "print(f\"Repo Folder:      {REPO_FOLDER}\")\n",
        "print(f\"Repository:       https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}\")\n",
        "print(f\"Branch:           {BRANCH}\")\n",
        "print(f\"Git Repo Exists:  {(REPO_FOLDER / '.git').exists()}\")\n",
        "print(f\"FOREX_PAT Set:    {'‚úÖ Yes' if FOREX_PAT else '‚ùå No'}\")\n",
        "\n",
        "# Check critical paths\n",
        "print(\"\\nüìã Critical Paths:\")\n",
        "critical_paths = {\n",
        "    \"Repo .git\": REPO_FOLDER / \".git\",\n",
        "    \"Save Folder\": SAVE_FOLDER,\n",
        "    \"Repo Folder\": REPO_FOLDER\n",
        "}\n",
        "\n",
        "for name, path in critical_paths.items():\n",
        "    exists = path.exists()\n",
        "    icon = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"  {icon} {name}: {path} {'(exists)' if exists else '(missing)'}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ Setup completed successfully!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Export Variables for Downstream Cells\n",
        "# ======================================================\n",
        "# These variables are now available in subsequent cells:\n",
        "# - ENV_NAME: Environment name\n",
        "# - IN_COLAB: Boolean for Colab detection\n",
        "# - IN_GHA: Boolean for GitHub Actions detection\n",
        "# - SAVE_FOLDER: Path to save files\n",
        "# - REPO_FOLDER: Path to git repository\n",
        "# - GITHUB_USERNAME, GITHUB_REPO, BRANCH: Git config\n",
        "# - FOREX_PAT: GitHub token (if available)\n",
        "\n",
        "print(\"\\n‚úÖ All environment variables exported for downstream cells\")"
      ],
      "metadata": {
        "id": "yjIMteyqLs_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oih6CDfjAjG9"
      },
      "outputs": [],
      "source": [
        "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn river\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üöÄ COMPLETE ALPHA VANTAGE FX WORKFLOW - FULLY FIXED\n",
        "# ======================================================\n",
        "# ‚úÖ Works in GitHub Actions, Google Colab, and Local\n",
        "# ‚úÖ No nested repositories\n",
        "# ‚úÖ Proper path management\n",
        "# ‚úÖ Thread-safe operations\n",
        "# ‚úÖ API rate limit handling\n",
        "# ‚úÖ Automatic retry logic\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import requests\n",
        "import subprocess\n",
        "import threading\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ ENVIRONMENT DETECTION\n",
        "# ======================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ Alpha Vantage FX Data Fetcher\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üìç Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ SMART PATH CONFIGURATION (NO NESTED REPOS!)\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    # ‚úÖ GitHub Actions: Already in repo root\n",
        "    print(\"ü§ñ GitHub Actions detected - using repository root\")\n",
        "    BASE_FOLDER = Path.cwd()\n",
        "    REPO_FOLDER = BASE_FOLDER  # SAME as current directory!\n",
        "    CSV_FOLDER = BASE_FOLDER / \"csvs\"\n",
        "    PICKLE_FOLDER = BASE_FOLDER / \"pickles\"\n",
        "    LOG_FOLDER = BASE_FOLDER / \"logs\"\n",
        "\n",
        "elif IN_COLAB:\n",
        "    # ‚úÖ Colab: Separate workspace to avoid confusion\n",
        "    print(\"‚òÅÔ∏è Google Colab detected - creating workspace\")\n",
        "    BASE_FOLDER = Path(\"/content/forex_workspace\")\n",
        "    BASE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "    REPO_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    CSV_FOLDER = BASE_FOLDER / \"csvs\"\n",
        "    PICKLE_FOLDER = BASE_FOLDER / \"pickles\"\n",
        "    LOG_FOLDER = BASE_FOLDER / \"logs\"\n",
        "\n",
        "else:\n",
        "    # ‚úÖ Local: Workspace in current directory\n",
        "    print(\"üíª Local environment detected - creating workspace\")\n",
        "    BASE_FOLDER = Path(\"./forex_workspace\").resolve()\n",
        "    BASE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "    REPO_FOLDER = BASE_FOLDER / \"forex-ai-models\"\n",
        "    CSV_FOLDER = BASE_FOLDER / \"csvs\"\n",
        "    PICKLE_FOLDER = BASE_FOLDER / \"pickles\"\n",
        "    LOG_FOLDER = BASE_FOLDER / \"logs\"\n",
        "\n",
        "# Create output directories\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOG_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"üìÇ Base folder: {BASE_FOLDER}\")\n",
        "print(f\"üì¶ Repo folder: {REPO_FOLDER}\")\n",
        "print(f\"üíæ CSV folder: {CSV_FOLDER}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GITHUB CONFIGURATION\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# Get GitHub PAT from environment\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "\n",
        "# Try Colab secrets if available\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not access Colab secrets: {e}\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    print(\"‚ùå ERROR: FOREX_PAT not found!\")\n",
        "    print(\"   Set it in:\")\n",
        "    print(\"   - GitHub Secrets (for Actions)\")\n",
        "    print(\"   - Colab Secrets (for Colab)\")\n",
        "    print(\"   - Environment variables (for Local)\")\n",
        "    raise ValueError(\"FOREX_PAT is required\")\n",
        "\n",
        "# URL-encode the PAT for safe use in URLs\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "print(\"‚úÖ GitHub credentials configured\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ REPOSITORY MANAGEMENT\n",
        "# ======================================================\n",
        "def ensure_repository():\n",
        "    \"\"\"\n",
        "    Ensure repository is available and up-to-date\n",
        "    Behavior depends on environment\n",
        "    \"\"\"\n",
        "    if IN_GHA:\n",
        "        # GitHub Actions: Repo already checked out\n",
        "        print(\"\\nü§ñ GitHub Actions: Repository already available\")\n",
        "        if not (REPO_FOLDER / \".git\").exists():\n",
        "            print(\"‚ö†Ô∏è Warning: .git directory not found\")\n",
        "            print(\"   Make sure actions/checkout@v4 is in your workflow\")\n",
        "        else:\n",
        "            print(\"‚úÖ Git repository verified\")\n",
        "        return\n",
        "\n",
        "    # For Colab and Local: Clone or update\n",
        "    print(\"\\nüì• Managing repository...\")\n",
        "\n",
        "    if REPO_FOLDER.exists():\n",
        "        if (REPO_FOLDER / \".git\").exists():\n",
        "            # Repository exists - update it\n",
        "            print(f\"üîÑ Updating existing repository...\")\n",
        "            try:\n",
        "                # Fetch latest\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"-C\", str(REPO_FOLDER), \"fetch\", \"origin\"],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                # Checkout branch\n",
        "                subprocess.run(\n",
        "                    [\"git\", \"-C\", str(REPO_FOLDER), \"checkout\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True\n",
        "                )\n",
        "\n",
        "                # Pull latest changes\n",
        "                result = subprocess.run(\n",
        "                    [\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(\"‚úÖ Repository updated successfully\")\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Pull had warnings: {result.stderr}\")\n",
        "\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(\"‚ö†Ô∏è Update timed out - continuing with existing repo\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Update failed: {e} - continuing with existing repo\")\n",
        "        else:\n",
        "            # Folder exists but not a git repo - remove it\n",
        "            print(\"üóëÔ∏è Removing incomplete repository folder...\")\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "    # Clone if needed\n",
        "    if not REPO_FOLDER.exists() or not (REPO_FOLDER / \".git\").exists():\n",
        "        print(f\"üì• Cloning repository to {REPO_FOLDER}...\")\n",
        "\n",
        "        # Skip LFS to speed up clone\n",
        "        env = os.environ.copy()\n",
        "        env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                env=env,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Repository cloned successfully\")\n",
        "            else:\n",
        "                raise RuntimeError(f\"Clone failed: {result.stderr}\")\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            raise TimeoutError(\"Repository clone timed out after 60 seconds\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Clone failed: {e}\")\n",
        "\n",
        "ensure_repository()\n",
        "\n",
        "# Configure Git identity\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME],\n",
        "               capture_output=True, check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL],\n",
        "               capture_output=True, check=False)\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ ALPHA VANTAGE CONFIGURATION\n",
        "# ======================================================\n",
        "ALPHA_VANTAGE_KEY = os.environ.get(\"ALPHA_VANTAGE_KEY\", \"1W58NPZXOG5SLHZ6\")\n",
        "\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    raise ValueError(\"‚ùå ALPHA_VANTAGE_KEY is required\")\n",
        "\n",
        "print(f\"‚úÖ Alpha Vantage API key: {ALPHA_VANTAGE_KEY[:4]}...{ALPHA_VANTAGE_KEY[-4:]}\")\n",
        "\n",
        "# FX pairs to fetch\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "# Thread lock for file operations\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ HELPER FUNCTIONS\n",
        "# ======================================================\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"\n",
        "    Remove timezone information from DataFrame index\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    \"\"\"\n",
        "    Calculate MD5 hash of file to detect changes\n",
        "    \"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def fetch_alpha_vantage_fx(pair, outputsize='full', max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Fetch FX data from Alpha Vantage API with retry logic\n",
        "\n",
        "    Args:\n",
        "        pair: FX pair (e.g., \"EUR/USD\")\n",
        "        outputsize: 'compact' (100 rows) or 'full' (all available)\n",
        "        max_retries: Number of retry attempts\n",
        "        retry_delay: Seconds between retries\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with OHLC data or empty DataFrame on failure\n",
        "    \"\"\"\n",
        "    base_url = 'https://www.alphavantage.co/query'\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "\n",
        "    params = {\n",
        "        'function': 'FX_DAILY',\n",
        "        'from_symbol': from_currency,\n",
        "        'to_symbol': to_currency,\n",
        "        'outputsize': outputsize,\n",
        "        'datatype': 'json',\n",
        "        'apikey': ALPHA_VANTAGE_KEY\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  Fetching {pair} (attempt {attempt + 1}/{max_retries})...\")\n",
        "\n",
        "            r = requests.get(base_url, params=params, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "\n",
        "            # Check for API errors\n",
        "            if 'Error Message' in data:\n",
        "                raise ValueError(f\"API Error: {data['Error Message']}\")\n",
        "\n",
        "            if 'Note' in data:\n",
        "                print(f\"  ‚ö†Ô∏è API rate limit reached for {pair}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay * 2)  # Longer wait for rate limit\n",
        "                    continue\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            if 'Time Series FX (Daily)' not in data:\n",
        "                raise ValueError(f\"Unexpected response format: {list(data.keys())}\")\n",
        "\n",
        "            # Parse time series data\n",
        "            ts = data['Time Series FX (Daily)']\n",
        "            df = pd.DataFrame(ts).T\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df.sort_index()\n",
        "\n",
        "            # Rename columns\n",
        "            df = df.rename(columns={\n",
        "                '1. open': 'open',\n",
        "                '2. high': 'high',\n",
        "                '3. low': 'low',\n",
        "                '4. close': 'close'\n",
        "            })\n",
        "\n",
        "            # Convert to float\n",
        "            df = df.astype(float)\n",
        "\n",
        "            # Remove timezone\n",
        "            df = ensure_tz_naive(df)\n",
        "\n",
        "            print(f\"  ‚úÖ Fetched {len(df)} rows for {pair}\")\n",
        "            return df\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"  ‚ö†Ô∏è Network error: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ‚ùå Failed after {max_retries} attempts\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Error: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ‚ùå Failed after {max_retries} attempts\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ PAIR PROCESSING\n",
        "# ======================================================\n",
        "def process_pair(pair):\n",
        "    \"\"\"\n",
        "    Process single FX pair: fetch, merge with existing, save\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (filepath if changed, status message)\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîÑ Processing {pair}...\")\n",
        "\n",
        "    filename = pair.replace(\"/\", \"_\") + \".csv\"\n",
        "\n",
        "    # Determine file locations based on environment\n",
        "    if IN_GHA:\n",
        "        # In GitHub Actions: Save directly to repo root\n",
        "        csv_path = REPO_FOLDER / filename\n",
        "        repo_path = csv_path  # Same file\n",
        "    else:\n",
        "        # In Colab/Local: Save to CSV folder AND repo folder\n",
        "        csv_path = CSV_FOLDER / filename\n",
        "        repo_path = REPO_FOLDER / filename\n",
        "\n",
        "    # Load existing data\n",
        "    existing_df = pd.DataFrame()\n",
        "    if csv_path.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
        "            existing_df = ensure_tz_naive(existing_df)\n",
        "            print(f\"  üìä Loaded {len(existing_df)} existing rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not load existing data: {e}\")\n",
        "\n",
        "    # Get hash before changes\n",
        "    old_hash = file_hash(csv_path)\n",
        "\n",
        "    # Fetch new data\n",
        "    new_df = fetch_alpha_vantage_fx(pair)\n",
        "\n",
        "    if new_df.empty:\n",
        "        return None, f\"‚ùå {pair}: No data fetched\"\n",
        "\n",
        "    # Merge with existing data\n",
        "    if not existing_df.empty:\n",
        "        combined_df = pd.concat([existing_df, new_df])\n",
        "        # Remove duplicates, keeping latest\n",
        "        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
        "    else:\n",
        "        combined_df = new_df\n",
        "\n",
        "    # Sort by date\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    # Save files (thread-safe)\n",
        "    with lock:\n",
        "        # Save to CSV folder\n",
        "        combined_df.to_csv(csv_path)\n",
        "\n",
        "        # Also save to repo folder if different\n",
        "        if not IN_GHA and csv_path != repo_path:\n",
        "            combined_df.to_csv(repo_path)\n",
        "\n",
        "    # Check if file changed\n",
        "    new_hash = file_hash(csv_path)\n",
        "    changed = (old_hash != new_hash)\n",
        "\n",
        "    status = \"‚úÖ Updated\" if changed else \"‚ÑπÔ∏è No changes\"\n",
        "    print(f\"  {status} - Total rows: {len(combined_df)}\")\n",
        "\n",
        "    return (str(repo_path) if changed else None), f\"{status} {pair} ({len(combined_df)} rows)\"\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ PARALLEL EXECUTION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Fetching FX data from Alpha Vantage...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "changed_files = []\n",
        "results = []\n",
        "\n",
        "# Process pairs in parallel (max 4 at a time to respect API limits)\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = {executor.submit(process_pair, pair): pair for pair in FX_PAIRS}\n",
        "\n",
        "    for future in as_completed(futures):\n",
        "        pair = futures[future]\n",
        "        try:\n",
        "            filepath, message = future.result()\n",
        "            results.append(message)\n",
        "            if filepath:\n",
        "                changed_files.append(filepath)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {pair} processing failed: {e}\")\n",
        "            results.append(f\"‚ùå {pair}: Failed\")\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ RESULTS SUMMARY\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PROCESSING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "print(f\"\\nTotal pairs processed: {len(FX_PAIRS)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "\n",
        "# ======================================================\n",
        "# üîü GIT COMMIT & PUSH (Skip in GitHub Actions)\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"   (Workflow will handle commit and push)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        # Change to repo directory\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        # Stage changed files\n",
        "        print(f\"üìù Staging {len(changed_files)} files...\")\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        # Commit\n",
        "        commit_msg = f\"Update Alpha Vantage FX data - {len(changed_files)} files\"\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Changes committed\")\n",
        "        elif \"nothing to commit\" in result.stdout:\n",
        "            print(\"‚ÑπÔ∏è No changes to commit\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Commit warning: {result.stderr}\")\n",
        "\n",
        "        # Push with retry logic\n",
        "        max_push_attempts = 3\n",
        "        for attempt in range(max_push_attempts):\n",
        "            print(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/{max_push_attempts})...\")\n",
        "\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Successfully pushed to GitHub\")\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_push_attempts - 1:\n",
        "                    print(f\"‚ö†Ô∏è Push failed, retrying...\")\n",
        "                    # Pull latest and try again\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "                else:\n",
        "                    print(f\"‚ùå Push failed after {max_push_attempts} attempts\")\n",
        "                    print(f\"   Error: {result.stderr}\")\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"‚ùå Git operation timed out\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Git error: {e}\")\n",
        "    finally:\n",
        "        # Return to base folder\n",
        "        os.chdir(BASE_FOLDER)\n",
        "\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚ÑπÔ∏è No changes to commit\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ COMPLETION\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALPHA VANTAGE WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"Pairs processed: {len(FX_PAIRS)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Status: {'Success' if len(results) == len(FX_PAIRS) else 'Partial'}\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "ZGwB1Pp2LJ6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# FULLY IMPROVED FOREX DATA WORKFLOW - YFINANCE\n",
        "# ‚úÖ Works in: Colab + GitHub Actions + Local\n",
        "# ‚úÖ No permission errors\n",
        "# ‚úÖ 403-Proof, Large History Support\n",
        "# ‚úÖ Environment-aware paths\n",
        "# ======================================================\n",
        "\n",
        "import os, time, hashlib, subprocess, shutil, threading\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ YFinance FX Data Fetcher - Multi-Environment Edition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ FIXED: Environment Detection\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "IN_LOCAL = not IN_COLAB and not IN_GHA\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Detected Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ FIXED: Working Directories (Environment-Aware)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    # Colab: Use /content (has permissions)\n",
        "    BASE_DIR = Path(\"/content/forex-alpha-models\")\n",
        "    BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "elif IN_GHA:\n",
        "    # GitHub Actions: Use current working directory (repo root)\n",
        "    BASE_DIR = Path.cwd()\n",
        "    print(f\"üìÇ GitHub Actions: Using repo root: {BASE_DIR}\")\n",
        "else:\n",
        "    # Local: Use subdirectory\n",
        "    BASE_DIR = Path(\"./forex-alpha-models\").resolve()\n",
        "    BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Change to base directory (safe for all environments)\n",
        "os.chdir(BASE_DIR)\n",
        "\n",
        "# Setup subdirectories\n",
        "PICKLE_FOLDER = BASE_DIR / \"pickles\"\n",
        "CSV_FOLDER = BASE_DIR / \"csvs\"\n",
        "LOG_FOLDER = BASE_DIR / \"logs\"\n",
        "\n",
        "# Create all subdirectories with parents=True\n",
        "for folder in [PICKLE_FOLDER, CSV_FOLDER, LOG_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Working directory: {BASE_DIR.resolve()}\")\n",
        "print(f\"‚úÖ Pickle folder: {PICKLE_FOLDER}\")\n",
        "print(f\"‚úÖ CSV folder: {CSV_FOLDER}\")\n",
        "print(f\"‚úÖ Log folder: {LOG_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Git Configuration\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT environment variable is required!\")\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
        "\n",
        "# Configure git\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
        "\n",
        "# Store credentials\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ FIXED: Repository Management (Environment-Aware)\n",
        "# ======================================================\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "REPO_FOLDER = BASE_DIR / GITHUB_REPO\n",
        "\n",
        "def ensure_repo_cloned(repo_url, repo_folder, branch=\"main\"):\n",
        "    \"\"\"\n",
        "    Clone or update repository with environment-aware handling\n",
        "    \"\"\"\n",
        "    repo_folder = Path(repo_folder)\n",
        "\n",
        "    if IN_GHA:\n",
        "        # GitHub Actions: Repo already checked out\n",
        "        print(\"ü§ñ GitHub Actions: Repository already available\")\n",
        "        if not (repo_folder / \".git\").exists() and (Path.cwd() / \".git\").exists():\n",
        "            # We're in the repo root, use current directory\n",
        "            print(f\"‚úÖ Using current directory as repo: {Path.cwd()}\")\n",
        "            return Path.cwd()\n",
        "        elif (repo_folder / \".git\").exists():\n",
        "            print(f\"‚úÖ Repository found at: {repo_folder}\")\n",
        "            return repo_folder\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Warning: .git directory not found\")\n",
        "            print(\"   Make sure actions/checkout@v4 is in your workflow\")\n",
        "            return repo_folder\n",
        "\n",
        "    # For Colab and Local: Clone or update\n",
        "    tmp_folder = repo_folder.parent / (repo_folder.name + \"_tmp\")\n",
        "\n",
        "    if tmp_folder.exists():\n",
        "        shutil.rmtree(tmp_folder)\n",
        "\n",
        "    if not (repo_folder / \".git\").exists():\n",
        "        print(f\"üî• Cloning repository to {tmp_folder}...\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", branch, repo_url, str(tmp_folder)],\n",
        "                check=True,\n",
        "                timeout=60\n",
        "            )\n",
        "\n",
        "            if repo_folder.exists():\n",
        "                shutil.rmtree(repo_folder)\n",
        "\n",
        "            tmp_folder.rename(repo_folder)\n",
        "            print(f\"‚úÖ Repository cloned successfully\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ùå Clone timed out after 60 seconds\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Clone failed: {e}\")\n",
        "            raise\n",
        "    else:\n",
        "        print(\"üîÑ Repository exists, pulling latest changes...\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(repo_folder), \"fetch\", \"origin\"],\n",
        "                check=True,\n",
        "                timeout=30\n",
        "            )\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(repo_folder), \"checkout\", branch],\n",
        "                check=False\n",
        "            )\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(repo_folder), \"pull\", \"origin\", branch],\n",
        "                check=False,\n",
        "                timeout=30\n",
        "            )\n",
        "            print(\"‚úÖ Repository updated successfully\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ö†Ô∏è Update timed out - continuing with existing repo\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Update failed: {e} - continuing with existing repo\")\n",
        "\n",
        "    print(f\"‚úÖ Repository ready at: {repo_folder.resolve()}\")\n",
        "    return repo_folder\n",
        "\n",
        "# Ensure repository is available\n",
        "REPO_FOLDER = ensure_repo_cloned(REPO_URL, REPO_FOLDER, BRANCH)\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ FX Pairs & Timeframes Configuration\n",
        "# ======================================================\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "\n",
        "TIMEFRAMES = {\n",
        "    \"1d_5y\": (\"1d\", \"5y\"),      # Daily data, 5 years\n",
        "    \"1h_2y\": (\"1h\", \"2y\"),      # Hourly data, 2 years\n",
        "    \"15m_60d\": (\"15m\", \"60d\"),  # 15-minute data, 60 days\n",
        "    \"5m_1mo\": (\"5m\", \"1mo\"),    # 5-minute data, 1 month\n",
        "    \"1m_7d\": (\"1m\", \"7d\")       # 1-minute data, 7 days\n",
        "}\n",
        "\n",
        "print(f\"\\nüìä Configuration:\")\n",
        "print(f\"   Pairs: {len(FX_PAIRS)}\")\n",
        "print(f\"   Timeframes: {len(TIMEFRAMES)}\")\n",
        "print(f\"   Total tasks: {len(FX_PAIRS) * len(TIMEFRAMES)}\")\n",
        "\n",
        "# Thread lock for file operations\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Helper Functions\n",
        "# ======================================================\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    \"\"\"Calculate MD5 hash of file to detect changes\"\"\"\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def merge_data(existing_df, new_df):\n",
        "    \"\"\"Merge existing and new data, removing duplicates\"\"\"\n",
        "    existing_df = ensure_tz_naive(existing_df)\n",
        "    new_df = ensure_tz_naive(new_df)\n",
        "\n",
        "    if existing_df.empty:\n",
        "        return new_df\n",
        "    if new_df.empty:\n",
        "        return existing_df\n",
        "\n",
        "    # Combine dataframes\n",
        "    combined = pd.concat([existing_df, new_df])\n",
        "\n",
        "    # Remove duplicates, keeping the latest\n",
        "    combined = combined[~combined.index.duplicated(keep=\"last\")]\n",
        "\n",
        "    # Sort by date\n",
        "    combined.sort_index(inplace=True)\n",
        "\n",
        "    return combined\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Worker Function for Pair/Timeframe Processing\n",
        "# ======================================================\n",
        "def process_pair_tf(pair, tf_name, interval, period, max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Download and process data for a single pair/timeframe combination\n",
        "\n",
        "    Args:\n",
        "        pair: FX pair (e.g., \"EUR/USD\")\n",
        "        tf_name: Timeframe name (e.g., \"1d_5y\")\n",
        "        interval: YFinance interval (e.g., \"1d\")\n",
        "        period: YFinance period (e.g., \"5y\")\n",
        "        max_retries: Number of retry attempts\n",
        "        retry_delay: Seconds between retries\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (status_message, filepath_if_changed)\n",
        "    \"\"\"\n",
        "    # Convert pair to YFinance symbol (e.g., \"EUR/USD\" -> \"EURUSD=X\")\n",
        "    symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
        "\n",
        "    # Create filename\n",
        "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
        "    filepath = REPO_FOLDER / filename\n",
        "\n",
        "    # Load existing data if available\n",
        "    existing_df = pd.DataFrame()\n",
        "    if filepath.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "            print(f\"  üìÇ Loaded {len(existing_df)} existing rows for {pair} {tf_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not load existing data: {e}\")\n",
        "\n",
        "    # Get hash before changes\n",
        "    old_hash = file_hash(filepath)\n",
        "\n",
        "    # Attempt to download with retries\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  üîΩ Fetching {pair} {tf_name} (attempt {attempt + 1}/{max_retries})...\")\n",
        "\n",
        "            # Download data from YFinance\n",
        "            df = yf.download(\n",
        "                symbol,\n",
        "                interval=interval,\n",
        "                period=period,\n",
        "                progress=False,\n",
        "                auto_adjust=False,\n",
        "                threads=True\n",
        "            )\n",
        "\n",
        "            if df.empty:\n",
        "                raise ValueError(\"No data returned from YFinance\")\n",
        "\n",
        "            # Select and rename columns\n",
        "            available_cols = [c for c in ['Open', 'High', 'Low', 'Close', 'Volume'] if c in df.columns]\n",
        "            df = df[available_cols]\n",
        "            df.rename(columns=lambda x: x.lower(), inplace=True)\n",
        "\n",
        "            # Remove timezone information\n",
        "            df = ensure_tz_naive(df)\n",
        "\n",
        "            # Merge with existing data\n",
        "            combined_df = merge_data(existing_df, df)\n",
        "\n",
        "            # Save to CSV (thread-safe)\n",
        "            with lock:\n",
        "                combined_df.to_csv(filepath)\n",
        "\n",
        "            # Check if file changed\n",
        "            new_hash = file_hash(filepath)\n",
        "            changed = (old_hash != new_hash)\n",
        "\n",
        "            if changed:\n",
        "                print(f\"  ‚úÖ Updated {pair} {tf_name} - Total rows: {len(combined_df)}\")\n",
        "                return f\"üìà Updated {pair} {tf_name} ({len(combined_df)} rows)\", str(filepath)\n",
        "            else:\n",
        "                print(f\"  ‚ÑπÔ∏è No changes {pair} {tf_name}\")\n",
        "                return f\"‚úÖ No changes {pair} {tf_name}\", None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Attempt {attempt + 1}/{max_retries} failed for {pair} {tf_name}: {e}\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  ‚è≥ Waiting {retry_delay} seconds before retry...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  ‚ùå All attempts failed for {pair} {tf_name}\")\n",
        "                return f\"‚ùå Failed {pair} {tf_name}: {e}\", None\n",
        "\n",
        "    return f\"‚ùå Failed {pair} {tf_name}\", None\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Parallel Execution\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Starting parallel data download...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "changed_files = []\n",
        "results = []\n",
        "tasks = []\n",
        "\n",
        "# Create all tasks\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    for pair in FX_PAIRS:\n",
        "        for tf_name, (interval, period) in TIMEFRAMES.items():\n",
        "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period))\n",
        "\n",
        "    # Process results as they complete\n",
        "    for future in as_completed(tasks):\n",
        "        try:\n",
        "            msg, filename = future.result()\n",
        "            results.append(msg)\n",
        "            if filename:\n",
        "                changed_files.append(filename)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Task failed with error: {e}\")\n",
        "            results.append(f\"‚ùå Task failed: {e}\")\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Results Summary\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PROCESSING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "print(f\"\\nTotal tasks: {len(results)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "\n",
        "# ======================================================\n",
        "# üîü Git Commit & Push (Skip in GitHub Actions)\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"   (Workflow will handle commit and push)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        # Change to repo directory\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        # Stage changed files\n",
        "        print(f\"üìù Staging {len(changed_files)} files...\")\n",
        "        subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "\n",
        "        # Commit\n",
        "        commit_msg = f\"Update YFinance FX data - {len(changed_files)} files\"\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", commit_msg],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Changes committed\")\n",
        "        elif \"nothing to commit\" in result.stdout:\n",
        "            print(\"‚ÑπÔ∏è No changes to commit\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Commit warning: {result.stderr}\")\n",
        "\n",
        "        # Push with retry logic\n",
        "        max_push_attempts = 3\n",
        "        for attempt in range(max_push_attempts):\n",
        "            print(f\"üì§ Pushing to GitHub (attempt {attempt + 1}/{max_push_attempts})...\")\n",
        "\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"push\", \"origin\", BRANCH],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Successfully pushed to GitHub\")\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_push_attempts - 1:\n",
        "                    print(f\"‚ö†Ô∏è Push failed, retrying...\")\n",
        "                    # Pull latest and try again\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        capture_output=True\n",
        "                    )\n",
        "                    time.sleep(3)\n",
        "                else:\n",
        "                    print(f\"‚ùå Push failed after {max_push_attempts} attempts\")\n",
        "                    print(f\"   Error: {result.stderr}\")\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"‚ùå Git operation timed out\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Git error: {e}\")\n",
        "    finally:\n",
        "        # Return to base folder\n",
        "        os.chdir(BASE_DIR)\n",
        "\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚ÑπÔ∏è No changes to commit\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ Completion\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ YFINANCE WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"Pairs processed: {len(FX_PAIRS)}\")\n",
        "print(f\"Timeframes per pair: {len(TIMEFRAMES)}\")\n",
        "print(f\"Files updated: {len(changed_files)}\")\n",
        "print(f\"Status: {'Success' if len(results) == len(FX_PAIRS) * len(TIMEFRAMES) else 'Partial'}\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüéØ All FX pairs & timeframes processed with maximum historical data!\")"
      ],
      "metadata": {
        "id": "E8P3UAnfGxbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# FX CSV Combine + Incremental Indicators Pipeline v3.7\n",
        "# ‚úÖ FIXED: Looks for CSVs in correct location\n",
        "# ‚úÖ FIXED: No nested paths for GitHub Actions\n",
        "# ‚úÖ Works in: Colab + GitHub Actions + Local\n",
        "# ‚úÖ Thread-safe, timezone-safe, Git-push-safe\n",
        "# ======================================================\n",
        "\n",
        "import os, time, hashlib, subprocess, shutil\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîß CSV Combiner & Indicator Generator v3.7 - FIXED FILE DISCOVERY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ======================================================\n",
        "# 0Ô∏è‚É£ FIXED: Environment Detection\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Detected Environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ FIXED: Path Setup (NO NESTED DIRECTORIES)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    # Colab: Use /content\n",
        "    ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "    ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "elif IN_GHA:\n",
        "    # ‚úÖ GitHub Actions: Use current directory (NO NESTING)\n",
        "    ROOT_DIR = Path.cwd()\n",
        "    REPO_FOLDER = ROOT_DIR  # No nested folder\n",
        "    print(f\"üìÇ GitHub Actions: Using repo root: {ROOT_DIR}\")\n",
        "else:\n",
        "    # Local: Use subdirectory\n",
        "    ROOT_DIR = Path(\"./forex-alpha-models\")\n",
        "    ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "\n",
        "# Setup subdirectories\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER, REPO_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Root directory: {ROOT_DIR}\")\n",
        "print(f\"‚úÖ Repo folder: {REPO_FOLDER}\")\n",
        "print(f\"‚úÖ CSV folder: {CSV_FOLDER}\")\n",
        "print(f\"‚úÖ Pickle folder: {PICKLE_FOLDER}\")\n",
        "print(f\"‚úÖ Logs folder: {LOGS_FOLDER}\")\n",
        "\n",
        "# Thread lock for file operations\n",
        "lock = threading.Lock()\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Print status messages with icons\"\"\"\n",
        "    levels = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\",\"error\":\"‚ùå\",\"debug\":\"üêû\"}\n",
        "    print(f\"{levels.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Git Configuration\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
        "\n",
        "if FOREX_PAT and not IN_GHA:\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
        "\n",
        "    cred_file = Path.home() / \".git-credentials\"\n",
        "    cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Repository Management (COMPLETE)\n",
        "# ======================================================\n",
        "def ensure_repo():\n",
        "    \"\"\"Ensure repository exists with environment-aware handling\"\"\"\n",
        "    if IN_GHA:\n",
        "        # GitHub Actions: Repo already checked out\n",
        "        print_status(\"ü§ñ GitHub Actions: Repository already available\", \"info\")\n",
        "        return\n",
        "\n",
        "    # For Colab and Local: Clone or update\n",
        "    REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        if REPO_FOLDER.exists():\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "        print_status(f\"Cloning repo into {REPO_FOLDER}...\", \"info\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)],\n",
        "                check=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            print_status(\"‚úÖ Repository cloned successfully\", \"success\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print_status(\"‚ùå Clone timed out after 60 seconds\", \"error\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ùå Clone failed: {e}\", \"error\")\n",
        "            raise\n",
        "    else:\n",
        "        print_status(\"Repo exists, pulling latest...\", \"info\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"fetch\", \"origin\"],\n",
        "                check=False,\n",
        "                timeout=30\n",
        "            )\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"checkout\", BRANCH],\n",
        "                check=False\n",
        "            )\n",
        "            subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH],\n",
        "                check=False,\n",
        "                timeout=30\n",
        "            )\n",
        "            print_status(\"‚úÖ Repo synced successfully\", \"success\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print_status(\"‚ö†Ô∏è Update timed out - continuing\", \"warn\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Update failed: {e} - continuing\", \"warn\")\n",
        "\n",
        "# Execute repository setup\n",
        "ensure_repo()\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Helper Functions\n",
        "# ======================================================\n",
        "def ensure_tz_naive(df):\n",
        "    \"\"\"Remove timezone information from DataFrame index\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_localize(None)\n",
        "\n",
        "    return df\n",
        "\n",
        "def safe_numeric(df):\n",
        "    \"\"\"Handle infinity/NaN robustly\"\"\"\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    required_columns = ['open', 'high', 'low', 'close']\n",
        "    existing_columns = [col for col in required_columns if col in df.columns]\n",
        "\n",
        "    if existing_columns:\n",
        "        df.dropna(subset=existing_columns, inplace=True)\n",
        "    else:\n",
        "        df.dropna(how='all', inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ CSV Combine\n",
        "# ======================================================\n",
        "def combine_csv(csv_path):\n",
        "    \"\"\"Combine CSV with existing data in REPO_FOLDER\"\"\"\n",
        "    target_file = REPO_FOLDER / csv_path.name\n",
        "\n",
        "    # Load existing data\n",
        "    if target_file.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(target_file, index_col=0, parse_dates=True)\n",
        "            existing_df = ensure_tz_naive(existing_df)\n",
        "            print_status(f\"  üìÇ Loaded {len(existing_df)} existing rows\", \"debug\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"  ‚ö†Ô∏è Could not load existing: {e}\", \"warn\")\n",
        "            existing_df = pd.DataFrame()\n",
        "    else:\n",
        "        existing_df = pd.DataFrame()\n",
        "\n",
        "    # Load new data\n",
        "    try:\n",
        "        new_df = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
        "        new_df = ensure_tz_naive(new_df)\n",
        "    except Exception as e:\n",
        "        print_status(f\"  ‚ùå Could not load new data: {e}\", \"error\")\n",
        "        return existing_df, target_file\n",
        "\n",
        "    # Combine\n",
        "    combined_df = pd.concat([existing_df, new_df])\n",
        "    combined_df = combined_df[~combined_df.index.duplicated(keep=\"last\")]\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    return combined_df, target_file\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Incremental Indicators\n",
        "# ======================================================\n",
        "def add_indicators_incremental(existing_df, combined_df):\n",
        "    \"\"\"Add indicators only to NEW rows\"\"\"\n",
        "    if not existing_df.empty:\n",
        "        new_rows = combined_df.loc[~combined_df.index.isin(existing_df.index)]\n",
        "    else:\n",
        "        new_rows = combined_df.copy()\n",
        "\n",
        "    if new_rows.empty:\n",
        "        return None\n",
        "\n",
        "    # Validate OHLC columns\n",
        "    required_cols = ['open', 'high', 'low', 'close']\n",
        "    if not all(col in new_rows.columns for col in required_cols):\n",
        "        print_status(f\"‚ö†Ô∏è Missing OHLC columns\", \"warn\")\n",
        "        return None\n",
        "\n",
        "    new_rows = safe_numeric(new_rows)\n",
        "\n",
        "    if new_rows.empty:\n",
        "        return None\n",
        "\n",
        "    new_rows.sort_index(inplace=True)\n",
        "\n",
        "    # Calculate indicators\n",
        "    try:\n",
        "        # Moving Averages\n",
        "        if len(new_rows) >= 10:\n",
        "            new_rows['SMA_10'] = ta.trend.sma_indicator(new_rows['close'], 10)\n",
        "            new_rows['EMA_10'] = ta.trend.ema_indicator(new_rows['close'], 10)\n",
        "\n",
        "        if len(new_rows) >= 50:\n",
        "            new_rows['SMA_50'] = ta.trend.sma_indicator(new_rows['close'], 50)\n",
        "            new_rows['EMA_50'] = ta.trend.ema_indicator(new_rows['close'], 50)\n",
        "\n",
        "        # RSI\n",
        "        if len(new_rows) >= 14:\n",
        "            new_rows['RSI_14'] = ta.momentum.rsi(new_rows['close'], 14)\n",
        "            new_rows['Williams_%R'] = WilliamsRIndicator(\n",
        "                new_rows['high'], new_rows['low'], new_rows['close'], 14\n",
        "            ).williams_r()\n",
        "\n",
        "        # ATR\n",
        "        if len(new_rows) >= 14:\n",
        "            new_rows['ATR'] = AverageTrueRange(\n",
        "                new_rows['high'], new_rows['low'], new_rows['close'], 14\n",
        "            ).average_true_range()\n",
        "\n",
        "        # MACD\n",
        "        if len(new_rows) >= 26:\n",
        "            new_rows['MACD'] = ta.trend.macd(new_rows['close'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ö†Ô∏è Indicator error: {e}\", \"warn\")\n",
        "\n",
        "    # Scale (protect OHLC)\n",
        "    numeric_cols = new_rows.select_dtypes(include=[np.number]).columns\n",
        "    protected_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "    scalable_cols = [c for c in numeric_cols if c not in protected_cols]\n",
        "\n",
        "    if scalable_cols and not new_rows[scalable_cols].dropna(how='all').empty:\n",
        "        new_rows[scalable_cols] = new_rows[scalable_cols].replace([np.inf, -np.inf], np.nan)\n",
        "        new_rows[scalable_cols] = new_rows[scalable_cols].ffill().bfill().fillna(0)\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        try:\n",
        "            new_rows[scalable_cols] = scaler.fit_transform(new_rows[scalable_cols])\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Scaling warning: {e}\", \"warn\")\n",
        "\n",
        "    return new_rows\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Worker Function\n",
        "# ======================================================\n",
        "def process_csv_file(csv_file):\n",
        "    \"\"\"Process a single CSV file\"\"\"\n",
        "    try:\n",
        "        # Combine CSV\n",
        "        combined_df, target_file = combine_csv(csv_file)\n",
        "\n",
        "        # Validate\n",
        "        required_cols = ['open', 'high', 'low', 'close']\n",
        "        if not all(col in combined_df.columns for col in required_cols):\n",
        "            msg = f\"‚ö†Ô∏è Skipped {csv_file.name}: Missing OHLC\"\n",
        "            print_status(msg, \"warn\")\n",
        "            return None, msg\n",
        "\n",
        "        # Check for existing indicators\n",
        "        existing_pickle = PICKLE_FOLDER / f\"{csv_file.stem}_indicators.pkl\"\n",
        "\n",
        "        if existing_pickle.exists():\n",
        "            try:\n",
        "                existing_df = pd.read_pickle(existing_pickle)\n",
        "            except:\n",
        "                existing_df = pd.DataFrame()\n",
        "        else:\n",
        "            existing_df = pd.DataFrame()\n",
        "\n",
        "        # Add indicators for new rows\n",
        "        new_indicators = add_indicators_incremental(existing_df, combined_df)\n",
        "\n",
        "        if new_indicators is not None:\n",
        "            # Combine\n",
        "            updated_df = pd.concat([existing_df, new_indicators]).sort_index()\n",
        "\n",
        "            # Save (thread-safe)\n",
        "            with lock:\n",
        "                updated_df.to_pickle(existing_pickle, protocol=4)\n",
        "                combined_df.to_csv(target_file)\n",
        "\n",
        "            msg = f\"‚úÖ {csv_file.name} updated: {len(new_indicators)} new rows\"\n",
        "            print_status(msg, \"success\")\n",
        "            return str(existing_pickle), msg\n",
        "        else:\n",
        "            msg = f\"‚ÑπÔ∏è {csv_file.name} no new rows\"\n",
        "            print_status(msg, \"info\")\n",
        "            return None, msg\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = f\"‚ùå Failed {csv_file.name}: {e}\"\n",
        "        print_status(msg, \"error\")\n",
        "        return None, msg\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ FIXED: Enhanced CSV Discovery\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Processing CSV files...\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# ‚úÖ FIXED: Search in multiple locations and patterns\n",
        "csv_files = []\n",
        "\n",
        "# Search patterns for different CSV naming conventions\n",
        "search_patterns = [\n",
        "    CSV_FOLDER / \"*.csv\",           # Standard location\n",
        "    ROOT_DIR / \"*.csv\",             # Root directory\n",
        "    REPO_FOLDER / \"*.csv\",          # Repo folder\n",
        "]\n",
        "\n",
        "print_status(f\"üîç Searching for CSV files in multiple locations...\", \"info\")\n",
        "\n",
        "for pattern in search_patterns:\n",
        "    found = list(pattern.parent.glob(pattern.name))\n",
        "    if found:\n",
        "        print_status(f\"  üìÇ Found {len(found)} CSV(s) in: {pattern.parent}\", \"debug\")\n",
        "        csv_files.extend(found)\n",
        "\n",
        "# Remove duplicates (keep unique paths)\n",
        "csv_files = list(set(csv_files))\n",
        "\n",
        "# ‚úÖ DIAGNOSTIC: Show what we found\n",
        "if csv_files:\n",
        "    print_status(f\"üìä Total unique CSV files found: {len(csv_files)}\", \"success\")\n",
        "    for csv_file in csv_files[:5]:  # Show first 5\n",
        "        print_status(f\"  ‚Ä¢ {csv_file.name} ({csv_file.stat().st_size / 1024:.1f} KB)\", \"debug\")\n",
        "    if len(csv_files) > 5:\n",
        "        print_status(f\"  ... and {len(csv_files) - 5} more\", \"debug\")\n",
        "else:\n",
        "    print_status(\"‚ö™ No CSV files found in any location\", \"warn\")\n",
        "    print_status(\"üîç Searched in:\", \"info\")\n",
        "    print_status(f\"  ‚Ä¢ {CSV_FOLDER}\", \"debug\")\n",
        "    print_status(f\"  ‚Ä¢ {ROOT_DIR}\", \"debug\")\n",
        "    print_status(f\"  ‚Ä¢ {REPO_FOLDER}\", \"debug\")\n",
        "\n",
        "    # ‚úÖ List what's actually in CSV_FOLDER\n",
        "    if CSV_FOLDER.exists():\n",
        "        all_files = list(CSV_FOLDER.glob(\"*\"))\n",
        "        if all_files:\n",
        "            print_status(f\"üìÇ Files in CSV folder ({len(all_files)}):\", \"debug\")\n",
        "            for f in all_files[:10]:\n",
        "                print_status(f\"  ‚Ä¢ {f.name}\", \"debug\")\n",
        "        else:\n",
        "            print_status(\"üìÇ CSV folder is empty\", \"debug\")\n",
        "\n",
        "changed_files = []\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Process Files\n",
        "# ======================================================\n",
        "if csv_files:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"‚öôÔ∏è Processing {len(csv_files)} CSV file(s)...\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=min(8, len(csv_files))) as executor:\n",
        "        futures = [executor.submit(process_csv_file, f) for f in csv_files]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            file, msg = future.result()\n",
        "            if file:\n",
        "                changed_files.append(file)\n",
        "\n",
        "# ======================================================\n",
        "# üîü Git Push (Skip in GitHub Actions)\n",
        "# ======================================================\n",
        "if IN_GHA:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ü§ñ GitHub Actions: Skipping git operations\")\n",
        "    print(\"   (Workflow will handle commit and push)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "elif changed_files and FOREX_PAT:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ Committing changes to GitHub...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        # Stage files\n",
        "        subprocess.run(\n",
        "            [\"git\", \"-C\", str(REPO_FOLDER), \"add\"] + changed_files,\n",
        "            check=False\n",
        "        )\n",
        "\n",
        "        # Commit\n",
        "        subprocess.run(\n",
        "            [\"git\", \"-C\", str(REPO_FOLDER), \"commit\", \"-m\", \"üìà Auto-update CSVs & indicators\"],\n",
        "            check=False\n",
        "        )\n",
        "\n",
        "        # Push with retry\n",
        "        for attempt in range(3):\n",
        "            print_status(f\"üì§ Pushing (attempt {attempt + 1}/3)...\", \"info\")\n",
        "\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"-C\", str(REPO_FOLDER), \"push\", \"origin\", BRANCH],\n",
        "                capture_output=True,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print_status(\"‚úÖ Push successful\", \"success\")\n",
        "                break\n",
        "            else:\n",
        "                if attempt < 2:\n",
        "                    subprocess.run(\n",
        "                        [\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"--rebase\", \"origin\", BRANCH],\n",
        "                        check=False\n",
        "                    )\n",
        "                    time.sleep(5)\n",
        "                else:\n",
        "                    print_status(f\"‚ùå Push failed\", \"error\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ùå Git error: {e}\", \"error\")\n",
        "\n",
        "# ======================================================\n",
        "# ‚úÖ Completion\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ CSV COMBINER WORKFLOW COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Environment: {ENV_NAME}\")\n",
        "print(f\"CSV files found: {len(csv_files)}\")\n",
        "print(f\"CSV files processed: {len(csv_files)}\")\n",
        "print(f\"Pickle files updated: {len(changed_files)}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if csv_files:\n",
        "    print(\"\\nüéØ All CSVs combined with incremental indicators!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No CSV files found - check data source cells!\")"
      ],
      "metadata": {
        "id": "mGEjZ-GrvrFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "VERSION 3.7 ‚Äì ULTRA-PERSISTENT SELF-LEARNING HYBRID FX PIPELINE (ENHANCED)\n",
        "===========================================================================\n",
        "‚úÖ FIXED: No nested paths for GitHub Actions\n",
        "‚úÖ Uses memory_v85.db in repo folder (no nesting)\n",
        "‚úÖ Full production features with comprehensive analytics\n",
        "\"\"\"\n",
        "\n",
        "import os, time, json, re, shutil, subprocess, pickle, filecmp, sqlite3\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import ta\n",
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from collections import defaultdict\n",
        "from contextlib import contextmanager\n",
        "import threading\n",
        "\n",
        "# ======================================================\n",
        "# 0Ô∏è‚É£ FIXED: Environment Detection & Path Setup\n",
        "# ======================================================\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Detected Environment: {ENV_NAME}\")\n",
        "\n",
        "# ‚úÖ FIXED: Set paths based on environment (NO NESTING IN GHA)\n",
        "if IN_COLAB:\n",
        "    ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "    ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "elif IN_GHA:\n",
        "    # ‚úÖ GitHub Actions: Use current directory directly\n",
        "    ROOT_DIR = Path.cwd()\n",
        "    REPO_FOLDER = ROOT_DIR  # No nested folder!\n",
        "    print(f\"üìÇ GitHub Actions: Using repo root: {ROOT_DIR}\")\n",
        "else:\n",
        "    ROOT_DIR = Path(\"./forex-alpha-models\")\n",
        "    ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "\n",
        "# Setup subdirectories\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "BACKUP_FOLDER = ROOT_DIR / \"backups\"\n",
        "\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER, BACKUP_FOLDER, REPO_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Root Directory: {ROOT_DIR}\")\n",
        "print(f\"‚úÖ Repo Folder: {REPO_FOLDER}\")\n",
        "print(f\"‚úÖ CSV Folder: {CSV_FOLDER}\")\n",
        "print(f\"‚úÖ Pickle Folder: {PICKLE_FOLDER}\")\n",
        "print(f\"‚úÖ Logs Folder: {LOGS_FOLDER}\")\n",
        "print(f\"‚úÖ Backup Folder: {BACKUP_FOLDER}\")\n",
        "\n",
        "# Enhanced logging setup with rotation\n",
        "log_formatter = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "# Main log file with rotation (10MB max, 5 backups)\n",
        "main_handler = RotatingFileHandler(\n",
        "    LOGS_FOLDER / \"pipeline.log\",\n",
        "    maxBytes=10*1024*1024,\n",
        "    backupCount=5\n",
        ")\n",
        "main_handler.setFormatter(log_formatter)\n",
        "\n",
        "# Error log file\n",
        "error_handler = RotatingFileHandler(\n",
        "    LOGS_FOLDER / \"errors.log\",\n",
        "    maxBytes=5*1024*1024,\n",
        "    backupCount=3\n",
        ")\n",
        "error_handler.setLevel(logging.ERROR)\n",
        "error_handler.setFormatter(log_formatter)\n",
        "\n",
        "# Configure root logger\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.addHandler(main_handler)\n",
        "logger.addHandler(error_handler)\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Enhanced status printing with better formatting\"\"\"\n",
        "    icons = {\n",
        "        \"info\": \"‚ÑπÔ∏è\",\n",
        "        \"success\": \"‚úÖ\",\n",
        "        \"warn\": \"‚ö†Ô∏è\",\n",
        "        \"debug\": \"üêû\",\n",
        "        \"error\": \"‚ùå\",\n",
        "        \"performance\": \"‚ö°\"\n",
        "    }\n",
        "    log_level = level if level != \"warn\" else \"warning\"\n",
        "    log_level = log_level if log_level != \"performance\" else \"info\"\n",
        "    getattr(logging, log_level, logging.info)(msg)\n",
        "    print(f\"{icons.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "print_status(f\"Environment: {ENV_NAME}\", \"success\")\n",
        "print_status(f\"Working Directory: {os.getcwd()}\", \"info\")\n",
        "\n",
        "# ======================================================\n",
        "# üÜï ENHANCED DATABASE - v3.7 (memory_v85.db)\n",
        "# ======================================================\n",
        "PERSISTENT_DB = REPO_FOLDER / \"memory_v85.db\"  # ‚úÖ In repo folder (no nesting)\n",
        "\n",
        "class EnhancedTradeMemoryDatabase:\n",
        "    \"\"\"\n",
        "    ENHANCED VERSION v3.7 - Production-ready database\n",
        "\n",
        "    ‚úÖ Fixed: No nested paths for GitHub Actions\n",
        "    ‚úÖ Uses memory_v85.db for backward compatibility\n",
        "    ‚úÖ Full production features with comprehensive analytics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, db_path=PERSISTENT_DB, max_retries=3):\n",
        "        self.db_path = db_path\n",
        "        self.db_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        self.conn = None\n",
        "        self.lock = threading.RLock()\n",
        "        self.min_age_hours = 1\n",
        "        self.max_retries = max_retries\n",
        "        self.performance_metrics = defaultdict(list)\n",
        "\n",
        "        print_status(f\"üìÅ Database path: {self.db_path}\", \"info\")\n",
        "        self.initialize_database()\n",
        "\n",
        "    @contextmanager\n",
        "    def get_cursor(self):\n",
        "        \"\"\"Context manager for database cursor with auto-commit\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        try:\n",
        "            yield cursor\n",
        "            self.conn.commit()\n",
        "        except Exception as e:\n",
        "            self.conn.rollback()\n",
        "            raise e\n",
        "        finally:\n",
        "            cursor.close()\n",
        "\n",
        "    def _execute_with_retry(self, operation, *args, **kwargs):\n",
        "        \"\"\"Execute database operation with retry logic\"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                return operation(*args, **kwargs)\n",
        "            except sqlite3.OperationalError as e:\n",
        "                if attempt < self.max_retries - 1:\n",
        "                    wait_time = (2 ** attempt) * 0.1\n",
        "                    print_status(\n",
        "                        f\"‚ö†Ô∏è Database busy, retrying in {wait_time:.1f}s... \"\n",
        "                        f\"(attempt {attempt + 1}/{self.max_retries})\",\n",
        "                        \"warn\"\n",
        "                    )\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "    def initialize_database(self):\n",
        "        \"\"\"Create database with optimized settings\"\"\"\n",
        "        try:\n",
        "            # Check if database exists\n",
        "            db_exists = self.db_path.exists()\n",
        "\n",
        "            self.conn = sqlite3.connect(\n",
        "                str(self.db_path),\n",
        "                timeout=30,\n",
        "                check_same_thread=False\n",
        "            )\n",
        "\n",
        "            # Optimized PRAGMA settings\n",
        "            pragmas = [\n",
        "                \"PRAGMA journal_mode=WAL\",\n",
        "                \"PRAGMA synchronous=NORMAL\",\n",
        "                \"PRAGMA cache_size=-64000\",\n",
        "                \"PRAGMA temp_store=MEMORY\",\n",
        "                \"PRAGMA mmap_size=30000000000\",\n",
        "                \"PRAGMA page_size=4096\",\n",
        "                \"PRAGMA auto_vacuum=INCREMENTAL\"\n",
        "            ]\n",
        "\n",
        "            for pragma in pragmas:\n",
        "                self.conn.execute(pragma)\n",
        "\n",
        "            with self.get_cursor() as cursor:\n",
        "                # ===== TABLE 1: Pending trades =====\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS pending_trades (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        created_at TEXT NOT NULL,\n",
        "                        iteration INTEGER NOT NULL,\n",
        "                        pair TEXT NOT NULL,\n",
        "                        timeframe TEXT NOT NULL,\n",
        "                        sgd_prediction INTEGER,\n",
        "                        rf_prediction INTEGER,\n",
        "                        ensemble_prediction INTEGER,\n",
        "                        entry_price REAL NOT NULL,\n",
        "                        sl_price REAL NOT NULL,\n",
        "                        tp_price REAL NOT NULL,\n",
        "                        confidence REAL,\n",
        "                        evaluated BOOLEAN DEFAULT 0,\n",
        "                        retry_count INTEGER DEFAULT 0,\n",
        "                        last_error TEXT\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "                # Create indexes\n",
        "                indexes = [\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_pending_eval ON pending_trades(evaluated, created_at)\",\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_pending_pair ON pending_trades(pair, evaluated)\",\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_pending_iteration ON pending_trades(iteration, evaluated)\"\n",
        "                ]\n",
        "\n",
        "                for index_sql in indexes:\n",
        "                    cursor.execute(index_sql)\n",
        "\n",
        "                # ===== TABLE 2: Completed trades =====\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS completed_trades (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        pending_trade_id INTEGER,\n",
        "                        created_at TEXT NOT NULL,\n",
        "                        evaluated_at TEXT NOT NULL,\n",
        "                        iteration_created INTEGER,\n",
        "                        iteration_evaluated INTEGER,\n",
        "                        pair TEXT NOT NULL,\n",
        "                        timeframe TEXT NOT NULL,\n",
        "                        model_used TEXT NOT NULL,\n",
        "                        entry_price REAL NOT NULL,\n",
        "                        exit_price REAL NOT NULL,\n",
        "                        sl_price REAL NOT NULL,\n",
        "                        tp_price REAL NOT NULL,\n",
        "                        prediction INTEGER,\n",
        "                        hit_tp BOOLEAN NOT NULL,\n",
        "                        pnl REAL NOT NULL,\n",
        "                        pnl_percent REAL,\n",
        "                        duration_hours REAL,\n",
        "                        price_movement REAL,\n",
        "                        FOREIGN KEY (pending_trade_id) REFERENCES pending_trades(id)\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "                # Create indexes\n",
        "                indexes = [\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_completed_model ON completed_trades(model_used, evaluated_at)\",\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_completed_pair ON completed_trades(pair, model_used, evaluated_at)\",\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_completed_timestamp ON completed_trades(evaluated_at)\",\n",
        "                    \"CREATE INDEX IF NOT EXISTS idx_completed_pnl ON completed_trades(model_used, pnl)\"\n",
        "                ]\n",
        "\n",
        "                for index_sql in indexes:\n",
        "                    cursor.execute(index_sql)\n",
        "\n",
        "                # ===== TABLE 3: Model performance cache =====\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS model_stats_cache (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        updated_at TEXT NOT NULL,\n",
        "                        pair TEXT NOT NULL,\n",
        "                        model_name TEXT NOT NULL,\n",
        "                        days INTEGER NOT NULL,\n",
        "                        total_trades INTEGER DEFAULT 0,\n",
        "                        winning_trades INTEGER DEFAULT 0,\n",
        "                        losing_trades INTEGER DEFAULT 0,\n",
        "                        accuracy_pct REAL DEFAULT 0.0,\n",
        "                        total_pnl REAL DEFAULT 0.0,\n",
        "                        avg_pnl REAL DEFAULT 0.0,\n",
        "                        max_pnl REAL DEFAULT 0.0,\n",
        "                        min_pnl REAL DEFAULT 0.0,\n",
        "                        sharpe_ratio REAL DEFAULT 0.0,\n",
        "                        max_drawdown REAL DEFAULT 0.0,\n",
        "                        avg_duration_hours REAL DEFAULT 0.0,\n",
        "                        UNIQUE(pair, model_name, days) ON CONFLICT REPLACE\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "                cursor.execute('''\n",
        "                    CREATE INDEX IF NOT EXISTS idx_stats_lookup\n",
        "                    ON model_stats_cache(pair, model_name, days)\n",
        "                ''')\n",
        "\n",
        "                # ===== TABLE 4: Pipeline execution log =====\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS execution_log (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        timestamp TEXT NOT NULL,\n",
        "                        iteration INTEGER NOT NULL,\n",
        "                        status TEXT NOT NULL,\n",
        "                        trades_stored INTEGER DEFAULT 0,\n",
        "                        trades_evaluated INTEGER DEFAULT 0,\n",
        "                        duration_seconds REAL,\n",
        "                        memory_usage_mb REAL,\n",
        "                        error_message TEXT\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "                # ===== TABLE 5: Performance metrics =====\n",
        "                cursor.execute('''\n",
        "                    CREATE TABLE IF NOT EXISTS performance_metrics (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        timestamp TEXT NOT NULL,\n",
        "                        operation TEXT NOT NULL,\n",
        "                        duration_ms REAL NOT NULL,\n",
        "                        rows_affected INTEGER DEFAULT 0,\n",
        "                        success BOOLEAN DEFAULT 1\n",
        "                    )\n",
        "                ''')\n",
        "\n",
        "            if db_exists:\n",
        "                print_status(f\"‚úÖ Connected to existing database: {self.db_path.name}\", \"success\")\n",
        "            else:\n",
        "                print_status(f\"‚úÖ Created new database: {self.db_path.name}\", \"success\")\n",
        "\n",
        "            print_status(\"‚úÖ Enhanced Database v3.7 initialized\", \"success\")\n",
        "            self._verify_database_integrity()\n",
        "            self._optimize_database()\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Database initialization failed: {e}\", \"error\")\n",
        "            raise\n",
        "\n",
        "    def _verify_database_integrity(self):\n",
        "        \"\"\"Verify database structure and run integrity check\"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                # Check integrity\n",
        "                cursor.execute(\"PRAGMA integrity_check\")\n",
        "                result = cursor.fetchone()\n",
        "                if result[0] != 'ok':\n",
        "                    print_status(f\"‚ö†Ô∏è Database integrity issue: {result[0]}\", \"warn\")\n",
        "\n",
        "                # Check tables\n",
        "                cursor.execute(\"\"\"\n",
        "                    SELECT name FROM sqlite_master\n",
        "                    WHERE type='table'\n",
        "                \"\"\")\n",
        "                tables = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "                expected_tables = [\n",
        "                    'pending_trades', 'completed_trades',\n",
        "                    'model_stats_cache', 'execution_log',\n",
        "                    'performance_metrics'\n",
        "                ]\n",
        "\n",
        "                for table in expected_tables:\n",
        "                    if table in tables:\n",
        "                        cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
        "                        count = cursor.fetchone()[0]\n",
        "                        print_status(f\"  ‚úì Table '{table}' exists ({count} rows)\", \"debug\")\n",
        "                    else:\n",
        "                        print_status(f\"  ‚úó Table '{table}' missing!\", \"error\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Database verification warning: {e}\", \"warn\")\n",
        "\n",
        "    def _optimize_database(self):\n",
        "        \"\"\"Optimize database performance\"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                # Analyze tables for query optimization\n",
        "                cursor.execute(\"ANALYZE\")\n",
        "\n",
        "                # Check if vacuum is needed\n",
        "                cursor.execute(\"PRAGMA page_count\")\n",
        "                page_count = cursor.fetchone()[0]\n",
        "\n",
        "                cursor.execute(\"PRAGMA freelist_count\")\n",
        "                freelist_count = cursor.fetchone()[0]\n",
        "\n",
        "                # Vacuum if more than 10% free pages\n",
        "                if page_count > 0 and (freelist_count / page_count) > 0.1:\n",
        "                    print_status(\"üîß Running database vacuum...\", \"info\")\n",
        "                    cursor.execute(\"PRAGMA incremental_vacuum\")\n",
        "                    print_status(\"‚úÖ Database optimized\", \"success\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Database optimization warning: {e}\", \"warn\")\n",
        "\n",
        "    def _track_performance(self, operation, duration_ms, rows_affected=0, success=True):\n",
        "        \"\"\"Track operation performance metrics\"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    INSERT INTO performance_metrics\n",
        "                    (timestamp, operation, duration_ms, rows_affected, success)\n",
        "                    VALUES (?, ?, ?, ?, ?)\n",
        "                ''', (\n",
        "                    datetime.now(timezone.utc).isoformat(),\n",
        "                    operation,\n",
        "                    duration_ms,\n",
        "                    rows_affected,\n",
        "                    success\n",
        "                ))\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Metrics tracking failed: {e}\", \"debug\")\n",
        "\n",
        "    def store_new_signals(self, aggregated_signals, current_iteration):\n",
        "        \"\"\"Store signals with batch insert for better performance\"\"\"\n",
        "        if not aggregated_signals:\n",
        "            print_status(\"‚ö†Ô∏è No signals to store\", \"warn\")\n",
        "            return 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        stored_count = 0\n",
        "        failed_count = 0\n",
        "\n",
        "        # Prepare batch data\n",
        "        batch_data = []\n",
        "\n",
        "        for pair, pair_data in aggregated_signals.items():\n",
        "            signals = pair_data.get('signals', {})\n",
        "\n",
        "            for tf_name, signal_data in signals.items():\n",
        "                if not signal_data:\n",
        "                    continue\n",
        "\n",
        "                # Validate required fields\n",
        "                required_fields = ['live', 'SL', 'TP']\n",
        "                if not all(signal_data.get(f, 0) > 0 for f in required_fields):\n",
        "                    failed_count += 1\n",
        "                    continue\n",
        "\n",
        "                batch_data.append((\n",
        "                    datetime.now(timezone.utc).isoformat(),\n",
        "                    current_iteration,\n",
        "                    pair,\n",
        "                    tf_name,\n",
        "                    signal_data.get('sgd_pred'),\n",
        "                    signal_data.get('rf_pred'),\n",
        "                    signal_data.get('signal'),\n",
        "                    signal_data.get('live', 0),\n",
        "                    signal_data.get('SL', 0),\n",
        "                    signal_data.get('TP', 0),\n",
        "                    signal_data.get('confidence', 0.5)\n",
        "                ))\n",
        "\n",
        "        if not batch_data:\n",
        "            print_status(\"‚ö†Ô∏è No valid signals to store\", \"warn\")\n",
        "            return 0\n",
        "\n",
        "        try:\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                # Batch insert\n",
        "                cursor.executemany('''\n",
        "                    INSERT INTO pending_trades\n",
        "                    (created_at, iteration, pair, timeframe,\n",
        "                     sgd_prediction, rf_prediction, ensemble_prediction,\n",
        "                     entry_price, sl_price, tp_price, confidence)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                ''', batch_data)\n",
        "\n",
        "                stored_count = len(batch_data)\n",
        "\n",
        "                # Log execution\n",
        "                cursor.execute('''\n",
        "                    INSERT INTO execution_log\n",
        "                    (timestamp, iteration, status, trades_stored, duration_seconds)\n",
        "                    VALUES (?, ?, 'signals_stored', ?, ?)\n",
        "                ''', (\n",
        "                    datetime.now(timezone.utc).isoformat(),\n",
        "                    current_iteration,\n",
        "                    stored_count,\n",
        "                    time.time() - start_time\n",
        "                ))\n",
        "\n",
        "            duration_ms = (time.time() - start_time) * 1000\n",
        "            self._track_performance('store_signals', duration_ms, stored_count, True)\n",
        "\n",
        "            print_status(\n",
        "                f\"üíæ Stored {stored_count} trades in {duration_ms:.0f}ms \"\n",
        "                f\"({failed_count} failed)\",\n",
        "                \"success\"\n",
        "            )\n",
        "            return stored_count\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Batch insert failed: {e}\", \"error\")\n",
        "            self._track_performance('store_signals', 0, 0, False)\n",
        "            return 0\n",
        "\n",
        "    def evaluate_pending_trades(self, current_prices, current_iteration):\n",
        "        \"\"\"Enhanced trade evaluation with better performance\"\"\"\n",
        "        if not current_prices:\n",
        "            print_status(\"‚ö†Ô∏è No current prices provided\", \"warn\")\n",
        "            return {}\n",
        "\n",
        "        start_time = time.time()\n",
        "        min_age = (datetime.now(timezone.utc) - timedelta(hours=self.min_age_hours)).isoformat()\n",
        "\n",
        "        try:\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    SELECT id, pair, timeframe, sgd_prediction, rf_prediction,\n",
        "                           ensemble_prediction, entry_price, sl_price, tp_price,\n",
        "                           created_at, iteration\n",
        "                    FROM pending_trades\n",
        "                    WHERE evaluated = 0 AND created_at < ?\n",
        "                    ORDER BY created_at ASC\n",
        "                    LIMIT 1000\n",
        "                ''', (min_age,))\n",
        "\n",
        "                pending_trades = cursor.fetchall()\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Failed to fetch pending trades: {e}\", \"error\")\n",
        "            return {}\n",
        "\n",
        "        if not pending_trades:\n",
        "            print_status(\n",
        "                f\"‚ÑπÔ∏è No trades old enough to evaluate (need {self.min_age_hours}+ hours)\",\n",
        "                \"info\"\n",
        "            )\n",
        "            return {}\n",
        "\n",
        "        print_status(\n",
        "            f\"üîç Evaluating {len(pending_trades)} trades from previous iteration(s)\",\n",
        "            \"info\"\n",
        "        )\n",
        "\n",
        "        results_by_model = defaultdict(lambda: {\n",
        "            'closed_trades': 0,\n",
        "            'wins': 0,\n",
        "            'losses': 0,\n",
        "            'total_pnl': 0.0,\n",
        "            'trades': []\n",
        "        })\n",
        "\n",
        "        evaluated_count = 0\n",
        "        skipped_count = 0\n",
        "        completed_trades_batch = []\n",
        "        evaluated_ids = []\n",
        "\n",
        "        for trade in pending_trades:\n",
        "            (trade_id, pair, timeframe, sgd_pred, rf_pred, ensemble_pred,\n",
        "             entry_price, sl_price, tp_price, created_at, created_iteration) = trade\n",
        "\n",
        "            current_price = current_prices.get(pair, 0)\n",
        "\n",
        "            if current_price <= 0:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            # Validate prices\n",
        "            if not self._validate_trade_prices(entry_price, sl_price, tp_price, current_price):\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            # Evaluate for each model\n",
        "            for model_name, prediction in [\n",
        "                ('SGD', sgd_pred),\n",
        "                ('RandomForest', rf_pred),\n",
        "                ('Ensemble', ensemble_pred)\n",
        "            ]:\n",
        "                if prediction is None:\n",
        "                    continue\n",
        "\n",
        "                # Check if TP or SL was hit\n",
        "                hit_tp, hit_sl, exit_price = self._evaluate_trade_outcome(\n",
        "                    prediction, current_price, tp_price, sl_price\n",
        "                )\n",
        "\n",
        "                # If trade closed, record result\n",
        "                if exit_price:\n",
        "                    pnl = self._calculate_pnl(prediction, entry_price, exit_price)\n",
        "                    pnl_percent = (pnl / entry_price) * 100\n",
        "                    duration_hours = self._calculate_duration_hours(created_at)\n",
        "                    price_movement = abs(exit_price - entry_price) / entry_price * 100\n",
        "\n",
        "                    completed_trades_batch.append((\n",
        "                        trade_id, created_at, datetime.now(timezone.utc).isoformat(),\n",
        "                        created_iteration, current_iteration,\n",
        "                        pair, timeframe, model_name, entry_price, exit_price,\n",
        "                        sl_price, tp_price, prediction, hit_tp, pnl, pnl_percent,\n",
        "                        duration_hours, price_movement\n",
        "                    ))\n",
        "\n",
        "                    # Accumulate results\n",
        "                    results_by_model[model_name]['closed_trades'] += 1\n",
        "                    results_by_model[model_name]['total_pnl'] += pnl\n",
        "\n",
        "                    if hit_tp:\n",
        "                        results_by_model[model_name]['wins'] += 1\n",
        "                    else:\n",
        "                        results_by_model[model_name]['losses'] += 1\n",
        "\n",
        "                    results_by_model[model_name]['trades'].append({\n",
        "                        'pair': pair,\n",
        "                        'timeframe': timeframe,\n",
        "                        'pnl': pnl,\n",
        "                        'hit_tp': hit_tp\n",
        "                    })\n",
        "\n",
        "                    status = \"WIN ‚úÖ\" if hit_tp else \"LOSS ‚ùå\"\n",
        "                    print_status(\n",
        "                        f\"{status} {model_name}: {pair} {timeframe} \"\n",
        "                        f\"P&L=${pnl:.5f} ({pnl_percent:+.2f}%) [{duration_hours:.1f}h]\",\n",
        "                        \"success\" if hit_tp else \"warn\"\n",
        "                    )\n",
        "\n",
        "            evaluated_ids.append(trade_id)\n",
        "            evaluated_count += 1\n",
        "\n",
        "        # Batch insert completed trades\n",
        "        if completed_trades_batch:\n",
        "            try:\n",
        "                with self.lock, self.get_cursor() as cursor:\n",
        "                    cursor.executemany('''\n",
        "                        INSERT INTO completed_trades\n",
        "                        (pending_trade_id, created_at, evaluated_at,\n",
        "                         iteration_created, iteration_evaluated,\n",
        "                         pair, timeframe, model_used, entry_price, exit_price,\n",
        "                         sl_price, tp_price, prediction, hit_tp, pnl, pnl_percent,\n",
        "                         duration_hours, price_movement)\n",
        "                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                    ''', completed_trades_batch)\n",
        "\n",
        "                    # Mark as evaluated\n",
        "                    if evaluated_ids:\n",
        "                        placeholders = ','.join('?' * len(evaluated_ids))\n",
        "                        cursor.execute(f'''\n",
        "                            UPDATE pending_trades\n",
        "                            SET evaluated = 1\n",
        "                            WHERE id IN ({placeholders})\n",
        "                        ''', evaluated_ids)\n",
        "\n",
        "                    # Log execution\n",
        "                    cursor.execute('''\n",
        "                        INSERT INTO execution_log\n",
        "                        (timestamp, iteration, status, trades_evaluated, duration_seconds)\n",
        "                        VALUES (?, ?, 'trades_evaluated', ?, ?)\n",
        "                    ''', (\n",
        "                        datetime.now(timezone.utc).isoformat(),\n",
        "                        current_iteration,\n",
        "                        evaluated_count,\n",
        "                        time.time() - start_time\n",
        "                    ))\n",
        "\n",
        "                duration_ms = (time.time() - start_time) * 1000\n",
        "                self._track_performance('evaluate_trades', duration_ms, evaluated_count, True)\n",
        "\n",
        "                print_status(\n",
        "                    f\"‚úÖ Evaluated {evaluated_count} trades in {duration_ms:.0f}ms \"\n",
        "                    f\"({skipped_count} skipped)\",\n",
        "                    \"success\"\n",
        "                )\n",
        "\n",
        "            except sqlite3.Error as e:\n",
        "                print_status(f\"‚ùå Evaluation batch insert failed: {e}\", \"error\")\n",
        "                return {}\n",
        "\n",
        "        # Calculate accuracies\n",
        "        for model_name, results in results_by_model.items():\n",
        "            if results['closed_trades'] > 0:\n",
        "                results['accuracy'] = (results['wins'] / results['closed_trades']) * 100\n",
        "            else:\n",
        "                results['accuracy'] = 0.0\n",
        "\n",
        "        # Update model stats cache\n",
        "        self._update_stats_cache()\n",
        "\n",
        "        return dict(results_by_model)\n",
        "\n",
        "    def _validate_trade_prices(self, entry, sl, tp, current):\n",
        "        \"\"\"Enhanced price validation\"\"\"\n",
        "        try:\n",
        "            if any(p <= 0 for p in [entry, sl, tp, current]):\n",
        "                return False\n",
        "            if any(not np.isfinite(p) for p in [entry, sl, tp, current]):\n",
        "                return False\n",
        "            prices = [entry, sl, tp, current]\n",
        "            avg_price = sum(prices) / len(prices)\n",
        "            for price in prices:\n",
        "                if abs(price - avg_price) / avg_price > 0.5:\n",
        "                    return False\n",
        "            sl_distance = abs(sl - entry) / entry\n",
        "            tp_distance = abs(tp - entry) / entry\n",
        "            if sl_distance > 0.2 or tp_distance > 0.5:\n",
        "                return False\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def _evaluate_trade_outcome(self, prediction, current_price, tp_price, sl_price):\n",
        "        \"\"\"Determine if trade hit TP or SL\"\"\"\n",
        "        hit_tp = False\n",
        "        hit_sl = False\n",
        "        exit_price = None\n",
        "\n",
        "        try:\n",
        "            if prediction == 1:  # Long\n",
        "                if current_price >= tp_price:\n",
        "                    hit_tp = True\n",
        "                    exit_price = tp_price\n",
        "                elif current_price <= sl_price:\n",
        "                    hit_sl = True\n",
        "                    exit_price = sl_price\n",
        "            elif prediction == 0:  # Short\n",
        "                if current_price <= tp_price:\n",
        "                    hit_tp = True\n",
        "                    exit_price = tp_price\n",
        "                elif current_price >= sl_price:\n",
        "                    hit_sl = True\n",
        "                    exit_price = sl_price\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Trade evaluation error: {e}\", \"warn\")\n",
        "\n",
        "        return hit_tp, hit_sl, exit_price\n",
        "\n",
        "    def _calculate_pnl(self, prediction, entry_price, exit_price):\n",
        "        \"\"\"Calculate profit/loss\"\"\"\n",
        "        try:\n",
        "            if prediction == 1:  # Long\n",
        "                return exit_price - entry_price\n",
        "            else:  # Short\n",
        "                return entry_price - exit_price\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_duration_hours(self, created_at):\n",
        "        \"\"\"Calculate trade duration in hours\"\"\"\n",
        "        try:\n",
        "            created_dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))\n",
        "            duration = (datetime.now(timezone.utc) - created_dt).total_seconds() / 3600\n",
        "            return max(0, duration)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _update_stats_cache(self):\n",
        "        \"\"\"Update cached model performance statistics\"\"\"\n",
        "        try:\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                cursor.execute('SELECT DISTINCT pair FROM completed_trades')\n",
        "                pairs = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "                cursor.execute('SELECT DISTINCT model_used FROM completed_trades')\n",
        "                models = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "                for pair in pairs:\n",
        "                    for model in models:\n",
        "                        for days in [7, 30, 90]:\n",
        "                            since = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
        "\n",
        "                            cursor.execute('''\n",
        "                                SELECT\n",
        "                                    COUNT(*) as total,\n",
        "                                    SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins,\n",
        "                                    SUM(CASE WHEN NOT hit_tp THEN 1 ELSE 0 END) as losses,\n",
        "                                    SUM(pnl) as total_pnl,\n",
        "                                    AVG(pnl) as avg_pnl,\n",
        "                                    MAX(pnl) as max_pnl,\n",
        "                                    MIN(pnl) as min_pnl,\n",
        "                                    AVG(duration_hours) as avg_duration\n",
        "                                FROM completed_trades\n",
        "                                WHERE pair = ? AND model_used = ? AND evaluated_at > ?\n",
        "                            ''', (pair, model, since))\n",
        "\n",
        "                            result = cursor.fetchone()\n",
        "                            if not result or not result[0]:\n",
        "                                continue\n",
        "\n",
        "                            total, wins, losses, total_pnl, avg_pnl, max_pnl, min_pnl, avg_duration = result\n",
        "                            accuracy = (wins / total * 100) if total > 0 else 0.0\n",
        "\n",
        "                            cursor.execute('''\n",
        "                                SELECT pnl FROM completed_trades\n",
        "                                WHERE pair = ? AND model_used = ? AND evaluated_at > ?\n",
        "                            ''', (pair, model, since))\n",
        "\n",
        "                            pnls = [row[0] for row in cursor.fetchall()]\n",
        "                            sharpe_ratio = 0.0\n",
        "                            max_drawdown = 0.0\n",
        "\n",
        "                            if len(pnls) > 1:\n",
        "                                pnl_std = np.std(pnls)\n",
        "                                if pnl_std > 0:\n",
        "                                    sharpe_ratio = (avg_pnl or 0) / pnl_std\n",
        "                                cumulative_pnl = np.cumsum(pnls)\n",
        "                                running_max = np.maximum.accumulate(cumulative_pnl)\n",
        "                                drawdown = running_max - cumulative_pnl\n",
        "                                max_drawdown = np.max(drawdown) if len(drawdown) > 0 else 0.0\n",
        "\n",
        "                            cursor.execute('''\n",
        "                                INSERT OR REPLACE INTO model_stats_cache\n",
        "                                (updated_at, pair, model_name, days, total_trades,\n",
        "                                 winning_trades, losing_trades, accuracy_pct,\n",
        "                                 total_pnl, avg_pnl, max_pnl, min_pnl,\n",
        "                                 sharpe_ratio, max_drawdown, avg_duration_hours)\n",
        "                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                            ''', (\n",
        "                                datetime.now(timezone.utc).isoformat(),\n",
        "                                pair, model, days, total, wins or 0, losses or 0,\n",
        "                                accuracy, total_pnl or 0.0, avg_pnl or 0.0,\n",
        "                                max_pnl or 0.0, min_pnl or 0.0,\n",
        "                                sharpe_ratio, max_drawdown, avg_duration or 0.0\n",
        "                            ))\n",
        "\n",
        "                print_status(\"‚úÖ Stats cache updated\", \"debug\")\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ö†Ô∏è Stats cache update failed: {e}\", \"warn\")\n",
        "\n",
        "    def get_model_performance(self, pair, model_name, days=7):\n",
        "        \"\"\"Get comprehensive model performance metrics\"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    SELECT total_trades, winning_trades, losing_trades,\n",
        "                           accuracy_pct, total_pnl, avg_pnl, max_pnl, min_pnl,\n",
        "                           sharpe_ratio, max_drawdown, avg_duration_hours, updated_at\n",
        "                    FROM model_stats_cache\n",
        "                    WHERE pair = ? AND model_name = ? AND days = ?\n",
        "                ''', (pair, model_name, days))\n",
        "\n",
        "                result = cursor.fetchone()\n",
        "\n",
        "                if not result:\n",
        "                    return {\n",
        "                        'total_trades': 0,\n",
        "                        'winning_trades': 0,\n",
        "                        'losing_trades': 0,\n",
        "                        'accuracy': 0.0,\n",
        "                        'total_pnl': 0.0,\n",
        "                        'avg_pnl': 0.0,\n",
        "                        'max_pnl': 0.0,\n",
        "                        'min_pnl': 0.0,\n",
        "                        'sharpe_ratio': 0.0,\n",
        "                        'max_drawdown': 0.0,\n",
        "                        'avg_duration_hours': 0.0\n",
        "                    }\n",
        "\n",
        "                (total, wins, losses, accuracy, total_pnl, avg_pnl,\n",
        "                 max_pnl, min_pnl, sharpe, drawdown, avg_duration, updated_at) = result\n",
        "\n",
        "                return {\n",
        "                    'total_trades': total,\n",
        "                    'winning_trades': wins,\n",
        "                    'losing_trades': losses,\n",
        "                    'accuracy': accuracy,\n",
        "                    'total_pnl': total_pnl,\n",
        "                    'avg_pnl': avg_pnl,\n",
        "                    'max_pnl': max_pnl,\n",
        "                    'min_pnl': min_pnl,\n",
        "                    'sharpe_ratio': sharpe,\n",
        "                    'max_drawdown': drawdown,\n",
        "                    'avg_duration_hours': avg_duration,\n",
        "                    'updated_at': updated_at\n",
        "                }\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to get model performance: {e}\", \"warn\")\n",
        "            return {\n",
        "                'total_trades': 0,\n",
        "                'winning_trades': 0,\n",
        "                'losing_trades': 0,\n",
        "                'accuracy': 0.0,\n",
        "                'total_pnl': 0.0,\n",
        "                'avg_pnl': 0.0,\n",
        "                'max_pnl': 0.0,\n",
        "                'min_pnl': 0.0,\n",
        "                'sharpe_ratio': 0.0,\n",
        "                'max_drawdown': 0.0,\n",
        "                'avg_duration_hours': 0.0\n",
        "            }\n",
        "\n",
        "    def get_best_model(self, pair, days=7, min_trades=3):\n",
        "        \"\"\"Determine best model using multiple criteria\"\"\"\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    SELECT model_name, accuracy_pct, total_trades,\n",
        "                           total_pnl, sharpe_ratio, avg_pnl\n",
        "                    FROM model_stats_cache\n",
        "                    WHERE pair = ? AND days = ? AND total_trades >= ?\n",
        "                    ORDER BY\n",
        "                        accuracy_pct DESC,\n",
        "                        sharpe_ratio DESC,\n",
        "                        total_pnl DESC\n",
        "                    LIMIT 1\n",
        "                ''', (pair, days, min_trades))\n",
        "\n",
        "                result = cursor.fetchone()\n",
        "\n",
        "                if result:\n",
        "                    model_name, accuracy, trades, pnl, sharpe, avg_pnl = result\n",
        "                    print_status(\n",
        "                        f\"üèÜ Best model for {pair}: {model_name} \"\n",
        "                        f\"(Acc: {accuracy:.1f}%, Sharpe: {sharpe:.2f}, \"\n",
        "                        f\"PnL: ${pnl:.5f})\",\n",
        "                        \"performance\"\n",
        "                    )\n",
        "                    return model_name\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to get best model: {e}\", \"warn\")\n",
        "\n",
        "        return 'Ensemble'\n",
        "\n",
        "    def get_database_stats(self):\n",
        "        \"\"\"Get comprehensive database statistics\"\"\"\n",
        "        stats = {}\n",
        "\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('SELECT COUNT(*) FROM pending_trades WHERE evaluated = 0')\n",
        "                stats['pending_trades'] = cursor.fetchone()[0]\n",
        "\n",
        "                cursor.execute('SELECT COUNT(*) FROM completed_trades')\n",
        "                stats['completed_trades'] = cursor.fetchone()[0]\n",
        "\n",
        "                cursor.execute('SELECT SUM(pnl) FROM completed_trades')\n",
        "                result = cursor.fetchone()\n",
        "                stats['total_pnl'] = result[0] if result[0] else 0.0\n",
        "\n",
        "                cursor.execute('''\n",
        "                    SELECT\n",
        "                        COUNT(*) as total,\n",
        "                        SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins\n",
        "                    FROM completed_trades\n",
        "                ''')\n",
        "                result = cursor.fetchone()\n",
        "                if result and result[0] > 0:\n",
        "                    stats['overall_accuracy'] = (result[1] / result[0]) * 100\n",
        "                else:\n",
        "                    stats['overall_accuracy'] = 0.0\n",
        "\n",
        "                cursor.execute('SELECT AVG(duration_hours) FROM completed_trades')\n",
        "                result = cursor.fetchone()\n",
        "                stats['avg_duration_hours'] = result[0] if result[0] else 0.0\n",
        "\n",
        "                cursor.execute('''\n",
        "                    SELECT model_used, COUNT(*) as trades,\n",
        "                           SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins\n",
        "                    FROM completed_trades\n",
        "                    GROUP BY model_used\n",
        "                    ORDER BY wins DESC\n",
        "                    LIMIT 1\n",
        "                ''')\n",
        "                result = cursor.fetchone()\n",
        "                if result:\n",
        "                    stats['best_model'] = result[0]\n",
        "                    stats['best_model_trades'] = result[1]\n",
        "                    stats['best_model_wins'] = result[2]\n",
        "                else:\n",
        "                    stats['best_model'] = 'None'\n",
        "                    stats['best_model_trades'] = 0\n",
        "                    stats['best_model_wins'] = 0\n",
        "\n",
        "                if self.db_path.exists():\n",
        "                    stats['db_size_mb'] = self.db_path.stat().st_size / (1024 * 1024)\n",
        "                else:\n",
        "                    stats['db_size_mb'] = 0.0\n",
        "\n",
        "                yesterday = (datetime.now(timezone.utc) - timedelta(days=1)).isoformat()\n",
        "                cursor.execute('''\n",
        "                    SELECT COUNT(*) FROM completed_trades\n",
        "                    WHERE evaluated_at > ?\n",
        "                ''', (yesterday,))\n",
        "                stats['trades_last_24h'] = cursor.fetchone()[0]\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to get database stats: {e}\", \"warn\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def cleanup_old_data(self, days_to_keep=90):\n",
        "        \"\"\"Clean up old data with backup\"\"\"\n",
        "        cutoff_date = (datetime.now(timezone.utc) - timedelta(days=days_to_keep)).isoformat()\n",
        "\n",
        "        try:\n",
        "            self.create_backup()\n",
        "\n",
        "            with self.lock, self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    DELETE FROM pending_trades\n",
        "                    WHERE evaluated = 1 AND created_at < ?\n",
        "                ''', (cutoff_date,))\n",
        "                deleted_pending = cursor.rowcount\n",
        "\n",
        "                cursor.execute('''\n",
        "                    DELETE FROM execution_log\n",
        "                    WHERE timestamp < ?\n",
        "                ''', (cutoff_date,))\n",
        "                deleted_logs = cursor.rowcount\n",
        "\n",
        "                cursor.execute('''\n",
        "                    DELETE FROM performance_metrics\n",
        "                    WHERE timestamp < ?\n",
        "                ''', (cutoff_date,))\n",
        "                deleted_metrics = cursor.rowcount\n",
        "\n",
        "                print_status(\n",
        "                    f\"üßπ Cleanup complete: Removed {deleted_pending} pending trades, \"\n",
        "                    f\"{deleted_logs} logs, {deleted_metrics} metrics\",\n",
        "                    \"success\"\n",
        "                )\n",
        "\n",
        "                self._optimize_database()\n",
        "\n",
        "        except sqlite3.Error as e:\n",
        "            print_status(f\"‚ùå Cleanup failed: {e}\", \"error\")\n",
        "\n",
        "    def create_backup(self):\n",
        "        \"\"\"Create database backup\"\"\"\n",
        "        try:\n",
        "            if not self.db_path.exists():\n",
        "                return\n",
        "\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            backup_path = BACKUP_FOLDER / f\"db_backup_{timestamp}.db\"\n",
        "\n",
        "            shutil.copy2(self.db_path, backup_path)\n",
        "\n",
        "            backups = sorted(BACKUP_FOLDER.glob(\"db_backup_*.db\"))\n",
        "            while len(backups) > 5:\n",
        "                oldest = backups.pop(0)\n",
        "                oldest.unlink()\n",
        "                print_status(f\"üóëÔ∏è Removed old backup: {oldest.name}\", \"debug\")\n",
        "\n",
        "            print_status(f\"üíæ Backup created: {backup_path.name}\", \"success\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Backup failed: {e}\", \"warn\")\n",
        "\n",
        "    def get_performance_report(self, days=7):\n",
        "        \"\"\"Generate comprehensive performance report\"\"\"\n",
        "        report = {\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "            'period_days': days,\n",
        "            'models': {},\n",
        "            'pairs': {},\n",
        "            'overall': {}\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with self.get_cursor() as cursor:\n",
        "                cursor.execute('''\n",
        "                    SELECT pair, model_name, total_trades, accuracy_pct,\n",
        "                           total_pnl, sharpe_ratio, max_drawdown\n",
        "                    FROM model_stats_cache\n",
        "                    WHERE days = ?\n",
        "                    ORDER BY total_trades DESC\n",
        "                ''', (days,))\n",
        "\n",
        "                for row in cursor.fetchall():\n",
        "                    pair, model, trades, acc, pnl, sharpe, drawdown = row\n",
        "\n",
        "                    if model not in report['models']:\n",
        "                        report['models'][model] = {\n",
        "                            'total_trades': 0,\n",
        "                            'total_pnl': 0.0,\n",
        "                            'avg_accuracy': 0.0,\n",
        "                            'pairs': []\n",
        "                        }\n",
        "\n",
        "                    report['models'][model]['total_trades'] += trades\n",
        "                    report['models'][model]['total_pnl'] += pnl\n",
        "                    report['models'][model]['pairs'].append({\n",
        "                        'pair': pair,\n",
        "                        'accuracy': acc,\n",
        "                        'pnl': pnl\n",
        "                    })\n",
        "\n",
        "                    if pair not in report['pairs']:\n",
        "                        report['pairs'][pair] = {\n",
        "                            'total_trades': 0,\n",
        "                            'best_model': None,\n",
        "                            'best_accuracy': 0.0\n",
        "                        }\n",
        "\n",
        "                    report['pairs'][pair]['total_trades'] += trades\n",
        "                    if acc > report['pairs'][pair]['best_accuracy']:\n",
        "                        report['pairs'][pair]['best_model'] = model\n",
        "                        report['pairs'][pair]['best_accuracy'] = acc\n",
        "\n",
        "                for model, data in report['models'].items():\n",
        "                    if len(data['pairs']) > 0:\n",
        "                        data['avg_accuracy'] = sum(p['accuracy'] for p in data['pairs']) / len(data['pairs'])\n",
        "\n",
        "                stats = self.get_database_stats()\n",
        "                report['overall'] = stats\n",
        "\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Failed to generate report: {e}\", \"warn\")\n",
        "\n",
        "        return report\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close database connection with cleanup\"\"\"\n",
        "        try:\n",
        "            if self.conn:\n",
        "                self._optimize_database()\n",
        "                self.conn.close()\n",
        "                print_status(\"‚úÖ Database connection closed\", \"success\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Error closing database: {e}\", \"warn\")\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Destructor to ensure connection is closed\"\"\"\n",
        "        self.close()\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# üéØ USAGE EXAMPLE & TEST\n",
        "# ======================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print_status(\"=\"*60, \"info\")\n",
        "    print_status(\"ENHANCED FX PIPELINE v3.7 - Database Test\", \"success\")\n",
        "    print_status(\"=\"*60, \"info\")\n",
        "\n",
        "    # Initialize database\n",
        "    db = EnhancedTradeMemoryDatabase()\n",
        "\n",
        "    # Get database stats\n",
        "    stats = db.get_database_stats()\n",
        "    print_status(\"\\nüìä Current Database Statistics:\", \"info\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    # Example: Store test signals\n",
        "    test_signals = {\n",
        "        'EUR/USD': {\n",
        "            'signals': {\n",
        "                'H1': {\n",
        "                    'live': 1.0950,\n",
        "                    'SL': 1.0920,\n",
        "                    'TP': 1.1000,\n",
        "                    'signal': 1,\n",
        "                    'sgd_pred': 1,\n",
        "                    'rf_pred': 1,\n",
        "                    'confidence': 0.85\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    stored = db.store_new_signals(test_signals, current_iteration=1)\n",
        "    print_status(f\"\\n‚úÖ Stored {stored} test signals\", \"success\")\n",
        "\n",
        "    # Example: Evaluate trades\n",
        "    current_prices = {\n",
        "        'EUR/USD': 1.0980\n",
        "    }\n",
        "\n",
        "    results = db.evaluate_pending_trades(current_prices, current_iteration=2)\n",
        "    if results:\n",
        "        print_status(\"\\nüìà Evaluation Results:\", \"info\")\n",
        "        for model, data in results.items():\n",
        "            print(f\"  {model}: {data['wins']}/{data['closed_trades']} wins \"\n",
        "                  f\"({data['accuracy']:.1f}% accuracy)\")\n",
        "\n",
        "    # Get model performance\n",
        "    print_status(\"\\nüìä Model Performance (Last 7 days):\", \"info\")\n",
        "    for model in ['SGD', 'RandomForest', 'Ensemble']:\n",
        "        perf = db.get_model_performance('EUR/USD', model, days=7)\n",
        "        if perf['total_trades'] > 0:\n",
        "            print(f\"  {model}:\")\n",
        "            print(f\"    Trades: {perf['total_trades']}\")\n",
        "            print(f\"    Accuracy: {perf['accuracy']:.1f}%\")\n",
        "            print(f\"    Total PnL: ${perf['total_pnl']:.5f}\")\n",
        "            print(f\"    Sharpe Ratio: {perf['sharpe_ratio']:.2f}\")\n",
        "\n",
        "    # Get best model\n",
        "    best = db.get_best_model('EUR/USD', days=7, min_trades=3)\n",
        "    print_status(f\"\\nüèÜ Best Model: {best}\", \"performance\")\n",
        "\n",
        "    # Generate performance report\n",
        "    report = db.get_performance_report(days=7)\n",
        "    print_status(\"\\nüìã Performance Report:\", \"info\")\n",
        "    print(f\"  Total Models: {len(report['models'])}\")\n",
        "    print(f\"  Total Pairs: {len(report['pairs'])}\")\n",
        "\n",
        "    for model_name, model_data in report['models'].items():\n",
        "        print(f\"\\n  {model_name}:\")\n",
        "        print(f\"    Total Trades: {model_data['total_trades']}\")\n",
        "        print(f\"    Total PnL: ${model_data['total_pnl']:.5f}\")\n",
        "        print(f\"    Avg Accuracy: {model_data['avg_accuracy']:.1f}%\")\n",
        "\n",
        "    # Cleanup and close\n",
        "    print_status(\"\\nüßπ Running cleanup...\", \"info\")\n",
        "    db.cleanup_old_data(days_to_keep=90)\n",
        "    db.close()\n",
        "\n",
        "    print_status(\"\\n‚úÖ Enhanced Database v3.7 Test Complete!\", \"success\")\n",
        "    print_status(f\"‚úÖ Database location: {PERSISTENT_DB}\", \"success\")\n",
        "    print_status(f\"‚úÖ Fixed for GitHub Actions (no nested paths)\", \"success\")"
      ],
      "metadata": {
        "id": "DuaE5mDEk9dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "VERSION 3.7 ‚Äì Unified Loader + Merge Pickles (Production Ready)\n",
        "‚úÖ UPDATED: Paths for GitHub Actions compatibility\n",
        "‚úÖ UPDATED: Environment detection (Colab/GHA/Local)\n",
        "‚úÖ KEPT: All original logic from Document 6\n",
        "\"\"\"\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import warnings\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime\n",
        "\n",
        "# ======================================================\n",
        "# 0Ô∏è‚É£ UPDATED: Environment Detection & Path Setup\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    ENV_NAME = \"Google Colab\"\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    ENV_NAME = \"Local/GitHub Actions\"\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "if IN_GHA:\n",
        "    ENV_NAME = \"GitHub Actions\"\n",
        "\n",
        "print(f\"üåç Detected Environment: {ENV_NAME}\")\n",
        "\n",
        "# ‚úÖ UPDATED: Dynamic path setup based on environment\n",
        "if IN_COLAB:\n",
        "    ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "    ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "elif IN_GHA:\n",
        "    # GitHub Actions: Use current directory\n",
        "    ROOT_DIR = Path.cwd()\n",
        "    REPO_FOLDER = ROOT_DIR\n",
        "    print(f\"üìÇ GitHub Actions: Using repo root: {ROOT_DIR}\")\n",
        "else:\n",
        "    # Local: Use subdirectory\n",
        "    ROOT_DIR = Path(\"./forex-alpha-models\")\n",
        "    ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "\n",
        "# ‚úÖ UPDATED: Consistent folder structure\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"  # ‚Üê CHANGED: Was \"merged_data_pickles\"\n",
        "TEMP_PICKLE_FOLDER = ROOT_DIR / \"temp_pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, TEMP_PICKLE_FOLDER, LOGS_FOLDER, REPO_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Root Directory: {ROOT_DIR}\")\n",
        "print(f\"‚úÖ CSV Folder: {CSV_FOLDER}\")\n",
        "print(f\"‚úÖ Pickle Folder: {PICKLE_FOLDER}\")\n",
        "print(f\"‚úÖ Temp Folder: {TEMP_PICKLE_FOLDER}\")\n",
        "print(f\"‚úÖ Repo Folder: {REPO_FOLDER}\")\n",
        "\n",
        "JSON_FILE = REPO_FOLDER / \"latest_signals.json\"\n",
        "\n",
        "print(f\"‚úÖ JSON File: {JSON_FILE}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Safe Indicator Generator (UNCHANGED from Document 6)\n",
        "# ======================================================\n",
        "def add_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Add technical indicators to DataFrame\"\"\"\n",
        "    df = df.copy()\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    df = df[(df[[\"open\", \"high\", \"low\", \"close\"]] > 0).all(axis=1)]\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # --- Preserve raw OHLC prices for GA ---\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if col in df.columns:\n",
        "            df[f\"raw_{col}\"] = df[col].copy()\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "\n",
        "        try:\n",
        "            if len(df['close']) >= 10:\n",
        "                df['SMA_10'] = ta.trend.sma_indicator(df['close'], 10)\n",
        "                df['EMA_10'] = ta.trend.ema_indicator(df['close'], 10)\n",
        "            if len(df['close']) >= 50:\n",
        "                df['SMA_50'] = ta.trend.sma_indicator(df['close'], 50)\n",
        "                df['EMA_50'] = ta.trend.ema_indicator(df['close'], 50)\n",
        "            if len(df['close']) >= 14:\n",
        "                df['RSI_14'] = ta.momentum.rsi(df['close'], 14)\n",
        "            if all(col in df.columns for col in ['high', 'low', 'close']) and len(df['close']) >= 14:\n",
        "                df['Williams_%R'] = WilliamsRIndicator(df['high'], df['low'], df['close'], 14).williams_r()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Indicator calculation failed: {e}\")\n",
        "\n",
        "        # --- Safe ATR ---\n",
        "        try:\n",
        "            if all(col in df.columns for col in ['high', 'low', 'close']):\n",
        "                window = 14\n",
        "                if len(df) >= window:\n",
        "                    df['ATR'] = AverageTrueRange(\n",
        "                        df['high'], df['low'], df['close'], window=window\n",
        "                    ).average_true_range().fillna(1e-5).clip(lower=1e-4)\n",
        "                else:\n",
        "                    df['ATR'] = 1e-4\n",
        "        except Exception as e:\n",
        "            df['ATR'] = 1e-4\n",
        "            print(f\"‚ö†Ô∏è ATR calculation failed: {e}\")\n",
        "\n",
        "        # --- Scale only non-price numeric columns ---\n",
        "        numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns if not df[c].isna().all()]\n",
        "        protected_cols = [\n",
        "            \"open\", \"high\", \"low\", \"close\",\n",
        "            \"raw_open\", \"raw_high\", \"raw_low\", \"raw_close\"\n",
        "        ]\n",
        "        numeric_cols = [c for c in numeric_cols if c not in protected_cols]\n",
        "\n",
        "        if numeric_cols:\n",
        "            scaler = MinMaxScaler()\n",
        "            df[numeric_cols] = scaler.fit_transform(df[numeric_cols].fillna(0) + 1e-8)\n",
        "\n",
        "    return df\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Safe CSV Processing (UNCHANGED from Document 6)\n",
        "# ======================================================\n",
        "def process_csv_file(csv_file: Path, save_folder: Path):\n",
        "    \"\"\"Process a single CSV file and save as pickle\"\"\"\n",
        "    try:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\", category=pd.errors.ParserWarning)\n",
        "            df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö™ Skipped empty CSV: {csv_file.name}\")\n",
        "            return None\n",
        "\n",
        "        df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "        df = add_indicators(df)\n",
        "        if df.empty:\n",
        "            print(f\"‚ö™ Skipped CSV after filtering invalid prices: {csv_file.name}\")\n",
        "            return None\n",
        "\n",
        "        out_file = save_folder / f\"{csv_file.stem}.pkl\"\n",
        "        df.to_pickle(out_file)\n",
        "        print(f\"‚úÖ Processed CSV {csv_file.name} ‚Üí {out_file.name}\")\n",
        "        return out_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed CSV {csv_file.name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ JSON Processing (UNCHANGED from Document 6)\n",
        "# ======================================================\n",
        "def process_json_file(json_file: Path, save_folder: Path):\n",
        "    \"\"\"Process JSON signals and save as pickles\"\"\"\n",
        "    try:\n",
        "        with open(json_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load JSON: {e}\")\n",
        "        return []\n",
        "\n",
        "    signals_data = data.get(\"pairs\", {})\n",
        "    timestamp = pd.to_datetime(data.get(\"timestamp\"), utc=True)\n",
        "    processed_files = []\n",
        "\n",
        "    print(f\"üìä Processing JSON signals for {len(signals_data)} pairs...\")\n",
        "\n",
        "    for pair, info in signals_data.items():\n",
        "        signals = info.get(\"signals\", {})\n",
        "        dfs = []\n",
        "\n",
        "        for tf_name, tf_info in signals.items():\n",
        "            # Validate price data\n",
        "            live = tf_info.get(\"live\")\n",
        "            sl = tf_info.get(\"SL\")\n",
        "            tp = tf_info.get(\"TP\")\n",
        "\n",
        "            if not all([live, sl, tp]) or any(v <= 0 for v in [live, sl, tp] if v is not None):\n",
        "                print(f\"‚ö†Ô∏è No valid price data after filtering\")\n",
        "                continue\n",
        "\n",
        "            df = pd.DataFrame({\n",
        "                \"live\": [live],\n",
        "                \"SL\": [sl],\n",
        "                \"TP\": [tp],\n",
        "                \"signal\": [tf_info.get(\"signal\")]\n",
        "            }, index=[timestamp])\n",
        "            df[\"timeframe\"] = tf_name\n",
        "            df = add_indicators(df)\n",
        "            if not df.empty:\n",
        "                dfs.append(df)\n",
        "\n",
        "        if dfs:\n",
        "            df_pair = pd.concat(dfs)\n",
        "            out_file = save_folder / f\"{pair.replace('/', '_')}.pkl\"\n",
        "            df_pair.to_pickle(out_file)\n",
        "            print(f\"‚úÖ Processed JSON {pair} ‚Üí {out_file.name}\")\n",
        "            processed_files.append(out_file)\n",
        "\n",
        "    return processed_files\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Safe Pickle Merger (UNCHANGED from Document 6)\n",
        "# ======================================================\n",
        "def merge_pickles(temp_folder: Path, final_folder: Path, keep_last: int = 5):\n",
        "    \"\"\"Merge temporary pickles into final consolidated pickles\"\"\"\n",
        "    pickles = list(temp_folder.glob(\"*.pkl\"))\n",
        "    if not pickles:\n",
        "        print(\"‚ö™ No temporary pickles to merge.\")\n",
        "        return\n",
        "\n",
        "    pairs = set(p.stem.split('.')[0] for p in pickles)\n",
        "\n",
        "    for pair in pairs:\n",
        "        pair_files = [p for p in pickles if p.stem.startswith(pair)]\n",
        "        dfs = [pd.read_pickle(p) for p in pair_files if p.exists() and p.stat().st_size > 0]\n",
        "\n",
        "        if not dfs:\n",
        "            print(f\"‚ö™ Skipped {pair} (no valid pickles)\")\n",
        "            continue\n",
        "\n",
        "        merged_df = pd.concat(dfs, ignore_index=False).sort_index().drop_duplicates()\n",
        "        # Create final merged pickle with _2244 suffix\n",
        "        merged_file = final_folder / f\"{pair}_2244.pkl\"\n",
        "        merged_df.to_pickle(merged_file)\n",
        "        print(f\"üîó Merged {len(pair_files)} files ‚Üí {merged_file.name}\")\n",
        "\n",
        "        # Clean up old versions (keep only last N)\n",
        "        existing = sorted(final_folder.glob(f\"{pair}_*.pkl\"), key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "        for old_file in existing[keep_last:]:\n",
        "            try:\n",
        "                old_file.unlink()\n",
        "                print(f\"üßπ Removed old file: {old_file.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not remove {old_file.name}: {e}\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Unified Pipeline Runner\n",
        "# ======================================================\n",
        "def run_unified_pipeline():\n",
        "    \"\"\"Main pipeline execution\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üöÄ UNIFIED PICKLE MERGER v3.7\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Environment: {ENV_NAME}\")\n",
        "    print(f\"Root: {ROOT_DIR}\")\n",
        "    print(f\"CSV Folder: {CSV_FOLDER}\")\n",
        "    print(f\"Output Folder: {PICKLE_FOLDER}\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "    temp_files = []\n",
        "\n",
        "    # ===== Step 1: Process JSON signals =====\n",
        "    print(\"üìã Step 1: Processing JSON signals...\")\n",
        "    if JSON_FILE.exists():\n",
        "        temp_files += process_json_file(JSON_FILE, TEMP_PICKLE_FOLDER)\n",
        "        print(f\"‚úÖ JSON processing complete: {len(temp_files)} files\")\n",
        "    else:\n",
        "        print(f\"‚ö™ No JSON file found at {JSON_FILE}\")\n",
        "\n",
        "    # ===== Step 2: Process CSV files =====\n",
        "    print(\"\\nüìã Step 2: Processing CSV files...\")\n",
        "    # ‚úÖ UPDATED: Look in ROOT directory as well as CSV folder\n",
        "    csv_locations = [CSV_FOLDER, ROOT_DIR]\n",
        "    csv_files = []\n",
        "\n",
        "    for location in csv_locations:\n",
        "        found = list(location.glob(\"*.csv\"))\n",
        "        if found:\n",
        "            csv_files.extend(found)\n",
        "\n",
        "    # Remove duplicates\n",
        "    csv_files = list(set(csv_files))\n",
        "\n",
        "    if csv_files:\n",
        "        print(f\"üìä Found {len(csv_files)} CSV files\")\n",
        "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "            futures = [executor.submit(process_csv_file, f, TEMP_PICKLE_FOLDER) for f in csv_files]\n",
        "            for fut in as_completed(futures):\n",
        "                result = fut.result()\n",
        "                if result:\n",
        "                    temp_files.append(result)\n",
        "    else:\n",
        "        print(\"‚ö™ No CSV files found\")\n",
        "\n",
        "    # ===== Step 3: Merge all pickles =====\n",
        "    print(\"\\nüìã Step 3: Merging pickle files...\")\n",
        "    merge_pickles(TEMP_PICKLE_FOLDER, PICKLE_FOLDER)\n",
        "\n",
        "    # ===== Final verification =====\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä FINAL OUTPUT VERIFICATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    final_pickles = list(PICKLE_FOLDER.glob(\"*_2244.pkl\"))\n",
        "    if final_pickles:\n",
        "        print(f\"‚úÖ Created {len(final_pickles)} merged pickle files:\")\n",
        "        for pkl in final_pickles:\n",
        "            df = pd.read_pickle(pkl)\n",
        "            print(f\"  ‚Ä¢ {pkl.name}: {len(df)} rows\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No merged pickle files created!\")\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üéØ Unified pipeline complete!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(f\"\\n‚úÖ Pipeline completed successfully!\")\n",
        "    print(f\"üìÅ Final pickles saved in: {PICKLE_FOLDER}\")\n",
        "\n",
        "    return PICKLE_FOLDER\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Execute\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "    import os  # Import here for IN_GHA check\n",
        "    final_folder = run_unified_pipeline()"
      ],
      "metadata": {
        "id": "P-EaMD_T6L_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TAG: pipeline_main\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Ultimate Forex Pipeline v8.5.2 - FIXED FOR GITHUB ACTIONS\n",
        "==========================================================\n",
        "‚úÖ FIXED: No nested paths (forex-alpha-models/forex-ai-models)\n",
        "‚úÖ FIXED: All files save to repo root in GitHub Actions\n",
        "‚úÖ Enhanced Git operations with proper error handling\n",
        "‚úÖ Memory system using pickle (lightweight)\n",
        "‚úÖ All v8.5.1 features preserved + path corrections\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import smtplib\n",
        "import subprocess\n",
        "import time\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# ======================================================\n",
        "# FIXED CONFIGURATION & PATH SETUP\n",
        "# ======================================================\n",
        "logging.basicConfig(\n",
        "    filename='forex_pipeline_v85.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(message)s'\n",
        ")\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    icons = {\"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"warn\": \"‚ö†Ô∏è\", \"error\": \"‚ùå\",\n",
        "             \"rocket\": \"üöÄ\", \"chart\": \"üìä\", \"brain\": \"üß†\", \"money\": \"üí∞\"}\n",
        "    getattr(logging, level if level != \"warn\" else \"warning\", logging.info)(msg)\n",
        "    print(f\"{icons.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# ‚úÖ FIXED: Environment detection\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    IN_GHA = False\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "\n",
        "ENV_NAME = \"GitHub Actions\" if IN_GHA else (\"Colab\" if IN_COLAB else \"Local\")\n",
        "print_status(f\"üåç Environment: {ENV_NAME}\", \"info\")\n",
        "\n",
        "# ‚úÖ FIXED: Path setup - NO MORE NESTED DIRECTORIES\n",
        "if IN_GHA:\n",
        "    # GitHub Actions: Use current directory directly\n",
        "    ROOT_PATH = Path.cwd()\n",
        "    REPO_FOLDER = ROOT_PATH  # No nesting!\n",
        "    PICKLE_FOLDER = ROOT_PATH / \"pickles\"\n",
        "    print_status(f\"ü§ñ GitHub Actions: Using {ROOT_PATH}\", \"info\")\n",
        "elif IN_COLAB:\n",
        "    # Colab: Use standard path\n",
        "    ROOT_PATH = Path(\"/content/forex-alpha-models\")\n",
        "    REPO_FOLDER = ROOT_PATH / \"forex-ai-models\"\n",
        "    PICKLE_FOLDER = ROOT_PATH / \"pickles\"\n",
        "else:\n",
        "    # Local: Use relative path\n",
        "    ROOT_PATH = Path(\"./forex-alpha-models\")\n",
        "    REPO_FOLDER = ROOT_PATH / \"forex-ai-models\"\n",
        "    PICKLE_FOLDER = ROOT_PATH / \"pickles\"\n",
        "\n",
        "# Create directories\n",
        "for folder in [PICKLE_FOLDER, REPO_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print_status(f\"‚úÖ Root: {ROOT_PATH}\", \"success\")\n",
        "print_status(f\"‚úÖ Repo: {REPO_FOLDER}\", \"success\")\n",
        "print_status(f\"‚úÖ Pickles: {PICKLE_FOLDER}\", \"success\")\n",
        "\n",
        "# Git configuration\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "\n",
        "if not IN_GHA:\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "    subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "\n",
        "# Email configuration\n",
        "GMAIL_USER = os.environ.get(\"GMAIL_USER\", \"nakatonabira3@gmail.com\")\n",
        "GMAIL_APP_PASSWORD = os.environ.get(\"GMAIL_APP_PASSWORD\", \"\")\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/rahim-dotAI/forex-ai-models/main/IMG_1599.jpeg\"\n",
        "\n",
        "# Trading parameters\n",
        "PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "ATR_PERIOD = 14\n",
        "MIN_ATR = 1e-5\n",
        "BASE_CAPITAL = 100\n",
        "MAX_POSITION_FRACTION = 0.1\n",
        "MAX_TRADE_CAP = BASE_CAPITAL * 0.05\n",
        "EPS = 1e-8\n",
        "MAX_ATR_SL = 3.0\n",
        "MAX_ATR_TP = 3.0\n",
        "TOURNAMENT_SIZE = 3\n",
        "SLIPPAGE_PCT = 0.0001\n",
        "COMMISSION_PCT = 0.0002\n",
        "\n",
        "# ‚úÖ FIXED: File paths - All in REPO_FOLDER (no nesting)\n",
        "SIGNALS_JSON_PATH = REPO_FOLDER / \"broker_signals.json\"\n",
        "ENSEMBLE_SIGNALS_FILE = REPO_FOLDER / \"ensemble_signals.json\"\n",
        "LEARNING_FILE = REPO_FOLDER / \"learning_v85.pkl\"\n",
        "ITERATION_FILE = REPO_FOLDER / \"iteration_v85.pkl\"\n",
        "MEMORY_FILE = REPO_FOLDER / \"memory_v85.pkl\"  # Using pickle instead of sqlite\n",
        "WEIGHTS_FILE = REPO_FOLDER / \"weights_v85.pkl\"\n",
        "MONDAY_FILE = REPO_FOLDER / \"monday_runs.pkl\"\n",
        "\n",
        "# Ensure parent directories exist\n",
        "for file_path in [SIGNALS_JSON_PATH, LEARNING_FILE, ITERATION_FILE, MEMORY_FILE]:\n",
        "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print_status(f\"üìÅ Memory file: {MEMORY_FILE}\", \"info\")\n",
        "print_status(f\"üìÅ Learning file: {LEARNING_FILE}\", \"info\")\n",
        "print_status(f\"üìÅ Iteration file: {ITERATION_FILE}\", \"info\")\n",
        "\n",
        "# Model configurations\n",
        "COMPETITION_MODELS = {\n",
        "    \"Alpha Momentum\": {\n",
        "        \"color\": \"üî¥\", \"hex_color\": \"#E74C3C\",\n",
        "        \"strategy\": \"Aggressive momentum with adaptive stops\",\n",
        "        \"atr_sl_range\": (1.5, 2.5), \"atr_tp_range\": (2.0, 3.5),\n",
        "        \"risk_range\": (0.015, 0.03), \"confidence_range\": (0.3, 0.5),\n",
        "        \"pop_size\": 15, \"generations\": 20, \"mutation_rate\": 0.3\n",
        "    },\n",
        "    \"Beta Conservative\": {\n",
        "        \"color\": \"üîµ\", \"hex_color\": \"#3498DB\",\n",
        "        \"strategy\": \"Conservative mean reversion\",\n",
        "        \"atr_sl_range\": (1.0, 1.8), \"atr_tp_range\": (1.5, 2.5),\n",
        "        \"risk_range\": (0.005, 0.015), \"confidence_range\": (0.5, 0.7),\n",
        "        \"pop_size\": 12, \"generations\": 15, \"mutation_rate\": 0.2\n",
        "    },\n",
        "    \"Gamma Adaptive\": {\n",
        "        \"color\": \"üü¢\", \"hex_color\": \"#2ECC71\",\n",
        "        \"strategy\": \"Adaptive volatility trading\",\n",
        "        \"atr_sl_range\": (1.2, 2.2), \"atr_tp_range\": (1.8, 3.0),\n",
        "        \"risk_range\": (0.01, 0.025), \"confidence_range\": (0.4, 0.6),\n",
        "        \"pop_size\": 18, \"generations\": 22, \"mutation_rate\": 0.25\n",
        "    }\n",
        "}\n",
        "\n",
        "# ======================================================\n",
        "# ITERATION COUNTER\n",
        "# ======================================================\n",
        "class IterationCounter:\n",
        "    def __init__(self, file=ITERATION_FILE):\n",
        "        self.file = file\n",
        "        self.data = self._load()\n",
        "        print_status(f\"üìä Iteration counter: Total runs = {self.data['total']}\", \"info\")\n",
        "\n",
        "    def _load(self):\n",
        "        if self.file.exists():\n",
        "            try:\n",
        "                with open(self.file, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "                    print_status(f\"‚úÖ Loaded: {data['total']} iterations\", \"success\")\n",
        "                    return data\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ö†Ô∏è Load failed: {e}, creating new\", \"warn\")\n",
        "        return {'total': 0, 'start': datetime.now(timezone.utc).isoformat(), 'history': []}\n",
        "\n",
        "    def increment(self, success=True):\n",
        "        self.data['total'] += 1\n",
        "        self.data['history'].append({\n",
        "            'iteration': self.data['total'],\n",
        "            'time': datetime.now(timezone.utc).isoformat(),\n",
        "            'success': success\n",
        "        })\n",
        "        if len(self.data['history']) > 1000:\n",
        "            self.data['history'] = self.data['history'][-1000:]\n",
        "        try:\n",
        "            with open(self.file, 'wb') as f:\n",
        "                pickle.dump(self.data, f, protocol=4)\n",
        "            print_status(f\"‚úÖ Saved: Iteration #{self.data['total']}\", \"success\")\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Save failed: {e}\", \"error\")\n",
        "        return self.data['total']\n",
        "\n",
        "    def get_stats(self):\n",
        "        days = max(1, (datetime.now(timezone.utc) - datetime.fromisoformat(self.data['start'])).days)\n",
        "        return {\n",
        "            'total': self.data['total'],\n",
        "            'days': days,\n",
        "            'per_day': self.data['total'] / days,\n",
        "            'start': self.data['start']\n",
        "        }\n",
        "\n",
        "COUNTER = IterationCounter()\n",
        "\n",
        "# ======================================================\n",
        "# MEMORY SYSTEM - PICKLE-BASED (Lightweight)\n",
        "# ======================================================\n",
        "class MemorySystem:\n",
        "    \"\"\"Lightweight pickle-based memory system\"\"\"\n",
        "\n",
        "    def __init__(self, file=MEMORY_FILE):\n",
        "        self.file = file\n",
        "        self.data = self._load()\n",
        "        print_status(f\"üìÅ Memory system initialized: {len(self.data['signals'])} signals\", \"info\")\n",
        "\n",
        "    def _load(self):\n",
        "        if self.file.exists():\n",
        "            try:\n",
        "                with open(self.file, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "                    print_status(f\"‚úÖ Loaded memory: {len(data.get('signals', []))} signals\", \"success\")\n",
        "                    return data\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ö†Ô∏è Memory load failed: {e}\", \"warn\")\n",
        "\n",
        "        return {\n",
        "            'signals': [],\n",
        "            'trades': [],\n",
        "            'performance': {},\n",
        "            'created_at': datetime.now(timezone.utc).isoformat()\n",
        "        }\n",
        "\n",
        "    def _save(self):\n",
        "        try:\n",
        "            self.data['updated_at'] = datetime.now(timezone.utc).isoformat()\n",
        "            with open(self.file, 'wb') as f:\n",
        "                pickle.dump(self.data, f, protocol=4)\n",
        "        except Exception as e:\n",
        "            print_status(f\"‚ö†Ô∏è Memory save failed: {e}\", \"error\")\n",
        "\n",
        "    def store_signals(self, signals_by_model, timestamp):\n",
        "        \"\"\"Store signals from all models\"\"\"\n",
        "        for model_name, signals in signals_by_model.items():\n",
        "            for pair, sig in signals.items():\n",
        "                if sig['direction'] != 'HOLD':\n",
        "                    self.data['signals'].append({\n",
        "                        'timestamp': timestamp.isoformat(),\n",
        "                        'model': model_name,\n",
        "                        'pair': pair,\n",
        "                        'direction': sig['direction'],\n",
        "                        'entry': sig['last_price'],\n",
        "                        'sl': sig['SL'],\n",
        "                        'tp': sig['TP'],\n",
        "                        'confidence': sig['score_1_100']\n",
        "                    })\n",
        "\n",
        "        # Keep only last 1000 signals\n",
        "        if len(self.data['signals']) > 1000:\n",
        "            self.data['signals'] = self.data['signals'][-1000:]\n",
        "\n",
        "        self._save()\n",
        "\n",
        "    def get_history(self, model_name, days=7):\n",
        "        \"\"\"Get historical performance\"\"\"\n",
        "        since = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
        "\n",
        "        trades = [t for t in self.data.get('trades', [])\n",
        "                 if t.get('model') == model_name and t.get('timestamp', '') > since]\n",
        "\n",
        "        total = len(trades)\n",
        "        wins = sum(1 for t in trades if t.get('hit_tp', False))\n",
        "\n",
        "        return {\n",
        "            'total_trades': total,\n",
        "            'wins': wins,\n",
        "            'accuracy': (wins / total * 100) if total > 0 else 0,\n",
        "            'total_pnl': sum(t.get('pnl', 0) for t in trades),\n",
        "            'avg_pnl': sum(t.get('pnl', 0) for t in trades) / total if total > 0 else 0\n",
        "        }\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Save and close\"\"\"\n",
        "        self._save()\n",
        "        print_status(\"‚úÖ Memory system saved\", \"success\")\n",
        "\n",
        "MEMORY = MemorySystem()\n",
        "\n",
        "# ======================================================\n",
        "# LEARNING SYSTEM\n",
        "# ======================================================\n",
        "class LearningSystem:\n",
        "    def __init__(self, file=LEARNING_FILE):\n",
        "        self.file = file\n",
        "        print_status(f\"üìö Learning system: {self.file}\", \"info\")\n",
        "        self.data = self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        if self.file.exists():\n",
        "            try:\n",
        "                with open(self.file, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "                    print_status(f\"‚úÖ Loaded learning: {data['iterations']} iterations\", \"success\")\n",
        "                    return data\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ö†Ô∏è Learning load failed: {e}\", \"warn\")\n",
        "        return {\n",
        "            'iterations': 0,\n",
        "            'successful_patterns': {},\n",
        "            'learning_curve': [],\n",
        "            'adaptation_score': 0.0\n",
        "        }\n",
        "\n",
        "    def record_iteration(self, results, outcomes=None):\n",
        "        self.data['iterations'] += 1\n",
        "\n",
        "        for model, result in results.items():\n",
        "            if not result or 'metrics' not in result:\n",
        "                continue\n",
        "\n",
        "            pnl = outcomes[model]['total_pnl'] if outcomes and model in outcomes else result['metrics']['total_pnl']\n",
        "            accuracy = outcomes[model]['accuracy'] if outcomes and model in outcomes else 0\n",
        "\n",
        "            if pnl > 0 and accuracy >= 50:\n",
        "                key = f\"{model}_success\"\n",
        "                if key not in self.data['successful_patterns']:\n",
        "                    self.data['successful_patterns'][key] = []\n",
        "\n",
        "                self.data['successful_patterns'][key].append({\n",
        "                    'chromosome': result.get('chromosome'),\n",
        "                    'pnl': pnl,\n",
        "                    'accuracy': accuracy,\n",
        "                    'time': datetime.now(timezone.utc).isoformat()\n",
        "                })\n",
        "\n",
        "                if len(self.data['successful_patterns'][key]) > 50:\n",
        "                    self.data['successful_patterns'][key] = sorted(\n",
        "                        self.data['successful_patterns'][key],\n",
        "                        key=lambda x: x['pnl'],\n",
        "                        reverse=True\n",
        "                    )[:50]\n",
        "\n",
        "        self.data['learning_curve'].append(sum(outcomes[m]['total_pnl'] for m in outcomes) if outcomes else 0)\n",
        "        if len(self.data['learning_curve']) > 100:\n",
        "            self.data['learning_curve'] = self.data['learning_curve'][-100:]\n",
        "\n",
        "        if len(self.data['learning_curve']) >= 10:\n",
        "            recent = np.mean(self.data['learning_curve'][-10:])\n",
        "            self.data['adaptation_score'] = min(100, max(0, 50 + recent))\n",
        "\n",
        "        try:\n",
        "            with open(self.file, 'wb') as f:\n",
        "                pickle.dump(self.data, f, protocol=4)\n",
        "            print_status(f\"‚úÖ Learning saved: Iteration {self.data['iterations']}\", \"success\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Learning save failed: {e}\")\n",
        "\n",
        "    def get_best_chromosomes(self, model, top_n=3):\n",
        "        key = f\"{model}_success\"\n",
        "        patterns = self.data['successful_patterns'].get(key, [])\n",
        "        return [p['chromosome'] for p in sorted(patterns, key=lambda x: x['pnl'], reverse=True)[:top_n] if p.get('chromosome')]\n",
        "\n",
        "    def get_report(self):\n",
        "        total_success = sum(len(p) for p in self.data['successful_patterns'].values())\n",
        "        return {\n",
        "            'iterations': self.data['iterations'],\n",
        "            'adaptation_score': self.data['adaptation_score'],\n",
        "            'total_successes': total_success,\n",
        "            'trend': \"üìà Improving\" if self.data['adaptation_score'] > 50 else \"üìâ Adjusting\"\n",
        "        }\n",
        "\n",
        "LEARNING = LearningSystem()\n",
        "\n",
        "# ======================================================\n",
        "# MODE MANAGER\n",
        "# ======================================================\n",
        "class ModeManager:\n",
        "    def __init__(self):\n",
        "        self.monday_data = self._load_monday()\n",
        "\n",
        "    def _load_monday(self):\n",
        "        if MONDAY_FILE.exists():\n",
        "            try:\n",
        "                data = pickle.load(open(MONDAY_FILE, \"rb\"))\n",
        "                if data.get('date') != datetime.now().strftime('%Y-%m-%d'):\n",
        "                    return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "                return data\n",
        "            except:\n",
        "                pass\n",
        "        return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "\n",
        "    def get_mode(self):\n",
        "        weekday = datetime.now().weekday()\n",
        "        if weekday in [5, 6]:\n",
        "            return \"weekend_replay\"\n",
        "        elif weekday == 0 and self.monday_data['count'] < 1:\n",
        "            return \"monday_replay\"\n",
        "        return \"normal\"\n",
        "\n",
        "    def should_send_email(self):\n",
        "        return self.get_mode() == \"normal\"\n",
        "\n",
        "MODE_MANAGER = ModeManager()\n",
        "\n",
        "# ======================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ======================================================\n",
        "def ensure_atr(df):\n",
        "    if \"atr\" in df.columns and not df[\"atr\"].isna().all():\n",
        "        df[\"atr\"] = df[\"atr\"].fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "        return df\n",
        "\n",
        "    high, low, close = df[\"high\"].values, df[\"low\"].values, df[\"close\"].values\n",
        "    tr = np.maximum.reduce([\n",
        "        high - low,\n",
        "        np.abs(high - np.roll(close, 1)),\n",
        "        np.abs(low - np.roll(close, 1))\n",
        "    ])\n",
        "    tr[0] = high[0] - low[0] if len(tr) > 0 else MIN_ATR\n",
        "    df[\"atr\"] = pd.Series(tr, index=df.index).rolling(ATR_PERIOD, min_periods=1).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "    return df\n",
        "\n",
        "def seed_hybrid_signal(df):\n",
        "    if \"hybrid_signal\" not in df.columns or df[\"hybrid_signal\"].abs().sum() == 0:\n",
        "        fast = df[\"close\"].rolling(10, min_periods=1).mean()\n",
        "        slow = df[\"close\"].rolling(50, min_periods=1).mean()\n",
        "        df[\"hybrid_signal\"] = (fast - slow).fillna(0)\n",
        "    return df\n",
        "\n",
        "def load_data(folder):\n",
        "    combined = {}\n",
        "    for pair in PAIRS:\n",
        "        combined[pair] = {}\n",
        "        prefix = pair.replace(\"/\", \"_\")\n",
        "        for pf in sorted(folder.glob(f\"{prefix}*.pkl\")):\n",
        "            try:\n",
        "                df = pd.read_pickle(pf)\n",
        "                if not isinstance(df, pd.DataFrame) or len(df) < 50:\n",
        "                    continue\n",
        "                df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "                if df.index.tz is not None:\n",
        "                    df.index = df.index.tz_convert(None)\n",
        "                df = ensure_atr(df)\n",
        "                df = seed_hybrid_signal(df)\n",
        "                tf = re.sub(rf\"{prefix}_?|\\.pkl\", \"\", pf.name).strip(\"_\") or \"merged\"\n",
        "                combined[pair][tf] = df\n",
        "            except:\n",
        "                continue\n",
        "    return combined\n",
        "\n",
        "def fetch_live_rate(pair):\n",
        "    token = os.environ.get(\"BROWSERLESS_TOKEN\", \"\")\n",
        "    if not token:\n",
        "        return 0.0\n",
        "    from_c, to_c = pair.split(\"/\")\n",
        "    try:\n",
        "        r = requests.post(\n",
        "            f\"https://production-sfo.browserless.io/content?token={token}\",\n",
        "            json={\"url\": f\"https://www.x-rates.com/calculator/?from={from_c}&to={to_c}&amount=1\"},\n",
        "            timeout=8\n",
        "        )\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', r.text)\n",
        "        return float(match.group(1).replace(\",\", \"\")) if match else 0.0\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def build_tf_map(data):\n",
        "    return {p: list(tfs.keys()) for p, tfs in data.items()}\n",
        "\n",
        "def create_chromosome(tf_map, config):\n",
        "    chrom = [\n",
        "        float(random.uniform(*config['atr_sl_range'])),\n",
        "        float(random.uniform(*config['atr_tp_range'])),\n",
        "        float(random.uniform(*config['risk_range'])),\n",
        "        float(random.uniform(*config['confidence_range']))\n",
        "    ]\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p, [])))\n",
        "        weights = np.random.dirichlet(np.ones(n)).tolist()\n",
        "        chrom.extend(weights)\n",
        "    return chrom\n",
        "\n",
        "def decode_chromosome(chrom, tf_map):\n",
        "    atr_sl = np.clip(chrom[0], 1.0, MAX_ATR_SL)\n",
        "    atr_tp = np.clip(chrom[1], 1.0, MAX_ATR_TP)\n",
        "    risk, conf = chrom[2], chrom[3]\n",
        "\n",
        "    tf_w = {}\n",
        "    idx = 4\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p, [])))\n",
        "        weights = np.array(chrom[idx:idx+n], dtype=float)\n",
        "        weights = weights / (weights.sum() + EPS) if weights.sum() > 0 else np.ones(n) / n\n",
        "        tf_w[p] = {tf: float(w) for tf, w in zip(tf_map.get(p, []), weights)}\n",
        "        idx += n\n",
        "\n",
        "    return atr_sl, atr_tp, risk, conf, tf_w\n",
        "\n",
        "def calculate_sharpe(equity_curve):\n",
        "    if len(equity_curve) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    equity_array = np.array(equity_curve, dtype=float)\n",
        "    returns = np.diff(equity_array) / (equity_array[:-1] + EPS)\n",
        "    if len(returns) == 0 or np.std(returns) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return float(np.mean(returns) / (np.std(returns) + EPS))\n",
        "\n",
        "# ======================================================\n",
        "# BACKTESTING\n",
        "# ======================================================\n",
        "def backtest_strategy(data, tf_map, chromosome):\n",
        "    atr_sl, atr_tp, risk, conf, tf_w = decode_chromosome(chromosome, tf_map)\n",
        "\n",
        "    equity = BASE_CAPITAL\n",
        "    equity_curve = [equity]\n",
        "    trades = []\n",
        "    position = None\n",
        "\n",
        "    all_times = sorted(set().union(*[df.index for tfs in data.values() for df in tfs.values()]))\n",
        "\n",
        "    for t in all_times:\n",
        "        if position:\n",
        "            pair = position['pair']\n",
        "            price = 0\n",
        "            for tf in tf_map.get(pair, []):\n",
        "                if tf in data.get(pair, {}) and t in data[pair][tf].index:\n",
        "                    price = data[pair][tf].loc[t, 'close']\n",
        "                    break\n",
        "\n",
        "            if price > 0:\n",
        "                hit_tp = (position['dir'] == 'BUY' and price >= position['tp']) or (position['dir'] == 'SELL' and price <= position['tp'])\n",
        "                hit_sl = (position['dir'] == 'BUY' and price <= position['sl']) or (position['dir'] == 'SELL' and price >= position['sl'])\n",
        "\n",
        "                if hit_tp or hit_sl:\n",
        "                    exit_price = position['tp'] if hit_tp else position['sl']\n",
        "                    pnl = (exit_price - position['entry']) * position['size'] if position['dir'] == 'BUY' else (position['entry'] - exit_price) * position['size']\n",
        "                    equity += pnl\n",
        "                    equity_curve.append(equity)\n",
        "                    trades.append({'pnl': pnl, 'correct': hit_tp})\n",
        "                    position = None\n",
        "\n",
        "        if position is None:\n",
        "            for pair in PAIRS:\n",
        "                signal = 0\n",
        "                price = 0\n",
        "                atr = MIN_ATR\n",
        "\n",
        "                for tf, weight in tf_w.get(pair, {}).items():\n",
        "                    if tf in data.get(pair, {}) and t in data[pair][tf].index:\n",
        "                        row = data[pair][tf].loc[t]\n",
        "                        signal += row.get('hybrid_signal', 0) * weight\n",
        "                        price = row['close']\n",
        "                        atr = max(row.get('atr', MIN_ATR), MIN_ATR)\n",
        "\n",
        "                if abs(signal) > conf and price > 0:\n",
        "                    direction = 'BUY' if signal > 0 else 'SELL'\n",
        "                    size = min(equity * risk, MAX_TRADE_CAP) / (atr * atr_sl)\n",
        "\n",
        "                    if direction == 'BUY':\n",
        "                        sl = price - (atr * atr_sl)\n",
        "                        tp = price + (atr * atr_tp)\n",
        "                    else:\n",
        "                        sl = price + (atr * atr_sl)\n",
        "                        tp = price - (atr * atr_tp)\n",
        "\n",
        "                    position = {'pair': pair, 'dir': direction, 'entry': price, 'sl': sl, 'tp': tp, 'size': size}\n",
        "                    break\n",
        "\n",
        "    total = len(trades)\n",
        "    wins = sum(1 for t in trades if t['correct'])\n",
        "    return {\n",
        "        'total_trades': total,\n",
        "        'winning_trades': wins,\n",
        "        'accuracy': (wins / total * 100) if total > 0 else 0,\n",
        "        'total_pnl': sum(t['pnl'] for t in trades),\n",
        "        'sharpe': calculate_sharpe(equity_curve)\n",
        "    }\n",
        "\n",
        "# ======================================================\n",
        "# GENETIC ALGORITHM\n",
        "# ======================================================\n",
        "def run_ga(data, tf_map, model_name, config):\n",
        "    print_status(f\"{config['color']} Training {model_name}...\", \"info\")\n",
        "\n",
        "    pop_size = config['pop_size']\n",
        "    generations = config['generations']\n",
        "    mutation_rate = config['mutation_rate']\n",
        "\n",
        "    try:\n",
        "        population = []\n",
        "        best_hist = LEARNING.get_best_chromosomes(model_name, top_n=3)\n",
        "        for chrom in best_hist:\n",
        "            if chrom:\n",
        "                metrics = backtest_strategy(data, tf_map, chrom)\n",
        "                fitness = metrics['total_pnl'] + (metrics['accuracy'] / 100) * 10\n",
        "                population.append((fitness, chrom))\n",
        "\n",
        "        while len(population) < pop_size:\n",
        "            chrom = create_chromosome(tf_map, config)\n",
        "            metrics = backtest_strategy(data, tf_map, chrom)\n",
        "            fitness = metrics['total_pnl'] + (metrics['accuracy'] / 100) * 10\n",
        "            population.append((fitness, chrom))\n",
        "\n",
        "        population.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "        for gen in range(generations):\n",
        "            new_pop = []\n",
        "            elite_count = max(1, int(pop_size * 0.2))\n",
        "            new_pop.extend(population[:elite_count])\n",
        "\n",
        "            while len(new_pop) < pop_size:\n",
        "                parent1 = max(random.sample(population, TOURNAMENT_SIZE), key=lambda x: x[0])[1]\n",
        "                parent2 = max(random.sample(population, TOURNAMENT_SIZE), key=lambda x: x[0])[1]\n",
        "\n",
        "                point = random.randint(1, len(parent1) - 1)\n",
        "                child = [float(x) for x in parent1[:point]] + [float(x) for x in parent2[point:]]\n",
        "\n",
        "                for i in range(len(child)):\n",
        "                    if random.random() < mutation_rate:\n",
        "                        if i == 0:\n",
        "                            child[i] = float(np.clip(child[i] + random.gauss(0, 0.3), *config['atr_sl_range']))\n",
        "                        elif i == 1:\n",
        "                            child[i] = float(np.clip(child[i] + random.gauss(0, 0.3), *config['atr_tp_range']))\n",
        "                        elif i == 2:\n",
        "                            child[i] = float(np.clip(child[i] + random.gauss(0, 0.005), *config['risk_range']))\n",
        "                        elif i == 3:\n",
        "                            child[i] = float(np.clip(child[i] + random.gauss(0, 0.1), *config['confidence_range']))\n",
        "                        else:\n",
        "                            child[i] = float(max(0.01, child[i] + random.gauss(0, 0.2)))\n",
        "\n",
        "                metrics = backtest_strategy(data, tf_map, child)\n",
        "                fitness = metrics['total_pnl'] + (metrics['accuracy'] / 100) * 10\n",
        "                new_pop.append((fitness, child))\n",
        "\n",
        "            population = sorted(new_pop, reverse=True, key=lambda x: x[0])\n",
        "\n",
        "            if (gen + 1) % 5 == 0:\n",
        "                print_status(f\"  Gen {gen+1}/{generations}: Best={population[0][0]:.4f}\", \"info\")\n",
        "\n",
        "        best_chrom = population[0][1]\n",
        "        final_metrics = backtest_strategy(data, tf_map, best_chrom)\n",
        "\n",
        "        print_status(\n",
        "            f\"  ‚úÖ {model_name}: {final_metrics['accuracy']:.1f}% accuracy | \"\n",
        "            f\"${final_metrics['total_pnl']:.4f} PnL | {final_metrics['total_trades']} trades\",\n",
        "            \"success\"\n",
        "        )\n",
        "\n",
        "        return {'chromosome': best_chrom, 'metrics': final_metrics}\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.exception(f\"{model_name} GA error\")\n",
        "        raise\n",
        "\n",
        "# ======================================================\n",
        "# SIGNAL GENERATION\n",
        "# ======================================================\n",
        "def generate_signals(data, tf_map, chromosome, model_name, current_time):\n",
        "    atr_sl, atr_tp, risk, conf, tf_w = decode_chromosome(chromosome, tf_map)\n",
        "    signals = {}\n",
        "\n",
        "    for pair in PAIRS:\n",
        "        signal_strength = 0\n",
        "        price = 0\n",
        "        atr = MIN_ATR\n",
        "\n",
        "        for tf, weight in tf_w.get(pair, {}).items():\n",
        "            if tf in data.get(pair, {}):\n",
        "                df = data[pair][tf]\n",
        "                if len(df) > 0:\n",
        "                    row = df.iloc[-1]\n",
        "                    signal_strength += row.get('hybrid_signal', 0) * weight\n",
        "                    price = row['close']\n",
        "                    atr = max(row.get('atr', MIN_ATR), MIN_ATR)\n",
        "\n",
        "        direction = 'HOLD'\n",
        "        sl = tp = price\n",
        "\n",
        "        if abs(signal_strength) > conf and price > 0:\n",
        "            direction = 'BUY' if signal_strength > 0 else 'SELL'\n",
        "\n",
        "            if direction == 'BUY':\n",
        "                sl = price - (atr * atr_sl)\n",
        "                tp = price + (atr * atr_tp)\n",
        "            else:\n",
        "                sl = price + (atr * atr_sl)\n",
        "                tp = price - (atr * atr_tp)\n",
        "\n",
        "        signals[pair] = {\n",
        "            'direction': direction,\n",
        "            'last_price': float(price),\n",
        "            'SL': float(sl),\n",
        "            'TP': float(tp),\n",
        "            'atr': float(atr),\n",
        "            'score_1_100': int(abs(signal_strength) * 100),\n",
        "            'model': model_name,\n",
        "            'timestamp': current_time.isoformat()\n",
        "        }\n",
        "\n",
        "    return signals\n",
        "\n",
        "# ======================================================\n",
        "# EMAIL SYSTEM\n",
        "# ======================================================\n",
        "def send_email(signals_by_model, iteration_stats, learning_report):\n",
        "    if not MODE_MANAGER.should_send_email() or not GMAIL_APP_PASSWORD:\n",
        "        print_status(\"Email skipped (replay mode or no credentials)\", \"info\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        msg = MIMEMultipart('alternative')\n",
        "        msg['Subject'] = f\"ü§ñ Forex AI Signals - Iteration #{iteration_stats['iteration']}\"\n",
        "        msg['From'] = GMAIL_USER\n",
        "        msg['To'] = GMAIL_USER\n",
        "\n",
        "        html = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "<style>\n",
        "body {{font-family: Arial, sans-serif; background: #f4f4f4; margin: 0; padding: 20px;}}\n",
        ".container {{max-width: 800px; margin: 0 auto; background: white; border-radius: 10px; overflow: hidden; box-shadow: 0 2px 10px rgba(0,0,0,0.1);}}\n",
        ".header {{background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; text-align: center;}}\n",
        ".header h1 {{margin: 0; font-size: 28px;}}\n",
        ".stats {{display: flex; justify-content: space-around; padding: 20px; background: #f8f9fa;}}\n",
        ".stat {{text-align: center;}}\n",
        ".stat-value {{font-size: 24px; font-weight: bold; color: #667eea;}}\n",
        ".model-section {{padding: 20px; border-bottom: 1px solid #eee;}}\n",
        ".signal {{padding: 15px; background: #f8f9fa; border-radius: 5px; margin: 10px 0;}}\n",
        ".signal-buy {{border-left: 4px solid #28a745;}}\n",
        ".signal-sell {{border-left: 4px solid #dc3545;}}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <div class=\"header\">\n",
        "        <h1>ü§ñ Forex AI Trading Signals</h1>\n",
        "        <p>Iteration #{iteration_stats['iteration']} | {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}</p>\n",
        "    </div>\n",
        "    <div class=\"stats\">\n",
        "        <div class=\"stat\">\n",
        "            <div class=\"stat-value\">{iteration_stats['total_iterations']}</div>\n",
        "            <div>Total Iterations</div>\n",
        "        </div>\n",
        "        <div class=\"stat\">\n",
        "            <div class=\"stat-value\">{learning_report['adaptation_score']:.1f}/100</div>\n",
        "            <div>Adaptation Score</div>\n",
        "        </div>\n",
        "        <div class=\"stat\">\n",
        "            <div class=\"stat-value\">{learning_report['trend']}</div>\n",
        "            <div>Trend</div>\n",
        "        </div>\n",
        "    </div>\n",
        "\"\"\"\n",
        "\n",
        "        for model_name, signals in signals_by_model.items():\n",
        "            config = COMPETITION_MODELS[model_name]\n",
        "            html += f\"\"\"\n",
        "    <div class=\"model-section\">\n",
        "        <div style=\"font-size: 20px; font-weight: bold;\">{config['color']} {model_name}</div>\n",
        "        <div style=\"color: #666; margin: 10px 0;\">{config['strategy']}</div>\n",
        "\"\"\"\n",
        "\n",
        "            for pair, sig in signals.items():\n",
        "                if sig['direction'] != 'HOLD':\n",
        "                    direction_class = sig['direction'].lower()\n",
        "                    html += f\"\"\"\n",
        "        <div class=\"signal signal-{direction_class}\">\n",
        "            <div style=\"font-weight: bold;\">{pair}: {sig['direction']} @ {sig['last_price']:.5f}</div>\n",
        "            <div style=\"color: #666; margin-top: 5px;\">\n",
        "                SL: {sig['SL']:.5f} | TP: {sig['TP']:.5f} | Confidence: {sig['score_1_100']}/100\n",
        "            </div>\n",
        "        </div>\n",
        "\"\"\"\n",
        "\n",
        "            html += \"    </div>\"\n",
        "\n",
        "        html += \"\"\"\n",
        "    <div style=\"padding: 20px; text-align: center; background: #f8f9fa; color: #666;\">\n",
        "        Powered by Advanced AI Trading System | v8.5.2\n",
        "    </div>\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "        msg.attach(MIMEText(html, 'html'))\n",
        "\n",
        "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
        "            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)\n",
        "            server.send_message(msg)\n",
        "\n",
        "        print_status(\"‚úÖ Email sent successfully\", \"success\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ö†Ô∏è Email failed: {e}\", \"warn\")\n",
        "\n",
        "# ======================================================\n",
        "# ENHANCED GIT OPERATIONS (Skip in GitHub Actions)\n",
        "# ======================================================\n",
        "def push_to_github(files, message):\n",
        "    \"\"\"Enhanced Git push - SKIPS in GitHub Actions\"\"\"\n",
        "\n",
        "    # ‚úÖ Skip Git operations in GitHub Actions (workflow handles it)\n",
        "    if IN_GHA:\n",
        "        print_status(\"ü§ñ GitHub Actions: Skipping Git push (workflow handles it)\", \"info\")\n",
        "        return True\n",
        "\n",
        "    if not FOREX_PAT:\n",
        "        print_status(\"‚ö†Ô∏è No FOREX_PAT - skipping Git push\", \"warn\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "        # Ensure repo exists\n",
        "        if not (REPO_FOLDER / \".git\").exists():\n",
        "            print_status(\"Cloning repository...\", \"info\")\n",
        "            subprocess.run(\n",
        "                [\"git\", \"clone\", REPO_URL, str(REPO_FOLDER)],\n",
        "                capture_output=True,\n",
        "                timeout=60,\n",
        "                check=True\n",
        "            )\n",
        "\n",
        "        os.chdir(REPO_FOLDER)\n",
        "\n",
        "        # Stage files\n",
        "        print_status(\"Staging files...\", \"info\")\n",
        "        files_added = 0\n",
        "        for f in files:\n",
        "            file_path = REPO_FOLDER / f\n",
        "            if file_path.exists():\n",
        "                subprocess.run([\"git\", \"add\", str(f)], check=False)\n",
        "                files_added += 1\n",
        "\n",
        "        if files_added == 0:\n",
        "            print_status(\"No files to stage\", \"warn\")\n",
        "            return True\n",
        "\n",
        "        # Commit\n",
        "        subprocess.run(\n",
        "            [\"git\", \"commit\", \"-m\", message],\n",
        "            capture_output=True,\n",
        "            check=False\n",
        "        )\n",
        "\n",
        "        # Pull before push\n",
        "        subprocess.run(\n",
        "            [\"git\", \"pull\", \"--rebase\", \"origin\", \"main\"],\n",
        "            capture_output=True,\n",
        "            check=False\n",
        "        )\n",
        "\n",
        "        # Push with retry\n",
        "        for attempt in range(3):\n",
        "            print_status(f\"Pushing (attempt {attempt + 1}/3)...\", \"info\")\n",
        "\n",
        "            result = subprocess.run(\n",
        "                [\"git\", \"push\", \"origin\", \"main\"],\n",
        "                capture_output=True,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print_status(\"‚úÖ Successfully pushed to GitHub\", \"success\")\n",
        "                return True\n",
        "\n",
        "            if attempt < 2:\n",
        "                time.sleep(2)\n",
        "\n",
        "        print_status(\"‚ùå Push failed after 3 attempts\", \"error\")\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print_status(f\"‚ùå Git error: {e}\", \"error\")\n",
        "        return False\n",
        "    finally:\n",
        "        try:\n",
        "            os.chdir(ROOT_PATH)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# ======================================================\n",
        "# MAIN EXECUTION\n",
        "# ======================================================\n",
        "def main():\n",
        "    print_status(\"=\" * 70, \"rocket\")\n",
        "    print_status(\"üöÄ FOREX PIPELINE v8.5.2 - FIXED PATHS EDITION\", \"rocket\")\n",
        "    print_status(\"=\" * 70, \"rocket\")\n",
        "\n",
        "    success = False\n",
        "\n",
        "    try:\n",
        "        # Display stats\n",
        "        current_iter = COUNTER.data['total'] + 1\n",
        "        stats = COUNTER.get_stats()\n",
        "        mode = MODE_MANAGER.get_mode()\n",
        "\n",
        "        print_status(f\"\\nüìä Iteration #{current_iter} | Mode: {mode.upper()}\", \"info\")\n",
        "        print_status(f\"Total Runs: {stats['total']} | Days: {stats['days']} | Avg/Day: {stats['per_day']:.1f}\", \"info\")\n",
        "\n",
        "        # Load data\n",
        "        print_status(\"\\nüì¶ Loading data...\", \"info\")\n",
        "        data = load_data(PICKLE_FOLDER)\n",
        "\n",
        "        if not data:\n",
        "            raise ValueError(\"‚ùå No data loaded - check PICKLE_FOLDER\")\n",
        "\n",
        "        print_status(f\"‚úÖ Loaded {len(data)} pairs\", \"success\")\n",
        "\n",
        "        tf_map = build_tf_map(data)\n",
        "\n",
        "        # Run competition\n",
        "        print_status(\"\\nüèÜ Running Competition...\", \"chart\")\n",
        "        competition_results = {}\n",
        "        signals_by_model = {}\n",
        "\n",
        "        for model_name, config in COMPETITION_MODELS.items():\n",
        "            try:\n",
        "                result = run_ga(data, tf_map, model_name, config)\n",
        "                competition_results[model_name] = result\n",
        "\n",
        "                # Generate signals\n",
        "                signals = generate_signals(\n",
        "                    data, tf_map, result['chromosome'],\n",
        "                    model_name, datetime.now(timezone.utc)\n",
        "                )\n",
        "                signals_by_model[model_name] = signals\n",
        "\n",
        "            except Exception as e:\n",
        "                print_status(f\"‚ùå {model_name} failed: {e}\", \"error\")\n",
        "\n",
        "        # Store signals in memory\n",
        "        MEMORY.store_signals(signals_by_model, datetime.now(timezone.utc))\n",
        "\n",
        "        # Update learning\n",
        "        LEARNING.record_iteration(competition_results)\n",
        "        learning_report = LEARNING.get_report()\n",
        "\n",
        "        print_status(\n",
        "            f\"\\nüß† Learning: {learning_report['trend']} | \"\n",
        "            f\"Score: {learning_report['adaptation_score']:.1f}/100\",\n",
        "            \"brain\"\n",
        "        )\n",
        "\n",
        "        # Save signals to JSON\n",
        "        print_status(\"\\nüíæ Saving signals...\", \"info\")\n",
        "\n",
        "        # Ensure parent directories exist\n",
        "        SIGNALS_JSON_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        with open(SIGNALS_JSON_PATH, 'w') as f:\n",
        "            json.dump(signals_by_model, f, indent=2, default=str)\n",
        "        print_status(f\"‚úÖ Saved: {SIGNALS_JSON_PATH}\", \"success\")\n",
        "\n",
        "        with open(ENSEMBLE_SIGNALS_FILE, 'w') as f:\n",
        "            json.dump({\n",
        "                'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "                'iteration': current_iter,\n",
        "                'models': signals_by_model\n",
        "            }, f, indent=2, default=str)\n",
        "        print_status(f\"‚úÖ Saved: {ENSEMBLE_SIGNALS_FILE}\", \"success\")\n",
        "\n",
        "        # Send email\n",
        "        iteration_stats = {\n",
        "            'iteration': current_iter,\n",
        "            'total_iterations': stats['total']\n",
        "        }\n",
        "        send_email(signals_by_model, iteration_stats, learning_report)\n",
        "\n",
        "        # Push to GitHub (skipped in GHA)\n",
        "        print_status(\"\\nüîÑ Git operations...\", \"info\")\n",
        "        files = [\n",
        "            SIGNALS_JSON_PATH.name,\n",
        "            ENSEMBLE_SIGNALS_FILE.name\n",
        "        ]\n",
        "        push_to_github(\n",
        "            files,\n",
        "            f\"ü§ñ Auto-update: Iteration #{current_iter} - {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}\"\n",
        "        )\n",
        "\n",
        "        # Summary\n",
        "        print_status(\"\\n\" + \"=\" * 70, \"success\")\n",
        "        print_status(\"‚úÖ PIPELINE COMPLETED SUCCESSFULLY\", \"success\")\n",
        "        print_status(\"=\" * 70, \"success\")\n",
        "        print_status(f\"Iteration: #{current_iter}\", \"info\")\n",
        "        print_status(f\"Models: {len(competition_results)}\", \"info\")\n",
        "        print_status(\n",
        "            f\"Signals: {sum(1 for m in signals_by_model.values() for s in m.values() if s['direction'] != 'HOLD')}\",\n",
        "            \"info\"\n",
        "        )\n",
        "\n",
        "        success = True\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print_status(\"\\n‚ö†Ô∏è Shutdown requested\", \"warn\")\n",
        "    except Exception as e:\n",
        "        print_status(f\"\\n‚ùå Fatal error: {e}\", \"error\")\n",
        "        logging.exception(\"Fatal error\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        COUNTER.increment(success=success)\n",
        "        MEMORY.close()\n",
        "        print_status(\"Cleanup complete\", \"info\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    print_status(\"Pipeline shutdown complete\", \"info\")"
      ],
      "metadata": {
        "id": "Xc7K-6WveVQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}