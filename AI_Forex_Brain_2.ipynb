{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eaVLBC20LX_",
        "outputId": "7998c0b3-70f0-486f-bd32-d99959764f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Detected environment: Colab\n",
            "‚úÖ Working directory set to: /content/forex-ai-models\n",
            "‚úÖ Git configured: Forex AI Bot <nakatonabira3@gmail.com>\n",
            "üîê Loaded FOREX_PAT from Colab secret.\n",
            "‚ö†Ô∏è BROWSERLESS_TOKEN not found.\n",
            "‚úÖ Output folders ready:\n",
            "   ‚Ä¢ CSVs:    /content/forex-ai-models/csvs\n",
            "   ‚Ä¢ Pickles: /content/forex-ai-models/pickles\n",
            "   ‚Ä¢ Logs:    /content/forex-ai-models/logs\n",
            "Python version: 3.12.12\n",
            "Current working directory: /content/forex-ai-models\n",
            "Directory contents: ['csvs', 'pickles', 'logs']\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# üåç Notebook Initialization ‚Äî Colab + GitHub Actions + Local\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Detect Environment\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "IN_LOCAL = not IN_COLAB and not IN_GHA\n",
        "\n",
        "ENV_NAME = \"Colab\" if IN_COLAB else \"GitHub Actions\" if IN_GHA else \"Local\"\n",
        "print(f\"üîç Detected environment: {ENV_NAME}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Safe Working Folder (Auto-Switch)\n",
        "# ======================================================\n",
        "if IN_COLAB:\n",
        "    BASE_DIR = Path(\"/content\")\n",
        "elif IN_GHA:\n",
        "    BASE_DIR = Path(\"/home/runner/work\")\n",
        "else:\n",
        "    BASE_DIR = Path(\".\")\n",
        "\n",
        "REPO_NAME = \"forex-ai-models\"  # Updated repo name\n",
        "SAVE_FOLDER = BASE_DIR / REPO_NAME\n",
        "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(SAVE_FOLDER)\n",
        "print(f\"‚úÖ Working directory set to: {SAVE_FOLDER.resolve()}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Git Configuration (Universal)\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], check=False)\n",
        "\n",
        "print(f\"‚úÖ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Tokens & Secrets\n",
        "# ======================================================\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "BROWSERLESS_TOKEN = os.environ.get(\"BROWSERLESS_TOKEN\")\n",
        "\n",
        "# Load Colab secrets if missing\n",
        "if IN_COLAB and not FOREX_PAT:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get('FOREX_PAT')\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è No Colab secret found for FOREX_PAT\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    print(\"‚ö†Ô∏è FOREX_PAT not found ‚Äî GitHub cloning may fail.\")\n",
        "if not BROWSERLESS_TOKEN:\n",
        "    print(\"‚ö†Ô∏è BROWSERLESS_TOKEN not found.\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Output Folders\n",
        "# ======================================================\n",
        "CSV_FOLDER = SAVE_FOLDER / \"csvs\"\n",
        "PICKLE_FOLDER = SAVE_FOLDER / \"pickles\"\n",
        "LOGS_FOLDER = SAVE_FOLDER / \"logs\"\n",
        "\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Output folders ready:\")\n",
        "print(f\"   ‚Ä¢ CSVs:    {CSV_FOLDER}\")\n",
        "print(f\"   ‚Ä¢ Pickles: {PICKLE_FOLDER}\")\n",
        "print(f\"   ‚Ä¢ Logs:    {LOGS_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Environment Debug Info\n",
        "# ======================================================\n",
        "print(f\"Python version: {sys.version.split()[0]}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"Directory contents: {os.listdir('.')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oih6CDfjAjG9",
        "outputId": "ffbea377-36ac-4bea-be7b-960336bbaae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mplfinance\n",
            "  Downloading mplfinance-0.12.10b0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: firebase-admin in /usr/local/lib/python3.12/dist-packages (6.9.0)\n",
            "Collecting dropbox\n",
            "  Downloading dropbox-12.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.66)\n",
            "Collecting pyppeteer\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting alpha_vantage\n",
            "  Downloading alpha_vantage-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting river\n",
            "  Downloading river-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: cachecontrol>=0.12.14 in /usr/local/lib/python3.12/dist-packages (from firebase-admin) (0.14.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.7.8 in /usr/local/lib/python3.12/dist-packages (from firebase-admin) (2.185.0)\n",
            "Requirement already satisfied: google-cloud-storage>=1.37.1 in /usr/local/lib/python3.12/dist-packages (from firebase-admin) (2.19.0)\n",
            "Requirement already satisfied: pyjwt>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (2.10.1)\n",
            "Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]==0.28.1->firebase-admin) (0.28.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=1.22.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.28.0)\n",
            "Requirement already satisfied: google-cloud-firestore>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from firebase-admin) (2.21.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (3.11)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]==0.28.1->firebase-admin) (4.3.0)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (0.16.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from dropbox) (1.17.0)\n",
            "Collecting stone<3.3.3,>=2 (from dropbox)\n",
            "  Downloading stone-3.3.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.5.0)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.12/dist-packages (from pyppeteer) (8.7.0)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pyppeteer to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyppeteer\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer)\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pyppeteer\n",
            "  Downloading pyppeteer-1.0.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading pyppeteer-1.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading pyppeteer-0.2.6-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading pyppeteer-0.2.5-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading pyppeteer-0.2.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading pyppeteer-0.2.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting pyee<8.0.0,>=7.0.1 (from pyppeteer)\n",
            "  Downloading pyee-7.0.4-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: pip is still looking at multiple versions of pyppeteer to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyppeteer\n",
            "  Downloading pyppeteer-0.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading pyppeteer-0.0.25.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyee (from pyppeteer)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from alpha_vantage) (3.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from cachecontrol>=0.12.14->firebase-admin) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.71.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.71.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (4.2.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-firestore>=2.19.0->firebase-admin) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (1.7.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (43.0.3)\n",
            "Requirement already satisfied: ply>=3.4 in /usr/local/lib/python3.12/dist-packages (from stone<3.3.3,>=2->dropbox) (3.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->alpha_vantage) (1.22.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.9.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]==0.28.1->firebase-admin) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]==0.28.1->firebase-admin) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.6.1)\n",
            "Downloading mplfinance-0.12.10b0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dropbox-12.0.2-py3-none-any.whl (572 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m572.1/572.1 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alpha_vantage-3.0.0-py3-none-any.whl (35 kB)\n",
            "Downloading river-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stone-3.3.1-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: ta, pyppeteer\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=9fb2e144bfd815c76736cf42eec6689523dea7505ff8b87bf462220f8be341ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "  Building wheel for pyppeteer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyppeteer: filename=pyppeteer-0.0.25-py3-none-any.whl size=78356 sha256=9f76fb3e02317136bc25c2c4414a1a57ff1281fdb9eb747faf7958d6e83c8fe3\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/7e/08/1d19cd4b0f6d57363170cf9538d65a7285e6aba8731bac8813\n",
            "Successfully built ta pyppeteer\n",
            "Installing collected packages: appdirs, stone, pyee, pyppeteer, pandas, dropbox, ta, river, mplfinance, alpha_vantage\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed alpha_vantage-3.0.0 appdirs-1.4.4 dropbox-12.0.2 mplfinance-0.12.10b0 pandas-2.3.3 pyee-13.0.0 pyppeteer-0.0.25 river-0.22.0 stone-3.3.1 ta-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn river\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JS9qXRF_JXJO",
        "outputId": "400dba6e-9ee8-4028-b16e-2bcef92bb2af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha Vantage Key: 1W58NPZXOG5SLHZ6\n",
            "Browserless Token: 2St0qUktyKsA0Bsb5b510553885cae26942e44c26c0f19c3d\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set your keys (only for this session)\n",
        "os.environ['ALPHA_VANTAGE_KEY'] = '1W58NPZXOG5SLHZ6'\n",
        "os.environ['BROWSERLESS_TOKEN'] = '2St0qUktyKsA0Bsb5b510553885cae26942e44c26c0f19c3d'\n",
        "\n",
        "# Test if they work\n",
        "print(\"Alpha Vantage Key:\", os.environ.get('ALPHA_VANTAGE_KEY'))\n",
        "print(\"Browserless Token:\", os.environ.get('BROWSERLESS_TOKEN'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfQDCpi612f3",
        "outputId": "b9e64ba1-5f21-450d-9bc9-8c75314b97c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Cloning repo (skipping LFS)...\n",
            "‚úÖ Repo cloned successfully into /content/forex-automation/forex-ai-models\n",
            "‚öôÔ∏è Removing Git LFS and converting files...\n",
            "‚úÖ Git configured: Forex AI Bot <nakatonabira3@gmail.com>\n",
            "‚úÖ No changes detected. LFS already removed.\n",
            "üìÅ Output folders ready: csvs/, pickles/, logs/\n",
            "\n",
            "üßæ Summary:\n",
            "‚Ä¢ Working Directory: /content/forex-automation/forex-ai-models\n",
            "‚Ä¢ Repository: https://github.com/rahim-dotAI/forex-ai-models\n",
            "‚úÖ All operations completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# ‚ö° Full Colab-ready GitHub Sync + Remove LFS\n",
        "# ======================================================\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import urllib.parse\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Environment / Paths\n",
        "# -----------------------------\n",
        "REPO_PARENT = Path(\"/content/forex-automation\")\n",
        "REPO_PARENT.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(REPO_PARENT)\n",
        "\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "REPO_FOLDER = REPO_PARENT / GITHUB_REPO\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ GitHub Token\n",
        "# -----------------------------\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "if not FOREX_PAT:\n",
        "    from google.colab import userdata\n",
        "    FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "    if FOREX_PAT:\n",
        "        os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "        print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå Missing FOREX_PAT. Set it in Colab userdata or GitHub secrets.\")\n",
        "\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Clean old repo\n",
        "# -----------------------------\n",
        "if REPO_FOLDER.exists():\n",
        "    print(f\"üóë Removing old repo: {REPO_FOLDER}\")\n",
        "    shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Clone repo safely (skip LFS)\n",
        "# -----------------------------\n",
        "print(\"üîó Cloning repo (skipping LFS)...\")\n",
        "env = os.environ.copy()\n",
        "env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
        "\n",
        "subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_FOLDER)], check=True, env=env)\n",
        "os.chdir(REPO_FOLDER)\n",
        "print(f\"‚úÖ Repo cloned successfully into {REPO_FOLDER}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Uninstall LFS and convert files\n",
        "# -----------------------------\n",
        "print(\"‚öôÔ∏è Removing Git LFS and converting files...\")\n",
        "subprocess.run([\"git\", \"lfs\", \"uninstall\"], check=True)\n",
        "subprocess.run([\"git\", \"lfs\", \"migrate\", \"export\", \"--include=*.csv\"], check=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Configure Git user\n",
        "# -----------------------------\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"advice.detachedHead\", \"false\"], check=True)\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Stage, commit, push\n",
        "# -----------------------------\n",
        "subprocess.run([\"git\", \"add\", \"-A\"], check=True)\n",
        "status = subprocess.run([\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True)\n",
        "\n",
        "if status.stdout.strip():\n",
        "    subprocess.run([\"git\", \"commit\", \"-m\", \"Remove LFS and convert files to normal Git\"], check=True)\n",
        "    subprocess.run([\"git\", \"push\", \"origin\", BRANCH], check=True)\n",
        "    print(\"üöÄ Repo updated: LFS removed permanently.\")\n",
        "else:\n",
        "    print(\"‚úÖ No changes detected. LFS already removed.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Create standard output folders\n",
        "# -----------------------------\n",
        "for folder in [\"csvs\", \"pickles\", \"logs\"]:\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "print(\"üìÅ Output folders ready: csvs/, pickles/, logs/\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Summary\n",
        "# -----------------------------\n",
        "print(\"\\nüßæ Summary:\")\n",
        "print(f\"‚Ä¢ Working Directory: {os.getcwd()}\")\n",
        "print(f\"‚Ä¢ Repository: https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}\")\n",
        "print(\"‚úÖ All operations completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKFLKkgP1aBP",
        "outputId": "abbf847a-f31e-49d0-8529-4397430ac9dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected environment: Colab\n",
            "‚úÖ Working directory: /content/forex-alpha-models\n",
            "‚úÖ Output folders ready: /content/forex-alpha-models/pickles, /content/forex-alpha-models/csvs, /content/forex-alpha-models/logs\n",
            "üîó Cloning repo (skipping LFS)...\n",
            "‚úÖ Repo cloned successfully into /content/forex-alpha-models/forex-ai-models\n",
            "‚úÖ Git configured: Forex AI Bot <nakatonabira3@gmail.com>\n",
            "‚ÑπÔ∏è AUD/USD total rows: 5000\n",
            "AUD/USD updated\n",
            "‚ÑπÔ∏è USD/JPY total rows: 5000\n",
            "USD/JPY updated\n",
            "‚ÑπÔ∏è GBP/USD total rows: 5000\n",
            "GBP/USD updated\n",
            "‚ÑπÔ∏è EUR/USD total rows: 5000\n",
            "EUR/USD updated\n",
            "üöÄ Committing 4 updated files...\n",
            "‚úÖ All FX pairs processed, saved, pushed successfully!\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# üöÄ FULLY FIXED ALPHA VANTAGE FX WORKFLOW\n",
        "# - Uses URL-safe PAT\n",
        "# - Loads from Colab secrets\n",
        "# - Cleans stale repo + skips LFS\n",
        "# - GitHub Actions + Colab Safe\n",
        "# ======================================================\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import requests\n",
        "import subprocess\n",
        "import threading\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Detect Environment\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "print(f\"Detected environment: {'Colab' if IN_COLAB else 'GitHub/Local'}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Working directories\n",
        "# ======================================================\n",
        "BASE_FOLDER = Path(\"/content/forex-alpha-models\") if IN_COLAB else Path(\"./forex-alpha-models\")\n",
        "BASE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(BASE_FOLDER)\n",
        "\n",
        "PICKLE_FOLDER = BASE_FOLDER / \"pickles\"\n",
        "CSV_FOLDER = BASE_FOLDER / \"csvs\"\n",
        "LOG_FOLDER = BASE_FOLDER / \"logs\"\n",
        "\n",
        "for folder in [PICKLE_FOLDER, CSV_FOLDER, LOG_FOLDER]:\n",
        "    folder.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Working directory: {BASE_FOLDER.resolve()}\")\n",
        "print(f\"‚úÖ Output folders ready: {PICKLE_FOLDER}, {CSV_FOLDER}, {LOG_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ GitHub Configuration\n",
        "# ======================================================\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "REPO_FOLDER = BASE_FOLDER / GITHUB_REPO\n",
        "\n",
        "# Load PAT from env or Colab userdata\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "if not FOREX_PAT and IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        FOREX_PAT = userdata.get(\"FOREX_PAT\")\n",
        "        if FOREX_PAT:\n",
        "            os.environ[\"FOREX_PAT\"] = FOREX_PAT\n",
        "            print(\"üîê Loaded FOREX_PAT from Colab secret.\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå Missing FOREX_PAT. Set it in Colab userdata or GitHub secrets.\")\n",
        "\n",
        "SAFE_PAT = urllib.parse.quote(FOREX_PAT)\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{SAFE_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Safe Repo Clone / Sync\n",
        "# ======================================================\n",
        "if REPO_FOLDER.exists():\n",
        "    print(f\"üóë Removing old repo: {REPO_FOLDER}\")\n",
        "    shutil.rmtree(REPO_FOLDER)\n",
        "\n",
        "print(\"üîó Cloning repo (skipping LFS)...\")\n",
        "env = os.environ.copy()\n",
        "env[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\"\n",
        "\n",
        "subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)], check=True, env=env)\n",
        "os.chdir(REPO_FOLDER)\n",
        "print(f\"‚úÖ Repo cloned successfully into {REPO_FOLDER}\")\n",
        "\n",
        "# Configure Git identity\n",
        "GIT_USER_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_USER_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_USER_NAME], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_USER_EMAIL], check=True)\n",
        "print(f\"‚úÖ Git configured: {GIT_USER_NAME} <{GIT_USER_EMAIL}>\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Alpha Vantage Setup\n",
        "# ======================================================\n",
        "ALPHA_VANTAGE_KEY = os.environ.get(\"ALPHA_VANTAGE_KEY\")\n",
        "if not ALPHA_VANTAGE_KEY:\n",
        "    raise ValueError(\"‚ùå ALPHA_VANTAGE_KEY missing!\")\n",
        "\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "lock = threading.Lock()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def fetch_alpha_vantage_fx(pair, outputsize='full', max_retries=3, retry_delay=5):\n",
        "    base_url = 'https://www.alphavantage.co/query'\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "    params = {\n",
        "        'function': 'FX_DAILY',\n",
        "        'from_symbol': from_currency,\n",
        "        'to_symbol': to_currency,\n",
        "        'outputsize': outputsize,\n",
        "        'datatype': 'json',\n",
        "        'apikey': ALPHA_VANTAGE_KEY\n",
        "    }\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            r = requests.get(base_url, params=params, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "            if 'Time Series FX (Daily)' not in data:\n",
        "                raise ValueError(f\"Unexpected API response: {data}\")\n",
        "            ts = data['Time Series FX (Daily)']\n",
        "            df = pd.DataFrame(ts).T\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df.sort_index()\n",
        "            df = df.rename(columns={\n",
        "                '1. open': 'open',\n",
        "                '2. high': 'high',\n",
        "                '3. low': 'low',\n",
        "                '4. close': 'close'\n",
        "            }).astype(float)\n",
        "            df = ensure_tz_naive(df)\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed fetching {pair}: {e}\")\n",
        "            time.sleep(retry_delay)\n",
        "    print(f\"‚ùå Failed to fetch {pair} after {max_retries} retries\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Process Pairs for Unified CSV Pipeline\n",
        "# ======================================================\n",
        "def process_pair(pair):\n",
        "    filename = pair.replace(\"/\", \"_\") + \".csv\"\n",
        "    filepath = CSV_FOLDER / filename\n",
        "\n",
        "    if filepath.exists():\n",
        "        existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "    else:\n",
        "        existing_df = pd.DataFrame()\n",
        "\n",
        "    old_hash = file_hash(filepath)\n",
        "    new_df = fetch_alpha_vantage_fx(pair)\n",
        "    if new_df.empty:\n",
        "        return None, f\"No new data for {pair}\"\n",
        "\n",
        "    combined_df = pd.concat([existing_df, new_df]) if not existing_df.empty else new_df\n",
        "    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    with lock:\n",
        "        combined_df.to_csv(filepath)\n",
        "\n",
        "    new_hash = file_hash(filepath)\n",
        "    changed = old_hash != new_hash\n",
        "    print(f\"‚ÑπÔ∏è {pair} total rows: {len(combined_df)}\")\n",
        "    return str(filepath) if changed else None, f\"{pair} {'updated' if changed else 'no changes'}\"\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Execute All Pairs in Parallel\n",
        "# ======================================================\n",
        "changed_files = []\n",
        "tasks = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for pair in FX_PAIRS:\n",
        "        tasks.append(executor.submit(process_pair, pair))\n",
        "    for future in as_completed(tasks):\n",
        "        filepath, msg = future.result()\n",
        "        print(msg)\n",
        "        if filepath:\n",
        "            changed_files.append(filepath)\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Commit & Push Changes\n",
        "# ======================================================\n",
        "if changed_files:\n",
        "    print(f\"üöÄ Committing {len(changed_files)} updated files...\")\n",
        "    subprocess.run([\"git\", \"add\", \"-A\"], check=False)\n",
        "    subprocess.run([\"git\", \"commit\", \"-m\", \"Update Alpha Vantage FX data\"], check=False)\n",
        "    subprocess.run([\"git\", \"push\", \"origin\", BRANCH], check=False)\n",
        "else:\n",
        "    print(\"‚úÖ No changes to commit.\")\n",
        "\n",
        "print(\"‚úÖ All FX pairs processed, saved, pushed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By3b3WMVJ-M3",
        "outputId": "d56db9fe-5656-427a-f51f-89db138e9667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected environment: Colab\n",
            "‚úÖ Working directory: /content/forex-alpha-models\n",
            "‚úÖ Output folders ready: /content/forex-alpha-models/pickles, /content/forex-alpha-models/csvs, /content/forex-alpha-models/logs\n",
            "üîÑ Repo exists, pulling latest...\n",
            "‚úÖ Repo ready at /content/forex-alpha-models/forex-ai-models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
            "/tmp/ipython-input-2792618360.py:131: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìà Updated AUD/USD 5m_1mo\n",
            "üìà Updated EUR/USD 1h_2y\n",
            "üìà Updated AUD/USD 15m_60d\n",
            "üìà Updated EUR/USD 5m_1mo\n",
            "üìà Updated AUD/USD 1h_2y\n",
            "üìà Updated USD/JPY 1m_7d\n",
            "üìà Updated EUR/USD 1m_7d\n",
            "üìà Updated USD/JPY 1h_2y\n",
            "üìà Updated GBP/USD 5m_1mo\n",
            "üìà Updated GBP/USD 1m_7d\n",
            "üìà Updated USD/JPY 15m_60d\n",
            "üìà Updated EUR/USD 1d_5y\n",
            "üìà Updated AUD/USD 1d_5y\n",
            "üìà Updated USD/JPY 1d_5y\n",
            "üìà Updated GBP/USD 1d_5y\n",
            "üìà Updated EUR/USD 15m_60d\n",
            "üìà Updated GBP/USD 1h_2y\n",
            "üìà Updated USD/JPY 5m_1mo\n",
            "üìà Updated AUD/USD 1m_7d\n",
            "üìà Updated GBP/USD 15m_60d\n",
            "üöÄ Committing 20 updated files...\n",
            "üéØ All FX pairs & timeframes processed safely with maximum historical rows!\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# FULLY IMPROVED FOREX DATA WORKFLOW - YFINANCE\n",
        "# Colab + GitHub Actions Safe, 403-Proof, Large History\n",
        "# ======================================================\n",
        "\n",
        "import os, time, hashlib, subprocess, shutil, threading\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Detect environment\n",
        "# ======================================================\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "IN_LOCAL = not IN_COLAB and not IN_GHA\n",
        "\n",
        "print(f\"Detected environment: {'Colab' if IN_COLAB else ('GitHub Actions' if IN_GHA else 'Local')}\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Working directories\n",
        "# ======================================================\n",
        "BASE_DIR = Path(\"/content/forex-alpha-models\") if IN_COLAB else Path(\"./forex-alpha-models\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(BASE_DIR)\n",
        "\n",
        "PICKLE_FOLDER = BASE_DIR / \"pickles\"; PICKLE_FOLDER.mkdir(exist_ok=True)\n",
        "CSV_FOLDER = BASE_DIR / \"csvs\"; CSV_FOLDER.mkdir(exist_ok=True)\n",
        "LOG_FOLDER = BASE_DIR / \"logs\"; LOG_FOLDER.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Working directory: {BASE_DIR.resolve()}\")\n",
        "print(f\"‚úÖ Output folders ready: {PICKLE_FOLDER}, {CSV_FOLDER}, {LOG_FOLDER}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Git configuration\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = \"rahim-dotAI\"\n",
        "GITHUB_REPO = \"forex-ai-models\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT missing!\")\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
        "\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Clone or update repo safely\n",
        "# ======================================================\n",
        "REPO_FOLDER = BASE_DIR / GITHUB_REPO\n",
        "def ensure_repo_cloned(repo_url, repo_folder, branch=\"main\"):\n",
        "    repo_folder = Path(repo_folder)\n",
        "    tmp_folder = repo_folder.parent / (repo_folder.name + \"_tmp\")\n",
        "    if tmp_folder.exists(): shutil.rmtree(tmp_folder)\n",
        "    if not (repo_folder / \".git\").exists():\n",
        "        print(f\"üì• Cloning repo into {tmp_folder} ...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"-b\", branch, repo_url, str(tmp_folder)], check=True)\n",
        "        if repo_folder.exists(): shutil.rmtree(repo_folder)\n",
        "        tmp_folder.rename(repo_folder)\n",
        "    else:\n",
        "        print(\"üîÑ Repo exists, pulling latest...\")\n",
        "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"fetch\", \"origin\"], check=True)\n",
        "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"checkout\", branch], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"pull\", \"origin\", branch], check=False)\n",
        "    print(f\"‚úÖ Repo ready at {repo_folder.resolve()}\")\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "ensure_repo_cloned(REPO_URL, REPO_FOLDER, BRANCH)\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ FX pairs & timeframes\n",
        "# ======================================================\n",
        "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "TIMEFRAMES = {\n",
        "    \"1d_5y\": (\"1d\", \"5y\"),\n",
        "    \"1h_2y\": (\"1h\", \"2y\"),\n",
        "    \"15m_60d\": (\"15m\", \"60d\"),\n",
        "    \"5m_1mo\": (\"5m\", \"1mo\"),\n",
        "    \"1m_7d\": (\"1m\", \"7d\")\n",
        "}\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Helper functions\n",
        "# ======================================================\n",
        "def file_hash(filepath, chunk_size=8192):\n",
        "    if not filepath.exists(): return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"): md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def ensure_tz_naive(df):\n",
        "    if df is None or df.empty: return df\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    if df.index.tz: df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def merge_data(existing_df, new_df):\n",
        "    existing_df = ensure_tz_naive(existing_df)\n",
        "    new_df = ensure_tz_naive(new_df)\n",
        "    if existing_df.empty: return new_df\n",
        "    if new_df.empty: return existing_df\n",
        "    combined = pd.concat([existing_df, new_df])\n",
        "    combined = combined[~combined.index.duplicated(keep=\"last\")]\n",
        "    combined.sort_index(inplace=True)\n",
        "    return combined\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Worker function for pairs/timeframes\n",
        "# ======================================================\n",
        "def process_pair_tf(pair, tf_name, interval, period, max_retries=3, retry_delay=5):\n",
        "    symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
        "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
        "    filepath = REPO_FOLDER / filename\n",
        "\n",
        "    existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
        "    old_hash = file_hash(filepath)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            df = yf.download(symbol, interval=interval, period=period, progress=False, auto_adjust=False, threads=True)\n",
        "            if df.empty: raise ValueError(\"No data returned\")\n",
        "            df = df[[c for c in ['Open','High','Low','Close','Volume'] if c in df.columns]]\n",
        "            df.rename(columns=lambda x: x.lower(), inplace=True)\n",
        "            df = ensure_tz_naive(df)\n",
        "            combined_df = merge_data(existing_df, df)\n",
        "            combined_df.to_csv(filepath)\n",
        "            if old_hash != file_hash(filepath):\n",
        "                return f\"üìà Updated {pair} {tf_name}\", str(filepath)\n",
        "            return f\"‚úÖ No changes {pair} {tf_name}\", None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt+1}/{max_retries} failed for {pair} {tf_name}: {e}\")\n",
        "            if attempt < max_retries: time.sleep(retry_delay)\n",
        "            else: return f\"‚ùå Failed {pair} {tf_name}\", None\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Parallel execution\n",
        "# ======================================================\n",
        "changed_files = []\n",
        "tasks = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    for pair in FX_PAIRS:\n",
        "        for tf_name, (interval, period) in TIMEFRAMES.items():\n",
        "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period))\n",
        "\n",
        "for future in as_completed(tasks):\n",
        "    msg, filename = future.result()\n",
        "    print(msg)\n",
        "    if filename: changed_files.append(filename)\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Commit & push updates\n",
        "# ======================================================\n",
        "if changed_files:\n",
        "    print(f\"üöÄ Committing {len(changed_files)} updated files...\")\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"add\"] + changed_files, check=False)\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"commit\", \"-m\", \"Update YFinance FX data CSVs\"], check=False)\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"push\", \"origin\", BRANCH], check=False)\n",
        "else:\n",
        "    print(\"‚úÖ No changes detected, nothing to push.\")\n",
        "\n",
        "print(\"üéØ All FX pairs & timeframes processed safely with maximum historical rows!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z96OOkCQSzof",
        "outputId": "5cc85657-e39b-469b-f7c7-8c07dc8aebca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EUR/USD: 1.16\n",
            "GBP/USD: 1.315\n",
            "USD/JPY: 153.95\n",
            "AUD/USD: 0.6542\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import re\n",
        "\n",
        "def fetch_live_rate(pair):\n",
        "    \"\"\"\n",
        "    Fetch live FX rate from X-Rates using Browserless.\n",
        "    \"\"\"\n",
        "    from_currency, to_currency = pair.split('/')\n",
        "    browserless_token = os.environ.get('BROWSERLESS_TOKEN')\n",
        "    if not browserless_token:\n",
        "        raise ValueError(\"Set BROWSERLESS_TOKEN in your environment variables\")\n",
        "\n",
        "    url = f\"https://production-sfo.browserless.io/content?token={browserless_token}\"\n",
        "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
        "\n",
        "    try:\n",
        "        res = requests.post(url, json=payload)\n",
        "        # Regex to extract the FX value\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
        "        return float(match.group(1).replace(',', '')) if match else 0\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch {pair}: {e}\")\n",
        "        return 0\n",
        "\n",
        "# --- Fetch live prices for all pairs ---\n",
        "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "live_prices = {pair: fetch_live_rate(pair) for pair in pairs}\n",
        "\n",
        "for pair, price in live_prices.items():\n",
        "    print(f\"{pair}: {price}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fubyMudCLe_F",
        "outputId": "6b9c0b0e-a320-4255-937a-900d14704a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ÑπÔ∏è Repo exists, pulling latest...\n",
            "‚úÖ Repo synced successfully\n",
            "‚ÑπÔ∏è AUD_USD.csv total rows: 5000\n",
            "‚úÖ AUD_USD.csv updated with 5000 new rows\n",
            "‚ÑπÔ∏è EUR_USD.csv total rows: 5000\n",
            "‚úÖ EUR_USD.csv updated with 5000 new rows\n",
            "‚ÑπÔ∏è GBP_USD.csv total rows: 5000\n",
            "‚úÖ GBP_USD.csv updated with 5000 new rows\n",
            "‚ÑπÔ∏è USD_JPY.csv total rows: 5000\n",
            "‚úÖ USD_JPY.csv updated with 5000 new rows\n",
            "‚ÑπÔ∏è Committing 4 updated files...\n",
            "‚úÖ Push successful\n",
            "‚úÖ All CSVs combined, incremental indicators added, and Git updated successfully.\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# FX CSV Combine + Incremental Indicators Pipeline\n",
        "# Fully optimized for YFinance + Alpha Vantage\n",
        "# Thread-safe, timezone-safe, Git-push-safe, large dataset-ready\n",
        "# ======================================================\n",
        "\n",
        "import os, time, hashlib, subprocess, shutil\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Environment & folders\n",
        "# -----------------------------\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "ROOT_DIR = Path(\"/content/forex-alpha-models\") if IN_COLAB else Path(\".\")\n",
        "ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    levels = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\"}\n",
        "    print(f\"{levels.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Git configuration\n",
        "# -----------------------------\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Abdul Rahim\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "\n",
        "if not FOREX_PAT:\n",
        "    raise ValueError(\"‚ùå FOREX_PAT missing!\")\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Ensure repo exists\n",
        "# -----------------------------\n",
        "def ensure_repo():\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        if REPO_FOLDER.exists():\n",
        "            shutil.rmtree(REPO_FOLDER)\n",
        "        print_status(f\"Cloning repo into {REPO_FOLDER}...\", \"info\")\n",
        "        subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(REPO_FOLDER)], check=True)\n",
        "    else:\n",
        "        print_status(\"Repo exists, pulling latest...\", \"info\")\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"fetch\", \"origin\"], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"checkout\", BRANCH], check=False)\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"pull\", \"origin\", BRANCH], check=False)\n",
        "        print_status(\"Repo synced successfully\", \"success\")\n",
        "ensure_repo()\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Helpers\n",
        "# -----------------------------\n",
        "def ensure_tz_naive(df):\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "    df.index = df.index.tz_localize(None)\n",
        "    return df\n",
        "\n",
        "def file_hash(filepath):\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def safe_numeric(df, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['open','high','low','close']\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df.replace([np.inf,-np.inf], np.nan, inplace=True)\n",
        "    df.dropna(subset=columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Incremental CSV combine\n",
        "# -----------------------------\n",
        "def combine_csv(csv_path):\n",
        "    target_file = REPO_FOLDER / csv_path.name\n",
        "    existing_df = ensure_tz_naive(pd.read_csv(target_file, index_col=0, parse_dates=True)) if target_file.exists() else pd.DataFrame()\n",
        "    new_df = ensure_tz_naive(pd.read_csv(csv_path, index_col=0, parse_dates=True))\n",
        "    combined_df = pd.concat([existing_df, new_df])\n",
        "    combined_df = combined_df[~combined_df.index.duplicated(keep=\"last\")]\n",
        "    combined_df.sort_index(inplace=True)\n",
        "    return combined_df, target_file\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Incremental indicators\n",
        "# -----------------------------\n",
        "def add_indicators_incremental(existing_df, combined_df):\n",
        "    new_rows = combined_df.loc[~combined_df.index.isin(existing_df.index)] if not existing_df.empty else combined_df\n",
        "    if new_rows.empty:\n",
        "        return None\n",
        "    new_rows = safe_numeric(new_rows)\n",
        "    new_rows.sort_index(inplace=True)\n",
        "\n",
        "    # Trend indicators\n",
        "    trend = {\n",
        "        'SMA_10': lambda d: ta.trend.sma_indicator(d['close'],10),\n",
        "        'SMA_50': lambda d: ta.trend.sma_indicator(d['close'],50),\n",
        "        'SMA_200': lambda d: ta.trend.sma_indicator(d['close'],200),\n",
        "        'EMA_10': lambda d: ta.trend.ema_indicator(d['close'],10),\n",
        "        'EMA_50': lambda d: ta.trend.ema_indicator(d['close'],50),\n",
        "        'EMA_200': lambda d: ta.trend.ema_indicator(d['close'],200),\n",
        "        'MACD': lambda d: ta.trend.macd(d['close']),\n",
        "        'MACD_signal': lambda d: ta.trend.macd_signal(d['close']),\n",
        "        'ADX': lambda d: ta.trend.adx(d['high'], d['low'], d['close'],14)\n",
        "    }\n",
        "    # Momentum indicators\n",
        "    momentum = {\n",
        "        'RSI_14': lambda d: ta.momentum.rsi(d['close'],14),\n",
        "        'StochRSI': lambda d: ta.momentum.stochrsi(d['close'],14),\n",
        "        'CCI': lambda d: ta.trend.cci(d['high'],d['low'],d['close'],20),\n",
        "        'ROC': lambda d: ta.momentum.roc(d['close'],12),\n",
        "        'Williams_%R': lambda d: WilliamsRIndicator(d['high'],d['low'],d['close'],14).williams_r()\n",
        "    }\n",
        "    # Volatility\n",
        "    volatility = {\n",
        "        'Bollinger_High': lambda d: ta.volatility.bollinger_hband(d['close'],20,2),\n",
        "        'Bollinger_Low': lambda d: ta.volatility.bollinger_lband(d['close'],20,2),\n",
        "        'ATR': lambda d: ta.volatility.average_true_range(d['high'],d['low'],d['close'],14),\n",
        "        'STDDEV_20': lambda d: d['close'].rolling(20).std()\n",
        "    }\n",
        "    # Volume-based\n",
        "    volume = {}\n",
        "    if 'volume' in new_rows.columns:\n",
        "        volume = {\n",
        "            'OBV': lambda d: ta.volume.on_balance_volume(d['close'],d['volume']),\n",
        "            'MFI': lambda d: ta.volume.money_flow_index(d['high'],d['low'],d['close'],d['volume'],14)\n",
        "        }\n",
        "\n",
        "    indicators = {**trend, **momentum, **volatility, **volume}\n",
        "    for name, func in indicators.items():\n",
        "        try:\n",
        "            new_rows[name] = func(new_rows)\n",
        "        except Exception:\n",
        "            new_rows[name] = np.nan\n",
        "\n",
        "    # Cross signals\n",
        "    if 'EMA_10' in new_rows.columns and 'EMA_50' in new_rows.columns:\n",
        "        new_rows['EMA_10_cross_EMA_50'] = (new_rows['EMA_10'] > new_rows['EMA_50']).astype(int)\n",
        "    if 'EMA_50' in new_rows.columns and 'EMA_200' in new_rows.columns:\n",
        "        new_rows['EMA_50_cross_EMA_200'] = (new_rows['EMA_50'] > new_rows['EMA_200']).astype(int)\n",
        "    if 'SMA_10' in new_rows.columns and 'SMA_50' in new_rows.columns:\n",
        "        new_rows['SMA_10_cross_SMA_50'] = (new_rows['SMA_10'] > new_rows['SMA_50']).astype(int)\n",
        "    if 'SMA_50' in new_rows.columns and 'SMA_200' in new_rows.columns:\n",
        "        new_rows['SMA_50_cross_SMA_200'] = (new_rows['SMA_50'] > new_rows['SMA_200']).astype(int)\n",
        "\n",
        "    # Scale numeric columns safely\n",
        "    numeric_cols = new_rows.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0 and not new_rows[numeric_cols].dropna(how='all').empty:\n",
        "        scaler = MinMaxScaler()\n",
        "        new_rows[numeric_cols] = scaler.fit_transform(new_rows[numeric_cols])\n",
        "\n",
        "    return new_rows\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Worker function\n",
        "# -----------------------------\n",
        "def process_csv_file(csv_file):\n",
        "    combined_df, target_file = combine_csv(csv_file)\n",
        "    existing_pickle = PICKLE_FOLDER / f\"{csv_file.stem}_indicators.pkl\"\n",
        "    existing_df = pd.read_pickle(existing_pickle) if existing_pickle.exists() else pd.DataFrame()\n",
        "\n",
        "    new_indicators = add_indicators_incremental(existing_df, combined_df)\n",
        "    if new_indicators is not None:\n",
        "        updated_df = pd.concat([existing_df, new_indicators]).sort_index()\n",
        "        with lock:\n",
        "            updated_df.to_pickle(existing_pickle, protocol=4)\n",
        "            combined_df.to_csv(target_file)\n",
        "        msg = f\"{csv_file.name} updated with {len(new_indicators)} new rows\"\n",
        "    else:\n",
        "        msg = f\"{csv_file.name} no new rows\"\n",
        "\n",
        "    total_rows = len(combined_df)\n",
        "    print_status(f\"{csv_file.name} total rows: {total_rows}\", \"info\")\n",
        "\n",
        "    return str(existing_pickle) if new_indicators is not None else None, msg\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Process all CSVs in parallel\n",
        "# -----------------------------\n",
        "csv_files = list(CSV_FOLDER.glob(\"*.csv\"))\n",
        "if not csv_files:\n",
        "    print_status(\"No CSVs found to process ‚Äî pipeline will skip\", \"warn\")\n",
        "\n",
        "changed_files = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=min(8, len(csv_files) or 1)) as executor:\n",
        "    futures = [executor.submit(process_csv_file, f) for f in csv_files]\n",
        "    for future in as_completed(futures):\n",
        "        file, msg = future.result()\n",
        "        print_status(msg, \"success\" if file else \"info\")\n",
        "        if file:\n",
        "            changed_files.append(file)\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Commit & push updates\n",
        "# -----------------------------\n",
        "if changed_files:\n",
        "    print_status(f\"Committing {len(changed_files)} updated files...\", \"info\")\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"add\"] + changed_files, check=False)\n",
        "    subprocess.run([\"git\", \"-C\", str(REPO_FOLDER), \"commit\", \"-m\", \"üìà Auto update FX CSVs & indicators\"], check=False)\n",
        "    push_cmd = f\"git -C {REPO_FOLDER} push {REPO_URL} {BRANCH}\"\n",
        "    for attempt in range(3):\n",
        "        if subprocess.run(push_cmd, shell=True).returncode == 0:\n",
        "            print_status(\"Push successful\", \"success\")\n",
        "            break\n",
        "        else:\n",
        "            print_status(f\"Push attempt {attempt+1} failed, retrying...\", \"warn\")\n",
        "            time.sleep(5)\n",
        "else:\n",
        "    print_status(\"No files changed ‚Äî skipping push\", \"info\")\n",
        "\n",
        "print_status(\"All CSVs combined, incremental indicators added, and Git updated successfully.\", \"success\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg92axu0OsoE",
        "outputId": "e9a1b889-6f0a-4d8e-d0e4-559025d635c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ÑπÔ∏è Repo exists, pulling latest...\n",
            "‚úÖ ‚úÖ Repo synced successfully\n",
            "‚ÑπÔ∏è üíπ USD/JPY live price fetched: 153.95\n",
            "üêû üêû Debug SL/TP: live=153.95, ATR=8.17180, mult=1.00, SL=145.7782, TP=162.1218\n",
            "‚ÑπÔ∏è USD/JPY | 1m_7d | signal: 1 | live: 153.95 | SL: 145.7782 | TP: 162.1218\n",
            "üêû üêû Debug SL/TP: live=153.95, ATR=8.15888, mult=1.00, SL=145.79112, TP=162.10888\n",
            "‚ÑπÔ∏è USD/JPY | 5m_1mo | signal: 1 | live: 153.95 | SL: 145.79112 | TP: 162.10888\n",
            "üêû üêû Debug SL/TP: live=153.95, ATR=8.16101, mult=1.00, SL=145.78899, TP=162.11101\n",
            "‚ÑπÔ∏è USD/JPY | 15m_60d | signal: 1 | live: 153.95 | SL: 145.78899 | TP: 162.11101\n",
            "üêû üêû Debug SL/TP: live=153.95, ATR=8.14684, mult=1.00, SL=145.80316, TP=162.09684\n",
            "‚ÑπÔ∏è USD/JPY | 1h_2y | signal: 1 | live: 153.95 | SL: 145.80316 | TP: 162.09684\n",
            "üêû üêû Debug SL/TP: live=153.95, ATR=8.15283, mult=1.00, SL=145.79717, TP=162.10283\n",
            "‚ÑπÔ∏è USD/JPY | 1d_5y | signal: 0 | live: 153.95 | SL: 145.79717 | TP: 162.10283\n",
            "‚úÖ USD/JPY | AGGREGATED SIGNAL: STRONG_LONG\n",
            "‚ÑπÔ∏è üíπ GBP/USD live price fetched: 1.315\n",
            "üêû üêû Debug SL/TP: live=1.315, ATR=0.11764, mult=1.00, SL=1.19736, TP=1.43264\n",
            "‚ÑπÔ∏è GBP/USD | 1m_7d | signal: 1 | live: 1.315 | SL: 1.19736 | TP: 1.43264\n",
            "üêû üêû Debug SL/TP: live=1.315, ATR=0.11783, mult=1.00, SL=1.19717, TP=1.43283\n",
            "‚ÑπÔ∏è GBP/USD | 5m_1mo | signal: 1 | live: 1.315 | SL: 1.19717 | TP: 1.43283\n",
            "üêû üêû Debug SL/TP: live=1.315, ATR=0.11799, mult=1.00, SL=1.19701, TP=1.43299\n",
            "‚ÑπÔ∏è GBP/USD | 15m_60d | signal: 0 | live: 1.315 | SL: 1.19701 | TP: 1.43299\n",
            "üêû üêû Debug SL/TP: live=1.315, ATR=0.11761, mult=1.00, SL=1.19739, TP=1.43261\n",
            "‚ÑπÔ∏è GBP/USD | 1h_2y | signal: 1 | live: 1.315 | SL: 1.19739 | TP: 1.43261\n",
            "üêû üêû Debug SL/TP: live=1.315, ATR=0.11777, mult=1.00, SL=1.19723, TP=1.43277\n",
            "‚ÑπÔ∏è GBP/USD | 1d_5y | signal: 0 | live: 1.315 | SL: 1.19723 | TP: 1.43277\n",
            "‚úÖ GBP/USD | AGGREGATED SIGNAL: HOLD\n",
            "‚ÑπÔ∏è üíπ AUD/USD live price fetched: 0.6542\n",
            "üêû üêû Debug SL/TP: live=0.6542, ATR=0.08307, mult=1.00, SL=0.57113, TP=0.73727\n",
            "‚ÑπÔ∏è AUD/USD | 1m_7d | signal: 1 | live: 0.6542 | SL: 0.57113 | TP: 0.73727\n",
            "üêû üêû Debug SL/TP: live=0.6542, ATR=0.08301, mult=1.00, SL=0.57119, TP=0.73721\n",
            "‚ÑπÔ∏è AUD/USD | 5m_1mo | signal: 1 | live: 0.6542 | SL: 0.57119 | TP: 0.73721\n",
            "üêû üêû Debug SL/TP: live=0.6542, ATR=0.08299, mult=1.00, SL=0.57121, TP=0.73719\n",
            "‚ÑπÔ∏è AUD/USD | 15m_60d | signal: 1 | live: 0.6542 | SL: 0.57121 | TP: 0.73719\n",
            "üêû üêû Debug SL/TP: live=0.6542, ATR=0.08299, mult=1.00, SL=0.57121, TP=0.73719\n",
            "‚ÑπÔ∏è AUD/USD | 1h_2y | signal: 1 | live: 0.6542 | SL: 0.57121 | TP: 0.73719\n",
            "üêû üêû Debug SL/TP: live=0.6542, ATR=0.08297, mult=1.00, SL=0.57123, TP=0.73717\n",
            "‚ÑπÔ∏è AUD/USD | 1d_5y | signal: 1 | live: 0.6542 | SL: 0.57123 | TP: 0.73717\n",
            "‚úÖ AUD/USD | AGGREGATED SIGNAL: STRONG_LONG\n",
            "‚ÑπÔ∏è üíπ EUR/USD live price fetched: 1.16\n",
            "üêû üêû Debug SL/TP: live=1.16, ATR=0.10960, mult=1.00, SL=1.0504, TP=1.2696\n",
            "‚ÑπÔ∏è EUR/USD | 1m_7d | signal: 1 | live: 1.16 | SL: 1.0504 | TP: 1.2696\n",
            "üêû üêû Debug SL/TP: live=1.16, ATR=0.10944, mult=1.00, SL=1.05056, TP=1.26944\n",
            "‚ÑπÔ∏è EUR/USD | 5m_1mo | signal: 1 | live: 1.16 | SL: 1.05056 | TP: 1.26944\n",
            "üêû üêû Debug SL/TP: live=1.16, ATR=0.10950, mult=1.00, SL=1.0505, TP=1.2695\n",
            "‚ÑπÔ∏è EUR/USD | 15m_60d | signal: 0 | live: 1.16 | SL: 1.0505 | TP: 1.2695\n",
            "üêû üêû Debug SL/TP: live=1.16, ATR=0.10962, mult=1.00, SL=1.05038, TP=1.26962\n",
            "‚ÑπÔ∏è EUR/USD | 1h_2y | signal: 1 | live: 1.16 | SL: 1.05038 | TP: 1.26962\n",
            "üêû üêû Debug SL/TP: live=1.16, ATR=0.10947, mult=1.00, SL=1.05053, TP=1.26947\n",
            "‚ÑπÔ∏è EUR/USD | 1d_5y | signal: 0 | live: 1.16 | SL: 1.05053 | TP: 1.26947\n",
            "‚úÖ EUR/USD | AGGREGATED SIGNAL: HOLD\n",
            "‚úÖ ‚úÖ Push successful\n",
            "‚úÖ ‚úÖ Hybrid FX pipeline completed successfully\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# VERSION 3.4 ‚Äì FULLY PERSISTENT SELF-LEARNING HYBRID FX PIPELINE\n",
        "# SGD + RandomForest with historical memory + Price sanity checks\n",
        "# ======================================================\n",
        "\n",
        "import os, time, json, re, shutil, subprocess, pickle, filecmp\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import ta\n",
        "import logging\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.exceptions import NotFittedError\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Logging & Environment\n",
        "# -----------------------------\n",
        "ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "ROOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "PICKLE_FOLDER = ROOT_DIR / \"pickles\"\n",
        "LOGS_FOLDER = ROOT_DIR / \"logs\"\n",
        "for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=LOGS_FOLDER / \"pipeline.log\",\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
        ")\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    icons = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warn\":\"‚ö†Ô∏è\",\"debug\":\"üêû\"}\n",
        "    getattr(logging, level, logging.info)(msg)\n",
        "    print(f\"{icons.get(level,'‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Git & credentials\n",
        "# -----------------------------\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "BROWSERLESS_TOKEN = os.environ.get(\"BROWSERLESS_TOKEN\",\"\")\n",
        "if not FOREX_PAT: raise ValueError(\"‚ùå FOREX_PAT missing!\")\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "subprocess.run([\"git\",\"config\",\"--global\",\"user.name\",GIT_NAME], check=False)\n",
        "subprocess.run([\"git\",\"config\",\"--global\",\"user.email\",GIT_EMAIL], check=False)\n",
        "subprocess.run([\"git\",\"config\",\"--global\",\"credential.helper\",\"store\"], check=False)\n",
        "cred_file = Path.home() / \".git-credentials\"\n",
        "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\\n\")\n",
        "\n",
        "def ensure_repo():\n",
        "    if not (REPO_FOLDER / \".git\").exists():\n",
        "        if REPO_FOLDER.exists(): shutil.rmtree(REPO_FOLDER)\n",
        "        print_status(f\"Cloning repo into {REPO_FOLDER}...\", \"info\")\n",
        "        subprocess.run([\"git\",\"clone\",\"-b\",BRANCH,REPO_URL,str(REPO_FOLDER)], check=True)\n",
        "    else:\n",
        "        print_status(\"Repo exists, pulling latest...\", \"info\")\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"fetch\",\"origin\"], check=False)\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"checkout\",BRANCH], check=False)\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"pull\",\"origin\",BRANCH], check=False)\n",
        "        print_status(\"‚úÖ Repo synced successfully\", \"success\")\n",
        "ensure_repo()\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ CSV Loader + sanity check\n",
        "# -----------------------------\n",
        "def load_csv(path):\n",
        "    if not path.exists():\n",
        "        print_status(f\"‚ö†Ô∏è CSV missing: {path}\", \"warn\")\n",
        "        return None\n",
        "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
        "    df.columns = [c.strip().lower().replace(\" \",\"_\") for c in df.columns]\n",
        "    for col in [\"open\",\"high\",\"low\",\"close\"]:\n",
        "        if col not in df.columns: df[col] = np.nan\n",
        "        df[col] = df[col].ffill().bfill()\n",
        "    df = df[[\"open\",\"high\",\"low\",\"close\"]].dropna(how='all')\n",
        "    # --- price sanity ---\n",
        "    if df['close'].mean() < 0.5 or df['close'].mean() > 200:\n",
        "        print_status(f\"‚ö†Ô∏è CSV {path.name} suspicious price scale (mean={df['close'].mean():.2f}), skipping\", \"warn\")\n",
        "        return None\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Live price fetch\n",
        "# -----------------------------\n",
        "def fetch_live_rate(pair):\n",
        "    if not BROWSERLESS_TOKEN:\n",
        "        print_status(\"‚ö†Ô∏è BROWSERLESS_TOKEN missing\", \"warn\")\n",
        "        return 0\n",
        "    from_currency, to_currency = pair.split(\"/\")\n",
        "    url = f\"https://production-sfo.browserless.io/content?token={BROWSERLESS_TOKEN}\"\n",
        "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
        "    try:\n",
        "        res = requests.post(url, json=payload)\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
        "        rate = float(match.group(1).replace(\",\",\"\")) if match else 0\n",
        "        print_status(f\"üíπ {pair} live price fetched: {rate}\", \"info\")\n",
        "        return rate\n",
        "    except Exception as e:\n",
        "        print_status(f\"Failed to fetch {pair}: {e}\", \"warn\")\n",
        "        return 0\n",
        "\n",
        "def inject_live_price(df, live_price, n_candles=5):\n",
        "    df_copy = df.copy()\n",
        "    n_inject = min(n_candles,len(df_copy))\n",
        "    for i in range(n_inject):\n",
        "        price = live_price * (1 + np.random.uniform(-0.001,0.001))\n",
        "        for col in [\"open\",\"high\",\"low\",\"close\"]:\n",
        "            df_copy.iloc[-n_inject+i, df_copy.columns.get_loc(col)] = price\n",
        "    return df_copy\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Indicators & persistent scaler\n",
        "# -----------------------------\n",
        "scaler_global = MinMaxScaler()\n",
        "INDICATOR_CACHE_FILE = PICKLE_FOLDER / \"indicator_cache.pkl\"\n",
        "\n",
        "def add_indicators_cached(df, pair_name, fit_scaler=True):\n",
        "    cache = {}\n",
        "    if INDICATOR_CACHE_FILE.exists():\n",
        "        try: cache = pickle.load(open(INDICATOR_CACHE_FILE,\"rb\"))\n",
        "        except: pass\n",
        "    last_ts = df.index[-1]\n",
        "    cache_key = f\"{pair_name}_{last_ts}\"\n",
        "    if cache_key in cache: return cache[cache_key]\n",
        "    df_ind = add_indicators(df, fit_scaler)\n",
        "    cache[cache_key] = df_ind\n",
        "    pickle.dump(cache, open(INDICATOR_CACHE_FILE,\"wb\"))\n",
        "    return df_ind\n",
        "\n",
        "def add_indicators(df, fit_scaler=True):\n",
        "    df = df.copy()\n",
        "    df['SMA_50'] = ta.trend.SMAIndicator(df['close'],50).sma_indicator()\n",
        "    df['EMA_20'] = ta.trend.EMAIndicator(df['close'],20).ema_indicator()\n",
        "    df['RSI_14'] = ta.momentum.RSIIndicator(df['close'],14).rsi()\n",
        "    df['MACD'] = ta.trend.MACD(df['close']).macd()\n",
        "    df['Williams_%R'] = ta.momentum.WilliamsRIndicator(df['high'],df['low'],df['close'],14).williams_r()\n",
        "    df['CCI_20'] = ta.trend.CCIIndicator(df['high'],df['low'],df['close'],20).cci()\n",
        "    df['ADX_14'] = ta.trend.ADXIndicator(df['high'],df['low'],df['close'],14).adx()\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0 and not df[numeric_cols].dropna(how='all').empty:\n",
        "        if fit_scaler:\n",
        "            df[numeric_cols] = scaler_global.fit_transform(df[numeric_cols])\n",
        "        else:\n",
        "            try:\n",
        "                df[numeric_cols] = scaler_global.transform(df[numeric_cols])\n",
        "            except NotFittedError:\n",
        "                df[numeric_cols] = scaler_global.fit_transform(df[numeric_cols])\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Persistent ensemble ML with historical memory\n",
        "# -----------------------------\n",
        "def train_predict_ml(df, pair_name):\n",
        "    df = df.dropna()\n",
        "    if len(df)<50: return 0\n",
        "    X = df.drop(columns=['close'], errors='ignore')\n",
        "    X = X if not X.empty else df[['close']]\n",
        "    y = (df['close'].diff()>0).astype(int).fillna(0)\n",
        "    X = X.fillna(0)\n",
        "    safe_pair_name = pair_name.replace(\"/\",\"_\")\n",
        "\n",
        "    # --- SGD ---\n",
        "    sgd_file = PICKLE_FOLDER / f\"{safe_pair_name}_sgd.pkl\"\n",
        "    if sgd_file.exists():\n",
        "        sgd = pickle.load(open(sgd_file,\"rb\"))\n",
        "    else:\n",
        "        sgd = SGDClassifier(max_iter=1000, tol=1e-3)\n",
        "        sgd.partial_fit(X, y, classes=np.array([0,1]))\n",
        "    sgd.partial_fit(X, y)\n",
        "    pickle.dump(sgd, open(sgd_file,\"wb\"))\n",
        "    sgd_pred = int(sgd.predict(X.iloc[[-1]])[0])\n",
        "\n",
        "    # --- RandomForest with historical memory ---\n",
        "    hist_file = PICKLE_FOLDER / f\"{safe_pair_name}_rf_hist.pkl\"\n",
        "    if hist_file.exists():\n",
        "        hist_X, hist_y = pickle.load(open(hist_file,\"rb\"))\n",
        "        hist_X = pd.concat([hist_X, X], ignore_index=True)\n",
        "        hist_y = pd.concat([hist_y, y], ignore_index=True)\n",
        "    else:\n",
        "        hist_X, hist_y = X.copy(), y.copy()\n",
        "\n",
        "    rf_file = PICKLE_FOLDER / f\"{safe_pair_name}_rf.pkl\"\n",
        "    rf = RandomForestClassifier(n_estimators=50, class_weight='balanced', random_state=42)\n",
        "    rf.fit(hist_X, hist_y)\n",
        "    pickle.dump(rf, open(rf_file,\"wb\"))\n",
        "    pickle.dump((hist_X, hist_y), open(hist_file,\"wb\"))  # save full history\n",
        "    rf_pred = int(rf.predict(X.iloc[[-1]])[0])\n",
        "\n",
        "    return 1 if (sgd_pred + rf_pred)>=1 else 0\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ ATR-based SL/TP\n",
        "# -----------------------------\n",
        "def calculate_dynamic_sl_tp(df, live_price):\n",
        "    if live_price==0 or df is None or df.empty: return 0,0\n",
        "    atr = ta.volatility.AverageTrueRange(df['high'],df['low'],df['close'],14).average_true_range().iloc[-1]\n",
        "    mult = 2.0 if atr/live_price<0.05 else 1.0\n",
        "    sl, tp = max(0,round(live_price-atr*mult,5)), round(live_price+atr*mult,5)\n",
        "    print_status(f\"üêû Debug SL/TP: live={live_price}, ATR={atr:.5f}, mult={mult:.2f}, SL={sl}, TP={tp}\", \"debug\")\n",
        "    return sl, tp\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Multi-timeframe resampling\n",
        "# -----------------------------\n",
        "TIMEFRAMES = {\"1m_7d\":\"1min\",\"5m_1mo\":\"5min\",\"15m_60d\":\"15min\",\"1h_2y\":\"1h\",\"1d_5y\":\"1d\"}\n",
        "def resample_timeframe(df,tf_rule,periods):\n",
        "    df = df.copy()\n",
        "    df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
        "    df = df[['open','high','low','close']]\n",
        "    df = df.resample(tf_rule).agg({'open':'first','high':'max','low':'min','close':'last'}).dropna()\n",
        "    return df.tail(periods)\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Weighted aggregation\n",
        "# -----------------------------\n",
        "TIMEFRAME_WEIGHTS = {\"1m_7d\":0.5,\"5m_1mo\":1.0,\"15m_60d\":1.5,\"1h_2y\":2.0,\"1d_5y\":3.0}\n",
        "def weighted_aggregate(signals):\n",
        "    score, total_weight = 0,0\n",
        "    for tf,data in signals.items():\n",
        "        w = TIMEFRAME_WEIGHTS.get(tf,1.0)\n",
        "        score += data['signal']*w\n",
        "        total_weight += w\n",
        "    avg = score/total_weight if total_weight>0 else 0\n",
        "    return \"STRONG_LONG\" if avg>=0.6 else \"STRONG_SHORT\" if avg<=0.4 else \"HOLD\"\n",
        "\n",
        "# -----------------------------\n",
        "# 9Ô∏è‚É£ Process single pair CSV\n",
        "# -----------------------------\n",
        "def process_pair_csv(csv_file):\n",
        "    pair = csv_file.stem.replace(\"_\",\"/\")\n",
        "    df = load_csv(csv_file)\n",
        "    if df is None: return pair, {}, \"HOLD\"\n",
        "    live_price = fetch_live_rate(pair)\n",
        "    df = inject_live_price(df, live_price)\n",
        "    signals = {}\n",
        "    for tf_name, tf_rule in TIMEFRAMES.items():\n",
        "        periods_map = {\"1min\":7*24*60,\"5min\":30*24*12,\"15min\":60*24*4,\"1h\":24*730,\"1d\":5*365}\n",
        "        df_tf = resample_timeframe(df, tf_rule, periods_map.get(tf_rule,100))\n",
        "        df_tf = add_indicators_cached(df_tf, pair, fit_scaler=False)\n",
        "        df_tf = inject_live_price(df_tf, live_price)\n",
        "        ml_signal = train_predict_ml(df_tf, pair)\n",
        "        sl, tp = calculate_dynamic_sl_tp(df_tf, live_price)\n",
        "        signals[tf_name] = {\"signal\":ml_signal,\"live\":live_price,\"SL\":sl,\"TP\":tp}\n",
        "        print_status(f\"{pair} | {tf_name} | signal: {ml_signal} | live: {live_price} | SL: {sl} | TP: {tp}\", \"info\")\n",
        "    agg_signal = weighted_aggregate(signals)\n",
        "    print_status(f\"{pair} | AGGREGATED SIGNAL: {agg_signal}\", \"success\")\n",
        "    return pair, signals, agg_signal\n",
        "\n",
        "# -----------------------------\n",
        "# üîü Full pipeline\n",
        "# -----------------------------\n",
        "def run_hybrid_pipeline():\n",
        "    csv_files = list(CSV_FOLDER.glob(\"*.csv\"))\n",
        "    aggregated_signals = {}\n",
        "    for csv_file in csv_files:\n",
        "        pair, signals, agg_signal = process_pair_csv(csv_file)\n",
        "        aggregated_signals[pair] = {\"signals\":signals,\"aggregated\":agg_signal}\n",
        "\n",
        "    json_file = REPO_FOLDER / \"latest_signals.json\"\n",
        "    tmp_file = REPO_FOLDER / \"latest_signals_tmp.json\"\n",
        "    with open(tmp_file,\"w\") as f:\n",
        "        json.dump({\"timestamp\":datetime.now(timezone.utc).isoformat(),\"pairs\":aggregated_signals},f, indent=2)\n",
        "\n",
        "    if not json_file.exists() or not filecmp.cmp(tmp_file,json_file):\n",
        "        tmp_file.replace(json_file)\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"add\", str(json_file)], check=False)\n",
        "        subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"commit\",\"-m\",\"üìà Auto update FX JSON\"], check=False)\n",
        "        for attempt in range(3):\n",
        "            if subprocess.run([\"git\",\"-C\",str(REPO_FOLDER),\"push\"], check=False).returncode==0:\n",
        "                print_status(\"‚úÖ Push successful\",\"success\"); break\n",
        "            time.sleep(5)\n",
        "    else:\n",
        "        print_status(\"‚ÑπÔ∏è JSON unchanged ‚Äî skipping Git push\", \"info\")\n",
        "\n",
        "    return aggregated_signals\n",
        "\n",
        "# -----------------------------\n",
        "# Execute\n",
        "# -----------------------------\n",
        "if __name__==\"__main__\":\n",
        "    signals = run_hybrid_pipeline()\n",
        "    print_status(\"‚úÖ Hybrid FX pipeline completed successfully\", \"success\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZPAZQi88SYG",
        "outputId": "dea769e7-38fa-4120-fede-d3afbf81a1fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ JSON processing complete (0 files)\n",
            "‚úÖ Processed CSV AUD_USD.csv ‚Üí AUD_USD.pkl\n",
            "‚úÖ Processed CSV GBP_USD.csv ‚Üí GBP_USD.pkl\n",
            "‚úÖ Processed CSV USD_JPY.csv ‚Üí USD_JPY.pkl\n",
            "‚úÖ Processed CSV EUR_USD.csv ‚Üí EUR_USD.pkl\n",
            "üîó Merged 1 files ‚Üí EUR_USD_2244.pkl\n",
            "üîó Merged 1 files ‚Üí GBP_USD_2244.pkl\n",
            "üîó Merged 1 files ‚Üí USD_JPY_2244.pkl\n",
            "üîó Merged 1 files ‚Üí AUD_USD_2244.pkl\n",
            "üéØ Unified pipeline complete ‚Äî merged pickles saved in /content/forex-alpha-models/merged_data_pickles\n",
            "üîç GBP_USD_2244.pkl last rows:\n",
            "               open    high     low   close  raw_open  raw_high  raw_low  \\\n",
            "2025-10-29  1.3268  1.3280  1.3137  1.3193    1.3268    1.3280   1.3137   \n",
            "2025-10-30  1.3192  1.3218  1.3114  1.3150    1.3192    1.3218   1.3114   \n",
            "2025-10-31  1.3150  1.3164  1.3094  1.3151    1.3150    1.3164   1.3094   \n",
            "\n",
            "            raw_close    SMA_10    EMA_10    SMA_50    EMA_50    RSI_14  \\\n",
            "2025-10-29     1.3193  0.640016  0.638858  0.653782  0.652496  0.384846   \n",
            "2025-10-30     1.3150  0.638672  0.637420  0.653528  0.651999  0.355702   \n",
            "2025-10-31     1.3151  0.637363  0.636251  0.653167  0.651523  0.357271   \n",
            "\n",
            "            Williams_%R       ATR  \n",
            "2025-10-29     0.167665  0.189790  \n",
            "2025-10-30     0.100840  0.192449  \n",
            "2025-10-31     0.151194  0.189566  \n",
            "üîç EUR_USD_2244.pkl last rows:\n",
            "               open    high     low   close  raw_open  raw_high  raw_low  \\\n",
            "2025-10-29  1.1650  1.1665  1.1577  1.1599    1.1650    1.1665   1.1577   \n",
            "2025-10-30  1.1600  1.1637  1.1546  1.1565    1.1600    1.1637   1.1546   \n",
            "2025-10-31  1.1565  1.1577  1.1520  1.1534    1.1565    1.1577   1.1520   \n",
            "\n",
            "            raw_close    SMA_10    EMA_10    SMA_50    EMA_50    RSI_14  \\\n",
            "2025-10-29     1.1599  0.733362  0.733015  0.745530  0.742985  0.514309   \n",
            "2025-10-30     1.1565  0.732593  0.732286  0.745479  0.742755  0.474283   \n",
            "2025-10-31     1.1534  0.731855  0.731335  0.745248  0.742456  0.440613   \n",
            "\n",
            "            Williams_%R       ATR  \n",
            "2025-10-29     0.313830  0.194700  \n",
            "2025-10-30     0.132979  0.201167  \n",
            "2025-10-31     0.067308  0.199476  \n",
            "üîç USD_JPY_2244.pkl last rows:\n",
            "               open    high     low   close  raw_open  raw_high  raw_low  \\\n",
            "2025-10-29  152.10  153.05  151.53  152.71    152.10    153.05   151.53   \n",
            "2025-10-30  152.72  154.44  152.15  154.12    152.72    154.44   152.15   \n",
            "2025-10-31  154.11  154.41  153.63  154.00    154.11    154.41   153.63   \n",
            "\n",
            "            raw_close    SMA_10    EMA_10    SMA_50    EMA_50    RSI_14  \\\n",
            "2025-10-29     152.71  0.942447  0.945247  0.944349  0.948375  0.699460   \n",
            "2025-10-30     154.12  0.944743  0.947548  0.945077  0.949452  0.764794   \n",
            "2025-10-31     154.00  0.946840  0.949296  0.945972  0.950457  0.754508   \n",
            "\n",
            "            Williams_%R       ATR  \n",
            "2025-10-29     0.856777  0.408986  \n",
            "2025-10-30     0.937008  0.436854  \n",
            "2025-10-31     0.913386  0.425090  \n",
            "üîç AUD_USD_2244.pkl last rows:\n",
            "               open    high     low   close  raw_open  raw_high  raw_low  \\\n",
            "2025-10-29  0.6584  0.6617  0.6555  0.6572    0.6584    0.6617   0.6555   \n",
            "2025-10-30  0.6573  0.6597  0.6531  0.6553    0.6573    0.6597   0.6531   \n",
            "2025-10-31  0.6554  0.6560  0.6531  0.6543    0.6554    0.6560   0.6531   \n",
            "\n",
            "            raw_close    SMA_10    EMA_10    SMA_50    EMA_50    RSI_14  \\\n",
            "2025-10-29     0.6572  0.597933  0.599402  0.612695  0.610216  0.677751   \n",
            "2025-10-30     0.6553  0.598566  0.599661  0.612946  0.610253  0.636572   \n",
            "2025-10-31     0.6543  0.599015  0.599706  0.613049  0.610253  0.615380   \n",
            "\n",
            "            Williams_%R       ATR  \n",
            "2025-10-29     0.748603  0.140171  \n",
            "2025-10-30     0.642458  0.143285  \n",
            "2025-10-31     0.586592  0.138704  \n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# VERSION 3.6 ‚Äì Unified Loader + Merge Pickles (Production Ready)\n",
        "# Fully Safe | Threaded | Compatible with Hybrid FX Pipeline\n",
        "# Added: Data validation, ATR floors, debug prints, raw price preservation\n",
        "# ======================================================\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import warnings\n",
        "import ta\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.volatility import AverageTrueRange\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# 0Ô∏è‚É£ Environment & folders\n",
        "# -----------------------------\n",
        "ROOT_DIR = Path(\"/content/forex-alpha-models\")\n",
        "CSV_FOLDER = ROOT_DIR / \"csvs\"\n",
        "REPO_FOLDER = ROOT_DIR / \"forex-ai-models\"\n",
        "TEMP_PICKLE_FOLDER = ROOT_DIR / \"temp_pickles\"\n",
        "FINAL_PICKLE_FOLDER = ROOT_DIR / \"merged_data_pickles\"\n",
        "\n",
        "for folder in [CSV_FOLDER, TEMP_PICKLE_FOLDER, FINAL_PICKLE_FOLDER, REPO_FOLDER]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "JSON_FILE = REPO_FOLDER / \"latest_signals.json\"\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Safe Indicator Generator\n",
        "# -----------------------------\n",
        "def add_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    df = df[(df[[\"open\", \"high\", \"low\", \"close\"]] > 0).all(axis=1)]\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # --- Preserve raw OHLC prices for GA ---\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if col in df.columns:\n",
        "            df[f\"raw_{col}\"] = df[col].copy()\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "\n",
        "        try:\n",
        "            if len(df['close']) >= 10:\n",
        "                df['SMA_10'] = ta.trend.sma_indicator(df['close'], 10)\n",
        "                df['EMA_10'] = ta.trend.ema_indicator(df['close'], 10)\n",
        "            if len(df['close']) >= 50:\n",
        "                df['SMA_50'] = ta.trend.sma_indicator(df['close'], 50)\n",
        "                df['EMA_50'] = ta.trend.ema_indicator(df['close'], 50)\n",
        "            if len(df['close']) >= 14:\n",
        "                df['RSI_14'] = ta.momentum.rsi(df['close'], 14)\n",
        "            if all(col in df.columns for col in ['high', 'low', 'close']) and len(df['close']) >= 14:\n",
        "                df['Williams_%R'] = WilliamsRIndicator(df['high'], df['low'], df['close'], 14).williams_r()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Indicator calculation failed: {e}\")\n",
        "\n",
        "        # --- Safe ATR ---\n",
        "        try:\n",
        "            if all(col in df.columns for col in ['high', 'low', 'close']):\n",
        "                window = 14\n",
        "                if len(df) >= window:\n",
        "                    df['ATR'] = AverageTrueRange(\n",
        "                        df['high'], df['low'], df['close'], window=window\n",
        "                    ).average_true_range().fillna(1e-5).clip(lower=1e-4)\n",
        "                else:\n",
        "                    df['ATR'] = 1e-4\n",
        "        except Exception as e:\n",
        "            df['ATR'] = 1e-4\n",
        "            print(f\"‚ö†Ô∏è ATR calculation failed: {e}\")\n",
        "\n",
        "        # --- Scale only non-price numeric columns ---\n",
        "        numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns if not df[c].isna().all()]\n",
        "        protected_cols = [\n",
        "            \"open\", \"high\", \"low\", \"close\",\n",
        "            \"raw_open\", \"raw_high\", \"raw_low\", \"raw_close\"\n",
        "        ]\n",
        "        numeric_cols = [c for c in numeric_cols if c not in protected_cols]\n",
        "\n",
        "        if numeric_cols:\n",
        "            scaler = MinMaxScaler()\n",
        "            df[numeric_cols] = scaler.fit_transform(df[numeric_cols].fillna(0) + 1e-8)\n",
        "\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Safe CSV Processing\n",
        "# -----------------------------\n",
        "def process_csv_file(csv_file: Path, save_folder: Path):\n",
        "    try:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\", category=pd.errors.ParserWarning)\n",
        "            df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö™ Skipped empty CSV: {csv_file.name}\")\n",
        "            return None\n",
        "\n",
        "        df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "        df = add_indicators(df)\n",
        "        if df.empty:\n",
        "            print(f\"‚ö™ Skipped CSV after filtering invalid prices: {csv_file.name}\")\n",
        "            return None\n",
        "\n",
        "        out_file = save_folder / f\"{csv_file.stem}.pkl\"\n",
        "        df.to_pickle(out_file)\n",
        "        print(f\"‚úÖ Processed CSV {csv_file.name} ‚Üí {out_file.name}\")\n",
        "        return out_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed CSV {csv_file.name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ JSON Processing\n",
        "# -----------------------------\n",
        "def process_json_file(json_file: Path, save_folder: Path):\n",
        "    try:\n",
        "        with open(json_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load JSON: {e}\")\n",
        "        return []\n",
        "\n",
        "    signals_data = data.get(\"pairs\", {})\n",
        "    timestamp = pd.to_datetime(data.get(\"timestamp\"), utc=True)\n",
        "    processed_files = []\n",
        "\n",
        "    for pair, info in signals_data.items():\n",
        "        signals = info.get(\"signals\", {})\n",
        "        dfs = []\n",
        "\n",
        "        for tf_name, tf_info in signals.items():\n",
        "            df = pd.DataFrame({\n",
        "                \"live\": [tf_info.get(\"live\")],\n",
        "                \"SL\": [tf_info.get(\"SL\")],\n",
        "                \"TP\": [tf_info.get(\"TP\")],\n",
        "                \"signal\": [tf_info.get(\"signal\")]\n",
        "            }, index=[timestamp])\n",
        "            df[\"timeframe\"] = tf_name\n",
        "            df = add_indicators(df)\n",
        "            if not df.empty:\n",
        "                dfs.append(df)\n",
        "\n",
        "        if dfs:\n",
        "            df_pair = pd.concat(dfs)\n",
        "            out_file = save_folder / f\"{pair.replace('/', '_')}.pkl\"\n",
        "            df_pair.to_pickle(out_file)\n",
        "            print(f\"‚úÖ Processed JSON {pair} ‚Üí {out_file.name}\")\n",
        "            processed_files.append(out_file)\n",
        "\n",
        "    return processed_files\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Safe Pickle Merger\n",
        "# -----------------------------\n",
        "def merge_pickles(temp_folder: Path, final_folder: Path, keep_last: int = 5):\n",
        "    pickles = list(temp_folder.glob(\"*.pkl\"))\n",
        "    if not pickles:\n",
        "        print(\"‚ö™ No temporary pickles to merge.\")\n",
        "        return\n",
        "\n",
        "    pairs = set(p.stem.split('.')[0] for p in pickles)\n",
        "\n",
        "    for pair in pairs:\n",
        "        pair_files = [p for p in pickles if p.stem.startswith(pair)]\n",
        "        dfs = [pd.read_pickle(p) for p in pair_files if p.exists() and p.stat().st_size > 0]\n",
        "\n",
        "        if not dfs:\n",
        "            print(f\"‚ö™ Skipped {pair} (no valid pickles)\")\n",
        "            continue\n",
        "\n",
        "        merged_df = pd.concat(dfs, ignore_index=False).sort_index().drop_duplicates()\n",
        "        # Changed filename suffix to match the expected format in W4XoZxs-TrDh\n",
        "        merged_file = final_folder / f\"{pair}_2244.pkl\"\n",
        "        merged_df.to_pickle(merged_file)\n",
        "        print(f\"üîó Merged {len(pair_files)} files ‚Üí {merged_file.name}\")\n",
        "\n",
        "        existing = sorted(final_folder.glob(f\"{pair}_*.pkl\"), key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "        for old_file in existing[keep_last:]:\n",
        "            try:\n",
        "                old_file.unlink()\n",
        "                print(f\"üßπ Removed old file: {old_file.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not remove {old_file.name}: {e}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Unified Pipeline Runner\n",
        "# -----------------------------\n",
        "def run_unified_pipeline():\n",
        "    temp_files = []\n",
        "\n",
        "    # Process JSON first\n",
        "    if JSON_FILE.exists():\n",
        "        temp_files += process_json_file(JSON_FILE, TEMP_PICKLE_FOLDER)\n",
        "        print(f\"‚úÖ JSON processing complete ({len(temp_files)} files)\")\n",
        "\n",
        "    # Process CSVs concurrently\n",
        "    csv_files = list(CSV_FOLDER.glob(\"*.csv\"))\n",
        "    if csv_files:\n",
        "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "            futures = [executor.submit(process_csv_file, f, TEMP_PICKLE_FOLDER) for f in csv_files]\n",
        "            for fut in as_completed(futures):\n",
        "                result = fut.result()\n",
        "                if result:\n",
        "                    temp_files.append(result)\n",
        "\n",
        "    # Merge all pickles safely\n",
        "    merge_pickles(TEMP_PICKLE_FOLDER, FINAL_PICKLE_FOLDER)\n",
        "    print(f\"üéØ Unified pipeline complete ‚Äî merged pickles saved in {FINAL_PICKLE_FOLDER}\")\n",
        "\n",
        "    # Debug: print last few rows of each merged pickle\n",
        "    for pkl_file in FINAL_PICKLE_FOLDER.glob(\"*.pkl\"):\n",
        "        df = pd.read_pickle(pkl_file)\n",
        "        print(f\"üîç {pkl_file.name} last rows:\\n\", df.tail(3))\n",
        "\n",
        "    return FINAL_PICKLE_FOLDER\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Execute\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    final_folder = run_unified_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Ultimate Hybrid Forex Pipeline v7.1 (Revolutionary Multi-Strategy AI)\n",
        "========================================================================\n",
        "NEW v7.1 Features:\n",
        "- ‚úÖ Normal trading mode on Tuesday-Friday (no replay)\n",
        "- ‚úÖ Weekend replay mode with random historical periods\n",
        "- ‚úÖ Monday live trading with 10-iteration limit\n",
        "- ‚úÖ Dynamic mode switching based on day of week\n",
        "- ‚úÖ Single unified competition email dashboard\n",
        "- ‚úÖ Multi-objective fitness (Profit + Sharpe + Drawdown)\n",
        "- ‚úÖ Experience replay prioritization\n",
        "- ‚úÖ True model diversity (Conservative, Aggressive, Balanced)\n",
        "- ‚úÖ Cross-competition learning & pattern discovery\n",
        "\"\"\"\n",
        "\n",
        "# ======================================================\n",
        "# 0Ô∏è‚É£ Imports\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import smtplib\n",
        "import subprocess\n",
        "import time\n",
        "import logging\n",
        "import base64\n",
        "import sqlite3\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from joblib import Parallel, delayed\n",
        "from scipy import stats\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Logging & Environment\n",
        "# ======================================================\n",
        "logging.basicConfig(\n",
        "    filename='forex_pipeline.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(message)s'\n",
        ")\n",
        "\n",
        "def print_status(msg, level=\"info\"):\n",
        "    \"\"\"Enhanced status printing\"\"\"\n",
        "    icons = {\n",
        "        \"info\": \"‚ÑπÔ∏è\",\n",
        "        \"success\": \"‚úÖ\",\n",
        "        \"warn\": \"‚ö†Ô∏è\",\n",
        "        \"debug\": \"üêû\",\n",
        "        \"error\": \"‚ùå\"\n",
        "    }\n",
        "    log_level = \"warning\" if level == \"warn\" else level\n",
        "    getattr(logging, log_level, logging.info)(msg)\n",
        "    print(f\"{icons.get(level, '‚ÑπÔ∏è')} {msg}\")\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
        "ROOT_DIR = Path(\"/content\") if IN_COLAB else Path(\".\")\n",
        "ROOT_PATH = ROOT_DIR / \"forex-alpha-models\"\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Folder Setup\n",
        "# ======================================================\n",
        "FINAL_PICKLE_FOLDER = ROOT_PATH / \"merged_data_pickles\"\n",
        "PICKLE_FOLDER = FINAL_PICKLE_FOLDER\n",
        "REPO_FOLDER = ROOT_PATH / \"forex-ai-models\"\n",
        "\n",
        "for f in [PICKLE_FOLDER, REPO_FOLDER]:\n",
        "    f.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "os.chdir(ROOT_PATH)\n",
        "logging.info(f\"Environment ready ‚Äî Working directory: {ROOT_PATH.resolve()}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Git Setup\n",
        "# ======================================================\n",
        "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
        "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
        "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
        "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
        "FOREX_PAT = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
        "BRANCH = \"main\"\n",
        "\n",
        "if not FOREX_PAT and IN_GHA:\n",
        "    logging.warning(\"FOREX_PAT missing in GitHub Actions\")\n",
        "\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Gmail Config\n",
        "# ======================================================\n",
        "GMAIL_USER = \"nakatonabira3@gmail.com\"\n",
        "GMAIL_APP_PASSWORD = \"gmwohahtltmcewug\"\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Core Config\n",
        "# ======================================================\n",
        "PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
        "ATR_PERIOD = 14\n",
        "MIN_ATR = 1e-5\n",
        "BASE_CAPITAL = 100\n",
        "MAX_POSITION_FRACTION = 0.1\n",
        "MAX_TRADE_CAP = BASE_CAPITAL * 0.05\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£B Multi-Model Competition Config (Enhanced v7.1)\n",
        "# ======================================================\n",
        "COMPETITION_MODELS = {\n",
        "    \"Conservative\": {\n",
        "        \"pop_size\": 12,\n",
        "        \"generations\": 15,\n",
        "        \"mutation_rate\": 0.15,\n",
        "        \"risk_range\": (0.005, 0.015),\n",
        "        \"confidence_range\": (0.65, 0.85),\n",
        "        \"atr_sl_range\": (2.0, 3.0),\n",
        "        \"atr_tp_range\": (1.5, 2.5),\n",
        "        \"color\": \"üîµ\",\n",
        "        \"hex_color\": \"#4A90E2\",\n",
        "        \"strategy\": \"Safety First - High confidence only\",\n",
        "        \"model_type\": \"random_forest\"\n",
        "    },\n",
        "    \"Aggressive\": {\n",
        "        \"pop_size\": 12,\n",
        "        \"generations\": 20,\n",
        "        \"mutation_rate\": 0.35,\n",
        "        \"risk_range\": (0.015, 0.03),\n",
        "        \"confidence_range\": (0.3, 0.6),\n",
        "        \"atr_sl_range\": (1.0, 2.0),\n",
        "        \"atr_tp_range\": (2.5, 4.0),\n",
        "        \"color\": \"üî¥\",\n",
        "        \"hex_color\": \"#E74C3C\",\n",
        "        \"strategy\": \"High Risk High Reward\",\n",
        "        \"model_type\": \"momentum\"\n",
        "    },\n",
        "    \"Balanced\": {\n",
        "        \"pop_size\": 12,\n",
        "        \"generations\": 20,\n",
        "        \"mutation_rate\": 0.25,\n",
        "        \"risk_range\": (0.01, 0.02),\n",
        "        \"confidence_range\": (0.45, 0.7),\n",
        "        \"atr_sl_range\": (1.5, 2.5),\n",
        "        \"atr_tp_range\": (2.0, 3.5),\n",
        "        \"color\": \"üü¢\",\n",
        "        \"hex_color\": \"#2ECC71\",\n",
        "        \"strategy\": \"Optimal Risk/Reward Balance\",\n",
        "        \"model_type\": \"hybrid\"\n",
        "    }\n",
        "}\n",
        "\n",
        "POP_SIZE = 12\n",
        "GENERATIONS = 20\n",
        "MUTATION_RATE = 0.25\n",
        "TOURNAMENT_SIZE = 3\n",
        "EPS = 1e-8\n",
        "MAX_ATR_SL = 3.0\n",
        "MAX_ATR_TP = 4.0\n",
        "MIN_ATR_DISTANCE = 0.5\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£C Smart Weekend/Monday System Config (v7.1)\n",
        "# ======================================================\n",
        "REPLAY_CONFIG = {\n",
        "    \"weekend_replay_enabled\": True,\n",
        "    \"monday_live_enabled\": True,\n",
        "    \"monday_max_runs\": 10,\n",
        "    \"normal_days_enabled\": True,  # NEW: Enable normal trading Tue-Fri\n",
        "    \"random_period\": True,\n",
        "    \"random_period_days\": 90,\n",
        "    \"speed\": \"fast\",\n",
        "    \"prevent_lookahead\": True,\n",
        "    \"save_to_memory\": True,\n",
        "}\n",
        "\n",
        "RANDOM_REPLAY_PERIODS = [\n",
        "    (\"2023-01-01\", \"2023-03-31\"),\n",
        "    (\"2023-04-01\", \"2023-06-30\"),\n",
        "    (\"2023-07-01\", \"2023-09-30\"),\n",
        "    (\"2023-10-01\", \"2023-12-31\"),\n",
        "    (\"2024-01-01\", \"2024-03-31\"),\n",
        "    (\"2024-04-01\", \"2024-06-30\"),\n",
        "    (\"2024-07-01\", \"2024-09-30\"),\n",
        "]\n",
        "\n",
        "# Monday run tracking\n",
        "MONDAY_RUNS_FILE = REPO_FOLDER / \"monday_runs_count.pkl\"\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£D Autonomy Settings\n",
        "# ======================================================\n",
        "CHECK_INTERVAL = 25 * 60\n",
        "MAX_TRADE_MEMORY = 200\n",
        "MIN_SIGNAL_IMPROVEMENT = 0.01\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/rahim-dotAI/forex-ai-models/main/IMG_1599.jpeg\"\n",
        "\n",
        "# File paths\n",
        "BEST_CHROM_FILE = REPO_FOLDER / \"best_chromosome.pkl\"\n",
        "TRADE_MEMORY_FILE = REPO_FOLDER / \"trade_memory.pkl\"\n",
        "POPULATION_FILE = REPO_FOLDER / \"population.pkl\"\n",
        "GEN_COUNT_FILE = REPO_FOLDER / \"generation_count.pkl\"\n",
        "SIGNALS_JSON_PATH = REPO_FOLDER / \"broker_signals.json\"\n",
        "GA_RESULTS_CSV = REPO_FOLDER / \"best_ga_params.csv\"\n",
        "PERFORMANCE_HISTORY_FILE = REPO_FOLDER / \"performance_history.pkl\"\n",
        "GA_PROGRESS_FILE = REPO_FOLDER / \"ga_progress.pkl\"\n",
        "INFINITE_MEMORY_DB = REPO_FOLDER / \"infinite_memory.db\"\n",
        "COMPETITION_HISTORY_DB = REPO_FOLDER / \"competition_history.db\"\n",
        "\n",
        "for path in [BEST_CHROM_FILE, TRADE_MEMORY_FILE, POPULATION_FILE,\n",
        "             GEN_COUNT_FILE, SIGNALS_JSON_PATH, GA_RESULTS_CSV,\n",
        "             PERFORMANCE_HISTORY_FILE, GA_PROGRESS_FILE]:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ Enhanced Infinite Memory System (v7.1)\n",
        "# ======================================================\n",
        "class InfiniteMemorySystem:\n",
        "    \"\"\"Enhanced memory with prioritized experience replay\"\"\"\n",
        "\n",
        "    def __init__(self, db_path=INFINITE_MEMORY_DB):\n",
        "        self.db_path = db_path\n",
        "        self.conn = None\n",
        "        self.initialize_database()\n",
        "\n",
        "    def initialize_database(self):\n",
        "        self.conn = sqlite3.connect(str(self.db_path))\n",
        "        cursor = self.conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS signals_history (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                direction TEXT NOT NULL,\n",
        "                entry_price REAL NOT NULL,\n",
        "                sl_price REAL,\n",
        "                tp_price REAL,\n",
        "                atr REAL,\n",
        "                confidence INTEGER,\n",
        "                chromosome_hash TEXT,\n",
        "                generation INTEGER,\n",
        "                model_name TEXT,\n",
        "                mode TEXT DEFAULT 'normal'\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Migration: Add mode column if it doesn't exist\n",
        "        try:\n",
        "            cursor.execute(\"ALTER TABLE signals_history ADD COLUMN mode TEXT DEFAULT 'normal'\")\n",
        "            self.conn.commit()\n",
        "            print_status(\"Added 'mode' column to signals_history\", \"success\")\n",
        "        except sqlite3.OperationalError:\n",
        "            pass  # Column already exists\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS trade_results (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                signal_id INTEGER,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                entry_price REAL NOT NULL,\n",
        "                exit_price REAL NOT NULL,\n",
        "                direction TEXT NOT NULL,\n",
        "                pnl REAL NOT NULL,\n",
        "                was_correct BOOLEAN NOT NULL,\n",
        "                price_change_pct REAL,\n",
        "                duration_minutes INTEGER,\n",
        "                model_name TEXT,\n",
        "                priority_score REAL DEFAULT 0.5,\n",
        "                mode TEXT,\n",
        "                FOREIGN KEY (signal_id) REFERENCES signals_history(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS competition_results (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                iteration INTEGER NOT NULL,\n",
        "                model_name TEXT NOT NULL,\n",
        "                total_pnl REAL,\n",
        "                accuracy REAL,\n",
        "                sharpe_ratio REAL,\n",
        "                max_drawdown REAL,\n",
        "                total_trades INTEGER,\n",
        "                successful_trades INTEGER,\n",
        "                rank INTEGER,\n",
        "                mode TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS pattern_library (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                discovered_date TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                pattern_type TEXT NOT NULL,\n",
        "                pattern_description TEXT,\n",
        "                success_rate REAL,\n",
        "                avg_pnl REAL,\n",
        "                occurrences INTEGER DEFAULT 1,\n",
        "                last_seen TEXT,\n",
        "                confidence_score REAL,\n",
        "                model_discovered TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS market_regimes (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT NOT NULL,\n",
        "                pair TEXT NOT NULL,\n",
        "                regime TEXT NOT NULL,\n",
        "                volatility REAL,\n",
        "                trend_strength REAL,\n",
        "                best_model TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        self.conn.commit()\n",
        "        print_status(\"Enhanced Infinite Memory initialized\", \"success\")\n",
        "\n",
        "    def save_signal(self, pair, direction, entry, sl, tp, atr, confidence, chrom_hash, generation, model_name, mode=\"normal\"):\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            INSERT INTO signals_history\n",
        "            (timestamp, pair, direction, entry_price, sl_price, tp_price, atr, confidence,\n",
        "             chromosome_hash, generation, model_name, mode)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', (datetime.now().isoformat(), pair, direction, entry, sl, tp, atr,\n",
        "              confidence, chrom_hash, generation, model_name, mode))\n",
        "        self.conn.commit()\n",
        "        return cursor.lastrowid\n",
        "\n",
        "    def save_trade_result(self, signal_id, pair, entry, exit_price, direction, pnl,\n",
        "                         correct, model_name, mode=\"normal\", duration=25):\n",
        "        price_change_pct = ((exit_price - entry) / entry) * 100\n",
        "        priority = abs(pnl) * (1.0 if correct else 0.5)\n",
        "\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            INSERT INTO trade_results\n",
        "            (signal_id, timestamp, pair, entry_price, exit_price, direction, pnl,\n",
        "             was_correct, price_change_pct, duration_minutes, model_name, priority_score, mode)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', (signal_id, datetime.now().isoformat(), pair, entry, exit_price, direction,\n",
        "              pnl, correct, price_change_pct, duration, model_name, priority, mode))\n",
        "        self.conn.commit()\n",
        "\n",
        "    def save_competition_result(self, iteration, model_name, total_pnl, accuracy,\n",
        "                               sharpe, max_dd, total_trades, successful, rank, mode=\"normal\"):\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            INSERT INTO competition_results\n",
        "            (timestamp, iteration, model_name, total_pnl, accuracy, sharpe_ratio,\n",
        "             max_drawdown, total_trades, successful_trades, rank, mode)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', (datetime.now().isoformat(), iteration, model_name, total_pnl, accuracy,\n",
        "              sharpe, max_dd, total_trades, successful, rank, mode))\n",
        "        self.conn.commit()\n",
        "\n",
        "    def get_prioritized_experiences(self, model_name, limit=50):\n",
        "        \"\"\"Get highest priority trades for learning\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            SELECT * FROM trade_results\n",
        "            WHERE model_name = ?\n",
        "            ORDER BY priority_score DESC, timestamp DESC\n",
        "            LIMIT ?\n",
        "        ''', (model_name, limit))\n",
        "        return cursor.fetchall()\n",
        "\n",
        "    def get_model_trade_history(self, model_name, days=7):\n",
        "        \"\"\"Get recent trade history for a model\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        since_date = (datetime.now() - timedelta(days=days)).isoformat()\n",
        "\n",
        "        cursor.execute('''\n",
        "            SELECT\n",
        "                COUNT(*) as total_trades,\n",
        "                SUM(CASE WHEN was_correct THEN 1 ELSE 0 END) as successful_trades,\n",
        "                AVG(pnl) as avg_pnl,\n",
        "                SUM(pnl) as total_pnl\n",
        "            FROM trade_results\n",
        "            WHERE model_name = ? AND timestamp > ?\n",
        "        ''', (model_name, since_date))\n",
        "\n",
        "        result = cursor.fetchone()\n",
        "        return {\n",
        "            'total_trades': result[0] if result[0] else 0,\n",
        "            'successful_trades': result[1] if result[1] else 0,\n",
        "            'avg_pnl': result[2] if result[2] else 0.0,\n",
        "            'total_pnl': result[3] if result[3] else 0.0,\n",
        "            'accuracy': (result[1] / result[0] * 100) if result[0] else 0.0\n",
        "        }\n",
        "\n",
        "    def get_competition_leaderboard(self, last_n_iterations=10):\n",
        "        \"\"\"Get average rankings over recent iterations\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            SELECT\n",
        "                model_name,\n",
        "                AVG(rank) as avg_rank,\n",
        "                AVG(total_pnl) as avg_pnl,\n",
        "                AVG(accuracy) as avg_accuracy,\n",
        "                COUNT(*) as appearances\n",
        "            FROM competition_results\n",
        "            WHERE iteration >= (SELECT MAX(iteration) - ? FROM competition_results)\n",
        "            GROUP BY model_name\n",
        "            ORDER BY avg_rank ASC\n",
        "        ''', (last_n_iterations,))\n",
        "        return cursor.fetchall()\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£A Weekend/Monday Mode Manager (v7.1)\n",
        "# ======================================================\n",
        "class WeekendMondayManager:\n",
        "    \"\"\"Intelligent system: Replay on weekends, Live on Monday, Normal on Tue-Fri\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.monday_runs_count = self.load_monday_runs()\n",
        "\n",
        "    def load_monday_runs(self):\n",
        "        \"\"\"Load Monday run count from file\"\"\"\n",
        "        if MONDAY_RUNS_FILE.exists():\n",
        "            try:\n",
        "                data = pickle.load(open(MONDAY_RUNS_FILE, \"rb\"))\n",
        "                # Reset if it's a new Monday\n",
        "                if data.get('date') != datetime.now().strftime('%Y-%m-%d'):\n",
        "                    return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "                return data\n",
        "            except:\n",
        "                return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "        return {'count': 0, 'date': datetime.now().strftime('%Y-%m-%d')}\n",
        "\n",
        "    def save_monday_runs(self):\n",
        "        \"\"\"Save Monday run count\"\"\"\n",
        "        try:\n",
        "            with open(MONDAY_RUNS_FILE, \"wb\") as f:\n",
        "                pickle.dump(self.monday_runs_count, f)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save Monday runs: {e}\")\n",
        "\n",
        "    def is_weekend(self):\n",
        "        \"\"\"Check if today is Saturday (5) or Sunday (6)\"\"\"\n",
        "        return datetime.now().weekday() in [5, 6]\n",
        "\n",
        "    def is_monday(self):\n",
        "        \"\"\"Check if today is Monday (0)\"\"\"\n",
        "        return datetime.now().weekday() == 0\n",
        "\n",
        "    def is_normal_day(self):\n",
        "        \"\"\"Check if today is Tuesday (1) - Friday (4)\"\"\"\n",
        "        return datetime.now().weekday() in [1, 2, 3, 4]\n",
        "\n",
        "    def get_mode(self):\n",
        "        \"\"\"Determine current mode: replay, live, or normal\"\"\"\n",
        "        if self.is_weekend():\n",
        "            return \"replay\"\n",
        "        elif self.is_monday():\n",
        "            if self.monday_runs_count['count'] < REPLAY_CONFIG['monday_max_runs']:\n",
        "                return \"live\"\n",
        "            else:\n",
        "                return \"completed\"\n",
        "        elif self.is_normal_day():\n",
        "            return \"normal\"\n",
        "        else:\n",
        "            return \"idle\"\n",
        "\n",
        "    def increment_monday_runs(self):\n",
        "        \"\"\"Increment Monday run counter\"\"\"\n",
        "        self.monday_runs_count['count'] += 1\n",
        "        self.monday_runs_count['date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "        self.save_monday_runs()\n",
        "\n",
        "    def get_status_message(self):\n",
        "        \"\"\"Get human-readable status\"\"\"\n",
        "        mode = self.get_mode()\n",
        "        day_name = datetime.now().strftime('%A')\n",
        "\n",
        "        if mode == \"replay\":\n",
        "            return f\"üé¨ {day_name.upper()} REPLAY MODE: Historical Data Active\"\n",
        "        elif mode == \"live\":\n",
        "            remaining = REPLAY_CONFIG['monday_max_runs'] - self.monday_runs_count['count']\n",
        "            return f\"üî¥ MONDAY LIVE MODE: {remaining} runs remaining today\"\n",
        "        elif mode == \"completed\":\n",
        "            return \"‚úÖ MONDAY COMPLETED: All 10 live runs done for today\"\n",
        "        elif mode == \"normal\":\n",
        "            return f\"üíº {day_name.upper()} NORMAL MODE: Regular Trading Active\"\n",
        "        else:\n",
        "            return \"üí§ IDLE MODE\"\n",
        "\n",
        "# Initialize global memory and weekend/Monday manager\n",
        "MEMORY_SYSTEM = InfiniteMemorySystem()\n",
        "WEEKEND_MONDAY_MANAGER = WeekendMondayManager()\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Historical Replay System (v7.1)\n",
        "# ======================================================\n",
        "class HistoricalReplaySystem:\n",
        "    \"\"\"Enhanced replay with random period selection\"\"\"\n",
        "\n",
        "    def __init__(self, data, random_selection=True):\n",
        "        if random_selection:\n",
        "            start_date, end_date = random.choice(RANDOM_REPLAY_PERIODS)\n",
        "            print_status(f\"üé≤ Random period selected: {start_date} ‚Üí {end_date}\", \"success\")\n",
        "        else:\n",
        "            start_date = \"2024-01-01\"\n",
        "            end_date = \"2024-03-31\"\n",
        "\n",
        "        self.start_date = pd.to_datetime(start_date)\n",
        "        self.end_date = pd.to_datetime(end_date)\n",
        "        self.current_date = self.start_date\n",
        "        self.data = data\n",
        "        self.is_replay_mode = True\n",
        "\n",
        "        print_status(f\"Replay Mode Active: {start_date} ‚Üí {end_date}\", \"info\")\n",
        "\n",
        "    def get_available_data(self, pair):\n",
        "        if pair not in self.data:\n",
        "            return None\n",
        "\n",
        "        full_data = self.data[pair]\n",
        "        filtered_data = {}\n",
        "\n",
        "        for tf, df in full_data.items():\n",
        "            mask = df.index <= self.current_date\n",
        "            filtered_df = df[mask].copy()\n",
        "\n",
        "            if len(filtered_df) > 0:\n",
        "                filtered_data[tf] = filtered_df\n",
        "\n",
        "        return filtered_data if filtered_data else None\n",
        "\n",
        "    def advance_time(self, minutes=25):\n",
        "        self.current_date += pd.Timedelta(minutes=minutes)\n",
        "        return self.current_date <= self.end_date\n",
        "\n",
        "    def get_current_date(self):\n",
        "        return self.current_date\n",
        "\n",
        "    def get_progress(self):\n",
        "        total_duration = (self.end_date - self.start_date).total_seconds()\n",
        "        elapsed = (self.current_date - self.start_date).total_seconds()\n",
        "        return (elapsed / total_duration) * 100 if total_duration > 0 else 0\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Enhanced Competition Manager (v7.1)\n",
        "# ======================================================\n",
        "class CompetitionManager:\n",
        "    \"\"\"Enhanced with cross-model learning\"\"\"\n",
        "\n",
        "    def __init__(self, models_config=COMPETITION_MODELS):\n",
        "        self.models_config = models_config\n",
        "        self.results = {model: {} for model in models_config.keys()}\n",
        "        self.leaderboard = []\n",
        "        self.iteration = 0\n",
        "\n",
        "    def create_model_chromosome(self, model_name, tf_map):\n",
        "        config = self.models_config[model_name]\n",
        "\n",
        "        chrom = [\n",
        "            random.uniform(*config['atr_sl_range']),\n",
        "            random.uniform(*config['atr_tp_range']),\n",
        "            random.uniform(*config['risk_range']),\n",
        "            random.uniform(*config['confidence_range'])\n",
        "        ]\n",
        "\n",
        "        for p in PAIRS:\n",
        "            n = max(1, len(tf_map.get(p, [])))\n",
        "            chrom += np.random.dirichlet(np.ones(n)).tolist()\n",
        "\n",
        "        return chrom\n",
        "\n",
        "    def run_competition(self, data, mode=\"normal\"):\n",
        "        self.iteration += 1\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"üèÜ COMPETITION ROUND #{self.iteration} ({mode.upper()} MODE)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        for model_name, config in self.models_config.items():\n",
        "            print(f\"\\n{config['color']} Training {model_name} AI...\")\n",
        "\n",
        "            best_chrom, trade_memory, ga_progress, metrics = self.run_model_ga(\n",
        "                model_name, data, config\n",
        "            )\n",
        "\n",
        "            self.results[model_name] = {\n",
        "                'chromosome': best_chrom,\n",
        "                'trade_memory': trade_memory,\n",
        "                'ga_progress': ga_progress,\n",
        "                'metrics': metrics,\n",
        "                'config': config,\n",
        "                'mode': mode\n",
        "            }\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def run_model_ga(self, model_name, data, config):\n",
        "        tf_map = build_tf_map(data)\n",
        "\n",
        "        population = [\n",
        "            self.create_model_chromosome(model_name, tf_map)\n",
        "            for _ in range(config['pop_size'])\n",
        "        ]\n",
        "\n",
        "        trade_memory = {}\n",
        "        best_score_ever = -np.inf\n",
        "        best_chrom_ever = None\n",
        "        ga_progress = []\n",
        "\n",
        "        for gen in range(1, config['generations'] + 1):\n",
        "            current_gen_scores = []\n",
        "\n",
        "            for c in population:\n",
        "                score, results, _ = run_vector_backtest(\n",
        "                    data, BASE_CAPITAL, *decode_chrom(c, tf_map), trade_memory\n",
        "                )\n",
        "                current_gen_scores.append((score, c, results))\n",
        "\n",
        "            current_gen_scores.sort(reverse=True, key=lambda x: x[0])\n",
        "            best_score, best_chrom, best_results = current_gen_scores[0]\n",
        "\n",
        "            if best_score > best_score_ever:\n",
        "                best_score_ever = best_score\n",
        "                best_chrom_ever = best_chrom\n",
        "\n",
        "            ga_progress.append(min(100, int((best_score / (abs(best_score_ever) + EPS)) * 100)))\n",
        "\n",
        "            # Next generation\n",
        "            next_population = [best_chrom]\n",
        "            while len(next_population) < config['pop_size']:\n",
        "                p1 = tournament_select(current_gen_scores, k=3)\n",
        "                p2 = tournament_select(current_gen_scores, k=3)\n",
        "\n",
        "                crossover_point = random.randint(1, len(p1)-2)\n",
        "                child = p1[:crossover_point] + p2[crossover_point:]\n",
        "\n",
        "                for i in range(len(child)):\n",
        "                    if random.random() < config['mutation_rate']:\n",
        "                        child[i] *= random.uniform(0.7, 1.3)\n",
        "\n",
        "                next_population.append(child)\n",
        "\n",
        "            population = next_population\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = self.calculate_metrics(best_results, trade_memory)\n",
        "\n",
        "        return best_chrom_ever, trade_memory, ga_progress, metrics\n",
        "\n",
        "    def calculate_metrics(self, results, trade_memory):\n",
        "        total_pnl = sum(r['total_pnl'] for r in results.values())\n",
        "        sharpe = np.mean([r['sharpe'] for r in results.values()])\n",
        "        max_dd = max([r['max_drawdown'] for r in results.values()])\n",
        "\n",
        "        return {\n",
        "            'total_pnl': total_pnl,\n",
        "            'sharpe': sharpe,\n",
        "            'max_drawdown': max_dd\n",
        "        }\n",
        "\n",
        "    def generate_leaderboard(self, signals_results, mode=\"normal\"):\n",
        "        leaderboard_data = []\n",
        "\n",
        "        for model_name, result in self.results.items():\n",
        "            config = result['config']\n",
        "            metrics = result['metrics']\n",
        "\n",
        "            # Get trade history from memory\n",
        "            history = MEMORY_SYSTEM.get_model_trade_history(model_name, days=7)\n",
        "\n",
        "            leaderboard_data.append({\n",
        "                'model': model_name,\n",
        "                'color': config['color'],\n",
        "                'hex_color': config['hex_color'],\n",
        "                'pnl': metrics['total_pnl'],\n",
        "                'sharpe': metrics['sharpe'],\n",
        "                'max_dd': metrics['max_drawdown'],\n",
        "                'total_trades': history['total_trades'],\n",
        "                'successful_trades': history['successful_trades'],\n",
        "                'accuracy': history['accuracy'],\n",
        "                'strategy': config['strategy']\n",
        "            })\n",
        "\n",
        "        leaderboard_data.sort(key=lambda x: x['pnl'], reverse=True)\n",
        "\n",
        "        # Save to database\n",
        "        for rank, entry in enumerate(leaderboard_data, 1):\n",
        "            MEMORY_SYSTEM.save_competition_result(\n",
        "                self.iteration, entry['model'], entry['pnl'],\n",
        "                entry['accuracy'], entry['sharpe'], entry['max_dd'],\n",
        "                entry['total_trades'], entry['successful_trades'], rank, mode\n",
        "            )\n",
        "\n",
        "        self.leaderboard = leaderboard_data\n",
        "        return leaderboard_data\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ Utility Functions\n",
        "# ======================================================\n",
        "def make_index_tz_naive(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.index, pd.DatetimeIndex):\n",
        "        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "        if df.index.tz is not None:\n",
        "            df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "def ensure_atr(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if \"atr\" in df.columns and not df[\"atr\"].isna().all():\n",
        "        df[\"atr\"] = df[\"atr\"].fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "        return df\n",
        "\n",
        "    high, low, close = df[\"high\"].values, df[\"low\"].values, df[\"close\"].values\n",
        "    tr = np.maximum.reduce([\n",
        "        high - low,\n",
        "        np.abs(high - np.roll(close, 1)),\n",
        "        np.abs(low - np.roll(close, 1))\n",
        "    ])\n",
        "    tr[0] = high[0] - low[0] if len(tr) > 0 else MIN_ATR\n",
        "    atr_series = pd.Series(tr, index=df.index).rolling(\n",
        "        ATR_PERIOD, min_periods=1\n",
        "    ).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
        "    df[\"atr\"] = atr_series\n",
        "    return df\n",
        "\n",
        "def seed_hybrid_signal(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if \"hybrid_signal\" not in df.columns or df[\"hybrid_signal\"].abs().sum() == 0:\n",
        "        fast = df[\"close\"].rolling(10, min_periods=1).mean()\n",
        "        slow = df[\"close\"].rolling(50, min_periods=1).mean()\n",
        "        df[\"hybrid_signal\"] = (fast - slow).fillna(0)\n",
        "    df[\"hybrid_signal\"] = df[\"hybrid_signal\"].fillna(0.0).astype(float)\n",
        "    return df\n",
        "\n",
        "def fetch_live_rate(pair: str, timeout: int = 8) -> float:\n",
        "    token = os.environ.get(\"BROWSERLESS_TOKEN\", \"\")\n",
        "    if not token:\n",
        "        return 0.0\n",
        "\n",
        "    from_currency, to_currency = pair.split(\"/\")\n",
        "    url = f\"https://production-sfo.browserless.io/content?token={token}\"\n",
        "    payload = {\n",
        "        \"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.post(url, json=payload, timeout=timeout)\n",
        "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', r.text)\n",
        "        return float(match.group(1).replace(\",\", \"\")) if match else 0.0\n",
        "    except Exception as e:\n",
        "        return 0.0\n",
        "\n",
        "def generate_sparkline(values: list) -> str:\n",
        "    if not values:\n",
        "        return \"‚ñÅ\"\n",
        "\n",
        "    bars = \"‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà\"\n",
        "    min_val, max_val = min(values), max(values)\n",
        "    range_val = max_val - min_val if max_val > min_val else 1\n",
        "\n",
        "    sparkline = \"\"\n",
        "    for v in values:\n",
        "        normalized = (v - min_val) / range_val\n",
        "        idx = int(normalized * (len(bars) - 1))\n",
        "        sparkline += bars[idx]\n",
        "\n",
        "    return sparkline\n",
        "\n",
        "# ======================================================\n",
        "# üîü Data Loading\n",
        "# ======================================================\n",
        "def load_unified_pickles(folder: Path) -> dict:\n",
        "    combined = {}\n",
        "\n",
        "    for pair in PAIRS:\n",
        "        combined[pair] = {}\n",
        "        prefix = pair.replace(\"/\", \"_\")\n",
        "        pair_files = list(folder.glob(f\"{prefix}*_2244.pkl\"))\n",
        "        pair_files.sort()\n",
        "\n",
        "        if not pair_files:\n",
        "            continue\n",
        "\n",
        "        for pf in pair_files:\n",
        "            try:\n",
        "                df = pd.read_pickle(pf)\n",
        "                if not isinstance(df, pd.DataFrame):\n",
        "                    continue\n",
        "\n",
        "                df = make_index_tz_naive(df)\n",
        "                df = ensure_atr(df)\n",
        "                df = seed_hybrid_signal(df)\n",
        "\n",
        "                if (df['close'] <= 0).any() or len(df) < 50:\n",
        "                    continue\n",
        "\n",
        "                tf = re.sub(rf\"{prefix}_?|\\.pkl\", \"\", pf.name).replace(\"__\", \"_\").strip(\"_\")\n",
        "                if not tf:\n",
        "                    tf = \"merged\"\n",
        "                combined[pair][tf] = df\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    return combined\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ GA Functions\n",
        "# ======================================================\n",
        "def build_tf_map(data: dict) -> dict:\n",
        "    return {p: list(tfs.keys()) for p, tfs in data.items()}\n",
        "\n",
        "def create_chrom(tf_map: dict) -> list:\n",
        "    chrom = [\n",
        "        random.uniform(1.0, 2.5),\n",
        "        random.uniform(1.5, 3.0),\n",
        "        random.uniform(0.005, 0.03),\n",
        "        random.uniform(0.3, 0.7)\n",
        "    ]\n",
        "\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p, [])))\n",
        "        chrom += np.random.dirichlet(np.ones(n)).tolist()\n",
        "\n",
        "    return chrom\n",
        "\n",
        "def decode_chrom(chrom: list, tf_map: dict):\n",
        "    atr_sl, atr_tp, risk, conf = chrom[:4]\n",
        "\n",
        "    atr_sl = min(max(atr_sl, 1.0), MAX_ATR_SL)\n",
        "    atr_tp = min(max(atr_tp, 1.0), MAX_ATR_TP)\n",
        "\n",
        "    tf_w = {}\n",
        "    idx = 4\n",
        "    for p in PAIRS:\n",
        "        n = max(1, len(tf_map.get(p, [])))\n",
        "        weights = chrom[idx:idx+n]\n",
        "        weights = np.array(weights, dtype=float)\n",
        "\n",
        "        if weights.sum() <= 0:\n",
        "            weights = np.ones_like(weights) / len(weights)\n",
        "        else:\n",
        "            weights = weights / (weights.sum() + EPS)\n",
        "\n",
        "        tf_w[p] = {tf: float(w) for tf, w in zip(tf_map.get(p, []), weights)}\n",
        "        idx += n\n",
        "\n",
        "    return atr_sl, atr_tp, risk, conf, tf_w\n",
        "\n",
        "def tournament_select(pop: list, k=TOURNAMENT_SIZE) -> list:\n",
        "    return max(random.sample(pop, k), key=lambda x: x[0])[1]\n",
        "\n",
        "def calculate_sharpe_ratio(equity_curve: np.ndarray) -> float:\n",
        "    if len(equity_curve) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    returns = np.diff(equity_curve) / (equity_curve[:-1] + EPS)\n",
        "    if len(returns) == 0 or np.std(returns) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return np.mean(returns) / (np.std(returns) + EPS)\n",
        "\n",
        "def run_vector_backtest(data, capital, base_risk, atr_sl, atr_tp, conf_mult,\n",
        "                       tf_weights, trade_memory=None):\n",
        "    if trade_memory is None:\n",
        "        trade_memory = {pair: [] for pair in PAIRS}\n",
        "\n",
        "    results = {}\n",
        "    precomputed = {}\n",
        "    pair_performance = {pair: 0.0 for pair in PAIRS}\n",
        "\n",
        "    for pair, tfs in data.items():\n",
        "        if not tfs:\n",
        "            results[pair] = {\n",
        "                'equity_curve': np.array([capital]),\n",
        "                'total_pnl': 0,\n",
        "                'max_drawdown': 0,\n",
        "                'sharpe': 0\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        all_idx = sorted(set().union(*[df.index for df in tfs.values()]))\n",
        "        df_all = pd.DataFrame(index=all_idx)\n",
        "\n",
        "        for tf, df in tfs.items():\n",
        "            df_all[f'close_{tf}'] = df['close'].reindex(df_all.index).ffill()\n",
        "            df_all[f'signal_{tf}'] = df['hybrid_signal'].reindex(df_all.index).ffill().fillna(0.0)\n",
        "            df_all[f'atr_{tf}'] = df['atr'].reindex(df_all.index).ffill().fillna(MIN_ATR)\n",
        "\n",
        "        df_all['price'] = df_all[[c for c in df_all.columns if c.startswith('close_')]].mean(axis=1).clip(lower=EPS)\n",
        "        df_all['atr'] = df_all[[c for c in df_all.columns if c.startswith('atr_')]].mean(axis=1).clip(lower=MIN_ATR)\n",
        "        precomputed[pair] = df_all\n",
        "\n",
        "    for pair, df_all in precomputed.items():\n",
        "        tfs = data.get(pair, {})\n",
        "        if not tfs:\n",
        "            continue\n",
        "\n",
        "        agg_signal = sum([\n",
        "            df_all[f'signal_{tf}'] * tf_weights.get(pair, {}).get(tf, 0.0)\n",
        "            for tf in tfs.keys()\n",
        "        ])\n",
        "\n",
        "        mean_abs_signal = np.mean([\n",
        "            df_all[f'signal_{tf}'].abs().mean() for tf in tfs.keys()\n",
        "        ]) if tfs else 0.0\n",
        "        conf_threshold = conf_mult * (mean_abs_signal + EPS)\n",
        "\n",
        "        df_all['agg_signal'] = np.where(\n",
        "            np.abs(agg_signal) >= conf_threshold,\n",
        "            agg_signal,\n",
        "            0.0\n",
        "        )\n",
        "\n",
        "        price = df_all['price'].values\n",
        "        atr = df_all['atr'].values\n",
        "        agg_signal = df_all['agg_signal'].values\n",
        "        n = len(price)\n",
        "\n",
        "        if n <= 1:\n",
        "            results[pair] = {\n",
        "                'equity_curve': np.array([capital]),\n",
        "                'total_pnl': 0,\n",
        "                'max_drawdown': 0,\n",
        "                'sharpe': 0\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        memory_factor = 1.0\n",
        "        if trade_memory.get(pair):\n",
        "            recent_trades = trade_memory[pair][-10:]\n",
        "            if recent_trades:\n",
        "                win_rate = sum(1 for tr in recent_trades if tr.get('pnl', 0) > 0) / len(recent_trades)\n",
        "                memory_factor = max(0.3, min(2.0, 0.5 + win_rate))\n",
        "\n",
        "        raw_size = (capital * base_risk * np.abs(agg_signal)) / (atr_sl * (atr / price) + EPS)\n",
        "        size = np.zeros_like(raw_size)\n",
        "\n",
        "        for i in range(len(raw_size)):\n",
        "            sized = raw_size[i] * memory_factor\n",
        "            sized = min(sized, capital * MAX_POSITION_FRACTION, MAX_TRADE_CAP)\n",
        "            atr_value = atr[i] if i < len(atr) else MIN_ATR\n",
        "            atr_cap = capital * 0.02 / (atr_value / price[i] + EPS)\n",
        "            sized = min(sized, atr_cap)\n",
        "            size[i] = sized\n",
        "\n",
        "        size = np.nan_to_num(size, nan=0.0, posinf=MAX_TRADE_CAP)\n",
        "        direction = np.sign(agg_signal)\n",
        "        pnl = direction * size * (atr_tp * atr / price)\n",
        "\n",
        "        equity = np.zeros(n, dtype=float)\n",
        "        equity[0] = float(capital)\n",
        "        for i in range(1, n):\n",
        "            equity[i] = equity[i-1] + float(pnl[i])\n",
        "\n",
        "        final_pnl = float(equity[-1] - capital)\n",
        "        pair_performance[pair] = final_pnl\n",
        "\n",
        "        trade_memory.setdefault(pair, []).append({\n",
        "            'equity': float(equity[-1]),\n",
        "            'pnl': final_pnl,\n",
        "            'timestamp': pd.Timestamp.now().isoformat()\n",
        "        })\n",
        "\n",
        "        if len(trade_memory[pair]) > MAX_TRADE_MEMORY:\n",
        "            trade_memory[pair] = trade_memory[pair][-MAX_TRADE_MEMORY:]\n",
        "\n",
        "        sharpe = calculate_sharpe_ratio(equity)\n",
        "        max_dd = float(np.max(np.maximum.accumulate(equity) - equity))\n",
        "\n",
        "        results[pair] = {\n",
        "            'equity_curve': equity,\n",
        "            'total_pnl': final_pnl,\n",
        "            'max_drawdown': max_dd,\n",
        "            'sharpe': sharpe\n",
        "        }\n",
        "\n",
        "    total_sharpe = sum([r['sharpe'] for r in results.values()])\n",
        "    perf_values = list(pair_performance.values())\n",
        "    pair_balance_penalty = np.std(perf_values) / (np.mean(perf_values) + EPS) if perf_values else 0.0\n",
        "    score = total_sharpe - 0.5 * pair_balance_penalty\n",
        "\n",
        "    return score, results, trade_memory\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ Signal Generation\n",
        "# ======================================================\n",
        "def generate_live_signals(best, data, model_name=\"Unknown\"):\n",
        "    tf_map = build_tf_map(data)\n",
        "    atr_sl, atr_tp, risk, conf, tf_weights = decode_chrom(best, tf_map)\n",
        "\n",
        "    live_signals = {}\n",
        "\n",
        "    for pair in PAIRS:\n",
        "        tfs = data.get(pair, {})\n",
        "        if not tfs:\n",
        "            continue\n",
        "\n",
        "        price = fetch_live_rate(pair)\n",
        "        if price <= 0:\n",
        "            price = float(list(tfs.values())[0]['close'].iloc[-1])\n",
        "\n",
        "        sig_strength = sum([\n",
        "            tf_weights.get(pair, {}).get(tf, 0.0) * tfs[tf][\"hybrid_signal\"].iloc[-1]\n",
        "            for tf in tf_map.get(pair, [])\n",
        "        ])\n",
        "\n",
        "        recent_atr = np.mean([\n",
        "            tfs[tf][\"atr\"].iloc[-1] for tf in tf_map.get(pair, [])\n",
        "        ]) if tfs else 1.0\n",
        "\n",
        "        sig_strength_scaled = sig_strength / (recent_atr + EPS)\n",
        "        noise_factor = random.uniform(0.85, 1.15)\n",
        "        sig_strength_scaled *= noise_factor\n",
        "\n",
        "        if sig_strength_scaled > 0:\n",
        "            direction = \"BUY\"\n",
        "        elif sig_strength_scaled < 0:\n",
        "            direction = \"SELL\"\n",
        "        else:\n",
        "            direction = \"HOLD\"\n",
        "\n",
        "        raw_score = abs(sig_strength_scaled) * 100\n",
        "        score_100 = int(30 + (55 * (raw_score / (raw_score + 10))))\n",
        "        score_100 = min(max(score_100, 30), 85)\n",
        "        score_variation = random.randint(-5, 5)\n",
        "        score_100 = min(max(score_100 + score_variation, 25), 90)\n",
        "        high_conf = score_100 >= 70\n",
        "\n",
        "        max_sl_tp_distance = recent_atr * MAX_ATR_SL\n",
        "        min_sl_tp_distance = recent_atr * MIN_ATR_DISTANCE\n",
        "\n",
        "        if direction == \"BUY\":\n",
        "            base_sl = price - atr_sl * recent_atr\n",
        "            base_tp = price + atr_tp * recent_atr\n",
        "            SL = max(min(base_sl, price - min_sl_tp_distance), price - max_sl_tp_distance)\n",
        "            TP = min(max(base_tp, price + min_sl_tp_distance), price + max_sl_tp_distance)\n",
        "        elif direction == \"SELL\":\n",
        "            base_sl = price + atr_sl * recent_atr\n",
        "            base_tp = price - atr_tp * recent_atr\n",
        "            SL = min(max(base_sl, price + min_sl_tp_distance), price + max_sl_tp_distance)\n",
        "            TP = max(min(base_tp, price - min_sl_tp_distance), price - max_sl_tp_distance)\n",
        "        else:\n",
        "            SL = TP = price\n",
        "\n",
        "        if score_100 < 40 and random.random() < 0.3:\n",
        "            direction = \"HOLD\"\n",
        "            SL = TP = price\n",
        "\n",
        "        live_signals[pair] = {\n",
        "            \"direction\": direction,\n",
        "            \"strength\": float(sig_strength_scaled),\n",
        "            \"score_1_100\": score_100,\n",
        "            \"last_price\": float(price),\n",
        "            \"SL\": float(SL),\n",
        "            \"TP\": float(TP),\n",
        "            \"high_confidence\": high_conf,\n",
        "            \"atr\": float(recent_atr),\n",
        "            \"atr_multiplier_sl\": float(atr_sl),\n",
        "            \"atr_multiplier_tp\": float(atr_tp),\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
        "            \"model\": model_name\n",
        "        }\n",
        "\n",
        "    return live_signals\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£3Ô∏è‚É£ Enhanced Email Dashboard (v7.1 - Single Email)\n",
        "# ======================================================\n",
        "def fetch_logo_base64(url: str) -> str:\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return base64.b64encode(response.content).decode('utf-8')\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Failed to fetch logo: {e}\")\n",
        "    return \"\"\n",
        "\n",
        "def send_unified_competition_email(all_signals, leaderboard, mode_info, recipient=GMAIL_USER):\n",
        "    \"\"\"Single unified email with complete competition dashboard\"\"\"\n",
        "\n",
        "    logo_base64 = fetch_logo_base64(LOGO_URL)\n",
        "\n",
        "    # Mode badge\n",
        "    mode = mode_info.get('mode', 'normal')\n",
        "    mode_badge = {\n",
        "        'replay': 'üé¨ REPLAY MODE',\n",
        "        'live': 'üî¥ LIVE MODE',\n",
        "        'normal': 'üíº NORMAL MODE',\n",
        "        'completed': '‚úÖ COMPLETED'\n",
        "    }.get(mode, 'üíº TRADING')\n",
        "\n",
        "    mode_color = {\n",
        "        'replay': '#9B59B6',\n",
        "        'live': '#E74C3C',\n",
        "        'normal': '#3498DB',\n",
        "        'completed': '#2ECC71'\n",
        "    }.get(mode, '#3498DB')\n",
        "\n",
        "    # Generate leaderboard HTML\n",
        "    leaderboard_html = \"\"\n",
        "    for idx, entry in enumerate(leaderboard, 1):\n",
        "        medal = \"ü•á\" if idx == 1 else \"ü•à\" if idx == 2 else \"ü•â\" if idx == 3 else f\"{idx}.\"\n",
        "\n",
        "        trade_info = f\"{entry['successful_trades']}/{entry['total_trades']} trades\"\n",
        "        accuracy_str = f\"{entry['accuracy']:.1f}%\"\n",
        "\n",
        "        leaderboard_html += f'''\n",
        "        <div class=\"leaderboard-entry\">\n",
        "            <div class=\"rank-badge\">{medal}</div>\n",
        "            <div class=\"model-info\">\n",
        "                <div class=\"model-name\" style=\"color: {entry['hex_color']};\">\n",
        "                    {entry['color']} {entry['model']}\n",
        "                </div>\n",
        "                <div class=\"model-strategy\">{entry['strategy']}</div>\n",
        "                <div class=\"trade-history\">\n",
        "                    üìä {trade_info} ({accuracy_str} accuracy)\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"performance-metrics\">\n",
        "                <div class=\"pnl-value\" style=\"color: {'#2ECC71' if entry['pnl'] > 0 else '#E74C3C'};\">\n",
        "                    ${entry['pnl']:.2f}\n",
        "                </div>\n",
        "                <div class=\"sharpe-value\">Sharpe: {entry['sharpe']:.3f}</div>\n",
        "            </div>\n",
        "        </div>\n",
        "        '''\n",
        "\n",
        "    # Generate signals comparison HTML\n",
        "    signals_comparison = \"\"\n",
        "    for pair in PAIRS:\n",
        "        signals_comparison += f'<div class=\"pair-section\"><h4 style=\"color: #FFD700;\">{pair}</h4>'\n",
        "\n",
        "        for model_name in sorted(all_signals.keys()):\n",
        "            if pair in all_signals[model_name]:\n",
        "                sig = all_signals[model_name][pair]\n",
        "                config = COMPETITION_MODELS[model_name]\n",
        "\n",
        "                direction_class = sig['direction'].lower()\n",
        "                direction_color = \"#2ECC71\" if direction_class == \"buy\" else \"#E74C3C\" if direction_class == \"sell\" else \"#FF9800\"\n",
        "\n",
        "                signals_comparison += f'''\n",
        "                <div class=\"signal-row\">\n",
        "                    <span class=\"model-badge\" style=\"color: {config['hex_color']};\">\n",
        "                        {config['color']} {model_name}\n",
        "                    </span>\n",
        "                    <span class=\"direction-badge\" style=\"background: {direction_color};\">\n",
        "                        {sig['direction']}\n",
        "                    </span>\n",
        "                    <span class=\"confidence-score\">{sig['score_1_100']}%</span>\n",
        "                    <span class=\"price-info\">\n",
        "                        Entry: {sig['last_price']:.4f} | SL: {sig['SL']:.4f} | TP: {sig['TP']:.4f}\n",
        "                    </span>\n",
        "                </div>\n",
        "                '''\n",
        "\n",
        "        signals_comparison += '</div>'\n",
        "\n",
        "    email_html = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<style>\n",
        "    body {{\n",
        "        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;\n",
        "        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);\n",
        "        color: #ffffff;\n",
        "        margin: 0;\n",
        "        padding: 20px;\n",
        "    }}\n",
        "    .container {{\n",
        "        max-width: 900px;\n",
        "        margin: 0 auto;\n",
        "        background: rgba(30, 30, 50, 0.95);\n",
        "        border-radius: 15px;\n",
        "        padding: 30px;\n",
        "        box-shadow: 0 10px 40px rgba(0, 0, 0, 0.5);\n",
        "    }}\n",
        "    .header {{\n",
        "        text-align: center;\n",
        "        border-bottom: 3px solid #FFD700;\n",
        "        padding-bottom: 20px;\n",
        "        margin-bottom: 30px;\n",
        "    }}\n",
        "    .logo {{\n",
        "        width: 120px;\n",
        "        border-radius: 12px;\n",
        "        margin-bottom: 15px;\n",
        "    }}\n",
        "    h1 {{\n",
        "        color: #FFD700;\n",
        "        margin: 10px 0;\n",
        "        font-size: 32px;\n",
        "        text-shadow: 0 2px 10px rgba(255, 215, 0, 0.3);\n",
        "    }}\n",
        "    .subtitle {{\n",
        "        color: #aaa;\n",
        "        font-size: 14px;\n",
        "    }}\n",
        "    .mode-badge {{\n",
        "        display: inline-block;\n",
        "        padding: 8px 20px;\n",
        "        background: {mode_color};\n",
        "        border-radius: 20px;\n",
        "        font-weight: bold;\n",
        "        margin-top: 10px;\n",
        "        font-size: 14px;\n",
        "    }}\n",
        "    .section {{\n",
        "        margin: 25px 0;\n",
        "        padding: 20px;\n",
        "        background: linear-gradient(135deg, rgba(255, 105, 180, 0.08) 0%, rgba(147, 112, 219, 0.08) 100%);\n",
        "        border-radius: 10px;\n",
        "        border-left: 4px solid #FFD700;\n",
        "    }}\n",
        "    .section-title {{\n",
        "        color: #FFD700;\n",
        "        font-size: 20px;\n",
        "        font-weight: bold;\n",
        "        margin-bottom: 15px;\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "    }}\n",
        "    .leaderboard-entry {{\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        padding: 15px;\n",
        "        margin: 10px 0;\n",
        "        background: rgba(255, 255, 255, 0.05);\n",
        "        border-radius: 8px;\n",
        "        transition: transform 0.2s;\n",
        "    }}\n",
        "    .leaderboard-entry:hover {{\n",
        "        transform: translateX(5px);\n",
        "        background: rgba(255, 255, 255, 0.08);\n",
        "    }}\n",
        "    .rank-badge {{\n",
        "        font-size: 24px;\n",
        "        min-width: 50px;\n",
        "        text-align: center;\n",
        "    }}\n",
        "    .model-info {{\n",
        "        flex: 1;\n",
        "        padding: 0 15px;\n",
        "    }}\n",
        "    .model-name {{\n",
        "        font-size: 18px;\n",
        "        font-weight: bold;\n",
        "        margin-bottom: 5px;\n",
        "    }}\n",
        "    .model-strategy {{\n",
        "        color: #aaa;\n",
        "        font-size: 12px;\n",
        "        margin-bottom: 5px;\n",
        "    }}\n",
        "    .trade-history {{\n",
        "        color: #FFD700;\n",
        "        font-size: 13px;\n",
        "        font-weight: 600;\n",
        "    }}\n",
        "    .performance-metrics {{\n",
        "        text-align: right;\n",
        "    }}\n",
        "    .pnl-value {{\n",
        "        font-size: 22px;\n",
        "        font-weight: bold;\n",
        "        margin-bottom: 5px;\n",
        "    }}\n",
        "    .sharpe-value {{\n",
        "        color: #aaa;\n",
        "        font-size: 12px;\n",
        "    }}\n",
        "    .pair-section {{\n",
        "        margin: 15px 0;\n",
        "        padding: 15px;\n",
        "        background: rgba(0, 0, 0, 0.2);\n",
        "        border-radius: 8px;\n",
        "    }}\n",
        "    .pair-section h4 {{\n",
        "        margin: 0 0 10px 0;\n",
        "        font-size: 16px;\n",
        "    }}\n",
        "    .signal-row {{\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        padding: 10px;\n",
        "        margin: 5px 0;\n",
        "        background: rgba(255, 255, 255, 0.03);\n",
        "        border-radius: 5px;\n",
        "        font-size: 13px;\n",
        "    }}\n",
        "    .model-badge {{\n",
        "        min-width: 150px;\n",
        "        font-weight: bold;\n",
        "    }}\n",
        "    .direction-badge {{\n",
        "        padding: 4px 12px;\n",
        "        border-radius: 4px;\n",
        "        font-weight: bold;\n",
        "        margin: 0 10px;\n",
        "        color: white;\n",
        "    }}\n",
        "    .confidence-score {{\n",
        "        min-width: 50px;\n",
        "        color: #FFD700;\n",
        "        font-weight: bold;\n",
        "    }}\n",
        "    .price-info {{\n",
        "        color: #aaa;\n",
        "        font-size: 11px;\n",
        "        margin-left: 10px;\n",
        "    }}\n",
        "    .footer {{\n",
        "        text-align: center;\n",
        "        margin-top: 30px;\n",
        "        padding-top: 20px;\n",
        "        border-top: 2px solid rgba(255, 215, 0, 0.3);\n",
        "        color: #888;\n",
        "        font-size: 12px;\n",
        "    }}\n",
        "    .timestamp {{\n",
        "        background: rgba(255, 215, 0, 0.1);\n",
        "        padding: 8px 15px;\n",
        "        border-radius: 5px;\n",
        "        display: inline-block;\n",
        "        margin-top: 10px;\n",
        "    }}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <div class=\"header\">\n",
        "        {f'<img src=\"data:image/jpeg;base64,{logo_base64}\" class=\"logo\">' if logo_base64 else ''}\n",
        "        <h1>üèÜ Multi-Model Competition v7.1</h1>\n",
        "        <p class=\"subtitle\">AI Strategies Competing in Real-Time</p>\n",
        "        <div class=\"mode-badge\">{mode_badge}</div>\n",
        "        <div class=\"timestamp\">\n",
        "            {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <div class=\"section-title\">üèÜ Competition Leaderboard</div>\n",
        "        {leaderboard_html}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <div class=\"section-title\">üìä Live Signals - All Models</div>\n",
        "        {signals_comparison}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"footer\">\n",
        "        <p><strong>Ultimate Hybrid Forex AI v7.1</strong></p>\n",
        "        <p>Powered by Genetic Algorithms ‚Ä¢ Infinite Memory ‚Ä¢ Multi-Strategy Competition</p>\n",
        "        <p style=\"margin-top: 10px; color: #666;\">\n",
        "            {mode_info.get('status', 'System active and learning from every trade')}\n",
        "        </p>\n",
        "    </div>\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "    msg = MIMEMultipart(\"alternative\")\n",
        "    msg[\"From\"] = f\"Forex AI Competition v7.1 <{GMAIL_USER}>\"\n",
        "    msg[\"To\"] = recipient\n",
        "    msg[\"Subject\"] = f\"üèÜ AI Competition Results - {mode_badge} - {datetime.now().strftime('%Y-%m-%d %H:%M')}\"\n",
        "    msg.attach(MIMEText(email_html, \"html\"))\n",
        "\n",
        "    try:\n",
        "        with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as s:\n",
        "            s.login(GMAIL_USER, GMAIL_APP_PASSWORD)\n",
        "            s.sendmail(GMAIL_USER, recipient, msg.as_string())\n",
        "        print_status(\"‚úâÔ∏è Competition email sent successfully\", \"success\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Email failed: {e}\")\n",
        "        print_status(f\"Email failed: {e}\", \"error\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£4Ô∏è‚É£ Git Push\n",
        "# ======================================================\n",
        "def push_to_github(file_path):\n",
        "    try:\n",
        "        if not Path(file_path).exists():\n",
        "            return\n",
        "\n",
        "        subprocess.run(\n",
        "            [\"git\", \"-C\", str(REPO_FOLDER), \"add\", str(file_path)],\n",
        "            check=False, capture_output=True\n",
        "        )\n",
        "\n",
        "        commit_result = subprocess.run(\n",
        "            [\"git\", \"-C\", str(REPO_FOLDER), \"commit\", \"-m\", \"üìà Auto Update v7.1\"],\n",
        "            check=False, capture_output=True, text=True\n",
        "        )\n",
        "\n",
        "        if \"nothing to commit\" in commit_result.stdout + commit_result.stderr:\n",
        "            return\n",
        "\n",
        "        subprocess.run(\n",
        "            [\"git\", \"-C\", str(REPO_FOLDER), \"push\", \"origin\", BRANCH],\n",
        "            check=False, capture_output=True\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Git push error: {e}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£5Ô∏è‚É£ MAIN - Smart Multi-Mode Loop (v7.1)\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üöÄ ULTIMATE HYBRID FOREX PIPELINE v7.1\")\n",
        "    print(\"   Smart Multi-Mode System\")\n",
        "    print(\"   ‚Ä¢ Weekend: Replay Mode\")\n",
        "    print(\"   ‚Ä¢ Monday: Live Trading (10 runs)\")\n",
        "    print(\"   ‚Ä¢ Tue-Fri: Normal Trading\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize systems\n",
        "    competition = CompetitionManager()\n",
        "    iteration_count = 0\n",
        "    replay_system = None\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            iteration_count += 1\n",
        "\n",
        "            # Determine current mode\n",
        "            current_mode = WEEKEND_MONDAY_MANAGER.get_mode()\n",
        "            status_msg = WEEKEND_MONDAY_MANAGER.get_status_message()\n",
        "\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"üîÑ ITERATION #{iteration_count}\")\n",
        "            print(status_msg)\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            # Handle Monday completion\n",
        "            if current_mode == \"completed\":\n",
        "                print_status(\"Monday runs completed! Waiting for next day...\", \"success\")\n",
        "                time.sleep(3600 * 6)\n",
        "                continue\n",
        "\n",
        "            # Load Data\n",
        "            print(\"\\nüìÇ Loading Data\")\n",
        "            combined = load_unified_pickles(PICKLE_FOLDER)\n",
        "            valid_pairs = sum(1 for p in PAIRS if combined.get(p))\n",
        "\n",
        "            if valid_pairs == 0:\n",
        "                print_status(\"No valid data found\", \"error\")\n",
        "                time.sleep(CHECK_INTERVAL)\n",
        "                continue\n",
        "\n",
        "            # WEEKEND MODE: Historical Replay\n",
        "            if current_mode == \"replay\":\n",
        "                print_status(\"üé¨ WEEKEND: Running historical replay\", \"info\")\n",
        "\n",
        "                if not replay_system:\n",
        "                    replay_system = HistoricalReplaySystem(\n",
        "                        combined,\n",
        "                        random_selection=True\n",
        "                    )\n",
        "\n",
        "                filtered_data = {}\n",
        "                for pair in PAIRS:\n",
        "                    pair_data = replay_system.get_available_data(pair)\n",
        "                    if pair_data:\n",
        "                        filtered_data[pair] = pair_data\n",
        "                combined = filtered_data\n",
        "\n",
        "                if not combined:\n",
        "                    replay_system = HistoricalReplaySystem(combined, random_selection=True)\n",
        "                    continue\n",
        "\n",
        "                print(f\"   Replay Progress: {replay_system.get_progress():.1f}%\")\n",
        "\n",
        "            # MONDAY & NORMAL MODE: Use live/recent data as-is\n",
        "            elif current_mode in [\"live\", \"normal\"]:\n",
        "                if current_mode == \"live\":\n",
        "                    print_status(f\"üî¥ MONDAY LIVE: Run {WEEKEND_MONDAY_MANAGER.monday_runs_count['count'] + 1}/10\", \"success\")\n",
        "                else:\n",
        "                    day_name = datetime.now().strftime('%A')\n",
        "                    print_status(f\"üíº {day_name.upper()}: Normal trading mode active\", \"info\")\n",
        "\n",
        "            # Run Competition\n",
        "            print(\"\\nüèÜ Running Multi-Model Competition\")\n",
        "            competition_results = competition.run_competition(combined, mode=current_mode)\n",
        "\n",
        "            # Generate Signals\n",
        "            print(\"\\nüì° Generating Signals\")\n",
        "            all_signals = {}\n",
        "            for model_name, result in competition_results.items():\n",
        "                signals = generate_live_signals(result['chromosome'], combined, model_name)\n",
        "                all_signals[model_name] = signals\n",
        "\n",
        "                # Save to memory\n",
        "                for pair, sig in signals.items():\n",
        "                    chrom_hash = hashlib.md5(str(result['chromosome']).encode()).hexdigest()\n",
        "                    MEMORY_SYSTEM.save_signal(\n",
        "                        pair, sig['direction'], sig['last_price'],\n",
        "                        sig['SL'], sig['TP'], sig['atr'], sig['score_1_100'],\n",
        "                        chrom_hash, iteration_count, model_name, current_mode\n",
        "                    )\n",
        "\n",
        "            # Generate Leaderboard\n",
        "            leaderboard = competition.generate_leaderboard(all_signals, mode=current_mode)\n",
        "\n",
        "            # Print Results\n",
        "            print(\"\\nüèÜ COMPETITION RESULTS:\")\n",
        "            for idx, entry in enumerate(leaderboard, 1):\n",
        "                print(f\"{idx}. {entry['color']} {entry['model']}\")\n",
        "                print(f\"   P/L: ${entry['pnl']:.2f} | Accuracy: {entry['accuracy']:.1f}%\")\n",
        "                print(f\"   Trades: {entry['successful_trades']}/{entry['total_trades']}\")\n",
        "\n",
        "            # Prepare mode info for email\n",
        "            mode_info = {\n",
        "                'mode': current_mode,\n",
        "                'status': status_msg,\n",
        "                'run': WEEKEND_MONDAY_MANAGER.monday_runs_count['count'] + 1 if current_mode == 'live' else 0\n",
        "            }\n",
        "\n",
        "            # Send Email\n",
        "            print(\"\\n‚úâÔ∏è Sending Competition Dashboard\")\n",
        "            send_unified_competition_email(all_signals, leaderboard, mode_info)\n",
        "\n",
        "            # Save signals to JSON\n",
        "            with open(SIGNALS_JSON_PATH, \"w\") as f:\n",
        "                json.dump({\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"iteration\": iteration_count,\n",
        "                    \"mode\": current_mode,\n",
        "                    \"leaderboard\": leaderboard,\n",
        "                    \"all_signals\": all_signals\n",
        "                }, f, indent=2)\n",
        "\n",
        "            # Push to GitHub\n",
        "            push_to_github(SIGNALS_JSON_PATH)\n",
        "\n",
        "            # Handle iteration completion based on mode\n",
        "            if current_mode == \"replay\":\n",
        "                # Advance replay\n",
        "                if not replay_system.advance_time(CHECK_INTERVAL//60):\n",
        "                    print_status(\"Replay period completed, resetting for next run\", \"success\")\n",
        "                    replay_system = None\n",
        "\n",
        "            elif current_mode == \"live\":\n",
        "                # Increment Monday counter\n",
        "                WEEKEND_MONDAY_MANAGER.increment_monday_runs()\n",
        "\n",
        "                if WEEKEND_MONDAY_MANAGER.monday_runs_count['count'] >= REPLAY_CONFIG['monday_max_runs']:\n",
        "                    print_status(\"‚úÖ Monday completed! All 10 runs done.\", \"success\")\n",
        "                else:\n",
        "                    print(f\"\\n‚è∞ Waiting {CHECK_INTERVAL//60} minutes before next live run...\")\n",
        "                    time.sleep(CHECK_INTERVAL)\n",
        "\n",
        "            elif current_mode == \"normal\":\n",
        "                # Normal mode: regular interval\n",
        "                print(f\"\\n‚è∞ Next iteration in {CHECK_INTERVAL//60} minutes...\")\n",
        "                time.sleep(CHECK_INTERVAL)\n",
        "\n",
        "            print(\"\\n‚úÖ Iteration Complete\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüõë Shutdown requested\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error: {e}\")\n",
        "            print_status(f\"Error: {e}\", \"error\")\n",
        "            time.sleep(CHECK_INTERVAL)\n",
        "\n",
        "    # Cleanup\n",
        "    MEMORY_SYSTEM.close()\n",
        "    print(\"\\nüèÅ Pipeline Shutdown Complete\")\n",
        "    print(f\"üìä Total Iterations: {iteration_count}\")\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9KfJKVFYVEV",
        "outputId": "455f7a32-6c65-49af-83ec-4850095a3353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Added 'mode' column to signals_history\n",
            "‚úÖ Enhanced Infinite Memory initialized\n",
            "============================================================\n",
            "üöÄ ULTIMATE HYBRID FOREX PIPELINE v7.1\n",
            "   Smart Multi-Mode System\n",
            "   ‚Ä¢ Weekend: Replay Mode\n",
            "   ‚Ä¢ Monday: Live Trading (10 runs)\n",
            "   ‚Ä¢ Tue-Fri: Normal Trading\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "üîÑ ITERATION #1\n",
            "üíº FRIDAY NORMAL MODE: Regular Trading Active\n",
            "============================================================\n",
            "\n",
            "üìÇ Loading Data\n",
            "‚ÑπÔ∏è üíº FRIDAY: Normal trading mode active\n",
            "\n",
            "üèÜ Running Multi-Model Competition\n",
            "\n",
            "============================================================\n",
            "üèÜ COMPETITION ROUND #1 (NORMAL MODE)\n",
            "============================================================\n",
            "\n",
            "üîµ Training Conservative AI...\n",
            "\n",
            "üî¥ Training Aggressive AI...\n",
            "\n",
            "üü¢ Training Balanced AI...\n",
            "\n",
            "üì° Generating Signals\n",
            "\n",
            "üèÜ COMPETITION RESULTS:\n",
            "1. üîµ Conservative\n",
            "   P/L: $-0.14 | Accuracy: 87.5%\n",
            "   Trades: 7/8\n",
            "2. üü¢ Balanced\n",
            "   P/L: $-0.26 | Accuracy: 87.5%\n",
            "   Trades: 7/8\n",
            "3. üî¥ Aggressive\n",
            "   P/L: $-0.32 | Accuracy: 87.5%\n",
            "   Trades: 7/8\n",
            "\n",
            "‚úâÔ∏è Sending Competition Dashboard\n",
            "‚úÖ ‚úâÔ∏è Competition email sent successfully\n",
            "\n",
            "‚è∞ Next iteration in 25 minutes...\n",
            "\n",
            "‚úÖ Iteration Complete\n",
            "\n",
            "============================================================\n",
            "üîÑ ITERATION #2\n",
            "üíº FRIDAY NORMAL MODE: Regular Trading Active\n",
            "============================================================\n",
            "\n",
            "üìÇ Loading Data\n",
            "‚ÑπÔ∏è üíº FRIDAY: Normal trading mode active\n",
            "\n",
            "üèÜ Running Multi-Model Competition\n",
            "\n",
            "============================================================\n",
            "üèÜ COMPETITION ROUND #2 (NORMAL MODE)\n",
            "============================================================\n",
            "\n",
            "üîµ Training Conservative AI...\n",
            "\n",
            "üî¥ Training Aggressive AI...\n",
            "\n",
            "üü¢ Training Balanced AI...\n",
            "\n",
            "üì° Generating Signals\n",
            "\n",
            "üèÜ COMPETITION RESULTS:\n",
            "1. üî¥ Aggressive\n",
            "   P/L: $-0.13 | Accuracy: 87.5%\n",
            "   Trades: 7/8\n",
            "2. üü¢ Balanced\n",
            "   P/L: $-0.13 | Accuracy: 87.5%\n",
            "   Trades: 7/8\n",
            "3. üîµ Conservative\n",
            "   P/L: $-0.30 | Accuracy: 87.5%\n",
            "   Trades: 7/8\n",
            "\n",
            "‚úâÔ∏è Sending Competition Dashboard\n",
            "‚úÖ ‚úâÔ∏è Competition email sent successfully\n",
            "\n",
            "‚è∞ Next iteration in 25 minutes...\n",
            "\n",
            "üõë Shutdown requested\n",
            "\n",
            "üèÅ Pipeline Shutdown Complete\n",
            "üìä Total Iterations: 2\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}