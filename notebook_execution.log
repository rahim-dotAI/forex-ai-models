üìñ Reading notebook: AI_Forex_Brain_2.ipynb
‚úÖ Loaded 10 cells
   - Code cells: 10
   - Markdown cells: 0

üöÄ Executing notebook...
============================================================

============================================================
‚ùå ERROR: Cell execution failed!
Cell index: unknown
Error: An error occurred while executing the following cell:
------------------
# ======================================================
# FX CSV Combine + Incremental Indicators Pipeline
# ‚úÖ Works in: Colab + GitHub Actions + Local
# ‚úÖ Thread-safe, timezone-safe, Git-push-safe
# ‚úÖ Large dataset-ready with column validation
# ‚úÖ Environment-aware paths (NO permission errors)
# ======================================================

import os, time, hashlib, subprocess, shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import ta
from ta.momentum import WilliamsRIndicator
from ta.volatility import AverageTrueRange

print("=" * 70)
print("üîß CSV Combiner & Indicator Generator - Multi-Environment Edition")
print("=" * 70)

# ======================================================
# 0Ô∏è‚É£ FIXED: Environment Detection
# ======================================================
try:
    import google.colab
    IN_COLAB = True
    ENV_NAME = "Google Colab"
except ImportError:
    IN_COLAB = False
    ENV_NAME = "Local/GitHub Actions"

IN_GHA = "GITHUB_ACTIONS" in os.environ
IN_LOCAL = not IN_COLAB and not IN_GHA

if IN_GHA:
    ENV_NAME = "GitHub Actions"

print(f"üåç Detected Environment: {ENV_NAME}")

# ======================================================
# 1Ô∏è‚É£ FIXED: Working Directories (Environment-Aware)
# ======================================================
if IN_COLAB:
    # Colab: Use /content (has permissions)
    ROOT_DIR = Path("/content/forex-alpha-models")
    ROOT_DIR.mkdir(parents=True, exist_ok=True)
elif IN_GHA:
    # GitHub Actions: Use current working directory (repo root)
    ROOT_DIR = Path.cwd()
    print(f"üìÇ GitHub Actions: Using repo root: {ROOT_DIR}")
else:
    # Local: Use subdirectory
    ROOT_DIR = Path("./forex-alpha-models")
    ROOT_DIR.mkdir(parents=True, exist_ok=True)

# Setup subdirectories
REPO_FOLDER = ROOT_DIR / "forex-ai-models"
CSV_FOLDER = ROOT_DIR / "csvs"
PICKLE_FOLDER = ROOT_DIR / "pickles"
LOGS_FOLDER = ROOT_DIR / "logs"

for folder in [CSV_FOLDER, PICKLE_FOLDER, LOGS_FOLDER]:
    folder.mkdir(parents=True, exist_ok=True)

print(f"‚úÖ Root directory: {ROOT_DIR}")
print(f"‚úÖ Repo folder: {REPO_FOLDER}")
print(f"‚úÖ CSV folder: {CSV_FOLDER}")
print(f"‚úÖ Pickle folder: {PICKLE_FOLDER}")
print(f"‚úÖ Logs folder: {LOGS_FOLDER}")

# Thread lock for file operations
lock = threading.Lock()

def print_status(msg, level="info"):
    """Print status messages with icons"""
    levels = {"info":"‚ÑπÔ∏è","success":"‚úÖ","warn":"‚ö†Ô∏è","error":"‚ùå","debug":"üêû"}
    print(f"{levels.get(level, '‚ÑπÔ∏è')} {msg}")

# ======================================================
# 2Ô∏è‚É£ Git Configuration
# ======================================================
GIT_NAME = os.environ.get("GIT_USER_NAME", "Forex AI Bot")
GIT_EMAIL = os.environ.get("GIT_USER_EMAIL", "nakatonabira3@gmail.com")
GITHUB_USERNAME = os.environ.get("GITHUB_USERNAME", "rahim-dotAI")
GITHUB_REPO = os.environ.get("GITHUB_REPO", "forex-ai-models")
FOREX_PAT = os.environ.get("FOREX_PAT", "").strip()
BRANCH = "main"

if not FOREX_PAT:
    raise ValueError("‚ùå FOREX_PAT environment variable is required!")

REPO_URL = f"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git"

print(f"‚úÖ Git configured: {GIT_NAME} <{GIT_EMAIL}>")

subprocess.run(["git", "config", "--global", "user.name", GIT_NAME], check=False)
subprocess.run(["git", "config", "--global", "user.email", GIT_EMAIL], check=False)
subprocess.run(["git", "config", "--global", "credential.helper", "store"], check=False)

cred_file = Path.home() / ".git-credentials"
cred_file.write_text(f"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com\n")

# ======================================================
# 3Ô∏è‚É£ FIXED: Repository Management (Environment-Aware)
# ======================================================
def ensure_repo():
    """
    Ensure repository exists with environment-aware handling
    """
    if IN_GHA:
        # GitHub Actions: Repo already checked out
        print_status("ü§ñ GitHub Actions: Repository already available", "info")
        if not (REPO_FOLDER / ".git").exists() and (Path.cwd() / ".git").exists():
            # We're in the repo root, update global variable
            global REPO_FOLDER
            REPO_FOLDER = Path.cwd()
            print_status(f"‚úÖ Using current directory as repo: {REPO_FOLDER}", "success")
        elif (REPO_FOLDER / ".git").exists():
            print_status(f"‚úÖ Repository found at: {REPO_FOLDER}", "success")
        else:
            print_status("‚ö†Ô∏è Warning: .git directory not found", "warn")
            print_status("   Make sure actions/checkout@v4 is in your workflow", "warn")
        return

    # For Colab and Local: Clone or update
    if not (REPO_FOLDER / ".git").exists():
        if REPO_FOLDER.exists():
            shutil.rmtree(REPO_FOLDER)
        print_status(f"Cloning repo into {REPO_FOLDER}...", "info")
        try:
            subprocess.run(
                ["git", "clone", "-b", BRANCH, REPO_URL, str(REPO_FOLDER)],
                check=True,
                timeout=60
            )
            print_status("‚úÖ Repository cloned successfully", "success")
        except subprocess.TimeoutExpired:
            print_status("‚ùå Clone timed out after 60 seconds", "error")
            raise
        except Exception as e:
            print_status(f"‚ùå Clone failed: {e}", "error")
            raise
    else:
        print_status("Repo exists, pulling latest...", "info")
        try:
            subprocess.run(
                ["git", "-C", str(REPO_FOLDER), "fetch", "origin"],
                check=False,
                timeout=30
            )
            subprocess.run(
                ["git", "-C", str(REPO_FOLDER), "checkout", BRANCH],
                check=False
            )
            subprocess.run(
                ["git", "-C", str(REPO_FOLDER), "pull", "origin", BRANCH],
                check=False,
                timeout=30
            )
            print_status("‚úÖ Repo synced successfully", "success")
        except subprocess.TimeoutExpired:
            print_status("‚ö†Ô∏è Update timed out - continuing with existing repo", "warn")
        except Exception as e:
            print_status(f"‚ö†Ô∏è Update failed: {e} - continuing", "warn")

ensure_repo()

# ======================================================
# 4Ô∏è‚É£ Helper Functions with Safeguards
# ======================================================
def ensure_tz_naive(df):
    """Remove timezone information from DataFrame index"""
    if df is None or df.empty:
        return pd.DataFrame()

    df.index = pd.to_datetime(df.index, errors='coerce')

    if df.index.tz is not None:
        df.index = df.index.tz_localize(None)

    return df

def file_hash(filepath):
    """Calculate MD5 hash of file to detect changes"""
    if not filepath.exists():
        return None

    md5 = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            md5.update(chunk)

    return md5.hexdigest()

def safe_numeric(df):
    """
    Handle infinity/NaN robustly before any scaling
    WITH COLUMN VALIDATION
    """
    # Replace infinity values first
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    # Check if OHLC columns exist before trying to drop NaN rows
    required_columns = ['open', 'high', 'low', 'close']
    existing_columns = [col for col in required_columns if col in df.columns]

    # Only drop NaN if we have at least some OHLC columns
    if existing_columns:
        df.dropna(subset=existing_columns, inplace=True)
    else:
        # If no OHLC columns, just drop rows that are completely empty
        df.dropna(how='all', inplace=True)

    return df

# ======================================================
# 5Ô∏è‚É£ Incremental CSV Combine
# ======================================================
def combine_csv(csv_path):
    """
    Combine CSV from CSV_FOLDER with existing data in REPO_FOLDER

    Args:
        csv_path: Path to CSV file in CSV_FOLDER

    Returns:
        Tuple of (combined_df, target_file)
    """
    target_file = REPO_FOLDER / csv_path.name

    # Load existing data from repo if available
    if target_file.exists():
        try:
            existing_df = pd.read_csv(target_file, index_col=0, parse_dates=True)
            existing_df = ensure_tz_naive(existing_df)
            print_status(f"  üìÇ Loaded {len(existing_df)} existing rows from {csv_path.name}", "debug")
        except Exception as e:
            print_status(f"  ‚ö†Ô∏è Could not load existing data: {e}", "warn")
            existing_df = pd.DataFrame()
    else:
        existing_df = pd.DataFrame()

    # Load new data from CSV folder
    try:
        new_df = pd.read_csv(csv_path, index_col=0, parse_dates=True)
        new_df = ensure_tz_naive(new_df)
    except Exception as e:
        print_status(f"  ‚ùå Could not load new data: {e}", "error")
        return existing_df, target_file

    # Combine dataframes
    combined_df = pd.concat([existing_df, new_df])
    combined_df = combined_df[~combined_df.index.duplicated(keep="last")]
    combined_df.sort_index(inplace=True)

    return combined_df, target_file

# ======================================================
# 6Ô∏è‚É£ Incremental Indicators with Validation
# ======================================================
def add_indicators_incremental(existing_df, combined_df):
    """
    Add technical indicators only to NEW rows

    Args:
        existing_df: DataFrame with existing indicators
        combined_df: Combined DataFrame with OHLC data

    Returns:
        DataFrame with new rows and indicators, or None if no new rows
    """
    # Identify new rows
    if not existing_df.empty:
        new_rows = combined_df.loc[~combined_df.index.isin(existing_df.index)]
    else:
        new_rows = combined_df.copy()

    if new_rows.empty:
        return None

    # CRITICAL: Validate OHLC columns exist before processing
    required_cols = ['open', 'high', 'low', 'close']
    if not all(col in new_rows.columns for col in required_cols):
        print_status(f"‚ö†Ô∏è Missing required OHLC columns. Found: {list(new_rows.columns)}", "warn")
        return None

    # Clean numeric data with validation
    new_rows = safe_numeric(new_rows)

    # Check if we still have data after cleaning
    if new_rows.empty:
        print_status("‚ö†Ô∏è No rows left after cleaning", "warn")
        return None

    new_rows.sort_index(inplace=True)

    # Calculate indicators
    try:
        # Trend indicators
        trend_indicators = {
            'SMA_10': lambda d: ta.trend.sma_indicator(d['close'], 10) if len(d) >= 10 else np.nan,
            'SMA_50': lambda d: ta.trend.sma_indicator(d['close'], 50) if len(d) >= 50 else np.nan,
            'SMA_200': lambda d: ta.trend.sma_indicator(d['close'], 200) if len(d) >= 200 else np.nan,
            'EMA_10': lambda d: ta.trend.ema_indicator(d['close'], 10) if len(d) >= 10 else np.nan,
            'EMA_50': lambda d: ta.trend.ema_indicator(d['close'], 50) if len(d) >= 50 else np.nan,
            'EMA_200': lambda d: ta.trend.ema_indicator(d['close'], 200) if len(d) >= 200 else np.nan,
            'MACD': lambda d: ta.trend.macd(d['close']) if len(d) >= 26 else np.nan,
            'MACD_signal': lambda d: ta.trend.macd_signal(d['close']) if len(d) >= 26 else np.nan,
            'ADX': lambda d: ta.trend.adx(d['high'], d['low'], d['close'], 14) if len(d) >= 14 else np.nan
        }

        # Momentum indicators
        momentum_indicators = {
            'RSI_14': lambda d: ta.momentum.rsi(d['close'], 14) if len(d) >= 14 else np.nan,
            'StochRSI': lambda d: ta.momentum.stochrsi(d['close'], 14) if len(d) >= 14 else np.nan,
            'CCI': lambda d: ta.trend.cci(d['high'], d['low'], d['close'], 20) if len(d) >= 20 else np.nan,
            'ROC': lambda d: ta.momentum.roc(d['close'], 12) if len(d) >= 12 else np.nan,
            'Williams_%R': lambda d: WilliamsRIndicator(d['high'], d['low'], d['close'], 14).williams_r() if len(d) >= 14 else np.nan
        }

        # Volatility indicators
        volatility_indicators = {
            'Bollinger_High': lambda d: ta.volatility.bollinger_hband(d['close'], 20, 2) if len(d) >= 20 else np.nan,
            'Bollinger_Low': lambda d: ta.volatility.bollinger_lband(d['close'], 20, 2) if len(d) >= 20 else np.nan,
            'ATR': lambda d: ta.volatility.average_true_range(d['high'], d['low'], d['close'], 14) if len(d) >= 14 else np.nan,
            'STDDEV_20': lambda d: d['close'].rolling(20).std() if len(d) >= 20 else np.nan
        }

        # Volume-based indicators (if volume column exists)
        volume_indicators = {}
        if 'volume' in new_rows.columns:
            volume_indicators = {
                'OBV': lambda d: ta.volume.on_balance_volume(d['close'], d['volume']) if len(d) >= 1 else np.nan,
                'MFI': lambda d: ta.volume.money_flow_index(d['high'], d['low'], d['close'], d['volume'], 14) if len(d) >= 14 else np.nan
            }

        # Combine all indicators
        all_indicators = {**trend_indicators, **momentum_indicators, **volatility_indicators, **volume_indicators}

        # Calculate each indicator
        for name, func in all_indicators.items():
            try:
                new_rows[name] = func(new_rows)
            except Exception as e:
                print_status(f"‚ö†Ô∏è Failed to calculate {name}: {e}", "warn")
                new_rows[name] = np.nan

        # Cross signals
        if 'EMA_10' in new_rows.columns and 'EMA_50' in new_rows.columns:
            new_rows['EMA_10_cross_EMA_50'] = (new_rows['EMA_10'] > new_rows['EMA_50']).astype(int)

        if 'EMA_50' in new_rows.columns and 'EMA_200' in new_rows.columns:
            new_rows['EMA_50_cross_EMA_200'] = (new_rows['EMA_50'] > new_rows['EMA_200']).astype(int)

        if 'SMA_10' in new_rows.columns and 'SMA_50' in new_rows.columns:
            new_rows['SMA_10_cross_SMA_50'] = (new_rows['SMA_10'] > new_rows['SMA_50']).astype(int)

        if 'SMA_50' in new_rows.columns and 'SMA_200' in new_rows.columns:
            new_rows['SMA_50_cross_SMA_200'] = (new_rows['SMA_50'] > new_rows['SMA_200']).astype(int)

    except Exception as e:
        print_status(f"‚ö†Ô∏è Indicator calculation error: {e}", "warn")

    # Clean infinity/NaN values before scaling
    numeric_cols = new_rows.select_dtypes(include=[np.number]).columns

    if len(numeric_cols) > 0 and not new_rows[numeric_cols].dropna(how='all').empty:
        # Replace infinity values with NaN
        new_rows[numeric_cols] = new_rows[numeric_cols].replace([np.inf, -np.inf], np.nan)

        # Forward fill NaN values, then backward fill, then fill remaining with 0
        new_rows[numeric_cols] = new_rows[numeric_cols].ffill().bfill().fillna(0)

        # Clip extreme values to a reasonable range (5 standard deviations)
        for col in numeric_cols:
            if new_rows[col].std() > 0:
                mean_val = new_rows[col].mean()
                std_val = new_rows[col].std()
                lower_bound = mean_val - (5 * std_val)
                upper_bound = mean_val + (5 * std_val)
                new_rows[col] = new_rows[col].clip(lower=lower_bound, upper=upper_bound)

        # Scale numeric columns (except OHLC)
        protected_cols = ['open', 'high', 'low', 'close', 'volume']
        scalable_cols = [c for c in numeric_cols if c not in protected_cols]

        if scalable_cols:
            scaler = MinMaxScaler()
            try:
                new_rows[scalable_cols] = scaler.fit_transform(new_rows[scalable_cols])
            except Exception as e:
                print_status(f"‚ö†Ô∏è Scaling warning: {e} - using manual normalization", "warn")
                # Manual normalization fallback
                for col in scalable_cols:
                    col_min = new_rows[col].min()
                    col_max = new_rows[col].max()
                    if col_max > col_min:
                        new_rows[col] = (new_rows[col] - col_min) / (col_max - col_min)

    return new_rows

# ======================================================
# 7Ô∏è‚É£ Worker Function with Validation
# ======================================================
def process_csv_file(csv_file):
    """
    Process a single CSV file: combine with existing data and add indicators

    Args:
        csv_file: Path to CSV file

    Returns:
        Tuple of (pickle_filepath_if_changed, status_message)
    """
    try:
        # Combine CSV data
        combined_df, target_file = combine_csv(csv_file)

        # Validate combined dataframe has required columns
        required_cols = ['open', 'high', 'low', 'close']
        if not all(col in combined_df.columns for col in required_cols):
            msg = f"‚ö†Ô∏è Skipped {csv_file.name}: Missing OHLC columns"
            print_status(msg, "warn")
            return None, msg

        # Check for existing indicators pickle
        existing_pickle = PICKLE_FOLDER / f"{csv_file.stem}_indicators.pkl"

        if existing_pickle.exists():
            try:
                existing_df = pd.read_pickle(existing_pickle)
            except Exception as e:
                print_status(f"  ‚ö†Ô∏è Could not load existing pickle: {e}", "warn")
                existing_df = pd.DataFrame()
        else:
            existing_df = pd.DataFrame()

        # Calculate indicators for new rows only
        new_indicators = add_indicators_incremental(existing_df, combined_df)

        if new_indicators is not None:
            # Combine existing indicators with new ones
            updated_df = pd.concat([existing_df, new_indicators]).sort_index()

            # Save files (thread-safe)
            with lock:
                updated_df.to_pickle(existing_pickle, protocol=4)
                combined_df.to_csv(target_file)

            msg = f"‚úÖ {csv_file.name} updated with {len(new_indicators)} new rows (total: {len(combined_df)})"
            print_status(msg, "success")
            return str(existing_pickle), msg
        else:
            msg = f"‚ÑπÔ∏è {csv_file.name} no new rows (total: {len(combined_df)})"
            print_status(msg, "info")
            return None, msg

    except Exception as e:
        msg = f"‚ùå Failed {csv_file.name}: {e}"
        print_status(msg, "error")
        return None, msg

# ======================================================
# 8Ô∏è‚É£ Process All CSVs in Parallel
# ======================================================
print("\n" + "=" * 70)
print("üöÄ Processing CSV files...")
print("=" * 70 + "\n")

csv_files = list(CSV_FOLDER.glob("*.csv"))

if not csv_files:
    print_status("No CSVs found to process ‚Äì pipeline will skip", "warn")
else:
    print_status(f"Found {len(csv_files)} CSV files to process", "info")

changed_files = []

if csv_files:
    with ThreadPoolExecutor(max_workers=min(8, len(csv_files))) as executor:
        futures = [executor.submit(process_csv_file, f) for f in csv_files]

        for future in as_completed(futures):
            file, msg = future.result()
            if file:
                changed_files.append(file)

# ======================================================
# 9Ô∏è‚É£ Commit & Push Updates (Skip in GitHub Actions)
# ======================================================
if IN_GHA:
    print("\n" + "=" * 70)
    print("ü§ñ GitHub Actions: Skipping git operations")
    print("   (Workflow will handle commit and push)")
    print("=" * 70)

elif changed_files:
    print("\n" + "=" * 70)
    print("üöÄ Committing changes to GitHub...")
    print("=" * 70)

    try:
        print_status(f"Committing {len(changed_files)} updated files...", "info")

        # Stage files
        subprocess.run(
            ["git", "-C", str(REPO_FOLDER), "add"] + changed_files,
            check=False
        )

        # Commit
        commit_result = subprocess.run(
            ["git", "-C", str(REPO_FOLDER), "commit", "-m", "üìà Auto update FX CSVs & indicators"],
            capture_output=True,
            text=True,
            check=False
        )

        if commit_result.returncode == 0:
            print_status("‚úÖ Changes committed", "success")
        elif "nothing to commit" in commit_result.stdout:
            print_status("‚ÑπÔ∏è No changes to commit", "info")
        else:
            print_status(f"‚ö†Ô∏è Commit warning: {commit_result.stderr}", "warn")

        # Push with retry logic
        max_attempts = 3
        for attempt in range(max_attempts):
            print_status(f"üì§ Pushing to GitHub (attempt {attempt + 1}/{max_attempts})...", "info")

            push_result = subprocess.run(
                ["git", "-C", str(REPO_FOLDER), "push", "origin", BRANCH],
                capture_output=True,
                text=True,
                timeout=30
            )

            if push_result.returncode == 0:
                print_status("‚úÖ Push successful", "success")
                break
            else:
                if attempt < max_attempts - 1:
                    print_status(f"‚ö†Ô∏è Push attempt {attempt + 1} failed, retrying...", "warn")
                    # Pull before retry
                    subprocess.run(
                        ["git", "-C", str(REPO_FOLDER), "pull", "--rebase", "origin", BRANCH],
                        capture_output=True
                    )
                    time.sleep(5)
                else:
                    print_status(f"‚ùå Push failed after {max_attempts} attempts", "error")
                    print_status(f"   Error: {push_result.stderr}", "error")

    except subprocess.TimeoutExpired:
        print_status("‚ùå Git operation timed out", "error")
    except Exception as e:
        print_status(f"‚ùå Git error: {e}", "error")

else:
    print("\n" + "=" * 70)
    print("‚ÑπÔ∏è No files changed ‚Äì skipping git operations")
    print("=" * 70)

# ======================================================
# ‚úÖ Completion
# ======================================================
print("\n" + "=" * 70)
print("‚úÖ CSV COMBINER WORKFLOW COMPLETED")
print("=" * 70)
print(f"Environment: {ENV_NAME}")
print(f"CSV files processed: {len(csv_files)}")
print(f"Pickle files updated: {len(changed_files)}")
print(f"Status: {'Success' if csv_files else 'No CSVs found'}")
print("=" * 70)
print("\nüéØ All CSVs combined, incremental indicators added successfully!")
------------------

----- stdout -----
======================================================================
üîß CSV Combiner & Indicator Generator - Multi-Environment Edition
======================================================================
üåç Detected Environment: GitHub Actions
üìÇ GitHub Actions: Using repo root: /home/runner/work/forex-ai-models/forex-ai-models
‚úÖ Root directory: /home/runner/work/forex-ai-models/forex-ai-models
‚úÖ Repo folder: /home/runner/work/forex-ai-models/forex-ai-models/forex-ai-models
‚úÖ CSV folder: /home/runner/work/forex-ai-models/forex-ai-models/csvs
‚úÖ Pickle folder: /home/runner/work/forex-ai-models/forex-ai-models/pickles
‚úÖ Logs folder: /home/runner/work/forex-ai-models/forex-ai-models/logs
‚úÖ Git configured: Forex AI Bot <nakatonabira3@gmail.com>
------------------

  [36mCell[39m[36m [39m[32mIn[7][39m[32m, line 118[39m
[31m    [39m[31mglobal REPO_FOLDER[39m
    ^
[31mSyntaxError[39m[31m:[39m name 'REPO_FOLDER' is used prior to global declaration


============================================================
