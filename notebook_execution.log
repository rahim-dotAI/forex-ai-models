ğŸš€ Starting tagged cells execution...
ğŸ“Š Found 2 tagged cells

ğŸ”„ Tagged Cell 1/2
======================================================================
ğŸš€ Ultra-Persistent FX Pipeline v4.1 - Weekend-Aware Edition
======================================================================
ğŸŒ Environment: GitHub Actions
ğŸ“‚ Base Folder: /home/runner/work/forex-ai-models/forex-ai-models
ğŸ’¾ Save Folder: /home/runner/work/forex-ai-models/forex-ai-models
ğŸ“¦ Repo Folder: /home/runner/work/forex-ai-models/forex-ai-models
ğŸ”§ Processed: /home/runner/work/forex-ai-models/forex-ai-models/data/processed
ğŸ’¿ Database: /home/runner/work/forex-ai-models/forex-ai-models/database/memory_v85.db
ğŸ“… Current Time: 2025-11-22 08:20:36 UTC (Saturday)
â±ï¸  Trade Age: 0.5h (Weekend)
ğŸ¦ Market Status: CLOSED (Learning Only)
======================================================================
âœ… âœ… Git configured: Forex AI Bot

âœ… âœ… ML libraries loaded
â„¹ï¸ ğŸš€ Initializing Ultra-Persistent Pipeline...
â„¹ï¸ ======================================================================
âœ… ğŸš€ STARTING ULTRA-PERSISTENT PIPELINE v4.1
â„¹ï¸ ======================================================================
ğŸ–ï¸ ğŸ–ï¸ WEEKEND MODE: Learning continues, no new live signals
â„¹ï¸ â±ï¸  Trade evaluation age: 0.5h
â„¹ï¸ ğŸ“ Database: /home/runner/work/forex-ai-models/forex-ai-models/database/memory_v85.db
âœ… âœ… Created: memory_v85.db
ğŸ“Š ğŸ“Š Database Tables:
ğŸ“Š   âœ“ pending_trades: 0 rows
ğŸ“Š   âœ“ completed_trades: 0 rows
ğŸ“Š   âœ“ model_stats_cache: 0 rows
ğŸ“Š 
ğŸ“Š CURRENT DATABASE STATS
ğŸ“Š   Pending: 0
ğŸ“Š   Completed: 0
ğŸ“Š   Total P&L: $0.00000
ğŸ“Š   Accuracy: 0.0%
â„¹ï¸ 
ğŸ”„ LOADING PROCESSED PICKLES
â„¹ï¸ ğŸ“‚ Looking in: /home/runner/work/forex-ai-models/forex-ai-models/data/processed
âš ï¸ âš ï¸ No processed pickles found!
â„¹ï¸ â„¹ï¸  Run CSV combiner first to generate processed pickles
âš ï¸ 
âš ï¸ No signals generated
âœ… 
âœ… ALL OPERATIONS COMPLETED!

âœ… Completed in 1.1s

ğŸ”„ Tagged Cell 2/2
Traceback (most recent call last):
  File "/home/runner/work/forex-ai-models/forex-ai-models/run_tagged_cells.py", line 63, in <module>
    success = run_tagged_cells(notebook)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/forex-ai-models/forex-ai-models/run_tagged_cells.py", line 51, in run_tagged_cells
    ep.preprocess(nb, {'metadata': {'path': '.'}})
  File "/home/runner/work/forex-ai-models/forex-ai-models/run_tagged_cells.py", line 20, in preprocess
    return super().preprocess(nb, resources, km)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 103, in preprocess
    self.preprocess_cell(cell, resources, index)
  File "/home/runner/work/forex-ai-models/forex-ai-models/run_tagged_cells.py", line 30, in preprocess_cell
    cell, resources = super().preprocess_cell(cell, resources, cell_index)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 124, in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
#TAG: pipeline_main

#!/usr/bin/env python3
"""
TRADE BEACON v17.0 - INTEGRATED LEARNING EDITION
=================================================
ğŸ§  Deep Q-Learning with Experience Replay
ğŸ“Š Learns from Ultra-Persistent Pipeline Database
ğŸ“ˆ 6 Chart Pattern Detection
ğŸ’¾ Cloud-Synced Iteration Counter
ğŸ¯ Weekend Learning Mode (Backtesting + Database)
âš ï¸ LIVE TRADING - Real money at risk on weekdays!
"""

import os, sys, json, pickle, random, re, smtplib, subprocess, logging, warnings, shutil, sqlite3
from pathlib import Path
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime, timezone, timedelta
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Any
from contextlib import contextmanager
import numpy as np
import pandas as pd
import requests

warnings.filterwarnings('ignore')

print("="*70)
print("ğŸ§  TRADE BEACON v17.0 - INTEGRATED LEARNING")
print("="*70)

# Environment Setup
try:
    import google.colab
    IN_COLAB, IN_GHA, ENV_NAME = True, False, "Google Colab"
except ImportError:
    IN_COLAB, IN_GHA = False, "GITHUB_ACTIONS" in os.environ
    ENV_NAME = "GitHub Actions" if IN_GHA else "Local"

BASE_FOLDER = Path("/content" if IN_COLAB else Path.cwd())
SAVE_FOLDER = BASE_FOLDER if IN_GHA else (BASE_FOLDER / "forex-ai-models" if IN_COLAB else BASE_FOLDER)

DIRECTORIES = {k: SAVE_FOLDER / v for k, v in {
    "data_processed": "data/processed", "database": "database", "logs": "logs",
    "outputs": "outputs", "omega_state": "omega_state", "rl_memory": "rl_memory",
    "rl_models": "rl_models", "backups": "backups"
}.items()}

for d in DIRECTORIES.values():
    d.mkdir(parents=True, exist_ok=True)

PICKLE_FOLDER = DIRECTORIES["data_processed"]
DATABASE_FOLDER = DIRECTORIES["database"]
OUTPUTS_FOLDER = DIRECTORIES["outputs"]
OMEGA_STATE_FOLDER = DIRECTORIES["omega_state"]
RL_MEMORY_FOLDER = DIRECTORIES["rl_memory"]
RL_MODELS_FOLDER = DIRECTORIES["rl_models"]
BACKUP_FOLDER = DIRECTORIES["backups"]

# Ultra-Persistent Pipeline Database
PIPELINE_DB = DATABASE_FOLDER / "memory_v85.db"

logging.basicConfig(
    filename=str(DIRECTORIES["logs"] / f"trade_beacon_{datetime.now():%Y%m%d_%H%M%S}.log"),
    level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s'
)

def log(msg, lvl="info"):
    icons = {"info":"â„¹ï¸","success":"âœ…","warn":"âš ï¸","error":"âŒ","rocket":"ğŸš€","brain":"ğŸ§ ","money":"ğŸ’°","database":"ğŸ’¾"}
    getattr(logging, "warning" if lvl=="warn" else lvl, logging.info)(msg)
    print(f"{icons.get(lvl,'â„¹ï¸')} {msg}")

# Config
GITHUB_USERNAME, GITHUB_REPO = "rahim-dotAI", "forex-ai-models"
FOREX_PAT = os.getenv("FOREX_PAT", "").strip()
if not FOREX_PAT and IN_COLAB:
    try:
        from google.colab import userdata
        FOREX_PAT = userdata.get("FOREX_PAT")
        if FOREX_PAT: os.environ["FOREX_PAT"] = FOREX_PAT
    except: pass

GMAIL_USER = os.getenv("GMAIL_USER", "nakatonabira3@gmail.com")
GMAIL_APP_PASSWORD = os.getenv("GMAIL_APP_PASSWORD", "").strip() or "gmwohahtltmcewug"
BROWSERLESS_TOKEN = os.getenv("BROWSERLESS_TOKEN", "")

PAIRS = ["EUR/USD", "GBP/USD", "USD/JPY", "AUD/USD"]
ATR_PERIOD, MIN_ATR, EPS = 14, 1e-5, 1e-8
BASE_CAPITAL, MAX_RISK_PER_TRADE, MAX_POSITIONS, MAX_TRADE_CAP = 100, 0.02, 2, 10.0

# RL Hyperparameters
STATE_SIZE, ACTION_SPACE = 30, 3
LEARNING_RATE, GAMMA = 0.001, 0.95
EPSILON_START, EPSILON_MIN, EPSILON_DECAY = 1.0, 0.05, 0.995
BATCH_SIZE, MEMORY_SIZE, MIN_REPLAY_SIZE = 32, 10000, 100
TARGET_UPDATE_FREQUENCY = 10

# SL/TP and Reward Parameters
ATR_SL_MULTIPLIER, ATR_TP_MULTIPLIER = 2.0, 3.0
PROFIT_REWARD_SCALE, LOSS_PENALTY_SCALE = 100.0, 100.0
SHARPE_REWARD_SCALE, WIN_STREAK_BONUS, LOSS_STREAK_PENALTY = 10.0, 5.0, 3.0

# Weekend Backtest Parameters
WEEKEND_BACKTEST_STEPS = 50

# Files
OMEGA_SIGNALS_FILE = OUTPUTS_FOLDER / "omega_signals.json"
OMEGA_ITERATION_FILE = OMEGA_STATE_FOLDER / "omega_iteration.json"
RL_MEMORY_FILE = RL_MEMORY_FOLDER / "experience_replay.pkl"
RL_Q_NETWORK_FILE = RL_MODELS_FOLDER / "q_network.npz"
RL_TARGET_NETWORK_FILE = RL_MODELS_FOLDER / "target_network.npz"
RL_LEARNING_STATS_FILE = RL_MEMORY_FOLDER / "learning_stats.json"
TRADE_HISTORY_FILE = RL_MEMORY_FOLDER / "trade_history.json"
PIPELINE_SYNC_FILE = RL_MEMORY_FOLDER / "pipeline_sync.json"

log("ğŸ’° LIVE TRADING MODE ACTIVE", "money")

# ========================================================================
# WEEKEND MODE
# ========================================================================

def is_weekend():
    """Check if current day is weekend"""
    return datetime.now().weekday() in [5, 6]

def get_weekend_mode():
    """Determine trading mode based on day of week"""
    if is_weekend():
        return "WEEKEND_LEARNING"
    return "LIVE_TRADING"

# ========================================================================
# DATABASE CONNECTION FOR ULTRA-PERSISTENT PIPELINE
# ========================================================================

class PipelineDatabase:
    """Interface to Ultra-Persistent Pipeline Database"""

    def __init__(self, db_path=PIPELINE_DB):
        self.db_path = db_path
        self.conn = None

        if not self.db_path.exists():
            log(f"âš ï¸ Pipeline database not found: {self.db_path}", "warn")
            return

        try:
            self.conn = sqlite3.connect(str(self.db_path), timeout=30, check_same_thread=False)
            log(f"âœ… Connected to pipeline database", "database")
        except Exception as e:
            log(f"âŒ Failed to connect to pipeline DB: {e}", "error")

    @contextmanager
    def get_cursor(self):
        """Context manager for database cursor"""
        if not self.conn:
            yield None
            return

        cursor = self.conn.cursor()
        try:
            yield cursor
        finally:
            cursor.close()

    def get_completed_trades(self, since_timestamp=None, limit=1000):
        """Fetch completed trades from pipeline database"""
        if not self.conn:
            return []

        try:
            with self.get_cursor() as cursor:
                if since_timestamp:
                    cursor.execute('''
                        SELECT pair, timeframe, model_used, entry_price, exit_price,
                               sl_price, tp_price, prediction, hit_tp, pnl,
                               pnl_percent, duration_hours, created_at, evaluated_at
                        FROM completed_trades
                        WHERE evaluated_at > ?
                        ORDER BY evaluated_at DESC
                        LIMIT ?
                    ''', (since_timestamp, limit))
                else:
                    cursor.execute('''
                        SELECT pair, timeframe, model_used, entry_price, exit_price,
                               sl_price, tp_price, prediction, hit_tp, pnl,
                               pnl_percent, duration_hours, created_at, evaluated_at
                        FROM completed_trades
                        ORDER BY evaluated_at DESC
                        LIMIT ?
                    ''', (limit,))

                return cursor.fetchall()
        except Exception as e:
            log(f"âš ï¸ Failed to fetch trades: {e}", "warn")
            return []

    def get_pipeline_stats(self):
        """Get statistics from pipeline database"""
        if not self.conn:
            return {}

        try:
            with self.get_cursor() as cursor:
                cursor.execute('''
                    SELECT
                        COUNT(*) as total_trades,
                        SUM(CASE WHEN hit_tp THEN 1 ELSE 0 END) as wins,
                        SUM(pnl) as total_pnl,
                        AVG(pnl) as avg_pnl,
                        MAX(evaluated_at) as last_trade
                    FROM completed_trades
                ''')

                result = cursor.fetchone()
                if result:
                    return {
                        'total_trades': result[0] or 0,
                        'wins': result[1] or 0,
                        'total_pnl': result[2] or 0.0,
                        'avg_pnl': result[3] or 0.0,
                        'win_rate': (result[1] / result[0] * 100) if result[0] else 0.0,
                        'last_trade': result[4]
                    }
        except Exception as e:
            log(f"âš ï¸ Failed to get stats: {e}", "warn")

        return {}

    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()

# ========================================================================
# ITERATION COUNTER
# ========================================================================

def safe_json_load(filepath: Path, default=None):
    """Safely load JSON file"""
    if not filepath.exists():
        return default
    try:
        with open(filepath, 'r') as f:
            return json.load(f)
    except:
        return default

def safe_json_save(filepath: Path, data):
    """Safely save JSON file"""
    try:
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2, default=str)
        return True
    except:
        return False

def load_iteration_counter():
    """Load iteration counter from file"""
    if OMEGA_ITERATION_FILE.exists():
        try:
            data = safe_json_load(OMEGA_ITERATION_FILE)
            if data and isinstance(data, dict):
                return data
        except:
            pass
    return {'total': 0, 'start_date': datetime.now(timezone.utc).isoformat(), 'history': []}

def save_iteration_counter(data):
    """Save iteration counter to file"""
    safe_json_save(OMEGA_ITERATION_FILE, data)

def increment_iteration():
    """Increment iteration counter and save"""
    data = load_iteration_counter()
    data['total'] += 1
    data['last_update'] = datetime.now(timezone.utc).isoformat()
    data['history'].append({
        'iteration': data['total'],
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'environment': ENV_NAME,
        'mode': get_weekend_mode()
    })
    if len(data['history']) > 1000:
        data['history'] = data['history'][-1000:]
    save_iteration_counter(data)
    return data['total']

# ========================================================================
# SAFE I/O
# ========================================================================

def safe_pickle_load(filepath: Path, default=None, backup=True):
    """Safely load pickle file with backup recovery"""
    if not filepath.exists():
        return default
    try:
        with open(filepath, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        log(f"âš ï¸ Corrupted: {filepath.name} ({e})", "warn")
        if backup:
            backup_path = BACKUP_FOLDER / f"{filepath.stem}_backup.pkl"
            if backup_path.exists():
                try:
                    with open(backup_path, 'rb') as f:
                        return pickle.load(f)
                except:
                    pass
        return default

def safe_pickle_save(filepath: Path, data, backup=True):
    """Safely save pickle file with backup"""
    try:
        if backup and filepath.exists():
            backup_path = BACKUP_FOLDER / f"{filepath.stem}_backup.pkl"
            try:
                shutil.copy2(filepath, backup_path)
            except:
                pass
        with open(filepath, 'wb') as f:
            pickle.dump(data, f, protocol=4)
        return True
    except Exception as e:
        log(f"âŒ Failed to save {filepath.name}: {e}", "error")
        return False

# ========================================================================
# CONFIDENCE SYSTEM
# ========================================================================

class ImprovedRLConfidence:
    """Adaptive confidence estimation for trade execution"""
    def __init__(self, min_q_spread=0.1, softmax_temp=1.0,
                 early_thresh=0.35, mature_thresh=0.65, epsilon_trans=0.3):
        self.min_q_spread = min_q_spread
        self.temperature = softmax_temp
        self.early_threshold = early_thresh
        self.mature_threshold = mature_thresh
        self.epsilon_transition = epsilon_trans

    def softmax(self, q_values: np.ndarray) -> np.ndarray:
        """Calculate softmax probabilities"""
        q_shifted = q_values - np.max(q_values)
        exp_q = np.exp(q_shifted / self.temperature)
        return exp_q / np.sum(exp_q)

    def calculate_entropy(self, probs: np.ndarray) -> float:
        """Calculate Shannon entropy"""
        probs = np.clip(probs, 1e-10, 1.0)
        return -np.sum(probs * np.log(probs))

    def get_confidence_metrics(self, q_values: np.ndarray, epsilon: float) -> Dict:
        """Calculate comprehensive confidence metrics"""
        sorted_q = np.sort(q_values)[::-1]
        q_spread = sorted_q[0] - sorted_q[1] if len(sorted_q) > 1 else 0.0
        probabilities = self.softmax(q_values)
        best_action_prob = np.max(probabilities)
        entropy = self.calculate_entropy(probabilities)
        max_entropy = np.log(len(q_values))
        normalized_entropy = entropy / max_entropy

        if epsilon > self.epsilon_transition:
            confidence = (0.5 * np.tanh(q_spread) + 0.3 * best_action_prob +
                         0.2 * (1 - normalized_entropy)) * 100
        else:
            confidence = (0.3 * np.tanh(q_spread) + 0.5 * best_action_prob +
                         0.2 * (1 - normalized_entropy)) * 100

        progress = 1 - (epsilon - 0.05) / (1.0 - 0.05)
        adaptive_threshold = (self.early_threshold +
                            progress * (self.mature_threshold - self.early_threshold)) * 100

        return {
            'q_spread': float(q_spread),
            'best_action_prob': float(best_action_prob),
            'entropy': float(entropy),
            'normalized_entropy': float(normalized_entropy),
            'confidence': float(np.clip(confidence, 0, 100)),
            'adaptive_threshold': float(adaptive_threshold),
            'training_progress': float(progress)
        }

    def should_trade(self, q_values: np.ndarray, epsilon: float) -> Tuple[bool, float, Dict]:
        """Determine if trade should be executed based on confidence"""
        metrics = self.get_confidence_metrics(q_values, epsilon)
        confidence = metrics['confidence']
        threshold = metrics['adaptive_threshold']

        if metrics['q_spread'] < self.min_q_spread:
            return False, confidence, {**metrics, 'reason': 'q_spread_too_small'}

        should_execute = confidence >= threshold
        metrics['threshold_used'] = threshold
        metrics['decision'] = 'TRADE' if should_execute else 'HOLD'
        return should_execute, confidence, metrics

    def calculate_position_size(self, base_size: float, confidence: float, metrics: Dict) -> float:
        """Calculate position size based on confidence"""
        confidence_multiplier = 0.5 + (confidence / 100) * 1.0
        entropy_penalty = 1.0 - (metrics['normalized_entropy'] * 0.3)
        exploration_penalty = 1.0 if metrics['training_progress'] > 0.5 else 0.7
        return base_size * confidence_multiplier * entropy_penalty * exploration_penalty

# ========================================================================
# DATA STRUCTURES
# ========================================================================

@dataclass
class Experience:
    """Single experience for replay memory"""
    state: np.ndarray
    action: int
    reward: float
    next_state: np.ndarray
    done: bool
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())

@dataclass
class TradeOutcome:
    """Trade outcome for learning"""
    pair: str
    action: str
    entry_price: float
    exit_price: float
    sl: float
    tp: float
    position_size: float
    pnl: float
    duration: float
    hit_tp: bool
    timestamp_entry: str
    timestamp_exit: str
    state_at_entry: np.ndarray
    confidence: float
    regime: str
    session: str

# ========================================================================
# INDICATORS
# ========================================================================

def calculate_rsi(prices: pd.Series, period: int = 14) -> pd.Series:
    """Calculate RSI indicator"""
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()
    rs = gain / (loss + EPS)
    return 100 - (100 / (1 + rs))

def calculate_macd(prices: pd.Series, fast=12, slow=26, signal=9):
    """Calculate MACD indicator"""
    ema_fast = prices.ewm(span=fast, adjust=False).mean()
    ema_slow = prices.ewm(span=slow, adjust=False).mean()
    macd = ema_fast - ema_slow
    signal_line = macd.ewm(span=signal, adjust=False).mean()
    return macd, signal_line, macd - signal_line

def calculate_bollinger_bands(prices: pd.Series, period=20, std_dev=2):
    """Calculate Bollinger Bands"""
    sma = prices.rolling(window=period, min_periods=1).mean()
    std = prices.rolling(window=period, min_periods=1).std()
    return sma + (std * std_dev), sma, sma - (std * std_dev)

def detect_chart_patterns(df: pd.DataFrame) -> Dict[str, float]:
    """Detect 6 key chart patterns and return strength scores"""
    patterns = {}
    try:
        closes = df['close'].values
        highs = df['high'].values
        lows = df['low'].values

        if len(closes) < 20:
            return {p: 0.0 for p in ['double_top', 'double_bottom', 'head_shoulders',
                                     'triangle', 'channel', 'breakout']}

        recent_highs = highs[-20:]
        recent_lows = lows[-20:]

        peak_indices = [i for i in range(1, len(recent_highs)-1)
                       if recent_highs[i] > recent_highs[i-1] and recent_highs[i] > recent_highs[i+1]]
        trough_indices = [i for i in range(1, len(recent_lows)-1)
                         if recent_lows[i] < recent_lows[i-1] and recent_lows[i] < recent_lows[i+1]]

        patterns['double_top'] = 0.0
        if len(peak_indices) >= 2:
            last_peaks = [recent_highs[i] for i in peak_indices[-2:]]
            if abs(last_peaks[0] - last_peaks[1]) / last_peaks[0] < 0.01:
                patterns['double_top'] = -0.8

        patterns['double_bottom'] = 0.0
        if len(trough_indices) >= 2:
            last_troughs = [recent_lows[i] for i in trough_indices[-2:]]
            if abs(last_troughs[0] - last_troughs[1]) / last_troughs[0] < 0.01:
                patterns['double_bottom'] = 0.8

        patterns['head_shoulders'] = 0.0
        if len(peak_indices) >= 3:
            last_3_peaks = [recent_highs[i] for i in peak_indices[-3:]]
            if (last_3_peaks[1] > last_3_peaks[0] and last_3_peaks[1] > last_3_peaks[2] and
                abs(last_3_peaks[0] - last_3_peaks[2]) / last_3_peaks[0] < 0.015):
                patterns['head_shoulders'] = -0.9

        patterns['triangle'] = 0.0
        if len(closes) >= 15:
            upper_slope = (highs[-1] - highs[-15]) / 15
            lower_slope = (lows[-1] - lows[-15]) / 15
            if abs(upper_slope) < 0.0001 and lower_slope > 0:
                patterns['triangle'] = 0.6
            elif upper_slope < 0 and abs(lower_slope) < 0.0001:
                patterns['triangle'] = -0.6
            elif abs(upper_slope + lower_slope) < 0.0001:
                patterns['triangle'] = 0.3

        patterns['channel'] = 0.0
        if len(closes) >= 20:
            high_slope = (highs[-1] - highs[-20]) / 20
            low_slope = (lows[-1] - lows[-20]) / 20
            if abs(high_slope - low_slope) < 0.0002:
                patterns['channel'] = 0.5 if high_slope > 0 else -0.5

        patterns['breakout'] = 0.0
        if len(closes) >= 30:
            volatility = np.std(closes[-30:-5])
            recent_move = abs(closes[-1] - closes[-5])
            if recent_move > volatility * 2:
                patterns['breakout'] = 0.7 if closes[-1] > closes[-5] else -0.7

    except Exception as e:
        log(f"âš ï¸ Pattern detection error: {e}", "warn")
        patterns = {p: 0.0 for p in ['double_top', 'double_bottom', 'head_shoulders',
                                     'triangle', 'channel', 'breakout']}
    return patterns

def create_state_vector(df_1h: pd.DataFrame, df_1d: pd.DataFrame, pair: str) -> np.ndarray:
    """Create 30-feature state vector from market data"""
    if len(df_1h) < 50 or len(df_1d) < 30:
        return np.zeros(STATE_SIZE)

    features = []
    try:
        close_1h = df_1h['close'].iloc[-1]
        high_20, low_20 = df_1h['high'].iloc[-20:].max(), df_1h['low'].iloc[-20:].min()
        features.append((close_1h - low_20) / (high_20 - low_20 + EPS))

        features.extend(df_1h['close'].pct_change().iloc[-5:].values)

        rsi_1h = calculate_rsi(df_1h['close'], 14).iloc[-1] / 100.0
        rsi_1d = calculate_rsi(df_1d['close'], 14).iloc[-1] / 100.0
        features.extend([rsi_1h, rsi_1d])

        macd, signal, _ = calculate_macd(df_1h['close'])
        features.extend([np.tanh(macd.iloc[-1] * 100), np.tanh(signal.iloc[-1] * 100)])

        upper, middle, lower = calculate_bollinger_bands(df_1h['close'])
        bb_pos = (close_1h - lower.iloc[-1]) / (upper.iloc[-1] - lower.iloc[-1] + EPS)
        bb_width = (upper.iloc[-1] - lower.iloc[-1]) / middle.iloc[-1]
        features.extend([bb_pos, bb_width])

        atr = df_1h['atr'].iloc[-1]
        atr_ma = df_1h['atr'].rolling(20).mean().iloc[-1]
        features.extend([atr / (atr_ma + EPS), df_1h['close'].pct_change().std() * 100])

        ema_fast = df_1h['close'].ewm(span=12).mean().iloc[-1]
        ema_slow = df_1h['close'].ewm(span=26).mean().iloc[-1]
        trend_1h = (ema_fast - ema_slow) / ema_slow

        ema_fast_1d = df_1d['close'].ewm(span=12).mean().iloc[-1]
        ema_slow_1d = df_1d['close'].ewm(span=26).mean().iloc[-1]
        trend_1d = (ema_fast_1d - ema_slow_1d) / ema_slow_1d

        slope = (df_1h['close'].iloc[-1] - df_1h['close'].iloc[-20]) / df_1h['close'].iloc[-20]
        features.extend([trend_1h * 10, trend_1d * 10, slope * 10])

        vol_ratio = 1.0
        if 'volume' in df_1h.columns and df_1h['volume'].sum() > 0:
            vol_ratio = df_1h['volume'].iloc[-5:].mean() / (df_1h['volume'].iloc[-50:].mean() + EPS)
        features.append(vol_ratio)

        hour = datetime.now().hour
        features.extend([
            1.0 if 0 <= hour < 8 else 0.0,
            1.0 if 8 <= hour < 16 else 0.0,
            1.0 if 16 <= hour < 24 else 0.0
        ])

        features.extend([datetime.now().weekday() / 6.0, hour / 23.0])

        patterns = detect_chart_patterns(df_1h)
        features.extend([
            patterns.get('double_top', 0.0),
            patterns.get('double_bottom', 0.0),
            patterns.get('head_shoulders', 0.0),
            patterns.get('triangle', 0.0),
            patterns.get('channel', 0.0),
            patterns.get('breakout', 0.0)
        ])

        features = features[:STATE_SIZE]
        while len(features) < STATE_SIZE:
            features.append(0.0)

        return np.array(features, dtype=np.float32)
    except Exception as e:
        log(f"âš ï¸ State vector error: {e}", "warn")
        return np.zeros(STATE_SIZE)

# ========================================================================
# Q-NETWORK
# ========================================================================

class SimpleQNetwork:
    """Deep Q-Network with 2 hidden layers"""
    def __init__(self, state_size=STATE_SIZE, action_size=ACTION_SPACE, hidden_size=64):
        self.state_size, self.action_size = state_size, action_size
        self.w1 = np.random.randn(state_size, hidden_size) * np.sqrt(2 / state_size)
        self.b1 = np.zeros(hidden_size)
        self.w2 = np.random.randn(hidden_size, hidden_size) * np.sqrt(2 / hidden_size)
        self.b2 = np.zeros(hidden_size)
        self.w3 = np.random.randn(hidden_size, action_size) * np.sqrt(2 / hidden_size)
        self.b3 = np.zeros(action_size)

    def relu(self, x):
        return np.maximum(0, x)

    def forward(self, state):
        h1 = self.relu(np.dot(state, self.w1) + self.b1)
        h2 = self.relu(np.dot(h1, self.w2) + self.b2)
        return np.dot(h2, self.w3) + self.b3

    def predict(self, state):
        if state.ndim == 1:
            state = state.reshape(1, -1)
        return self.forward(state[0])

    def update(self, states, targets, learning_rate=LEARNING_RATE):
        for i in range(len(states)):
            state, target = states[i], targets[i]
            h1 = self.relu(np.dot(state, self.w1) + self.b1)
            h2 = self.relu(np.dot(h1, self.w2) + self.b2)
            q_values = np.dot(h2, self.w3) + self.b3
            error = q_values - target

            dw3 = np.outer(h2, error)
            db3 = error
            dh2 = np.dot(error, self.w3.T)
            dh2[h2 <= 0] = 0
            dw2 = np.outer(h1, dh2)
            db2 = dh2
            dh1 = np.dot(dh2, self.w2.T)
            dh1[h1 <= 0] = 0
            dw1 = np.outer(state, dh1)
            db1 = dh1

            self.w3 -= learning_rate * dw3
            self.b3 -= learning_rate * db3
            self.w2 -= learning_rate * dw2
            self.b2 -= learning_rate * db2
            self.w1 -= learning_rate * dw1
            self.b1 -= learning_rate * db1

    def clone(self):
        new_net = SimpleQNetwork(self.state_size, self.action_size)
        new_net.w1, new_net.b1 = self.w1.copy(), self.b1.copy()
        new_net.w2, new_net.b2 = self.w2.copy(), self.b2.copy()
        new_net.w3, new_net.b3 = self.w3.copy(), self.b3.copy()
        return new_net

    def save(self, filepath):
        try:
            np.savez(filepath, w1=self.w1, b1=self.b1, w2=self.w2, b2=self.b2, w3=self.w3, b3=self.b3)
            return True
        except:
            return False

    def load(self, filepath):
        if not filepath.exists():
            return False
        try:
            data = np.load(filepath)
            if data['w1'].shape != (self.state_size, 64):
                log(f"âš ï¸ Network shape mismatch. Resetting.", "warn")
                return False
            self.w1, self.b1 = data['w1'], data['b1']
            self.w2, self.b2 = data['w2'], data['b2']
            self.w3, self.b3 = data['w3'], data['b3']
            return True
        except Exception as e:
            log(f"âš ï¸ Network load error: {e}", "warn")
            return False

# ========================================================================
# RL AGENT WITH PIPELINE INTEGRATION
# ========================================================================

class DeepRLAgent:
    """Deep Q-Learning Agent with Experience Replay and Pipeline Integration"""
    def __init__(self):
        self.q_network = SimpleQNetwork()
        self.target_network = SimpleQNetwork()
        self.memory = deque(maxlen=MEMORY_SIZE)
        self.epsilon = EPSILON_START
        self.learning_count = 0
        self.trade_count = 0
        self.stats = {
            'total_updates': 0, 'total_trades': 0, 'profitable_trades': 0,
            'total_pnl': 0.0, 'win_rate': 0.0, 'avg_reward': 0.0,
            'epsilon_history': [], 'q_value_history': [],
            'pipeline_trades_learned': 0, 'last_pipeline_sync': None
        }
        self.load_state()
        log(f"ğŸ§  RL Agent initialized: {len(self.memory)} experiences", "brain")

    def select_action(self, state, force_greedy=False):
        if not force_greedy and random.random() < self.epsilon:
            return random.randint(0, ACTION_SPACE - 1)
        else:
            return int(np.argmax(self.q_network.predict(state)))

    def remember(self, experience: Experience):
        self.memory.append(experience)

    def learn(self):
        if len(self.memory) < MIN_REPLAY_SIZE:
            return

        batch = random.sample(self.memory, min(BATCH_SIZE, len(self.memory)))
        states = np.array([exp.state for exp in batch])
        actions = np.array([exp.action for exp in batch])
        rewards = np.array([exp.reward for exp in batch])
        next_states = np.array([exp.next_state for exp in batch])
        dones = np.array([exp.done for exp in batch])

        current_q_batch = np.array([self.q_network.forward(s) for s in states])
        next_q_batch = np.array([self.target_network.forward(s) for s in next_states])

        targets = current_q_batch.copy()
        for i in range(len(batch)):
            if dones[i]:
                targets[i][actions[i]] = rewards[i]
            else:
                targets[i][actions[i]] = rewards[i] + GAMMA * np.max(next_q_batch[i])

        self.q_network.update(states, targets, LEARNING_RATE)
        self.learning_count += 1
        self.stats['total_updates'] += 1

        avg_q = np.mean([np.max(q) for q in current_q_batch])
        self.stats['q_value_history'].append(float(avg_q))

        self.epsilon = max(EPSILON_MIN, self.epsilon * EPSILON_DECAY)
        self.stats['epsilon_history'].append(self.epsilon)

        if self.learning_count % TARGET_UPDATE_FREQUENCY == 0:
            self.target_network = self.q_network.clone()
            log(f"ğŸ¯ Target network updated (#{self.learning_count})", "brain")

    def record_trade(self, trade_outcome: TradeOutcome):
        self.trade_count += 1
        self.stats['total_trades'] += 1
        self.stats['total_pnl'] += trade_outcome.pnl

        if trade_outcome.pnl > 0:
            self.stats['profitable_trades'] += 1

        self.stats['win_rate'] = self.stats['profitable_trades'] / self.stats['total_trades']

        reward = self.calculate_reward(trade_outcome)
        self.stats['avg_reward'] = (
            (self.stats['avg_reward'] * (self.stats['total_trades'] - 1) + reward) /
            self.stats['total_trades']
        )

        action = 0 if trade_outcome.action == 'BUY' else 1
        experience = Experience(
            state=trade_outcome.state_at_entry, action=action, reward=reward,
            next_state=trade_outcome.state_at_entry, done=True,
            metadata={'pair': trade_outcome.pair, 'pnl': trade_outcome.pnl,
                     'hit_tp': trade_outcome.hit_tp, 'duration': trade_outcome.duration}
        )

        self.remember(experience)

        if len(self.memory) >= MIN_REPLAY_SIZE:
            self.learn()

    def learn_from_pipeline_trades(self, pipeline_db: PipelineDatabase, data: Dict):
        """ğŸ†• Learn from completed trades in Ultra-Persistent Pipeline database"""

        # Load last sync timestamp
        sync_data = safe_json_load(PIPELINE_SYNC_FILE, default={'last_sync': None, 'trades_learned': 0})
        last_sync = sync_data.get('last_sync')

        log(f"\nğŸ’¾ Syncing with Pipeline Database...", "database")

        # Fetch new completed trades
        completed_trades = pipeline_db.get_completed_trades(since_timestamp=last_sync)

        if not completed_trades:
            log("â„¹ï¸ No new pipeline trades to learn from", "info")
            return 0

        log(f"ğŸ“Š Found {len(completed_trades)} new pipeline trades", "database")

        trades_learned = 0

        for trade_data in completed_trades:
            try:
                # Unpack trade data
                (pair, timeframe, model_used, entry_price, exit_price, sl_price, tp_price,
                 prediction, hit_tp, pnl, pnl_percent, duration_hours, created_at, evaluated_at) = trade_data

                # Convert pair format (EUR_USD -> EUR/USD)
                if '/' not in pair:
                    pair = f"{pair[:3]}/{pair[4:]}"

                if pair not in data or '1h' not in data[pair] or '1d' not in data[pair]:
                    continue

                # Create approximate state vector (using current data as proxy)
                state = create_state_vector(data[pair]['1h'], data[pair]['1d'], pair)

                # Determine action (1=long/buy, 0=short/sell from prediction)
                action = 1 if prediction == 1 else 0
                action_str = 'BUY' if prediction == 1 else 'SELL'

                # Create trade outcome
                outcome = TradeOutcome(
                    pair=pair,
                    action=action_str,
                    entry_price=float(entry_price),
                    exit_price=float(exit_price),
                    sl=float(sl_price),
                    tp=float(tp_price),
                    position_size=1.0,  # Normalized
                    pnl=float(pnl),
                    duration=float(duration_hours),
                    hit_tp=bool(hit_tp),
                    timestamp_entry=created_at,
                    timestamp_exit=evaluated_at,
                    state_at_entry=state,
                    confidence=70.0,  # Default confidence for pipeline trades
                    regime=f'PIPELINE_{model_used}',
                    session=timeframe
                )

                # Learn from this trade
                self.record_trade(outcome)
                trades_learned += 1

            except Exception as e:
                log(f"âš ï¸ Failed to process pipeline trade: {e}", "warn")
                continue

        # Update sync timestamp
        if completed_trades:
            latest_timestamp = max(trade[13] for trade in completed_trades)  # evaluated_at
            sync_data['last_sync'] = latest_timestamp
            sync_data['trades_learned'] = sync_data.get('trades_learned', 0) + trades_learned
            safe_json_save(PIPELINE_SYNC_FILE, sync_data)

        self.stats['pipeline_trades_learned'] = sync_data.get('trades_learned', 0)
        self.stats['last_pipeline_sync'] = datetime.now(timezone.utc).isoformat()

        log(f"âœ… Learned from {trades_learned} pipeline trades (Total: {self.stats['pipeline_trades_learned']})", "success")
        return trades_learned

    def calculate_reward(self, trade: TradeOutcome) -> float:
        reward = 0.0

        if trade.pnl > 0:
            reward += trade.pnl * PROFIT_REWARD_SCALE
        else:
            reward += trade.pnl * LOSS_PENALTY_SCALE

        if trade.hit_tp:
            reward += WIN_STREAK_BONUS
        else:
            reward -= LOSS_STREAK_PENALTY

        if trade.pnl > 0 and trade.duration < 24:
            reward *= 1.2
        elif trade.pnl < 0 and trade.duration > 48:
            reward *= 1.5

        risk_adjusted = trade.pnl / (abs(trade.entry_price - trade.sl) + EPS)
        reward += risk_adjusted * SHARPE_REWARD_SCALE

        return float(reward)

    def save_state(self):
        try:
            self.q_network.save(RL_Q_NETWORK_FILE)
            self.target_network.save(RL_TARGET_NETWORK_FILE)
            safe_pickle_save(RL_MEMORY_FILE, list(self.memory))
            safe_json_save(RL_LEARNING_STATS_FILE, self.stats)
            log(f"ğŸ’¾ RL state saved: {len(self.memory)} exp, {self.stats['total_trades']} trades", "success")
        except Exception as e:
            log(f"âš ï¸ Failed to save RL state: {e}", "warn")

    def load_state(self):
        try:
            q_loaded = self.q_network.load(RL_Q_NETWORK_FILE)
            t_loaded = self.target_network.load(RL_TARGET_NETWORK_FILE)
            if q_loaded and t_loaded:
                log("âœ… Loaded Q-networks", "success")
            else:
                log("ğŸ”„ New Q-networks initialized", "info")

            loaded_mem = safe_pickle_load(RL_MEMORY_FILE, default=[])
            if loaded_mem:
                self.memory.extend(loaded_mem)
                log(f"âœ… Loaded {len(loaded_mem)} experiences", "success")

            loaded_stats = safe_json_load(RL_LEARNING_STATS_FILE)
            if loaded_stats:
                self.stats = loaded_stats
                if self.stats.get('epsilon_history'):
                    self.epsilon = self.stats['epsilon_history'][-1]
                log(f"âœ… Loaded stats: {self.stats['total_trades']} trades", "success")
        except Exception as e:
            log(f"âš ï¸ Could not load RL state: {e}", "warn")

# ========================================================================
# TRADING ENVIRONMENT
# ========================================================================

class TradingEnvironment:
    """Environment for executing and tracking trades"""
    def __init__(self):
        self.active_trades = {}
        self.trade_history = safe_json_load(TRADE_HISTORY_FILE, default=[])
        if self.trade_history:
            log(f"âœ… Loaded {len(self.trade_history)} historical trades", "success")

    def save_trade_history(self):
        safe_json_save(TRADE_HISTORY_FILE, self.trade_history)

    def execute_trade(self, pair: str, action: str, price: float, sl: float,
                     tp: float, size: float, state: np.ndarray, metadata: Dict) -> str:
        trade_id = f"{pair}_{datetime.now():%Y%m%d_%H%M%S}"
        self.active_trades[trade_id] = {
            'pair': pair, 'action': action, 'entry_price': price, 'sl': sl, 'tp': tp,
            'size': size, 'entry_time': datetime.now(timezone.utc).isoformat(),
            'state_at_entry': state.tolist(), 'metadata': metadata
        }
        log(f"ğŸ’° Trade: {trade_id} - {action} {pair} @ {price:.5f}", "money")
        return trade_id

    def check_exits(self, current_prices: Dict[str, float]) -> List[TradeOutcome]:
        completed_trades = []

        for trade_id, trade in list(self.active_trades.items()):
            pair = trade['pair']
            if pair not in current_prices:
                continue

            current_price = current_prices[pair]
            hit_tp = hit_sl = False

            if trade['action'] == 'BUY':
                hit_tp = current_price >= trade['tp']
                hit_sl = current_price <= trade['sl']
            else:
                hit_tp = current_price <= trade['tp']
                hit_sl = current_price >= trade['sl']

            if hit_tp or hit_sl:
                exit_price = trade['tp'] if hit_tp else trade['sl']

                if trade['action'] == 'BUY':
                    pnl = (exit_price - trade['entry_price']) * trade['size']
                else:
                    pnl = (trade['entry_price'] - exit_price) * trade['size']

                pnl -= exit_price * 0.0003 + exit_price * trade['size'] * 0.0005

                entry_time = datetime.fromisoformat(trade['entry_time'])
                exit_time = datetime.now(timezone.utc)
                duration = (exit_time - entry_time).total_seconds() / 3600.0

                outcome = TradeOutcome(
                    pair=pair, action=trade['action'], entry_price=trade['entry_price'],
                    exit_price=exit_price, sl=trade['sl'], tp=trade['tp'],
                    position_size=trade['size'], pnl=pnl, duration=duration,
                    hit_tp=hit_tp, timestamp_entry=trade['entry_time'],
                    timestamp_exit=exit_time.isoformat(),
                    state_at_entry=np.array(trade['state_at_entry']),
                    confidence=trade['metadata'].get('confidence', 0),
                    regime=trade['metadata'].get('regime', 'UNKNOWN'),
                    session=trade['metadata'].get('session', 'UNKNOWN')
                )

                completed_trades.append(outcome)

                self.trade_history.append({
                    'trade_id': trade_id, 'pair': pair, 'action': trade['action'],
                    'entry': trade['entry_price'], 'exit': exit_price, 'pnl': pnl,
                    'result': 'WIN' if hit_tp else 'LOSS', 'duration_hours': duration,
                    'timestamp': exit_time.isoformat()
                })

                del self.active_trades[trade_id]
                log(f"âœ… Closed: {trade_id} - {'WIN' if hit_tp else 'LOSS'} | ${pnl:.2f}",
                    "success" if pnl > 0 else "warn")

        if completed_trades:
            self.save_trade_history()

        return completed_trades

# ========================================================================
# WEEKEND BACKTEST ENGINE
# ========================================================================

def run_weekend_backtest(data: Dict, agent: DeepRLAgent, confidence_system: ImprovedRLConfidence):
    """Run backtest on historical data for weekend learning"""
    log("\nğŸ“ WEEKEND LEARNING MODE: Running backtest...", "brain")

    trades_learned = 0

    for pair in PAIRS:
        if pair not in data or '1h' not in data[pair] or '1d' not in data[pair]:
            continue

        df_1h = data[pair]['1h']
        df_1d = data[pair]['1d']

        for i in range(max(60, len(df_1h) - WEEKEND_BACKTEST_STEPS), len(df_1h) - 5):
            try:
                state = create_state_vector(df_1h.iloc[:i], df_1d.iloc[:max(0, i-24)], pair)
                q_values = agent.q_network.predict(state)

                should_trade, confidence, conf_metrics = confidence_system.should_trade(q_values, agent.epsilon)

                if not should_trade:
                    continue

                best_action = np.argmax(q_values)
                action_map = {0: 'BUY', 1: 'SELL', 2: 'HOLD'}
                direction = action_map[best_action]

                if direction == 'HOLD':
                    continue

                entry_price = df_1h['close'].iloc[i]
                atr = df_1h['atr'].iloc[i]

                if direction == 'BUY':
                    sl = entry_price - (atr * ATR_SL_MULTIPLIER)
                    tp = entry_price + (atr * ATR_TP_MULTIPLIER)
                else:
                    sl = entry_price + (atr * ATR_SL_MULTIPLIER)
                    tp = entry_price - (atr * ATR_TP_MULTIPLIER)

                hit_tp = False
                hit_sl = False
                exit_idx = i + 1

                for j in range(i + 1, min(i + 50, len(df_1h))):
                    current_price = df_1h['close'].iloc[j]

                    if direction == 'BUY':
                        if current_price >= tp:
                            hit_tp = True
                            exit_idx = j
                            break
                        elif current_price <= sl:
                            hit_sl = True
                            exit_idx = j
                            break
                    else:
                        if current_price <= tp:
                            hit_tp = True
                            exit_idx = j
                            break
                        elif current_price >= sl:
                            hit_sl = True
                            exit_idx = j
                            break

                if not hit_tp and not hit_sl:
                    exit_idx = min(i + 50, len(df_1h) - 1)
                    exit_price = df_1h['close'].iloc[exit_idx]
                else:
                    exit_price = tp if hit_tp else sl

                base_size = (BASE_CAPITAL * MAX_RISK_PER_TRADE) / (abs(entry_price - sl) + EPS)
                position_size = confidence_system.calculate_position_size(base_size, confidence, conf_metrics)
                position_size = min(position_size, MAX_TRADE_CAP / entry_price)

                if direction == 'BUY':
                    pnl = (exit_price - entry_price) * position_size
                else:
                    pnl = (entry_price - exit_price) * position_size

                pnl -= exit_price * 0.0003 + exit_price * position_size * 0.0005

                duration = (exit_idx - i) * 1.0

                outcome = TradeOutcome(
                    pair=pair, action=direction, entry_price=entry_price,
                    exit_price=exit_price, sl=sl, tp=tp, position_size=position_size,
                    pnl=pnl, duration=duration, hit_tp=hit_tp,
                    timestamp_entry=str(df_1h.index[i]),
                    timestamp_exit=str(df_1h.index[exit_idx]),
                    state_at_entry=state, confidence=confidence,
                    regime='BACKTEST', session='WEEKEND'
                )

                agent.record_trade(outcome)
                trades_learned += 1

            except Exception as e:
                continue

    log(f"âœ… Weekend backtest complete: {trades_learned} trades learned", "success")
    return trades_learned

# ========================================================================
# CORE FUNCTIONS
# ========================================================================

def fetch_price(pair, timeout=10):
    if not BROWSERLESS_TOKEN:
        return None
    try:
        fc, tc = pair.split("/")
        url = f"https://production-sfo.browserless.io/content?token={BROWSERLESS_TOKEN}"
        r = requests.post(
            url,
            json={"url": f"https://www.x-rates.com/calculator/?from={fc}&to={tc}&amount=1"},
            timeout=timeout
        )
        m = re.search(r'ccOutputRslt[^>]*>([\d,.]+)', r.text)
        return float(m.group(1).replace(",", "")) if m else None
    except:
        return None

def ensure_atr(df):
    if "atr" in df.columns and df["atr"].median() > MIN_ATR:
        return df.assign(atr=df["atr"].fillna(MIN_ATR).clip(lower=MIN_ATR))

    high, low, close = df["high"].values, df["low"].values, df["close"].values
    tr = np.maximum.reduce([
        high - low, np.abs(high - np.roll(close, 1)), np.abs(low - np.roll(close, 1))
    ])
    tr[0] = high[0] - low[0] if len(tr) > 0 else MIN_ATR

    df["atr"] = pd.Series(tr, index=df.index).rolling(
        ATR_PERIOD, min_periods=1
    ).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)
    return df

def update_pickle_data():
    log("ğŸ”„ Updating pickle data...", "info")
    updated_count = 0

    for pair in PAIRS:
        latest_price = fetch_price(pair)
        if not latest_price or latest_price <= 0:
            continue

        pair_key = pair.replace("/", "_")
        for pkl_file in PICKLE_FOLDER.glob(f"{pair_key}*.pkl"):
            if any(x in pkl_file.name for x in ['_model', '_sgd', '_rf', 'indicator_cache', '.bak']):
                continue

            try:
                try:
                    df = pd.read_pickle(pkl_file, compression='gzip')
                except:
                    df = pd.read_pickle(pkl_file, compression=None)

                if not isinstance(df, pd.DataFrame) or len(df) < 10:
                    continue
                if not all(c in df.columns for c in ['open', 'high', 'low', 'close']):
                    continue

                last_time = df.index[-1]
                new_time = datetime.now().replace(second=0, microsecond=0)

                if new_time > last_time:
                    new_row = pd.DataFrame({
                        'open': [float(latest_price)], 'high': [float(latest_price)],
                        'low': [float(latest_price)], 'close': [float(latest_price)],
                        'volume': [0]
                    }, index=[new_time])

                    df = pd.concat([df, new_row]).tail(5000).ffill().bfill()
                    df = ensure_atr(df)
                    df.to_pickle(pkl_file, compression='gzip')
                    updated_count += 1
            except:
                pass

    log(f"âœ… Updated {updated_count} files", "success")
    return updated_count

def load_data(folder):
    log(f"ğŸ“‚ Loading data from: {folder}", "info")
    if not folder.exists():
        return {}

    all_pkl = [p for p in folder.glob("*.pkl") if not any(s in p.name for s in
               ['_sgd_model','_rf_model','indicator_cache','ultra_','alpha_','_model.pkl','.bak'])]

    pair_files = defaultdict(list)
    currencies = ["EUR","GBP","USD","AUD","NZD","CAD","CHF","JPY"]

    for pkl in all_pkl:
        parts = pkl.stem.split('_')
        if len(parts) >= 2 and parts[0] in currencies and parts[1] in currencies:
            pair_files[f"{parts[0]}_{parts[1]}"].append(pkl)

    combined = {}
    for pk, files in pair_files.items():
        pair = f"{pk[:3]}/{pk[4:]}"
        if pair not in PAIRS:
            continue

        pair_data = {}
        for pkl in files:
            try:
                try:
                    df = pd.read_pickle(pkl, compression='gzip')
                except:
                    df = pd.read_pickle(pkl, compression=None)

                if not isinstance(df, pd.DataFrame) or len(df) < 50:
                    continue
                if not all(c in df.columns for c in ['open','high','low','close']):
                    continue

                df = df.ffill().bfill().dropna(subset=['open', 'high', 'low', 'close'])
                df.index = pd.to_datetime(df.index, errors="coerce")
                if df.index.tz:
                    df.index = df.index.tz_localize(None)
                df = df[df.index.notna()]

                tf = "1d" if "1d" in pkl.stem or "daily" in pkl.stem else "1h"
                if tf not in ["1d", "1h"]:
                    continue

                df = ensure_atr(df)
                pair_data[tf] = df
                log(f"âœ… {pair} [{tf}]: {len(df)} rows", "success")
            except:
                pass

        if pair_data:
            combined[pair] = pair_data

    log(f"âœ… Loaded {len(combined)} pairs", "success")
    return combined

def send_rl_email(signals, iteration, rl_stats, trade_history, mode, pipeline_stats):
    if not GMAIL_APP_PASSWORD:
        log("âš ï¸ Email skipped: No password", "warn")
        return

    try:
        msg = MIMEMultipart('alternative')
        msg['Subject'] = f"ğŸ§  BEACON v17 [{mode}] - Iter #{iteration}"
        msg['From'] = GMAIL_USER
        msg['To'] = GMAIL_USER

        active_signals = sum(1 for s in signals.values() if s.get('direction') != 'HOLD')
        recent_trades = trade_history[-10:]

        epsilon = rl_stats.get('epsilon_history', [EPSILON_START])[-1] if rl_stats.get('epsilon_history') else EPSILON_START

        mode_badge = "WEEKEND LEARNING" if mode == "WEEKEND_LEARNING" else "LIVE TRADING"
        mode_color = "#f59e0b" if mode == "WEEKEND_LEARNING" else "#10b981"

        pipeline_trades_learned = rl_stats.get('pipeline_trades_learned', 0)
        pipeline_pnl = pipeline_stats.get('total_pnl', 0.0)
        pipeline_wr = pipeline_stats.get('win_rate', 0.0)

        html = f"""<!DOCTYPE html><html><head><style>
body{{font-family:-apple-system,sans-serif;background:#0f172a;margin:0;padding:20px}}
.container{{max-width:1000px;margin:0 auto;background:white;border-radius:12px;box-shadow:0 10px 40px rgba(0,0,0,0.4)}}
.header{{background:linear-gradient(135deg,#7c3aed,#4c1d95);color:white;padding:50px;text-align:center}}
.header h1{{margin:0;font-size:38px;font-weight:900}}
.mode-badge{{background:{mode_color};padding:12px 24px;border-radius:30px;margin-top:18px;font-weight:800}}
.stats{{background:#fef3c7;padding:25px;margin:25px;border-radius:10px}}
.pipeline-stats{{background:#dbeafe;padding:25px;margin:25px;border-radius:10px}}
.stat-grid{{display:grid;grid-template-columns:repeat(auto-fit,minmax(150px,1fr));gap:15px}}
.stat-item{{background:white;padding:15px;border-radius:8px;text-align:center}}
.stat-value{{font-size:28px;font-weight:900;color:#7c3aed}}
.stat-label{{font-size:12px;color:#6b7280;margin-top:5px}}
.section-title{{font-size:20px;font-weight:800;color:#1f2937;margin-bottom:15px}}
</style></head><body><div class="container">
<div class="header">
<h1>ğŸ§  TRADE BEACON v17.0</h1>
<div class="mode-badge">{mode_badge}</div>
<p style="margin:20px 0 0">Iteration #{iteration} | {datetime.now():%Y-%m-%d %H:%M UTC}</p>
</div>
<div class="stats">
<div class="section-title">ğŸ§  RL Agent Stats</div>
<div class="stat-grid">
<div class="stat-item"><div class="stat-value">{rl_stats.get('total_trades',0)}</div><div class="stat-label">TRADES</div></div>
<div class="stat-item"><div class="stat-value">{rl_stats.get('win_rate',0)*100:.1f}%</div><div class="stat-label">WIN RATE</div></div>
<div class="stat-item"><div class="stat-value">${rl_stats.get('total_pnl',0):.2f}</div><div class="stat-label">TOTAL P&L</div></div>
<div class="stat-item"><div class="stat-value">{epsilon:.3f}</div><div class="stat-label">EPSILON</div></div>
<div class="stat-item"><div class="stat-value">{active_signals}</div><div class="stat-label">SIGNALS</div></div>
<div class="stat-item"><div class="stat-value">{len(recent_trades)}</div><div class="stat-label">RECENT</div></div>
</div></div>
<div class="pipeline-stats">
<div class="section-title">ğŸ’¾ Pipeline Integration Stats</div>
<div class="stat-grid">
<div class="stat-item"><div class="stat-value">{pipeline_trades_learned}</div><div class="stat-label">PIPELINE TRADES</div></div>
<div class="stat-item"><div class="stat-value">{pipeline_stats.get('total_trades',0)}</div><div class="stat-label">TOTAL PIPELINE</div></div>
<div class="stat-item"><div class="stat-value">{pipeline_wr:.1f}%</div><div class="stat-label">PIPELINE WR</div></div>
<div class="stat-item"><div class="stat-value">${pipeline_pnl:.2f}</div><div class="stat-label">PIPELINE P&L</div></div>
</div></div></div></body></html>"""

        msg.attach(MIMEText(html, 'html'))

        with smtplib.SMTP_SSL('smtp.gmail.com', 465, timeout=30) as srv:
            srv.login(GMAIL_USER, GMAIL_APP_PASSWORD)
            srv.send_message(msg)

        log(f"âœ… Email sent", "success")
    except Exception as e:
        log(f"âŒ Email failed: {e}", "error")

def push_git(files, msg):
    if IN_GHA or not FOREX_PAT:
        return False

    try:
        REPO_URL = f"https://{GITHUB_USERNAME}:{FOREX_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git"
        repo_path = SAVE_FOLDER if (SAVE_FOLDER / ".git").exists() else BASE_FOLDER

        if not (repo_path / ".git").exists():
            subprocess.run(["git", "clone", REPO_URL, str(repo_path)], capture_output=True, timeout=60, check=True)

        os.chdir(repo_path)

        for f in files:
            if (repo_path / f).exists():
                subprocess.run(["git", "add", str(f)], check=False)

        subprocess.run(["git", "commit", "-m", msg], capture_output=True, check=False)
        subprocess.run(["git", "pull", "--rebase", "origin", "main"], capture_output=True, check=False)
        result = subprocess.run(["git", "push", "origin", "main"], capture_output=True, timeout=30)

        return result.returncode == 0
    except:
        return False
    finally:
        try:
            os.chdir(SAVE_FOLDER)
        except:
            pass

# ========================================================================
# MAIN
# ========================================================================

def main():
    log("=" * 70, "rocket")
    log("ğŸ§  TRADE BEACON v17.0 - INTEGRATED LEARNING", "brain")
    log("=" * 70, "rocket")

    mode = get_weekend_mode()
    log(f"ğŸ“… Mode: {mode}", "info")

    iteration = increment_iteration()

    agent = DeepRLAgent()
    env = TradingEnvironment()
    confidence_system = ImprovedRLConfidence()

    # ğŸ†• Connect to Pipeline Database
    pipeline_db = PipelineDatabase()

    try:
        log(f"\nğŸ“Š Iteration #{iteration} | {ENV_NAME} | {mode}", "info")

        # Only update prices on weekdays
        if mode == "LIVE_TRADING":
            update_pickle_data()

        data = load_data(PICKLE_FOLDER)
        if not data:
            raise ValueError("âŒ No data loaded")

        # ğŸ†• LEARN FROM PIPELINE DATABASE (always, regardless of weekend)
        if pipeline_db.conn:
            pipeline_trades_learned = agent.learn_from_pipeline_trades(pipeline_db, data)
            pipeline_stats = pipeline_db.get_pipeline_stats()

            log(f"\nğŸ’¾ Pipeline Stats:", "database")
            log(f"  Total Trades: {pipeline_stats.get('total_trades', 0)}", "database")
            log(f"  Win Rate: {pipeline_stats.get('win_rate', 0):.1f}%", "database")
            log(f"  Total P&L: ${pipeline_stats.get('total_pnl', 0.0):.5f}", "database")
            log(f"  Trades Learned by RL: {agent.stats.get('pipeline_trades_learned', 0)}", "database")
        else:
            pipeline_trades_learned = 0
            pipeline_stats = {}
            log("âš ï¸ Pipeline database not available", "warn")

        # WEEKEND LEARNING MODE
        if mode == "WEEKEND_LEARNING":
            backtest_trades_learned = run_weekend_backtest(data, agent, confidence_system)
            log(f"ğŸ“ Weekend learning: {backtest_trades_learned} backtest + {pipeline_trades_learned} pipeline = {backtest_trades_learned + pipeline_trades_learned} total trades", "brain")

        # Get current prices
        log("\nğŸ’¹ Fetching prices...", "info")
        current_prices = {}
        for pair in PAIRS:
            if mode == "WEEKEND_LEARNING":
                if pair in data and '1h' in data[pair]:
                    price = data[pair]['1h'].iloc[-1]['close']
                    current_prices[pair] = price
            else:
                price = fetch_price(pair)
                if not price and pair in data and '1h' in data[pair]:
                    price = data[pair]['1h'].iloc[-1]['close']
                if price:
                    current_prices[pair] = price

        # Check for trade exits
        log("\nğŸ” Checking trades...", "info")
        completed_trades = env.check_exits(current_prices)

        if completed_trades:
            log(f"\nğŸ“ Learning from {len(completed_trades)} completed trades...", "brain")
            for trade_outcome in completed_trades:
                agent.record_trade(trade_outcome)

        # Generate signals
        log("\nğŸ§  Generating signals...", "brain")
        signals = {}

        for pair in PAIRS:
            if pair not in data or '1h' not in data[pair] or '1d' not in data[pair]:
                signals[pair] = {'direction': 'HOLD', 'last_price': current_prices.get(pair, 0)}
                continue

            state = create_state_vector(data[pair]['1h'], data[pair]['1d'], pair)
            q_values = agent.q_network.predict(state)

            should_trade, confidence, conf_metrics = confidence_system.should_trade(q_values, agent.epsilon)

            best_action = np.argmax(q_values)
            action_map = {0: 'BUY', 1: 'SELL', 2: 'HOLD'}
            direction = action_map[best_action]

            if not should_trade:
                direction = 'HOLD'

            price = current_prices.get(pair, 0)
            atr = data[pair]['1h']['atr'].iloc[-1]

            if direction == 'BUY':
                sl = price - (atr * ATR_SL_MULTIPLIER)
                tp = price + (atr * ATR_TP_MULTIPLIER)
            elif direction == 'SELL':
                sl = price + (atr * ATR_SL_MULTIPLIER)
                tp = price - (atr * ATR_TP_MULTIPLIER)
            else:
                sl = tp = price

            signals[pair] = {
                'direction': direction, 'last_price': price, 'SL': float(sl), 'TP': float(tp),
                'confidence': confidence, 'threshold': conf_metrics['adaptive_threshold'],
                'timestamp': datetime.now(timezone.utc).isoformat()
            }

            log(f"  {pair}: Q={q_values[best_action]:.3f}, Conf={confidence:.1f}%, "
                f"Thresh={conf_metrics['adaptive_threshold']:.1f}%, {direction}", "brain")

            # Execute trades only on weekdays (LIVE_TRADING mode)
            if direction != 'HOLD' and len(env.active_trades) < MAX_POSITIONS and mode == "LIVE_TRADING":
                base_size = (BASE_CAPITAL * MAX_RISK_PER_TRADE) / (abs(price - sl) + EPS)
                position_size = confidence_system.calculate_position_size(base_size, confidence, conf_metrics)
                position_size = min(position_size, MAX_TRADE_CAP / price)

                env.execute_trade(pair, direction, price, sl, tp, position_size, state,
                                {'confidence': confidence, 'regime': 'RL', 'session': 'RL'})

        # Save results
        log("\nğŸ’¾ Saving...", "info")

        output = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'iteration': iteration,
            'version': 'v17.0-integrated',
            'mode': mode,
            'signals': signals,
            'rl_stats': agent.stats,
            'active_trades': len(env.active_trades),
            'pipeline_stats': pipeline_stats,
            'pipeline_trades_learned': pipeline_trades_learned
        }

        safe_json_save(OMEGA_SIGNALS_FILE, output)
        agent.save_state()

        # Only send emails on weekdays
        if mode == "LIVE_TRADING":
            send_rl_email(signals, iteration, agent.stats, env.trade_history, mode, pipeline_stats)
        else:
            log("ğŸ“§ Email skipped (weekend mode)", "info")

        # Push to Git
        files = [
            f"outputs/{OMEGA_SIGNALS_FILE.name}",
            f"omega_state/{OMEGA_ITERATION_FILE.name}",
            f"rl_memory/{RL_MEMORY_FILE.name}",
            f"rl_memory/{RL_LEARNING_STATS_FILE.name}",
            f"rl_memory/{TRADE_HISTORY_FILE.name}",
            f"rl_memory/{PIPELINE_SYNC_FILE.name}",
            f"rl_models/q_network.npz",
            f"rl_models/target_network.npz"
        ]

        commit_msg = f"ğŸ§  v17 #{iteration} [{mode}] WR={agent.stats['win_rate']*100:.1f}% | Pipeline: {pipeline_trades_learned} trades"
        push_git(files, commit_msg)

        # Summary
        log("\n" + "=" * 70, "success")
        log("âœ… CYCLE COMPLETE", "success")
        log("=" * 70, "success")
        log(f"Iteration: #{iteration} ({ENV_NAME})", "info")
        log(f"Mode: {mode}", "info")
        log(f"RL Trades Learned: {agent.stats['total_trades']}", "brain")
        log(f"Pipeline Trades Learned: {agent.stats.get('pipeline_trades_learned', 0)}", "database")
        log(f"Win Rate: {agent.stats['win_rate']*100:.1f}%", "info")
        log(f"Total P&L: ${agent.stats['total_pnl']:.2f}", "money")
        log(f"Active Trades: {len(env.active_trades)}", "info")
        log(f"Epsilon: {agent.epsilon:.3f}", "info")

        if pipeline_stats:
            log(f"Pipeline DB Trades: {pipeline_stats.get('total_trades', 0)}", "database")
            log(f"Pipeline Win Rate: {pipeline_stats.get('win_rate', 0):.1f}%", "database")

    except Exception as e:
        log(f"\nâŒ Error: {e}", "error")
        logging.exception("Fatal error")
        raise

    finally:
        if pipeline_db.conn:
            pipeline_db.close()
        log(f"\nğŸ§  Cycle complete (Iteration #{iteration})", "brain")

if __name__ == "__main__":
    main()
------------------

----- stdout -----
======================================================================
ğŸ§  TRADE BEACON v17.0 - INTEGRATED LEARNING
======================================================================
ğŸ’° ğŸ’° LIVE TRADING MODE ACTIVE
ğŸš€ ======================================================================
ğŸ§  ğŸ§  TRADE BEACON v17.0 - INTEGRATED LEARNING
ğŸš€ ======================================================================
â„¹ï¸ ğŸ“… Mode: WEEKEND_LEARNING
âœ… âœ… Loaded Q-networks
âœ… âœ… Loaded 189 experiences
âœ… âœ… Loaded stats: 189 trades
ğŸ§  ğŸ§  RL Agent initialized: 189 experiences
ğŸ’¾ âœ… Connected to pipeline database
â„¹ï¸ 
ğŸ“Š Iteration #5 | GitHub Actions | WEEKEND_LEARNING
â„¹ï¸ ğŸ“‚ Loading data from: /home/runner/work/forex-ai-models/forex-ai-models/data/processed
âœ… âœ… Loaded 0 pairs
âŒ 
âŒ Error: âŒ No data loaded
ğŸ§  
ğŸ§  Cycle complete (Iteration #5)
------------------

[31m---------------------------------------------------------------------------[39m
[31mValueError[39m                                Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[2][39m[32m, line 1555[39m
[32m   1552[39m         log([33mf[39m[33m"[39m[38;5;130;01m\n[39;00m[33mğŸ§  Cycle complete (Iteration #[39m[38;5;132;01m{[39;00miteration[38;5;132;01m}[39;00m[33m)[39m[33m"[39m, [33m"[39m[33mbrain[39m[33m"[39m)
[32m   1554[39m [38;5;28;01mif[39;00m [34m__name__[39m == [33m"[39m[33m__main__[39m[33m"[39m:
[32m-> [39m[32m1555[39m     [43mmain[49m[43m([49m[43m)[49m

[36mCell[39m[36m [39m[32mIn[2][39m[32m, line 1391[39m, in [36mmain[39m[34m()[39m
[32m   1389[39m data = load_data(PICKLE_FOLDER)
[32m   1390[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m data:
[32m-> [39m[32m1391[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([33m"[39m[33mâŒ No data loaded[39m[33m"[39m)
[32m   1393[39m [38;5;66;03m# ğŸ†• LEARN FROM PIPELINE DATABASE (always, regardless of weekend)[39;00m
[32m   1394[39m [38;5;28;01mif[39;00m pipeline_db.conn:

[31mValueError[39m: âŒ No data loaded

