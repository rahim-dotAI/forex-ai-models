{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac07e2c7",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [12]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b5125d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:15:25.295772Z",
     "iopub.status.busy": "2025-10-05T15:15:25.295610Z",
     "iopub.status.idle": "2025-10-05T15:15:25.301545Z",
     "shell.execute_reply": "2025-10-05T15:15:25.301040Z"
    },
    "id": "rTpnXB4JwTyx",
    "papermill": {
     "duration": 0.010749,
     "end_time": "2025-10-05T15:15:25.302404",
     "exception": false,
     "start_time": "2025-10-05T15:15:25.291655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/runner/work/forex-ai-models/forex-ai-models/forex_ai_outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "SAVE_DIR = \"forex_ai_outputs\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.chdir(SAVE_DIR)\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae288a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:15:25.309692Z",
     "iopub.status.busy": "2025-10-05T15:15:25.309538Z",
     "iopub.status.idle": "2025-10-05T15:15:36.559813Z",
     "shell.execute_reply": "2025-10-05T15:15:36.559056Z"
    },
    "id": "N8Oonb6gP_ho",
    "papermill": {
     "duration": 11.254569,
     "end_time": "2025-10-05T15:15:36.560939",
     "exception": false,
     "start_time": "2025-10-05T15:15:25.306370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mplfinance\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading mplfinance-0.12.10b0-py3-none-any.whl.metadata (19 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting firebase-admin\r\n",
      "  Downloading firebase_admin-7.1.0-py3-none-any.whl.metadata (1.7 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dropbox\r\n",
      "  Downloading dropbox-12.0.2-py3-none-any.whl.metadata (4.3 kB)\r\n",
      "Requirement already satisfied: requests in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (2.32.5)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (4.14.2)\r\n",
      "Requirement already satisfied: pandas in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: numpy in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: ta in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (0.11.0)\r\n",
      "Requirement already satisfied: yfinance in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (0.2.66)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyppeteer\r\n",
      "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: nest_asyncio in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (1.6.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\r\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (1.5.2)\r\n",
      "Requirement already satisfied: matplotlib in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (3.10.6)\r\n",
      "Collecting alpha_vantage\r\n",
      "  Downloading alpha_vantage-3.0.0-py3-none-any.whl.metadata (12 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (4.67.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (1.7.2)\r\n",
      "Collecting cachecontrol>=0.14.3 (from firebase-admin)\r\n",
      "  Downloading cachecontrol-0.14.3-py3-none-any.whl.metadata (3.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-core<3.0.0dev,>=2.25.1 (from google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading google_api_core-2.25.2-py3-none-any.whl.metadata (3.0 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-firestore>=2.21.0 (from firebase-admin)\r\n",
      "  Downloading google_cloud_firestore-2.21.0-py3-none-any.whl.metadata (9.9 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage>=3.1.1 (from firebase-admin)\r\n",
      "  Downloading google_cloud_storage-3.4.0-py3-none-any.whl.metadata (13 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyjwt>=2.10.1 (from pyjwt[crypto]>=2.10.1->firebase-admin)\r\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx==0.28.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpx[http2]==0.28.1->firebase-admin) (0.28.1)\r\n",
      "Requirement already satisfied: anyio in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (4.11.0)\r\n",
      "Requirement already satisfied: certifi in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (2025.10.5)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (1.0.9)\r\n",
      "Requirement already satisfied: idna in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (3.10)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h2<5,>=3 (from httpx[http2]==0.28.1->firebase-admin)\r\n",
      "  Downloading h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\r\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin) (6.32.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading google_auth-2.41.1-py2.py3-none-any.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from requests) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from requests) (2.5.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading grpcio-1.75.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\r\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.15.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]==0.28.1->firebase-admin)\r\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]==0.28.1->firebase-admin)\r\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Requirement already satisfied: h11>=0.16 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpcore==1.*->httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (0.16.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.12.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from dropbox) (1.17.0)\r\n",
      "Collecting stone<3.3.3,>=2 (from dropbox)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading stone-3.3.1-py3-none-any.whl.metadata (8.0 kB)\r\n",
      "Collecting ply>=3.4 (from stone<3.3.3,>=2->dropbox)\r\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soupsieve>1.2 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from beautifulsoup4) (2.8)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (0.0.12)\r\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (4.4.0)\r\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (2.4.6)\r\n",
      "Requirement already satisfied: peewee>=3.16.2 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (3.18.2)\r\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (0.13.0)\r\n",
      "Requirement already satisfied: websockets>=13.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (15.0.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer)\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting importlib-metadata>=1.4 (from pyppeteer)\r\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer)\r\n",
      "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3<3,>=1.21.1 (from requests)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: pip is looking at multiple versions of pyppeteer to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting pyppeteer\r\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Collecting pyppeteer\r\n",
      "  Downloading pyppeteer-1.0.1-py3-none-any.whl.metadata (6.9 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyppeteer-1.0.0-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "  Downloading pyppeteer-0.2.6-py3-none-any.whl.metadata (6.9 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyppeteer-0.2.5-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "  Downloading pyppeteer-0.2.4-py3-none-any.whl.metadata (6.8 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyppeteer-0.2.3-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting pyee<8.0.0,>=7.0.1 (from pyppeteer)\r\n",
      "  Downloading pyee-7.0.4-py2.py3-none-any.whl.metadata (1.8 kB)\r\n",
      "INFO: pip is still looking at multiple versions of pyppeteer to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting pyppeteer\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyppeteer-0.2.2-py3-none-any.whl.metadata (6.6 kB)\r\n",
      "  Downloading pyppeteer-0.0.25.tar.gz (1.2 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \b\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting pyee (from pyppeteer)\r\n",
      "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: scipy in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from lightgbm) (1.16.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (1.3.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (4.60.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (1.4.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (25.0)\r\n",
      "Requirement already satisfied: pillow>=8 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (11.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (3.2.5)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp (from alpha_vantage)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting msgpack<2.0.0,>=0.5.2 (from cachecontrol>=0.14.3->firebase-admin)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\r\n",
      "Requirement already satisfied: cffi>=1.12.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-core<3.0.0,>=1.4.1 (from google-cloud-firestore>=2.21.0->firebase-admin)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-resumable-media<3.0.0,>=2.7.2 (from google-cloud-storage>=3.1.1->firebase-admin)\r\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-crc32c<2.0.0,>=1.1.3 (from google-cloud-storage>=3.1.1->firebase-admin)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading google_crc32c-1.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cryptography>=3.4.0 (from pyjwt[crypto]>=2.10.1->firebase-admin)\r\n",
      "  Downloading cryptography-46.0.2-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->alpha_vantage)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->alpha_vantage)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (25.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting frozenlist>=1.1.1 (from aiohttp->alpha_vantage)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multidict<7.0,>=4.5 (from aiohttp->alpha_vantage)\r\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting propcache>=0.2.0 (from aiohttp->alpha_vantage)\r\n",
      "  Downloading propcache-0.4.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->alpha_vantage)\r\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sniffio>=1.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from anyio->httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (1.3.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mplfinance-0.12.10b0-py3-none-any.whl (75 kB)\r\n",
      "Downloading firebase_admin-7.1.0-py3-none-any.whl (137 kB)\r\n",
      "Downloading google_api_core-2.25.2-py3-none-any.whl (162 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading google_auth-2.41.1-py2.py3-none-any.whl (221 kB)\r\n",
      "Downloading cachetools-6.2.0-py3-none-any.whl (11 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\r\n",
      "Downloading grpcio-1.75.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.5 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading grpcio_status-1.75.1-py3-none-any.whl (14 kB)\r\n",
      "Downloading h2-4.3.0-py3-none-any.whl (61 kB)\r\n",
      "Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\r\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\r\n",
      "Downloading dropbox-12.0.2-py3-none-any.whl (572 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/572.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.1/572.1 kB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading stone-3.3.1-py3-none-any.whl (162 kB)\r\n",
      "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m215.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading alpha_vantage-3.0.0-py3-none-any.whl (35 kB)\r\n",
      "Downloading cachecontrol-0.14.3-py3-none-any.whl (21 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (429 kB)\r\n",
      "Downloading google_cloud_firestore-2.21.0-py3-none-any.whl (368 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\r\n",
      "Downloading google_cloud_storage-3.4.0-py3-none-any.whl (278 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading google_crc32c-1.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\r\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\r\n",
      "Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\r\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\r\n",
      "Downloading cryptography-46.0.2-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m168.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m200.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\r\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\r\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading propcache-0.4.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (209 kB)\r\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pyee-13.0.0-py3-none-any.whl (15 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: pyppeteer\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for pyppeteer (pyproject.toml) ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyppeteer: filename=pyppeteer-0.0.25-py3-none-any.whl size=78442 sha256=6c4fa4884e1e109ca0c3abc371a74e9f6f24580327e5b315c4b247eba1c887b9\r\n",
      "  Stored in directory: /home/runner/.cache/pip/wheels/73/a3/1e/3a15782836222d82b917f1ebf652f9db54eec93f3268a42bcf\r\n",
      "Successfully built pyppeteer\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: ply, appdirs, stone, pyjwt, pyee, pyasn1, proto-plus, propcache, multidict, msgpack, hyperframe, hpack, grpcio, googleapis-common-protos, google-crc32c, frozenlist, cachetools, aiohappyeyeballs, yarl, rsa, pyppeteer, pyasn1-modules, lightgbm, h2, grpcio-status, google-resumable-media, dropbox, cryptography, cachecontrol, aiosignal, mplfinance, google-auth, aiohttp, google-api-core, alpha_vantage, google-cloud-core, google-cloud-storage, google-cloud-firestore, firebase-admin\r\n",
      "\u001b[?25l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/39\u001b[0m [pyee]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/39\u001b[0m [msgpack]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/39\u001b[0m [grpcio]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/39\u001b[0m [pyppeteer]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/39\u001b[0m [pyasn1-modules]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/39\u001b[0m [google-resumable-media]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m26/39\u001b[0m [dropbox]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m26/39\u001b[0m [dropbox]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m28/39\u001b[0m [cachecontrol]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m32/39\u001b[0m [aiohttp]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m33/39\u001b[0m [google-api-core]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m36/39\u001b[0m [google-cloud-storage]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m37/39\u001b[0m [google-cloud-firestore]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39/39\u001b[0m [firebase-admin]\r\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 alpha_vantage-3.0.0 appdirs-1.4.4 cachecontrol-0.14.3 cachetools-6.2.0 cryptography-46.0.2 dropbox-12.0.2 firebase-admin-7.1.0 frozenlist-1.7.0 google-api-core-2.25.2 google-auth-2.41.1 google-cloud-core-2.4.3 google-cloud-firestore-2.21.0 google-cloud-storage-3.4.0 google-crc32c-1.7.1 google-resumable-media-2.7.2 googleapis-common-protos-1.70.0 grpcio-1.75.1 grpcio-status-1.75.1 h2-4.3.0 hpack-4.1.0 hyperframe-6.1.0 lightgbm-4.6.0 mplfinance-0.12.10b0 msgpack-1.1.1 multidict-6.6.4 ply-3.11 propcache-0.4.0 proto-plus-1.26.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pyee-13.0.0 pyjwt-2.10.1 pyppeteer-0.0.25 rsa-4.9.1 stone-3.3.1 yarl-1.20.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e60e2eca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:15:36.576488Z",
     "iopub.status.busy": "2025-10-05T15:15:36.576272Z",
     "iopub.status.idle": "2025-10-05T15:15:41.721661Z",
     "shell.execute_reply": "2025-10-05T15:15:41.721042Z"
    },
    "id": "dW7wdHj3poOk",
    "papermill": {
     "duration": 5.15401,
     "end_time": "2025-10-05T15:15:41.722625",
     "exception": false,
     "start_time": "2025-10-05T15:15:36.568615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Checking Git and Git LFS...\n",
      "✅ Git already installed.\n",
      "✅ Git LFS already installed.\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "🔧 Configuring Git identity...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Cloning repo 'forex-ai-models' from GitHub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'forex-ai-models'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:   6% (8/132)\r",
      "Filtering content:   7% (10/132)\r",
      "Filtering content:   8% (11/132)\r",
      "Filtering content:   9% (12/132)\r",
      "Filtering content:  10% (14/132)\r",
      "Filtering content:  11% (15/132)\r",
      "Filtering content:  12% (16/132)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  13% (18/132)\r",
      "Filtering content:  14% (19/132)\r",
      "Filtering content:  15% (20/132)\r",
      "Filtering content:  16% (22/132)\r",
      "Filtering content:  17% (23/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  18% (24/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  19% (26/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  20% (27/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  21% (28/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  22% (30/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  23% (31/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  24% (32/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  25% (33/132), 72.55 MiB | 134.90 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  26% (35/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  27% (36/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  28% (37/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  29% (39/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  30% (40/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  31% (41/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  32% (43/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  33% (44/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  34% (45/132), 72.55 MiB | 134.90 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  35% (47/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  36% (48/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  37% (49/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  38% (51/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  39% (52/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  40% (53/132), 72.55 MiB | 134.90 MiB/s\r",
      "Filtering content:  41% (55/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  42% (56/132), 120.50 MiB | 114.39 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  43% (57/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  44% (59/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  45% (60/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  46% (61/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  47% (63/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  48% (64/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  49% (65/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  50% (66/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  51% (68/132), 120.50 MiB | 114.39 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  52% (69/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  53% (70/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  54% (72/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  55% (73/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  56% (74/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  57% (76/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  58% (77/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  59% (78/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  60% (80/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  61% (81/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  62% (82/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  63% (84/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  64% (85/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  65% (86/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  66% (88/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  67% (89/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  68% (90/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  69% (92/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  70% (93/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  71% (94/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  72% (96/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  73% (97/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  74% (98/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  75% (99/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  76% (101/132), 120.50 MiB | 114.39 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  77% (102/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  78% (103/132), 120.50 MiB | 114.39 MiB/s\r",
      "Filtering content:  79% (105/132), 143.79 MiB | 91.45 MiB/s \r",
      "Filtering content:  80% (106/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  81% (107/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  82% (109/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  83% (110/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  84% (111/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  85% (113/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  86% (114/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  87% (115/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  88% (117/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  89% (118/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  90% (119/132), 143.79 MiB | 91.45 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  91% (121/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  92% (122/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  93% (123/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  94% (125/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  95% (126/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  96% (127/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  97% (129/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  98% (130/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content:  99% (131/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content: 100% (132/132), 143.79 MiB | 91.45 MiB/s\r",
      "Filtering content: 100% (132/132), 143.88 MiB | 53.81 MiB/s, done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Tracking CSV/PKL files with Git LFS...\n",
      "\"*.csv\" already supported\n",
      "\"*.pkl\" already supported\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "No .gitattributes changes\n",
      "📂 Staging all new/modified files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "No new changes to commit\n",
      "🚀 Pushing changes to GitHub...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 LFS-tracked files:\n",
      "62ba7af037 * AUD_USD.csv\n",
      "9aeb6c1661 * AUD_USD_15m_60d.csv\n",
      "51faae17e2 * AUD_USD_15m_60d_combined.csv\n",
      "60c231b720 * AUD_USD_1d_5y.csv\n",
      "81027227cf * AUD_USD_1d_5y_combined.csv\n",
      "5a2398a1e7 * AUD_USD_1h_2y.csv\n",
      "5a2398a1e7 * AUD_USD_1h_2y_combined.csv\n",
      "5bbdcf4286 * AUD_USD_1m_7d.csv\n",
      "67a33d97b3 * AUD_USD_1m_7d_combined.csv\n",
      "d331f178c2 * AUD_USD_5m_1mo.csv\n",
      "54d9cebc94 * AUD_USD_5m_1mo_combined.csv\n",
      "839982f0fa * EUR_USD.csv\n",
      "409e748308 * EUR_USD_15m_60d.csv\n",
      "5d118b259f * EUR_USD_15m_60d_combined.csv\n",
      "8eeeae7e65 * EUR_USD_1d_5y.csv\n",
      "1fa52a4d17 * EUR_USD_1d_5y_combined.csv\n",
      "08f9da0ddf * EUR_USD_1h_2y.csv\n",
      "e8369dcaba * EUR_USD_1h_2y_combined.csv\n",
      "6ee4ebfb32 * EUR_USD_1m_7d.csv\n",
      "df9c86c994 * EUR_USD_1m_7d_combined.csv\n",
      "f63aa6b9cc * EUR_USD_5m_1mo.csv\n",
      "df9c86c994 * EUR_USD_5m_1mo_combined.csv\n",
      "a517aaf597 * GBP_USD.csv\n",
      "20ff3c031b * GBP_USD_15m_60d.csv\n",
      "c74a84ce57 * GBP_USD_15m_60d_combined.csv\n",
      "27a76fec92 * GBP_USD_1d_5y.csv\n",
      "404c85df78 * GBP_USD_1d_5y_combined.csv\n",
      "b969b5361a * GBP_USD_1h_2y.csv\n",
      "b969b5361a * GBP_USD_1h_2y_combined.csv\n",
      "08f9da0ddf * GBP_USD_1m_7d.csv\n",
      "e8369dcaba * GBP_USD_1m_7d_combined.csv\n",
      "409e748308 * GBP_USD_5m_1mo.csv\n",
      "5d118b259f * GBP_USD_5m_1mo_combined.csv\n",
      "5f7007605b * USD_JPY.csv\n",
      "b7b20ae1f0 * USD_JPY_15m_60d.csv\n",
      "f65b12f7ed * USD_JPY_15m_60d_combined.csv\n",
      "d6dfe78887 * USD_JPY_1d_5y.csv\n",
      "cf020a40e9 * USD_JPY_1d_5y_combined.csv\n",
      "f7a46be5c7 * USD_JPY_1h_2y.csv\n",
      "48a7cb7d7d * USD_JPY_1h_2y_combined.csv\n",
      "99dc4321ce * USD_JPY_1m_7d.csv\n",
      "0c2374debe * USD_JPY_1m_7d_combined.csv\n",
      "ee7decd6b0 * USD_JPY_5m_1mo.csv\n",
      "bcfdc0c9fc * USD_JPY_5m_1mo_combined.csv\n",
      "1411d03052 * best_chromosome.pkl\n",
      "cb6d634fd9 * broker_signals_log.csv\n",
      "530fd64279 * combined_data/AUD_USD_15m_60d_combined.pkl\n",
      "294388c07b * combined_data/AUD_USD_1d_5y_combined.pkl\n",
      "1329ef90a8 * combined_data/AUD_USD_1h_2y_combined.pkl\n",
      "c847a1510d * combined_data/AUD_USD_1m_7d_combined.pkl\n",
      "530fd64279 * combined_data/AUD_USD_5m_1mo_combined.pkl\n",
      "a00d5cde1a * combined_data/EUR_USD_15m_60d_combined.pkl\n",
      "daa21c075e * combined_data/EUR_USD_1d_5y_combined.pkl\n",
      "c586a988d4 * combined_data/EUR_USD_1h_2y_combined.pkl\n",
      "c586a988d4 * combined_data/EUR_USD_1m_7d_combined.pkl\n",
      "c586a988d4 * combined_data/EUR_USD_5m_1mo_combined.pkl\n",
      "c586a988d4 * combined_data/GBP_USD_15m_60d_combined.pkl\n",
      "0369c29448 * combined_data/GBP_USD_1d_5y_combined.pkl\n",
      "0efd067b50 * combined_data/GBP_USD_1h_2y_combined.pkl\n",
      "c586a988d4 * combined_data/GBP_USD_1m_7d_combined.pkl\n",
      "a00d5cde1a * combined_data/GBP_USD_5m_1mo_combined.pkl\n",
      "0efd067b50 * combined_data/USD_JPY_15m_60d_combined.pkl\n",
      "0369c29448 * combined_data/USD_JPY_1d_5y_combined.pkl\n",
      "0efd067b50 * combined_data/USD_JPY_1h_2y_combined.pkl\n",
      "0efd067b50 * combined_data/USD_JPY_1m_7d_combined.pkl\n",
      "0efd067b50 * combined_data/USD_JPY_5m_1mo_combined.pkl\n",
      "8b9683b7b3 * combined_with_indicators/AUD_USD_15m_60d_combined.pkl\n",
      "55ca4426a5 * combined_with_indicators/AUD_USD_15m_60d_combined_combined.pkl\n",
      "9ee08a9bb1 * combined_with_indicators/AUD_USD_1d_5y_combined.pkl\n",
      "8b252f0b90 * combined_with_indicators/AUD_USD_1d_5y_combined_combined.pkl\n",
      "f9b05fac88 * combined_with_indicators/AUD_USD_1h_2y_combined.pkl\n",
      "3adb3ab338 * combined_with_indicators/AUD_USD_1h_2y_combined_combined.pkl\n",
      "4dcf63be33 * combined_with_indicators/AUD_USD_1m_7d_combined.pkl\n",
      "99f9d845ea * combined_with_indicators/AUD_USD_1m_7d_combined_combined.pkl\n",
      "f2165c4f1c * combined_with_indicators/AUD_USD_5m_1mo_combined.pkl\n",
      "55ca4426a5 * combined_with_indicators/AUD_USD_5m_1mo_combined_combined.pkl\n",
      "3e44fe01e3 * combined_with_indicators/AUD_USD_AUD_USD_combined.pkl\n",
      "debc2271d0 * combined_with_indicators/EUR_USD_15m_60d_combined.pkl\n",
      "8edcaa12ce * combined_with_indicators/EUR_USD_15m_60d_combined_combined.pkl\n",
      "bbdb7e7abc * combined_with_indicators/EUR_USD_1d_5y_combined.pkl\n",
      "8d328d50ad * combined_with_indicators/EUR_USD_1d_5y_combined_combined.pkl\n",
      "4455792a8e * combined_with_indicators/EUR_USD_1h_2y_combined.pkl\n",
      "8b938c03b4 * combined_with_indicators/EUR_USD_1h_2y_combined_combined.pkl\n",
      "fe89f585bc * combined_with_indicators/EUR_USD_1m_7d_combined.pkl\n",
      "8b938c03b4 * combined_with_indicators/EUR_USD_1m_7d_combined_combined.pkl\n",
      "fe89f585bc * combined_with_indicators/EUR_USD_5m_1mo_combined.pkl\n",
      "8b938c03b4 * combined_with_indicators/EUR_USD_5m_1mo_combined_combined.pkl\n",
      "99ee46bac5 * combined_with_indicators/EUR_USD_EUR_USD_combined.pkl\n",
      "31d7e9fa88 * combined_with_indicators/GBP_USD_15m_60d_combined.pkl\n",
      "8b938c03b4 * combined_with_indicators/GBP_USD_15m_60d_combined_combined.pkl\n",
      "84fe2d65b8 * combined_with_indicators/GBP_USD_1d_5y_combined.pkl\n",
      "fbecbb6da0 * combined_with_indicators/GBP_USD_1d_5y_combined_combined.pkl\n",
      "8dc7a979bf * combined_with_indicators/GBP_USD_1h_2y_combined.pkl\n",
      "bb4e780d62 * combined_with_indicators/GBP_USD_1h_2y_combined_combined.pkl\n",
      "4455792a8e * combined_with_indicators/GBP_USD_1m_7d_combined.pkl\n",
      "8b938c03b4 * combined_with_indicators/GBP_USD_1m_7d_combined_combined.pkl\n",
      "debc2271d0 * combined_with_indicators/GBP_USD_5m_1mo_combined.pkl\n",
      "8edcaa12ce * combined_with_indicators/GBP_USD_5m_1mo_combined_combined.pkl\n",
      "d27a1d82a0 * combined_with_indicators/GBP_USD_GBP_USD_combined.pkl\n",
      "5940e1ebbf * combined_with_indicators/USD_JPY_15m_60d_combined.pkl\n",
      "bb4e780d62 * combined_with_indicators/USD_JPY_15m_60d_combined_combined.pkl\n",
      "29a4402069 * combined_with_indicators/USD_JPY_1d_5y_combined.pkl\n",
      "fbecbb6da0 * combined_with_indicators/USD_JPY_1d_5y_combined_combined.pkl\n",
      "41f8570556 * combined_with_indicators/USD_JPY_1h_2y_combined.pkl\n",
      "bb4e780d62 * combined_with_indicators/USD_JPY_1h_2y_combined_combined.pkl\n",
      "b48e432d7b * combined_with_indicators/USD_JPY_1m_7d_combined.pkl\n",
      "bb4e780d62 * combined_with_indicators/USD_JPY_1m_7d_combined_combined.pkl\n",
      "92822f0e15 * combined_with_indicators/USD_JPY_5m_1mo_combined.pkl\n",
      "bb4e780d62 * combined_with_indicators/USD_JPY_5m_1mo_combined_combined.pkl\n",
      "9b163c9249 * combined_with_indicators/USD_JPY_USD_JPY_combined.pkl\n",
      "4f85dea4e0 * generation_count.pkl\n",
      "d3952dadaa * models/AUD_USD_15m_60d.pkl\n",
      "0b0fca22ee * models/AUD_USD_1d_5y.pkl\n",
      "49fbacc236 * models/AUD_USD_1h_2y.pkl\n",
      "25e253e1dc * models/AUD_USD_1m_7d.pkl\n",
      "e702fc9e4f * models/AUD_USD_5m_1mo.pkl\n",
      "791105e278 * models/EUR_USD_15m_60d.pkl\n",
      "71a1c3de81 * models/EUR_USD_1d_5y.pkl\n",
      "3fb4b7591a * models/EUR_USD_1h_2y.pkl\n",
      "79246dc05c * models/EUR_USD_1m_7d.pkl\n",
      "02bcf3399a * models/EUR_USD_5m_1mo.pkl\n",
      "b513ebdf08 * models/GBP_USD_15m_60d.pkl\n",
      "8e39bea713 * models/GBP_USD_1d_5y.pkl\n",
      "22e28669b9 * models/GBP_USD_1h_2y.pkl\n",
      "d80a79c31e * models/GBP_USD_1m_7d.pkl\n",
      "ae8a688e53 * models/GBP_USD_5m_1mo.pkl\n",
      "f04193ad8b * models/USD_JPY_15m_60d.pkl\n",
      "dffe8dad49 * models/USD_JPY_1d_5y.pkl\n",
      "a5a49ccdc2 * models/USD_JPY_1m_7d.pkl\n",
      "b10050c0ac * models/USD_JPY_5m_1mo.pkl\n",
      "84ad53ab35 * population.pkl\n",
      "926248e52d * trade_memory.pkl\n",
      "✅ Fresh-run GitHub repo workflow complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Fully Automatic Fresh-Run GitHub Workflow in Colab / GitHub Actions\n",
    "# -------------------------------\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# -------------------------------\n",
    "# 0️⃣ User Config\n",
    "# -------------------------------\n",
    "GITHUB_USERNAME = \"rahim-dotAI\"\n",
    "GITHUB_REPO = \"forex-ai-models\"\n",
    "REPO_FOLDER = GITHUB_REPO  # Local folder\n",
    "GIT_USER_EMAIL = \"nakatonabira3@gmail.com\"  # Replace with your email\n",
    "\n",
    "# Use environment variable for token\n",
    "GITHUB_PAT = os.environ.get(\"FOREX_PAT\")\n",
    "if not GITHUB_PAT:\n",
    "    raise ValueError(\"❌ Token not set! Define environment variable FOREX_PAT in Colab or as a GitHub Actions secret.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ Install Git and Git LFS (Safe across environments)\n",
    "# -------------------------------\n",
    "print(\"⚙️ Checking Git and Git LFS...\")\n",
    "\n",
    "def safe_run(cmd):\n",
    "    \"\"\"Run shell command safely with clear logging.\"\"\"\n",
    "    try:\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"⚠️ Skipped or failed: {cmd}\\n   Reason: {e}\")\n",
    "\n",
    "# Detect whether sudo is available (GitHub Actions needs sudo)\n",
    "USE_SUDO = shutil.which(\"sudo\") is not None\n",
    "\n",
    "if shutil.which(\"git\") is None:\n",
    "    cmd = \"apt-get update -qq && apt-get install -y git\"\n",
    "    if USE_SUDO:\n",
    "        cmd = \"sudo \" + cmd\n",
    "    safe_run(cmd)\n",
    "else:\n",
    "    print(\"✅ Git already installed.\")\n",
    "\n",
    "if shutil.which(\"git-lfs\") is None:\n",
    "    cmd = \"apt-get install -y git-lfs\"\n",
    "    if USE_SUDO:\n",
    "        cmd = \"sudo \" + cmd\n",
    "    safe_run(cmd)\n",
    "else:\n",
    "    print(\"✅ Git LFS already installed.\")\n",
    "\n",
    "safe_run(\"git lfs install\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ Remove local repo (fresh run)\n",
    "# -------------------------------\n",
    "if os.path.exists(REPO_FOLDER):\n",
    "    print(f\"🗑️ Removing existing local repo '{REPO_FOLDER}' for a fresh run...\")\n",
    "    shutil.rmtree(REPO_FOLDER)\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Configure Git identity\n",
    "# -------------------------------\n",
    "print(\"🔧 Configuring Git identity...\")\n",
    "safe_run(f'git config --global user.name \"{GITHUB_USERNAME}\"')\n",
    "safe_run(f'git config --global user.email \"{GIT_USER_EMAIL}\"')\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Clone repo fresh\n",
    "# -------------------------------\n",
    "REPO_URL = f\"https://{GITHUB_USERNAME}:{GITHUB_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
    "print(f\"📥 Cloning repo '{REPO_FOLDER}' from GitHub...\")\n",
    "safe_run(f\"git clone {REPO_URL}\")\n",
    "os.chdir(REPO_FOLDER)\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ Track CSV/PKL files with Git LFS\n",
    "# -------------------------------\n",
    "print(\"📌 Tracking CSV/PKL files with Git LFS...\")\n",
    "safe_run(\"git lfs track '*.csv'\")\n",
    "safe_run(\"git lfs track '*.pkl'\")\n",
    "safe_run(\"git add .gitattributes\")\n",
    "safe_run('git commit -m \"Track CSV/PKL files with Git LFS\" || echo \"No .gitattributes changes\"')\n",
    "\n",
    "# -------------------------------\n",
    "# 6️⃣ Stage, commit, and push changes\n",
    "# -------------------------------\n",
    "print(\"📂 Staging all new/modified files...\")\n",
    "safe_run(\"git add -A\")\n",
    "safe_run('git commit -m \"Auto-update: new or modified files\" || echo \"No new changes to commit\"')\n",
    "\n",
    "print(\"🚀 Pushing changes to GitHub...\")\n",
    "safe_run(\"git push origin main\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7️⃣ List LFS-tracked files\n",
    "# -------------------------------\n",
    "print(\"📋 LFS-tracked files:\")\n",
    "safe_run(\"git lfs ls-files\")\n",
    "\n",
    "print(\"✅ Fresh-run GitHub repo workflow complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a56ae3",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-05T15:15:41.739093Z",
     "iopub.status.busy": "2025-10-05T15:15:41.738928Z",
     "iopub.status.idle": "2025-10-05T15:15:41.742618Z",
     "shell.execute_reply": "2025-10-05T15:15:41.742112Z"
    },
    "id": "JS9qXRF_JXJO",
    "papermill": {
     "duration": 0.012598,
     "end_time": "2025-10-05T15:15:41.743460",
     "exception": false,
     "start_time": "2025-10-05T15:15:41.730862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Vantage Key: 1W58NPZXOG5SLHZ6\n",
      "Browserless Token: 2St0qUktyKsA0Bsb5b510553885cae26942e44c26c0f19c3d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set your keys (only for this session)\n",
    "os.environ['ALPHA_VANTAGE_KEY'] = '1W58NPZXOG5SLHZ6'\n",
    "os.environ['BROWSERLESS_TOKEN'] = '2St0qUktyKsA0Bsb5b510553885cae26942e44c26c0f19c3d'\n",
    "\n",
    "# Test if they work\n",
    "print(\"Alpha Vantage Key:\", os.environ.get('ALPHA_VANTAGE_KEY'))\n",
    "print(\"Browserless Token:\", os.environ.get('BROWSERLESS_TOKEN'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a72b5bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:15:41.759777Z",
     "iopub.status.busy": "2025-10-05T15:15:41.759622Z",
     "iopub.status.idle": "2025-10-05T15:16:02.328128Z",
     "shell.execute_reply": "2025-10-05T15:16:02.327555Z"
    },
    "id": "N_fMCgLPqlPV",
    "papermill": {
     "duration": 20.577738,
     "end_time": "2025-10-05T15:16:02.329023",
     "exception": false,
     "start_time": "2025-10-05T15:15:41.751285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Repo exists, pulling latest changes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Already on 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/rahim-dotAI/forex-ai-models\n",
      " * branch            main       -> FETCH_HEAD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "[main 5aba7e7] Initial commit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: could not read Username for 'https://github.com': No such device or address\n",
      "/tmp/ipykernel_2372/617912145.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EUR/USD: 2 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Updating EUR/USD on GitHub...\n",
      "[main afe931e] Update EUR/USD historical FX data\n",
      " 1 file changed, 2 insertions(+), 2 deletions(-)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading LFS objects: 100% (1/1), 3.9 KB | 0 B/s, done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/rahim-dotAI/forex-ai-models.git\n",
      "   4ea6d21..afe931e  main -> main\n",
      "/tmp/ipykernel_2372/617912145.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GBP/USD: 2 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Updating GBP/USD on GitHub...\n",
      "[main c174706] Update GBP/USD historical FX data\n",
      " 1 file changed, 2 insertions(+), 2 deletions(-)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading LFS objects: 100% (1/1), 3.9 KB | 0 B/s, done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/rahim-dotAI/forex-ai-models.git\n",
      "   afe931e..c174706  main -> main\n",
      "/tmp/ipykernel_2372/617912145.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded USD/JPY: 2 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Updating USD/JPY on GitHub...\n",
      "[main ab78dc7] Update USD/JPY historical FX data\n",
      " 1 file changed, 2 insertions(+), 2 deletions(-)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading LFS objects: 100% (1/1), 3.9 KB | 0 B/s, done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/rahim-dotAI/forex-ai-models.git\n",
      "   c174706..ab78dc7  main -> main\n",
      "/tmp/ipykernel_2372/617912145.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded AUD/USD: 2 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Updating AUD/USD on GitHub...\n",
      "[main 4b3a980] Update AUD/USD historical FX data\n",
      " 1 file changed, 2 insertions(+), 2 deletions(-)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading LFS objects: 100% (1/1), 3.9 KB | 0 B/s, done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All pairs processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/rahim-dotAI/forex-ai-models.git\n",
      "   ab78dc7..4b3a980  main -> main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "SAVE_FOLDER = \"/content/forex-ai-models\"  # Cloned GitHub repo folder\n",
    "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GIT_NAME = \"Abdul Rahim\"\n",
    "GIT_EMAIL = \"nakatonabira3@gmail.com\"\n",
    "\n",
    "# -----------------------------\n",
    "# GitHub Auth & Repo Info\n",
    "# -----------------------------\n",
    "GITHUB_USERNAME = \"rahim-dotAI\"\n",
    "GITHUB_REPO = \"forex-ai-models\"\n",
    "GITHUB_TOKEN = os.environ.get(\"FOREX_PAT\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"❌ Token not set! Define environment variable FOREX_PAT in Colab or as a GitHub Actions secret.\")\n",
    "BRANCH = \"main\"\n",
    "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
    "\n",
    "# -----------------------------\n",
    "# Setup Git identity (once per session)\n",
    "# -----------------------------\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=True)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def fetch_alpha_vantage_fx(pair, outputsize='compact'):\n",
    "    \"\"\"Fetch FX data from Alpha Vantage.\"\"\"\n",
    "    base_url = 'https://www.alphavantage.co/query'\n",
    "    from_currency, to_currency = pair.split('/')\n",
    "    params = {\n",
    "        'function': 'FX_DAILY',\n",
    "        'from_symbol': from_currency,\n",
    "        'to_symbol': to_currency,\n",
    "        'outputsize': outputsize,\n",
    "        'datatype': 'json',\n",
    "        'apikey': os.environ['ALPHA_VANTAGE_KEY']\n",
    "    }\n",
    "    response = requests.get(base_url, params=params, timeout=30)\n",
    "    data = response.json()\n",
    "    if 'Time Series FX (Daily)' not in data:\n",
    "        print(f\"Failed to fetch {pair}: {data}\")\n",
    "        return pd.DataFrame()\n",
    "    ts = data['Time Series FX (Daily)']\n",
    "    df = pd.DataFrame(ts).T\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df.rename(columns={\n",
    "        '1. open': 'open',\n",
    "        '2. high': 'high',\n",
    "        '3. low': 'low',\n",
    "        '4. close': 'close'\n",
    "    }).astype(float)\n",
    "\n",
    "def file_hash(filepath, chunk_size=8192):\n",
    "    \"\"\"Compute MD5 hash of a file in chunks (faster, memory efficient).\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "def ensure_repo_cloned(repo_url, repo_folder, branch=\"main\"):\n",
    "    \"\"\"Clone repo if missing or pull latest if exists.\"\"\"\n",
    "    if not os.path.exists(os.path.join(repo_folder, \".git\")):\n",
    "        if os.path.exists(repo_folder):\n",
    "            subprocess.run([\"rm\", \"-rf\", repo_folder], check=True)\n",
    "        print(\"📥 Cloning repo...\")\n",
    "        try:\n",
    "            subprocess.run([\"git\", \"clone\", \"-b\", branch, repo_url, repo_folder], check=True)\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"⚠️ Branch '{branch}' not found. Cloning default branch...\")\n",
    "            subprocess.run([\"git\", \"clone\", repo_url, repo_folder], check=True)\n",
    "    else:\n",
    "        print(\"🔄 Repo exists, pulling latest changes...\")\n",
    "        os.chdir(repo_folder)\n",
    "        subprocess.run([\"git\", \"fetch\", \"origin\"], check=True)\n",
    "        subprocess.run([\"git\", \"checkout\", branch], check=False)\n",
    "        subprocess.run([\"git\", \"pull\", \"origin\", branch], check=False)\n",
    "        os.chdir(\"..\")\n",
    "\n",
    "def ensure_repo_initialized(repo_folder):\n",
    "    \"\"\"Handle empty repo / initial commit and set main branch.\"\"\"\n",
    "    ensure_repo_cloned(REPO_URL, repo_folder, BRANCH)\n",
    "    os.chdir(repo_folder)\n",
    "    result = subprocess.run([\"git\", \"status\"], capture_output=True, text=True)\n",
    "    if \"nothing to commit\" in result.stdout:\n",
    "        return  # Already initialized\n",
    "    subprocess.run([\"git\", \"commit\", \"--allow-empty\", \"-m\", \"Initial commit\"], check=False)\n",
    "    subprocess.run([\"git\", \"branch\", \"-M\", \"main\"], check=False)\n",
    "    subprocess.run([\"git\", \"push\", \"-u\", \"origin\", \"main\"], check=False)\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize repo (first time only)\n",
    "# -----------------------------\n",
    "ensure_repo_initialized(SAVE_FOLDER)\n",
    "\n",
    "# -----------------------------\n",
    "# List of currency pairs\n",
    "# -----------------------------\n",
    "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Fetch, merge, save, commit & push\n",
    "# -----------------------------\n",
    "for pair in pairs:\n",
    "    filename = pair.replace(\"/\", \"_\") + \".csv\"\n",
    "    filepath = os.path.join(SAVE_FOLDER, filename)\n",
    "\n",
    "    # Load existing data (only if file exists)\n",
    "    if os.path.exists(filepath):\n",
    "        existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
    "        print(f\"Loaded {pair}: {existing_df.shape[0]} rows\")\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "        print(f\"No existing data for {pair}\")\n",
    "\n",
    "    old_hash = file_hash(filepath)\n",
    "\n",
    "    # Fetch new data\n",
    "    new_df = fetch_alpha_vantage_fx(pair)\n",
    "    if new_df.empty:\n",
    "        print(f\"No new data fetched for {pair}\")\n",
    "        continue\n",
    "\n",
    "    # Merge efficiently (drop duplicates directly)\n",
    "    if not existing_df.empty:\n",
    "        combined_df = pd.concat([existing_df, new_df])\n",
    "        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
    "    else:\n",
    "        combined_df = new_df\n",
    "\n",
    "    combined_df.sort_index(inplace=True)\n",
    "    combined_df.to_csv(filepath)\n",
    "\n",
    "    new_hash = file_hash(filepath)\n",
    "\n",
    "    # Commit & push only if changed\n",
    "    if old_hash != new_hash:\n",
    "        print(f\"🔄 Updating {pair} on GitHub...\")\n",
    "        os.chdir(SAVE_FOLDER)\n",
    "        subprocess.run([\"git\", \"add\", filename], check=False)\n",
    "        subprocess.run([\"git\", \"commit\", \"-m\", f\"Update {pair} historical FX data\"], check=False)\n",
    "        subprocess.run(\n",
    "            f\"git push https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git {BRANCH}\",\n",
    "            shell=True\n",
    "        )\n",
    "        os.chdir(\"..\")\n",
    "    else:\n",
    "        print(f\"No changes for {pair}, skipping push.\")\n",
    "\n",
    "print(\"✅ All pairs processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2c7ab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:16:02.347481Z",
     "iopub.status.busy": "2025-10-05T15:16:02.347288Z",
     "iopub.status.idle": "2025-10-05T15:16:13.527625Z",
     "shell.execute_reply": "2025-10-05T15:16:13.527015Z"
    },
    "id": "i1vFh6qtJo6M",
    "papermill": {
     "duration": 11.190447,
     "end_time": "2025-10-05T15:16:13.528578",
     "exception": false,
     "start_time": "2025-10-05T15:16:02.338131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Repo exists, pulling latest changes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/rahim-dotAI/forex-ai-models\n",
      "   4ea6d21..4b3a980  main       -> origin/main\n",
      "Already on 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/rahim-dotAI/forex-ai-models\n",
      " * branch            main       -> FETCH_HEAD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "Reading package lists..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Git LFS initialized.\n",
      "\"*.csv\" already supported\n",
      "\"*.pkl\" already supported\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\tforex_ai_outputs/\n",
      "\toutput/\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n",
      "No changes\n",
      "Loaded existing EUR/USD 1m_7d, 2 rows\n",
      "Loaded existing EUR/USD 5m_1mo, 2 rows\n",
      "Loaded existing EUR/USD 15m_60d, 2 rows\n",
      "Loaded existing EUR/USD 1h_2y, 2 rows\n",
      "Loaded existing EUR/USD 1d_5y, 2 rows\n",
      "Loaded existing GBP/USD 1m_7d, 2 rows\n",
      "Loaded existing GBP/USD 5m_1mo, 2 rows\n",
      "Loaded existing GBP/USD 15m_60d, 2 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing GBP/USD 1h_2y, 2 rows\n",
      "Loaded existing GBP/USD 1d_5y, 2 rows\n",
      "Loaded existing USD/JPY 1m_7d, 2 rows\n",
      "Loaded existing USD/JPY 15m_60d, 2 rows\n",
      "Loaded existing USD/JPY 1h_2y, 2 rows\n",
      "Loaded existing USD/JPY 5m_1mo, 2 rows\n",
      "Loaded existing USD/JPY 1d_5y, 2 rows\n",
      "Loaded existing AUD/USD 1m_7d, 2 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2352257564.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing AUD/USD 5m_1mo, 2 rows\n",
      "Loaded existing AUD/USD 15m_60d, 2 rows\n",
      "Loaded existing AUD/USD 1h_2y, 2 rows\n",
      "Loaded existing AUD/USD 1d_5y, 2 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Updated AUD/USD 1m_7d\n",
      "📌 Updated GBP/USD 1h_2y\n",
      "📌 Updated GBP/USD 15m_60d\n",
      "📌 Updated USD/JPY 1d_5y\n",
      "📌 Updated AUD/USD 1d_5y\n",
      "📌 Updated GBP/USD 5m_1mo\n",
      "📌 Updated USD/JPY 1h_2y\n",
      "📌 Updated EUR/USD 1d_5y\n",
      "📌 Updated EUR/USD 1m_7d\n",
      "📌 Updated AUD/USD 1h_2y\n",
      "📌 Updated GBP/USD 1d_5y\n",
      "📌 Updated GBP/USD 1m_7d\n",
      "📌 Updated USD/JPY 1m_7d\n",
      "📌 Updated USD/JPY 15m_60d\n",
      "📌 Updated AUD/USD 15m_60d\n",
      "📌 Updated EUR/USD 5m_1mo\n",
      "📌 Updated USD/JPY 5m_1mo\n",
      "📌 Updated EUR/USD 1h_2y\n",
      "📌 Updated AUD/USD 5m_1mo\n",
      "📌 Updated EUR/USD 15m_60d\n",
      "🚀 Committing 20 updated files in one push...\n",
      "[main aaa5276] Update multiple FX files\n",
      " 20 files changed, 40 insertions(+), 40 deletions(-)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading LFS objects: 100% (7/7), 3.4 MB | 0 B/s, done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 All yfinance pairs & timeframes processed (parallel, single push).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/rahim-dotAI/forex-ai-models.git\n",
      "   4b3a980..aaa5276  main -> main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "SAVE_FOLDER = \"/content/forex-ai-models\"  # Your repo folder\n",
    "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GIT_NAME = \"Abdul Rahim\"\n",
    "GIT_EMAIL = \"nakatonabira3@gmail.com\"\n",
    "\n",
    "GITHUB_USERNAME = \"rahim-dotAI\"\n",
    "GITHUB_REPO = \"forex-ai-models\"\n",
    "GITHUB_TOKEN = os.environ.get(\"FOREX_PAT\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"❌ Token not set! Define environment variable FOREX_PAT in Colab or as a GitHub Actions secret.\")\n",
    "BRANCH = \"main\"\n",
    "\n",
    "# -----------------------------\n",
    "# Git identity\n",
    "# -----------------------------\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: File hash\n",
    "# -----------------------------\n",
    "def file_hash(filepath, chunk_size=8192):\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure repo exists and pull\n",
    "# -----------------------------\n",
    "def ensure_repo():\n",
    "    if not os.path.exists(os.path.join(SAVE_FOLDER, \".git\")):\n",
    "        if os.path.exists(SAVE_FOLDER):\n",
    "            subprocess.run([\"rm\", \"-rf\", SAVE_FOLDER], check=True)\n",
    "        print(\"📥 Cloning repo...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"-b\", BRANCH,\n",
    "                        f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\",\n",
    "                        SAVE_FOLDER], check=True)\n",
    "    else:\n",
    "        print(\"🔄 Repo exists, pulling latest changes...\")\n",
    "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"fetch\", \"origin\"], check=True)\n",
    "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"checkout\", BRANCH], check=False)\n",
    "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"pull\", \"origin\", BRANCH], check=False)\n",
    "\n",
    "    # Git LFS setup\n",
    "    subprocess.run(\"apt-get update && apt-get install git-lfs -y\", shell=True)\n",
    "    subprocess.run([\"git\", \"lfs\", \"install\"], check=False)\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"lfs\", \"track\", \"*.csv\"], check=False)\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"lfs\", \"track\", \"*.pkl\"], check=False)\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"add\", \".gitattributes\"], check=False)\n",
    "    subprocess.run(f'git -C {SAVE_FOLDER} commit -m \"Track CSV/PKL files with Git LFS\" || echo \"No changes\"', shell=True)\n",
    "\n",
    "ensure_repo()\n",
    "\n",
    "# -----------------------------\n",
    "# FX pairs & timeframes\n",
    "# -----------------------------\n",
    "fx_pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "timeframes = {\n",
    "    \"1m_7d\": (\"1m\", \"7d\"),\n",
    "    \"5m_1mo\": (\"5m\", \"1mo\"),\n",
    "    \"15m_60d\": (\"15m\", \"60d\"),\n",
    "    \"1h_2y\": (\"1h\", \"2y\"),\n",
    "    \"1d_5y\": (\"1d\", \"5y\")\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Worker function with safe OHLC handling\n",
    "# -----------------------------\n",
    "def process_pair_tf(pair, tf_name, interval, period, max_retries=3, retry_delay=5):\n",
    "    \"\"\"\n",
    "    Download FX pair data from yfinance, merge with existing CSV,\n",
    "    handle variable columns safely, and return status & filename.\n",
    "    \"\"\"\n",
    "    symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
    "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
    "    filepath = os.path.join(SAVE_FOLDER, filename)\n",
    "\n",
    "    # Load existing data\n",
    "    if os.path.exists(filepath) and os.path.getsize(filepath) > 0:\n",
    "        existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
    "        print(f\"Loaded existing {pair} {tf_name}, {existing_df.shape[0]} rows\")\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "        print(f\"No existing data for {pair} {tf_name}\")\n",
    "\n",
    "    old_hash = file_hash(filepath)\n",
    "\n",
    "    # Retry loop\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            df = yf.download(symbol, period=period, interval=interval, progress=False, auto_adjust=False, threads=True)\n",
    "            if df.empty:\n",
    "                raise ValueError(\"No data returned from yfinance\")\n",
    "\n",
    "            # Keep only required columns if they exist\n",
    "            cols_to_keep = [col for col in ['Open', 'High', 'Low', 'Close', 'Volume'] if col in df.columns]\n",
    "            if not all(col in df.columns for col in ['Open', 'High', 'Low', 'Close']):\n",
    "                raise ValueError(f\"Missing OHLC columns: {df.columns.tolist()}\")\n",
    "\n",
    "            df = df[cols_to_keep].copy()\n",
    "            df.rename(columns=lambda x: x.lower(), inplace=True)\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "            # Merge with existing\n",
    "            if not existing_df.empty:\n",
    "                combined_df = pd.concat([existing_df, df])\n",
    "                combined_df = combined_df[~combined_df.index.duplicated(keep=\"last\")]\n",
    "            else:\n",
    "                combined_df = df\n",
    "\n",
    "            combined_df.sort_index(inplace=True)\n",
    "            combined_df.to_csv(filepath)\n",
    "\n",
    "            new_hash = file_hash(filepath)\n",
    "            if old_hash != new_hash:\n",
    "                return f\"📌 Updated {pair} {tf_name}\", filename\n",
    "            else:\n",
    "                return f\"✅ No changes for {pair} {tf_name}\", None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Attempt {attempt}/{max_retries} failed for {pair} {tf_name}: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                return f\"❌ Failed {pair} {tf_name} after {max_retries} retries: {e}\", None\n",
    "\n",
    "# -----------------------------\n",
    "# Run all downloads in parallel\n",
    "# -----------------------------\n",
    "changed_files = []\n",
    "tasks = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for pair in fx_pairs:\n",
    "        for tf_name, (interval, period) in timeframes.items():\n",
    "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period))\n",
    "\n",
    "for future in as_completed(tasks):\n",
    "    msg, filename = future.result()\n",
    "    print(msg)\n",
    "    if filename:\n",
    "        changed_files.append(filename)\n",
    "\n",
    "# -----------------------------\n",
    "# Commit & push once if needed\n",
    "# -----------------------------\n",
    "if changed_files:\n",
    "    print(f\"🚀 Committing {len(changed_files)} updated files in one push...\")\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"add\"] + changed_files, check=False)\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"commit\", \"-m\", \"Update multiple FX files\"], check=False)\n",
    "    subprocess.run(f\"git -C {SAVE_FOLDER} push https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git {BRANCH}\", shell=True)\n",
    "else:\n",
    "    print(\"✅ No changes detected, nothing to push.\")\n",
    "\n",
    "print(\"🎯 All yfinance pairs & timeframes processed (parallel, single push).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c70399f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:16:13.548661Z",
     "iopub.status.busy": "2025-10-05T15:16:13.548488Z",
     "iopub.status.idle": "2025-10-05T15:16:16.132950Z",
     "shell.execute_reply": "2025-10-05T15:16:16.132358Z"
    },
    "id": "JvVIc-mYKTg5",
    "papermill": {
     "duration": 2.595408,
     "end_time": "2025-10-05T15:16:16.133853",
     "exception": false,
     "start_time": "2025-10-05T15:16:13.538445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/rahim-dotAI/forex-ai-models\n",
      "   4b3a980..aaa5276  main       -> origin/main\n",
      "Already on 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/rahim-dotAI/forex-ai-models\n",
      " * branch            main       -> FETCH_HEAD\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "\"*_combined.csv\" already supported\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\tforex_ai_outputs/\n",
      "\toutput/\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n",
      "No changes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
      "/tmp/ipykernel_2372/27089880.py:97: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes detected for EUR/USD 5m_1mo, saved combined CSV\n",
      "Changes detected for USD/JPY 5m_1mo, saved combined CSV\n",
      "Changes detected for GBP/USD 1m_7d, saved combined CSV\n",
      "Changes detected for EUR/USD 1m_7d, saved combined CSV\n",
      "Changes detected for USD/JPY 15m_60d, saved combined CSV\n",
      "Changes detected for USD/JPY 1m_7d, saved combined CSV\n",
      "Changes detected for EUR/USD 15m_60d, saved combined CSV\n",
      "Changes detected for AUD/USD 15m_60d, saved combined CSV\n",
      "Changes detected for GBP/USD 1d_5y, saved combined CSV\n",
      "Changes detected for AUD/USD 1m_7d, saved combined CSV\n",
      "Changes detected for GBP/USD 5m_1mo, saved combined CSV\n",
      "Changes detected for EUR/USD 1h_2y, saved combined CSV\n",
      "Changes detected for GBP/USD 1h_2y, saved combined CSV\n",
      "Changes detected for AUD/USD 5m_1mo, saved combined CSV\n",
      "Changes detected for EUR/USD 1d_5y, saved combined CSV\n",
      "Changes detected for AUD/USD 1d_5y, saved combined CSV\n",
      "Changes detected for USD/JPY 1d_5y, saved combined CSV\n",
      "Changes detected for USD/JPY 1h_2y, saved combined CSV\n",
      "Changes detected for AUD/USD 1h_2y, saved combined CSV\n",
      "Changes detected for GBP/USD 15m_60d, saved combined CSV\n",
      "🚀 Committing 20 combined files in one push...\n",
      "[main 3c55edb] Update combined FX data\n",
      " 20 files changed, 40 insertions(+), 40 deletions(-)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All FX pairs processed and combined (parallel, single push).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/rahim-dotAI/forex-ai-models.git\n",
      "   aaa5276..3c55edb  main -> main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "SAVE_FOLDER = \"/content/forex-ai-models\"\n",
    "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GIT_NAME = \"Abdul Rahim\"\n",
    "GIT_EMAIL = \"nakatonabira3@gmail.com\"\n",
    "\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
    "\n",
    "GITHUB_USERNAME = \"rahim-dotAI\"\n",
    "GITHUB_REPO = \"forex-ai-models\"\n",
    "GITHUB_TOKEN = os.environ.get(\"FOREX_PAT\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"❌ Token not set! Define environment variable FOREX_PAT in Colab or GitHub Actions.\")\n",
    "BRANCH = \"main\"\n",
    "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure repo exists\n",
    "# -----------------------------\n",
    "def ensure_repo():\n",
    "    if not os.path.exists(os.path.join(SAVE_FOLDER, \".git\")):\n",
    "        if os.path.exists(SAVE_FOLDER):\n",
    "            subprocess.run([\"rm\", \"-rf\", SAVE_FOLDER], check=True)\n",
    "        subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, SAVE_FOLDER], check=True)\n",
    "    else:\n",
    "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"fetch\", \"origin\"], check=True)\n",
    "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"checkout\", BRANCH], check=False)\n",
    "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"pull\", \"origin\", BRANCH], check=False)\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"lfs\", \"track\", \"*_combined.csv\"], check=False)\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"add\", \".gitattributes\"], check=False)\n",
    "    subprocess.run(f'git -C {SAVE_FOLDER} commit -m \"Track combined CSVs with Git LFS\" || echo \"No changes\"', shell=True)\n",
    "\n",
    "ensure_repo()\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def file_hash(filepath, chunk_size=8192):\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "def combine_fx_data(av_df, yf_df):\n",
    "    if av_df is None or av_df.empty:\n",
    "        return yf_df\n",
    "    if yf_df is None or yf_df.empty:\n",
    "        return av_df\n",
    "\n",
    "    for df in [av_df, yf_df]:\n",
    "        if df.index.tz is not None:\n",
    "            df.index = df.index.tz_convert(None)\n",
    "\n",
    "    combined_df = pd.merge(\n",
    "        yf_df, av_df[['open','high','low','close']],\n",
    "        left_index=True, right_index=True,\n",
    "        how='outer', suffixes=('','_av')\n",
    "    )\n",
    "\n",
    "    for col in ['open','high','low','close']:\n",
    "        combined_df[col] = combined_df[col].fillna(combined_df[f'{col}_av'])\n",
    "    combined_df.drop(columns=[f'{col}_av' for col in ['open','high','low','close']], errors='ignore', inplace=True)\n",
    "    combined_df.sort_index(inplace=True)\n",
    "    combined_df.dropna(subset=['open','high','low','close'], inplace=True)\n",
    "    for col in ['open','high','low','close']:\n",
    "        combined_df[col] = combined_df[col].astype(float)\n",
    "    return combined_df\n",
    "\n",
    "# -----------------------------\n",
    "# Worker function\n",
    "# -----------------------------\n",
    "def process_pair_tf(pair, tf_name, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            av_file = os.path.join(SAVE_FOLDER, pair.replace(\"/\",\"_\")+\"_daily.csv\")\n",
    "            av_df = pd.read_csv(av_file, index_col=0, parse_dates=True) if os.path.exists(av_file) else pd.DataFrame()\n",
    "\n",
    "            yf_file = os.path.join(SAVE_FOLDER, f\"{pair.replace('/','_')}_{tf_name}.csv\")\n",
    "            yf_df = pd.read_csv(yf_file, index_col=0, parse_dates=True) if os.path.exists(yf_file) else pd.DataFrame()\n",
    "\n",
    "            combined_df = combine_fx_data(av_df, yf_df)\n",
    "            if combined_df.empty:\n",
    "                return f\"No data to combine for {pair} {tf_name}\", None\n",
    "\n",
    "            combined_file = os.path.join(SAVE_FOLDER, f\"{pair.replace('/','_')}_{tf_name}_combined.csv\")\n",
    "            old_hash = file_hash(combined_file)\n",
    "\n",
    "            with lock:\n",
    "                combined_df.to_csv(combined_file)\n",
    "\n",
    "            new_hash = file_hash(combined_file)\n",
    "            if old_hash != new_hash:\n",
    "                return f\"Changes detected for {pair} {tf_name}, saved combined CSV\", combined_file\n",
    "            else:\n",
    "                return f\"No changes for {pair} {tf_name}, skipping save\", None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Attempt {attempt+1} failed for {pair} {tf_name}: {e}\")\n",
    "            time.sleep(2)\n",
    "    return f\"❌ Failed to combine {pair} {tf_name} after {max_retries} retries\", None\n",
    "\n",
    "# -----------------------------\n",
    "# Run in parallel\n",
    "# -----------------------------\n",
    "pairs = [\"EUR/USD\",\"GBP/USD\",\"USD/JPY\",\"AUD/USD\"]\n",
    "timeframes = [\"1m_7d\", \"5m_1mo\", \"15m_60d\", \"1h_2y\", \"1d_5y\"]\n",
    "\n",
    "changed_files = []\n",
    "tasks = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for pair in pairs:\n",
    "        for tf_name in timeframes:\n",
    "            tasks.append(executor.submit(process_pair_tf, pair, tf_name))\n",
    "\n",
    "for future in as_completed(tasks):\n",
    "    msg, filename = future.result()\n",
    "    print(msg)\n",
    "    if filename:\n",
    "        changed_files.append(filename)\n",
    "\n",
    "# -----------------------------\n",
    "# Commit & push\n",
    "# -----------------------------\n",
    "if changed_files:\n",
    "    print(f\"🚀 Committing {len(changed_files)} combined files in one push...\")\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"add\"] + changed_files, check=False)\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"commit\", \"-m\", \"Update combined FX data\"], check=False)\n",
    "    subprocess.run(f\"git -C {SAVE_FOLDER} push https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git {BRANCH}\", shell=True)\n",
    "else:\n",
    "    print(\"✅ No combined files changed, nothing to push.\")\n",
    "\n",
    "print(\"✅ All FX pairs processed and combined (parallel, single push).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "defac033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:16:16.154302Z",
     "iopub.status.busy": "2025-10-05T15:16:16.154133Z",
     "iopub.status.idle": "2025-10-05T15:16:24.058734Z",
     "shell.execute_reply": "2025-10-05T15:16:24.058223Z"
    },
    "id": "z96OOkCQSzof",
    "papermill": {
     "duration": 7.91552,
     "end_time": "2025-10-05T15:16:24.059547",
     "exception": false,
     "start_time": "2025-10-05T15:16:16.144027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR/USD: 1.173\n",
      "GBP/USD: 1.347\n",
      "USD/JPY: 147.37\n",
      "AUD/USD: 0.6606\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def fetch_live_rate(pair):\n",
    "    \"\"\"\n",
    "    Fetch live FX rate from X-Rates using Browserless.\n",
    "    \"\"\"\n",
    "    from_currency, to_currency = pair.split('/')\n",
    "    browserless_token = os.environ.get('BROWSERLESS_TOKEN')\n",
    "    if not browserless_token:\n",
    "        raise ValueError(\"Set BROWSERLESS_TOKEN in your environment variables\")\n",
    "\n",
    "    url = f\"https://production-sfo.browserless.io/content?token={browserless_token}\"\n",
    "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
    "\n",
    "    try:\n",
    "        res = requests.post(url, json=payload)\n",
    "        # Regex to extract the FX value\n",
    "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
    "        return float(match.group(1).replace(',', '')) if match else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {pair}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# --- Fetch live prices for all pairs ---\n",
    "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "live_prices = {pair: fetch_live_rate(pair) for pair in pairs}\n",
    "\n",
    "for pair, price in live_prices.items():\n",
    "    print(f\"{pair}: {price}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb65898b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:16:24.080407Z",
     "iopub.status.busy": "2025-10-05T15:16:24.080214Z",
     "iopub.status.idle": "2025-10-05T15:16:31.851877Z",
     "shell.execute_reply": "2025-10-05T15:16:31.851274Z"
    },
    "id": "gxBvtCPm6eK4",
    "papermill": {
     "duration": 7.783094,
     "end_time": "2025-10-05T15:16:31.852807",
     "exception": false,
     "start_time": "2025-10-05T15:16:24.069713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Repo found, updating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/rahim-dotAI/forex-ai-models\n",
      "   aaa5276..3c55edb  main       -> origin/main\n",
      "Already on 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/rahim-dotAI/forex-ai-models\n",
      " * branch            main       -> FETCH_HEAD\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "EUR/USD (EUR_USD.csv) updated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP/USD (GBP_USD.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP/USD (GBP_USD_1d_5y.csv) updated\n",
      "EUR/USD (EUR_USD_15m_60d.csv) updated\n",
      "EUR/USD (EUR_USD_1d_5y.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR/USD (EUR_USD_1h_2y.csv) updated\n",
      "GBP/USD (GBP_USD_1h_2y.csv) updated\n",
      "GBP/USD (GBP_USD_5m_1mo.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USD/JPY (USD_JPY.csv) updated\n",
      "USD/JPY (USD_JPY_5m_1mo.csv) updated\n",
      "EUR/USD (EUR_USD_5m_1mo.csv) updated\n",
      "USD/JPY (USD_JPY_15m_60d.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USD/JPY (USD_JPY_1m_7d.csv) updated\n",
      "GBP/USD (GBP_USD_15m_60d.csv) updated\n",
      "EUR/USD (EUR_USD_1m_7d.csv) updated\n",
      "USD/JPY (USD_JPY_1h_2y.csv) updated\n",
      "AUD/USD (AUD_USD.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/4244329357.py:223: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(path, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USD/JPY (USD_JPY_1d_5y.csv) updated\n",
      "AUD/USD (AUD_USD_1m_7d.csv) updated\n",
      "GBP/USD (GBP_USD_1m_7d.csv) updated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUD/USD (AUD_USD_5m_1mo.csv) updated\n",
      "AUD/USD (AUD_USD_15m_60d.csv) updated\n",
      "AUD/USD (AUD_USD_1d_5y.csv) updated\n",
      "AUD/USD (AUD_USD_1h_2y.csv) updated\n",
      "🚀 Committing 24 modified files...\n",
      "[main 05722c9] 📈 Auto update combined indicators\n",
      " 24 files changed, 68 insertions(+), 8 deletions(-)\n",
      " create mode 100644 combined_with_indicators/AUD_USD_AUD_USD_15m_60d_combined.pkl\n",
      " create mode 100644 combined_with_indicators/AUD_USD_AUD_USD_1d_5y_combined.pkl\n",
      " create mode 100644 combined_with_indicators/AUD_USD_AUD_USD_1h_2y_combined.pkl\n",
      " create mode 100644 combined_with_indicators/AUD_USD_AUD_USD_1m_7d_combined.pkl\n",
      " create mode 100644 combined_with_indicators/AUD_USD_AUD_USD_5m_1mo_combined.pkl\n",
      " create mode 100644 combined_with_indicators/EUR_USD_EUR_USD_15m_60d_combined.pkl\n",
      " create mode 100644 combined_with_indicators/EUR_USD_EUR_USD_1d_5y_combined.pkl\n",
      " create mode 100644 combined_with_indicators/EUR_USD_EUR_USD_1h_2y_combined.pkl\n",
      " create mode 100644 combined_with_indicators/EUR_USD_EUR_USD_1m_7d_combined.pkl\n",
      " create mode 100644 combined_with_indicators/EUR_USD_EUR_USD_5m_1mo_combined.pkl\n",
      " create mode 100644 combined_with_indicators/GBP_USD_GBP_USD_15m_60d_combined.pkl\n",
      " create mode 100644 combined_with_indicators/GBP_USD_GBP_USD_1d_5y_combined.pkl\n",
      " create mode 100644 combined_with_indicators/GBP_USD_GBP_USD_1h_2y_combined.pkl\n",
      " create mode 100644 combined_with_indicators/GBP_USD_GBP_USD_1m_7d_combined.pkl\n",
      " create mode 100644 combined_with_indicators/GBP_USD_GBP_USD_5m_1mo_combined.pkl\n",
      " create mode 100644 combined_with_indicators/USD_JPY_USD_JPY_15m_60d_combined.pkl\n",
      " create mode 100644 combined_with_indicators/USD_JPY_USD_JPY_1d_5y_combined.pkl\n",
      " create mode 100644 combined_with_indicators/USD_JPY_USD_JPY_1h_2y_combined.pkl\n",
      " create mode 100644 combined_with_indicators/USD_JPY_USD_JPY_1m_7d_combined.pkl\n",
      " create mode 100644 combined_with_indicators/USD_JPY_USD_JPY_5m_1mo_combined.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading LFS objects: 100% (11/11), 10 MB | 0 B/s, done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Push complete.\n",
      "✅ All FX pairs processed and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/rahim-dotAI/forex-ai-models.git\n",
      "   3c55edb..05722c9  main -> main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import ta\n",
    "from ta.momentum import WilliamsRIndicator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# ======================================================\n",
    "# CONFIGURATION\n",
    "# ======================================================\n",
    "SAVE_FOLDER = \"/content/forex-ai-models\"\n",
    "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GIT_NAME = \"Abdul Rahim\"\n",
    "GIT_EMAIL = \"nakatonabira3@gmail.com\"\n",
    "\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
    "\n",
    "combined_save_path = os.path.join(SAVE_FOLDER, \"combined_with_indicators\")\n",
    "Path(combined_save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "\n",
    "# ======================================================\n",
    "# GITHUB AUTH CONFIGURATION\n",
    "# ======================================================\n",
    "GITHUB_USERNAME = \"rahim-dotAI\"\n",
    "GITHUB_REPO = \"forex-ai-models\"\n",
    "GITHUB_TOKEN = os.environ.get(\"FOREX_PAT\")\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"❌ Missing FOREX_PAT environment variable. Set it in Colab or GitHub Actions.\")\n",
    "\n",
    "BRANCH = \"main\"\n",
    "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
    "\n",
    "# ======================================================\n",
    "# ENSURE REPO EXISTS OR SYNCED\n",
    "# ======================================================\n",
    "def ensure_repo():\n",
    "    if not os.path.exists(os.path.join(SAVE_FOLDER, \".git\")):\n",
    "        print(\"📥 Cloning fresh repository...\")\n",
    "        subprocess.run([\"rm\", \"-rf\", SAVE_FOLDER], check=False)\n",
    "        subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, SAVE_FOLDER], check=True)\n",
    "    else:\n",
    "        print(\"🔄 Repo found, updating...\")\n",
    "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"fetch\", \"origin\"], check=False)\n",
    "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"checkout\", BRANCH], check=False)\n",
    "        subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"pull\", \"origin\", BRANCH], check=False)\n",
    "\n",
    "ensure_repo()\n",
    "\n",
    "# ======================================================\n",
    "# UTILITIES\n",
    "# ======================================================\n",
    "def file_hash(filepath, chunk_size=8192):\n",
    "    \"\"\"Compute MD5 hash for file to detect changes.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "\n",
    "def safe_numeric(df, columns=None):\n",
    "    \"\"\"Ensure numeric columns are properly typed to avoid aggregation errors.\"\"\"\n",
    "    if columns is None:\n",
    "        columns = ['open', 'high', 'low', 'close']\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(subset=columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def combine_fx_data(av_df, yf_df):\n",
    "    \"\"\"Combine AlphaVantage + Yahoo Finance data safely.\"\"\"\n",
    "    if av_df is None or av_df.empty:\n",
    "        return safe_numeric(yf_df)\n",
    "    if yf_df is None or yf_df.empty:\n",
    "        return safe_numeric(av_df)\n",
    "\n",
    "    if av_df.index.tz is not None:\n",
    "        av_df.index = av_df.index.tz_convert(None)\n",
    "    if yf_df.index.tz is not None:\n",
    "        yf_df.index = yf_df.index.tz_convert(None)\n",
    "\n",
    "    combined_df = pd.merge(\n",
    "        yf_df,\n",
    "        av_df[['open', 'high', 'low', 'close']],\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='outer',\n",
    "        suffixes=('', '_av')\n",
    "    )\n",
    "\n",
    "    for col in ['open', 'high', 'low', 'close']:\n",
    "        combined_df[col] = combined_df[col].fillna(combined_df[f'{col}_av'])\n",
    "\n",
    "    combined_df.drop(columns=[f'{col}_av' for col in ['open', 'high', 'low', 'close']], inplace=True, errors='ignore')\n",
    "    combined_df.sort_index(inplace=True)\n",
    "    combined_df = safe_numeric(combined_df)\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def add_all_indicators(df):\n",
    "    \"\"\"Add full indicator suite, with automatic numeric cleanup.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "\n",
    "    df = safe_numeric(df.copy())\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # ---------------- Trend Indicators ----------------\n",
    "    trend = {\n",
    "        'SMA_10': lambda d: ta.trend.sma_indicator(d['close'], 10),\n",
    "        'SMA_50': lambda d: ta.trend.sma_indicator(d['close'], 50),\n",
    "        'SMA_200': lambda d: ta.trend.sma_indicator(d['close'], 200),\n",
    "        'EMA_10': lambda d: ta.trend.ema_indicator(d['close'], 10),\n",
    "        'EMA_50': lambda d: ta.trend.ema_indicator(d['close'], 50),\n",
    "        'EMA_200': lambda d: ta.trend.ema_indicator(d['close'], 200),\n",
    "        'MACD': lambda d: ta.trend.macd(d['close']),\n",
    "        'MACD_signal': lambda d: ta.trend.macd_signal(d['close']),\n",
    "        'ADX': lambda d: ta.trend.adx(d['high'], d['low'], d['close'], 14)\n",
    "    }\n",
    "\n",
    "    # ---------------- Momentum Indicators ----------------\n",
    "    momentum = {\n",
    "        'RSI_14': lambda d: ta.momentum.rsi(d['close'], 14),\n",
    "        'StochRSI': lambda d: ta.momentum.stochrsi(d['close'], 14),\n",
    "        'CCI': lambda d: ta.trend.cci(d['high'], d['low'], d['close'], 20),\n",
    "        'ROC': lambda d: ta.momentum.roc(d['close'], 12),\n",
    "        'Williams_%R': lambda d: WilliamsRIndicator(d['high'], d['low'], d['close'], 14).williams_r()\n",
    "    }\n",
    "\n",
    "    # ---------------- Volatility Indicators ----------------\n",
    "    volatility = {\n",
    "        'Bollinger_High': lambda d: ta.volatility.bollinger_hband(d['close'], 20, 2),\n",
    "        'Bollinger_Low': lambda d: ta.volatility.bollinger_lband(d['close'], 20, 2),\n",
    "        'ATR': lambda d: ta.volatility.average_true_range(d['high'], d['low'], d['close'], 14),\n",
    "        'STDDEV_20': lambda d: d['close'].rolling(20).std()\n",
    "    }\n",
    "\n",
    "    # ---------------- Volume Indicators ----------------\n",
    "    volume = {}\n",
    "    if 'volume' in df.columns:\n",
    "        volume = {\n",
    "            'OBV': lambda d: ta.volume.on_balance_volume(d['close'], d['volume']),\n",
    "            'MFI': lambda d: ta.volume.money_flow_index(d['high'], d['low'], d['close'], d['volume'], 14)\n",
    "        }\n",
    "\n",
    "    indicators = {**trend, **momentum, **volatility, **volume}\n",
    "    for name, func in indicators.items():\n",
    "        try:\n",
    "            df[name] = func(df)\n",
    "        except Exception:\n",
    "            df[name] = np.nan\n",
    "\n",
    "    # ---------------- Cross + Derived Signals ----------------\n",
    "    df['EMA_10_cross_EMA_50'] = (df['EMA_10'] > df['EMA_50']).astype(int)\n",
    "    df['EMA_50_cross_EMA_200'] = (df['EMA_50'] > df['EMA_200']).astype(int)\n",
    "    df['SMA_10_cross_SMA_50'] = (df['SMA_10'] > df['SMA_50']).astype(int)\n",
    "    df['SMA_50_cross_SMA_200'] = (df['SMA_50'] > df['SMA_200']).astype(int)\n",
    "\n",
    "    df['recent_high'] = df['high'].rolling(20).max()\n",
    "    df['recent_low'] = df['low'].rolling(20).min()\n",
    "    df['pivot_point'] = (df['high'] + df['low'] + df['close']) / 3\n",
    "    df['support_1'] = 2 * df['pivot_point'] - df['high']\n",
    "    df['resistance_1'] = 2 * df['pivot_point'] - df['low']\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.ffill(inplace=True)\n",
    "    df.bfill(inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    df['long_score'] = (\n",
    "        df['EMA_10_cross_EMA_50']\n",
    "        + df['EMA_50_cross_EMA_200']\n",
    "        + df['SMA_10_cross_SMA_50'] * 0.5\n",
    "        + df['SMA_50_cross_SMA_200'] * 0.5\n",
    "        + df['ADX']\n",
    "        + df['RSI_14']\n",
    "    )\n",
    "\n",
    "    df['short_score'] = (\n",
    "        (1 - df['EMA_10_cross_EMA_50'])\n",
    "        + (1 - df['EMA_50_cross_EMA_200'])\n",
    "        + (1 - df['SMA_10_cross_SMA_50']) * 0.5\n",
    "        + (1 - df['SMA_50_cross_SMA_200']) * 0.5\n",
    "        + (1 - df['ADX'])\n",
    "        + (1 - df['RSI_14'])\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# ======================================================\n",
    "# WORKER FUNCTION\n",
    "# ======================================================\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_pair_file(pair, tf_file, max_retries=3):\n",
    "    av_file = os.path.join(SAVE_FOLDER, f\"{pair.replace('/','_')}_daily.csv\")\n",
    "\n",
    "    def safe_read_csv(path):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                if os.path.exists(path):\n",
    "                    return pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Retry {attempt+1} reading {path}: {e}\")\n",
    "                time.sleep(2)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    av_df = safe_read_csv(av_file)\n",
    "    yf_path = os.path.join(SAVE_FOLDER, tf_file)\n",
    "    yf_df = safe_read_csv(yf_path)\n",
    "\n",
    "    if yf_df.empty and av_df.empty:\n",
    "        return None, f\"{pair} ({tf_file}) skipped - no data\"\n",
    "\n",
    "    combined_df = combine_fx_data(av_df, yf_df)\n",
    "    if combined_df.empty:\n",
    "        return None, f\"{pair} ({tf_file}) empty after combine\"\n",
    "\n",
    "    combined_df = add_all_indicators(combined_df)\n",
    "\n",
    "    save_file = os.path.join(combined_save_path, f\"{pair.replace('/','_')}_{tf_file.replace('.csv','')}_combined.pkl\")\n",
    "    old_hash = file_hash(save_file)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with lock:\n",
    "                combined_df.to_pickle(save_file, protocol=4)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Retry {attempt+1} writing {save_file}: {e}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    new_hash = file_hash(save_file)\n",
    "    changed = old_hash != new_hash\n",
    "    return save_file if changed else None, f\"{pair} ({tf_file}) {'updated' if changed else 'no change'}\"\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# EXECUTE IN PARALLEL\n",
    "# ======================================================\n",
    "changed_files = []\n",
    "tasks = []\n",
    "files_to_process = []\n",
    "\n",
    "for pair in pairs:\n",
    "    for tf_file in os.listdir(SAVE_FOLDER):\n",
    "        if not tf_file.startswith(pair.replace('/','_')):\n",
    "            continue\n",
    "        if any(tf_file.endswith(x) for x in [\"daily.csv\", \"_combined.csv\", \"_combined.pkl\"]):\n",
    "            continue\n",
    "        files_to_process.append((pair, tf_file))\n",
    "\n",
    "max_workers = max(1, min(8, len(files_to_process), (os.cpu_count() or 4) * 2))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    for pair, tf_file in files_to_process:\n",
    "        tasks.append(executor.submit(process_pair_file, pair, tf_file))\n",
    "    for future in as_completed(tasks):\n",
    "        changed_file, msg = future.result()\n",
    "        print(msg)\n",
    "        if changed_file:\n",
    "            changed_files.append(changed_file)\n",
    "\n",
    "# ======================================================\n",
    "# COMMIT & PUSH ONCE\n",
    "# ======================================================\n",
    "if changed_files:\n",
    "    print(f\"🚀 Committing {len(changed_files)} modified files...\")\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"add\"] + changed_files, check=False)\n",
    "    subprocess.run([\"git\", \"-C\", SAVE_FOLDER, \"commit\", \"-m\", \"📈 Auto update combined indicators\"], check=False)\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            f\"git -C {SAVE_FOLDER} push https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git {BRANCH}\",\n",
    "            shell=True,\n",
    "            check=False\n",
    "        )\n",
    "        print(\"✅ Push complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Push failed, will retry next run: {e}\")\n",
    "else:\n",
    "    print(\"✅ No data changes detected — skipping push.\")\n",
    "\n",
    "print(\"✅ All FX pairs processed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d0a78d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:16:31.875866Z",
     "iopub.status.busy": "2025-10-05T15:16:31.875701Z",
     "iopub.status.idle": "2025-10-05T15:16:42.187058Z",
     "shell.execute_reply": "2025-10-05T15:16:42.186384Z"
    },
    "id": "T4ttBGtSvTEw",
    "papermill": {
     "duration": 10.323965,
     "end_time": "2025-10-05T15:16:42.188028",
     "exception": false,
     "start_time": "2025-10-05T15:16:31.864063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Broker JSON saved: forex-ai-models/broker_signals.json\n",
      "📑 Signals logged to CSV: forex-ai-models/broker_signals_log.csv\n",
      "\n",
      "📊 Latest Signals JSON:\n",
      " {\n",
      "  \"timestamp\": \"2025-10-05T15:16:39.196940+00:00\",\n",
      "  \"pairs\": {\n",
      "    \"EUR/USD\": {\n",
      "      \"15m_60d\": {\n",
      "        \"long\": 50,\n",
      "        \"short\": 1251,\n",
      "        \"hold\": 0,\n",
      "        \"latest_signal\": -1,\n",
      "        \"live_price\": 1.173\n",
      "      },\n",
      "      \"1h_2y\": {\n",
      "        \"long\": 463,\n",
      "        \"short\": 838,\n",
      "        \"hold\": 0,\n",
      "        \"latest_signal\": -1,\n",
      "        \"live_price\": 1.173\n",
      "      },\n",
      "      \"5m_1mo\": {\n",
      "        \"long\": 186,\n",
      "        \"short\": 1115,\n",
      "        \"hold\": 0,\n",
      "        \"latest_signal\": -1,\n",
      "        \"live_price\": 1.173\n",
      "      },\n",
      "      \"1m_7d\": {\n",
      "        \"long\": 849,\n",
      "        \"short\": 452,\n",
      "        \"hold\": 0,\n",
      "        \"latest_signal\": 1,\n",
      "        \"live_price\": 1.173\n",
      "      },\n",
      "      \"1d_5y\": {\n",
      "        \"long\": 1091,\n",
      "        \"short\": 210,\n",
      "        \"hold\": 0,\n",
      "        \"latest_signal\": 1,\n",
      "        \"live_price\": 1.173\n",
      "      }\n",
      "    },\n",
      "    \"GBP/USD\": {\n",
      "      \"1m_7d\": {\n",
      "        \"long\": 607,\n",
      "        \"short\": 690,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 1.347\n",
      "      },\n",
      "      \"1d_5y\": {\n",
      "        \"long\": 705,\n",
      "        \"short\": 5472,\n",
      "        \"hold\": 11,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 1.347\n",
      "      },\n",
      "      \"1h_2y\": {\n",
      "        \"long\": 2456,\n",
      "        \"short\": 135,\n",
      "        \"hold\": 3597,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 1.347\n",
      "      },\n",
      "      \"15m_60d\": {\n",
      "        \"long\": 6151,\n",
      "        \"short\": 27,\n",
      "        \"hold\": 10,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 1.347\n",
      "      },\n",
      "      \"5m_1mo\": {\n",
      "        \"long\": 6111,\n",
      "        \"short\": 66,\n",
      "        \"hold\": 11,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 1.347\n",
      "      }\n",
      "    },\n",
      "    \"USD/JPY\": {\n",
      "      \"1m_7d\": {\n",
      "        \"long\": 6184,\n",
      "        \"short\": 0,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 147.37\n",
      "      },\n",
      "      \"5m_1mo\": {\n",
      "        \"long\": 230,\n",
      "        \"short\": 5954,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 147.37\n",
      "      },\n",
      "      \"15m_60d\": {\n",
      "        \"long\": 1794,\n",
      "        \"short\": 4378,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 147.37\n",
      "      },\n",
      "      \"1d_5y\": {\n",
      "        \"long\": 5517,\n",
      "        \"short\": 655,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 147.37\n",
      "      },\n",
      "      \"1h_2y\": {\n",
      "        \"long\": 1619,\n",
      "        \"short\": 4553,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 147.37\n",
      "      }\n",
      "    },\n",
      "    \"AUD/USD\": {\n",
      "      \"1m_7d\": {\n",
      "        \"long\": 9,\n",
      "        \"short\": 6163,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 0.6606\n",
      "      },\n",
      "      \"5m_1mo\": {\n",
      "        \"long\": 1,\n",
      "        \"short\": 9883,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 0.6606\n",
      "      },\n",
      "      \"15m_60d\": {\n",
      "        \"long\": 10,\n",
      "        \"short\": 9874,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 0.6606\n",
      "      },\n",
      "      \"1d_5y\": {\n",
      "        \"long\": 9809,\n",
      "        \"short\": 15,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 0.6606\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import ta\n",
    "from ta.momentum import WilliamsRIndicator\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import datetime as dt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "BASE_DIR = Path(\"forex-ai-models\")\n",
    "BASE_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BROKER_JSON = BASE_DIR / \"broker_signals.json\"\n",
    "BROKER_LOG = BASE_DIR / \"broker_signals_log.csv\"\n",
    "\n",
    "PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "TIMEFRAMES = {\n",
    "    \"1m_7d\": (\"1m\", \"7d\"),\n",
    "    \"5m_1mo\": (\"5m\", \"1mo\"),\n",
    "    \"15m_60d\": (\"15m\", \"60d\"),\n",
    "    \"1h_2y\": (\"1h\", \"2y\"),\n",
    "    \"1d_5y\": (\"1d\", \"5y\"),\n",
    "}\n",
    "INJECT_CANDLES = 5\n",
    "\n",
    "# Delete old models\n",
    "for f in MODEL_DIR.glob(\"*.pkl\"):\n",
    "    f.unlink()\n",
    "\n",
    "# -----------------------------\n",
    "# LIVE PRICE FETCH\n",
    "# -----------------------------\n",
    "def fetch_live_rate(pair):\n",
    "    from_currency, to_currency = pair.split('/')\n",
    "    browserless_token = os.environ.get('BROWSERLESS_TOKEN')\n",
    "    if not browserless_token:\n",
    "        print(f\"⚠️ BROWSERLESS_TOKEN not set, skipping live rates for {pair}\")\n",
    "        return 0\n",
    "    url = f\"https://production-sfo.browserless.io/content?token={browserless_token}\"\n",
    "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
    "    try:\n",
    "        res = requests.post(url, json=payload)\n",
    "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
    "        return float(match.group(1).replace(',', '')) if match else 0\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Live price fetch failed for {pair}: {e}\")\n",
    "        return 0\n",
    "\n",
    "live_prices = {pair: fetch_live_rate(pair) for pair in PAIRS}\n",
    "\n",
    "# -----------------------------\n",
    "# DATA & INDICATORS\n",
    "# -----------------------------\n",
    "def fetch_data(symbol, interval, period):\n",
    "    df = yf.download(symbol.replace('/', '') + \"=X\", interval=interval, period=period,\n",
    "                     progress=False, auto_adjust=True)\n",
    "    df.dropna(inplace=True)\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [col[0] for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def inject_live_price(df, live_price, n_candles=INJECT_CANDLES):\n",
    "    df_copy = df.copy()\n",
    "    n_inject = min(n_candles, len(df_copy))\n",
    "    for col in [\"Open\",\"High\",\"Low\",\"Close\"]:\n",
    "        if col in df_copy.columns:\n",
    "            df_copy.iloc[-n_inject:, df_copy.columns.get_loc(col)] = live_price\n",
    "    return df_copy\n",
    "\n",
    "def add_all_indicators(df):\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"SMA_10\"] = ta.trend.sma_indicator(df[\"Close\"], 10)\n",
    "    df[\"SMA_50\"] = ta.trend.sma_indicator(df[\"Close\"], 50)\n",
    "    df[\"EMA_10\"] = ta.trend.ema_indicator(df[\"Close\"], 10)\n",
    "    df[\"EMA_50\"] = ta.trend.ema_indicator(df[\"Close\"], 50)\n",
    "    df[\"EMA_200\"] = ta.trend.ema_indicator(df[\"Close\"], 200)\n",
    "    df[\"MACD\"] = ta.trend.macd(df[\"Close\"])\n",
    "    df[\"MACD_signal\"] = ta.trend.macd_signal(df[\"Close\"])\n",
    "    df[\"ADX\"] = ta.trend.adx(df[\"High\"], df[\"Low\"], df[\"Close\"], window=14)\n",
    "    df[\"WilliamsR\"] = WilliamsRIndicator(df[\"High\"], df[\"Low\"], df[\"Close\"], lbp=14).williams_r()\n",
    "    df[\"Bollinger_High\"] = ta.volatility.bollinger_hband(df[\"Close\"], window=20)\n",
    "    df[\"Bollinger_Low\"] = ta.volatility.bollinger_lband(df[\"Close\"], window=20)\n",
    "    df[\"ATR\"] = ta.volatility.average_true_range(df[\"High\"], df[\"Low\"], df[\"Close\"], window=14)\n",
    "\n",
    "    # ML features\n",
    "    close = df[\"Close\"].values\n",
    "    df[\"return\"] = np.concatenate([[0], (close[1:] - close[:-1])/close[:-1]])\n",
    "    df[\"ma_fast\"] = pd.Series(close).rolling(5).mean()\n",
    "    df[\"ma_slow\"] = pd.Series(close).rolling(20).mean()\n",
    "    delta = np.diff(close, prepend=close[0])\n",
    "    gain = np.where(delta>0, delta, 0)\n",
    "    loss = np.where(delta<0, -delta, 0)\n",
    "    avg_gain = pd.Series(gain).rolling(14).mean()\n",
    "    avg_loss = pd.Series(loss).rolling(14).mean()\n",
    "    rs = avg_gain / avg_loss.replace(0, 1e-9)\n",
    "    df[\"rsi\"] = 100 - (100/(1+rs))\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "def generate_features(df):\n",
    "    df_features = add_all_indicators(df)\n",
    "    return df_features.select_dtypes(include=[np.number])\n",
    "\n",
    "def make_labels(df):\n",
    "    future = df[\"Close\"].shift(-1)\n",
    "    signal = np.where(future>df[\"Close\"], 1, np.where(future<df[\"Close\"], -1, 0))\n",
    "    return signal[:-1]\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL TRAIN/LOAD\n",
    "# -----------------------------\n",
    "def train_or_load_model(features, labels, model_path):\n",
    "    if features.empty or len(labels)==0:\n",
    "        return None, []\n",
    "    model = SGDClassifier(loss=\"log_loss\", max_iter=1000, tol=1e-3)\n",
    "    model.fit(features, labels)\n",
    "    joblib.dump({\"model\": model, \"features\": list(features.columns)}, model_path)\n",
    "    return model, list(features.columns)\n",
    "\n",
    "def hybrid_signal(model, features, trained_features):\n",
    "    if model is None or features.empty:\n",
    "        return pd.Series([0]*len(features), index=features.index)\n",
    "    for col in trained_features:\n",
    "        if col not in features.columns:\n",
    "            features[col] = 0\n",
    "    features_aligned = features[trained_features].copy()\n",
    "    features_aligned.fillna(0, inplace=True)\n",
    "    preds = model.predict(features_aligned)\n",
    "    return pd.Series(preds, index=features.index)\n",
    "\n",
    "# -----------------------------\n",
    "# PROCESS ONE PAIR/TIMEFRAME\n",
    "# -----------------------------\n",
    "def process_pair_tf(pair, tf_name, interval, period, live_price):\n",
    "    df = fetch_data(pair, interval, period)\n",
    "    if len(df)<2:\n",
    "        return None, None, None\n",
    "    df_live = inject_live_price(df, live_price)\n",
    "    features = generate_features(df_live)\n",
    "    labels = make_labels(df_live)\n",
    "    features = features.iloc[:len(labels)]\n",
    "    model_path = MODEL_DIR / f\"{pair.replace('/','_')}_{tf_name}.pkl\"\n",
    "    model, trained_features = train_or_load_model(features, labels, model_path)\n",
    "    signals = hybrid_signal(model, features, trained_features)\n",
    "    df_live = df_live.iloc[:len(signals)].copy()\n",
    "    df_live[\"hybrid_signal\"] = signals.values\n",
    "    latest_signal = int(df_live[\"hybrid_signal\"].iloc[-1])\n",
    "    long_count = int((df_live[\"hybrid_signal\"]==1).sum())\n",
    "    short_count = int((df_live[\"hybrid_signal\"]==-1).sum())\n",
    "    hold_count = int((df_live[\"hybrid_signal\"]==0).sum())\n",
    "    result = {\n",
    "        \"pair\": pair,\n",
    "        \"timeframe\": tf_name,\n",
    "        \"long\": long_count,\n",
    "        \"short\": short_count,\n",
    "        \"hold\": hold_count,\n",
    "        \"latest_signal\": latest_signal,\n",
    "        \"live_price\": live_price\n",
    "    }\n",
    "    return result, df, df_live\n",
    "\n",
    "# -----------------------------\n",
    "# RUN HYBRID PIPELINE\n",
    "# -----------------------------\n",
    "def run_hybrid():\n",
    "    broker_output = {\"timestamp\": dt.datetime.now(dt.timezone.utc).isoformat(), \"pairs\": {}}\n",
    "    log_rows = []\n",
    "    tasks = []\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        for pair in PAIRS:\n",
    "            live_price = live_prices[pair]\n",
    "            broker_output[\"pairs\"][pair] = {}\n",
    "            for tf_name, (interval, period) in TIMEFRAMES.items():\n",
    "                tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period, live_price))\n",
    "        for future in as_completed(tasks):\n",
    "            result, df, df_live = future.result()\n",
    "            if result is None:\n",
    "                continue\n",
    "            pair, tf_name = result[\"pair\"], result[\"timeframe\"]\n",
    "            broker_output[\"pairs\"][pair][tf_name] = {\n",
    "                \"long\": result[\"long\"],\n",
    "                \"short\": result[\"short\"],\n",
    "                \"hold\": result[\"hold\"],\n",
    "                \"latest_signal\": result[\"latest_signal\"],\n",
    "                \"live_price\": float(result[\"live_price\"])\n",
    "            }\n",
    "            log_rows.append({\n",
    "                \"timestamp\": dt.datetime.now(dt.timezone.utc),\n",
    "                \"pair\": pair,\n",
    "                \"timeframe\": tf_name,\n",
    "                **{k: result[k] for k in [\"long\",\"short\",\"hold\",\"latest_signal\"]},\n",
    "                \"live_price\": result[\"live_price\"]\n",
    "            })\n",
    "    # Save JSON\n",
    "    with open(BROKER_JSON, \"w\") as f:\n",
    "        json.dump(broker_output, f, indent=2)\n",
    "    # Append CSV log\n",
    "    log_df = pd.DataFrame(log_rows)\n",
    "    if not BROKER_LOG.exists():\n",
    "        log_df.to_csv(BROKER_LOG, index=False)\n",
    "    else:\n",
    "        log_df.to_csv(BROKER_LOG, mode=\"a\", header=False, index=False)\n",
    "    print(f\"💾 Broker JSON saved: {BROKER_JSON}\")\n",
    "    print(f\"📑 Signals logged to CSV: {BROKER_LOG}\")\n",
    "    return broker_output\n",
    "\n",
    "# -----------------------------\n",
    "# RUN SCRIPT\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    signals = run_hybrid()\n",
    "    print(\"\\n📊 Latest Signals JSON:\\n\", json.dumps(signals, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9719208d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:16:42.278579Z",
     "iopub.status.busy": "2025-10-05T15:16:42.266117Z",
     "iopub.status.idle": "2025-10-05T15:16:42.466499Z",
     "shell.execute_reply": "2025-10-05T15:16:42.465994Z"
    },
    "id": "qyFRcSiOmP4I",
    "papermill": {
     "duration": 0.26718,
     "end_time": "2025-10-05T15:16:42.467336",
     "exception": false,
     "start_time": "2025-10-05T15:16:42.200156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to process /content/forex-ai-models/EUR_USD_1d_5y_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/EUR_USD_1m_7d_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/EUR_USD_1h_2y_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/EUR_USD_15m_60d_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/EUR_USD_5m_1mo_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/GBP_USD_5m_1mo_combined.csv: No numeric types to aggregate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to process /content/forex-ai-models/GBP_USD_1m_7d_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/GBP_USD_1h_2y_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/GBP_USD_15m_60d_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/GBP_USD_1d_5y_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/USD_JPY_1d_5y_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/USD_JPY_1m_7d_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/USD_JPY_5m_1mo_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/USD_JPY_15m_60d_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/USD_JPY_1h_2y_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/AUD_USD_5m_1mo_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/AUD_USD_1h_2y_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/AUD_USD_1m_7d_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/AUD_USD_15m_60d_combined.csv: No numeric types to aggregate\n",
      "❌ Failed to process /content/forex-ai-models/AUD_USD_1d_5y_combined.csv: No numeric types to aggregate\n",
      "🎯 All CSVs processed, hybrid_signal generated, and converted to .pkl for backtest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_2372/2319423661.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "CSV_FOLDER = \"/content/forex-ai-models\"  # folder where fetched CSVs are\n",
    "SAVE_FOLDER = \"./combined_data\"          # folder for backtest-ready .pkl files\n",
    "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pairs = [\"EUR/USD\",\"GBP/USD\",\"USD/JPY\",\"AUD/USD\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Helper function: temporary hybrid signal\n",
    "# -----------------------------\n",
    "def generate_temp_signal(df, fast=5, slow=20):\n",
    "    if len(df) < slow:\n",
    "        return pd.Series([0]*len(df), index=df.index)\n",
    "    fast_ma = df['close'].rolling(fast).mean()\n",
    "    slow_ma = df['close'].rolling(slow).mean()\n",
    "    signal = (fast_ma - slow_ma).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    return signal.fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# Process all CSVs\n",
    "# -----------------------------\n",
    "for pair in pairs:\n",
    "    csv_files = list(Path(CSV_FOLDER).glob(f\"{pair.replace('/','_')}_*_combined.csv\"))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"⚠️ No CSV files found for {pair}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
    "\n",
    "            # Check required OHLC columns\n",
    "            for col in ['open','high','low','close']:\n",
    "                if col not in df.columns:\n",
    "                    print(f\"⚠️ {csv_file} missing column {col}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "            # Generate temporary hybrid_signal based on MA\n",
    "            df['hybrid_signal'] = generate_temp_signal(df)\n",
    "\n",
    "            # Compute ATR if missing\n",
    "            if 'atr' not in df.columns:\n",
    "                high = df['high'].values\n",
    "                low = df['low'].values\n",
    "                close = df['close'].values\n",
    "                tr = pd.Series(\n",
    "                    [max(h-l, abs(h-close[i-1]), abs(l-close[i-1])) if i>0 else h-l\n",
    "                     for i,(h,l) in enumerate(zip(high,low))]\n",
    "                )\n",
    "                df['atr'] = tr.rolling(14).mean().fillna(1e-5).clip(lower=1e-5)\n",
    "\n",
    "            # Save as pickle\n",
    "            pkl_file = Path(SAVE_FOLDER) / f\"{csv_file.stem}.pkl\"\n",
    "            df.to_pickle(pkl_file)\n",
    "            print(f\"✅ Saved {pkl_file} with temporary hybrid_signal\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to process {csv_file}: {e}\")\n",
    "\n",
    "print(\"🎯 All CSVs processed, hybrid_signal generated, and converted to .pkl for backtest.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba25fc",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6901ed27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:16:42.497643Z",
     "iopub.status.busy": "2025-10-05T15:16:42.497467Z",
     "iopub.status.idle": "2025-10-05T15:16:43.964857Z",
     "shell.execute_reply": "2025-10-05T15:16:43.964077Z"
    },
    "id": "MFBLL-2I6h0u",
    "papermill": {
     "duration": 1.486397,
     "end_time": "2025-10-05T15:16:43.965694",
     "exception": true,
     "start_time": "2025-10-05T15:16:42.479297",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined data...\n",
      "🎯 Running GA + vectorized backtest (parallelized)...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Timestamp' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 490, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/joblib/parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/joblib/parallel.py\", line 607, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2372/1619265312.py\", line 238, in evaluate_chrom\n  File \"/tmp/ipykernel_2372/1619265312.py\", line 133, in run_vector_backtest_vectorized\nTypeError: '<' not supported between instances of 'Timestamp' and 'str'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 397\u001b[39m\n\u001b[32m    394\u001b[39m combined_data = load_combined_data(SAVE_FOLDER)\n\u001b[32m    396\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🎯 Running GA + vectorized backtest (parallelized)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m best_chrom, trade_memory = \u001b[43mrun_ga_vectorized_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📡 Generating live signals with SL/TP, 1-100 score, and high-confidence flag...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    400\u001b[39m signals = generate_live_signals_with_sl_tp(best_chrom, combined_data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 242\u001b[39m, in \u001b[36mrun_ga_vectorized_parallel\u001b[39m\u001b[34m(combined_data, generations, population_size, mutation_rate)\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score, chrom\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(last_gen+\u001b[32m1\u001b[39m,last_gen+\u001b[32m1\u001b[39m+generations):\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     scored_population = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_chrom\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     scored_population.sort(reverse=\u001b[38;5;28;01mTrue\u001b[39;00m,key=\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[32m0\u001b[39m])\n\u001b[32m    244\u001b[39m     best_score,best_chrom=scored_population[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/joblib/parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/joblib/parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/joblib/parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/joblib/parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'Timestamp' and 'str'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Hybrid Vectorized Backtest + GA + Momentum-aware Live Browserless Signals + Email\n",
    "- Multi-timeframe calculations preserved\n",
    "- GA loads previous population and continues from last generation\n",
    "- Live signals with SL, TP, and 1-100 scoring\n",
    "- High-confidence trade flags included\n",
    "- Signals sent via Gmail (hardcoded App password)\n",
    "- GA evaluation fully parallelized\n",
    "\"\"\"\n",
    "\n",
    "import os, json, pickle, random, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "SAVE_FOLDER = \"/content/forex-ai-models/combined_with_indicators\"\n",
    "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "BEST_CHROM_FILE = \"/content/forex-ai-models/best_chromosome.pkl\"\n",
    "TRADE_MEMORY_FILE = \"/content/forex-ai-models/trade_memory.pkl\"\n",
    "POPULATION_FILE = \"/content/forex-ai-models/population.pkl\"\n",
    "GEN_COUNT_FILE = \"/content/forex-ai-models/generation_count.pkl\"\n",
    "SIGNALS_JSON_PATH = \"/content/forex-ai-models/broker_signals.json\"\n",
    "\n",
    "pairs = ['EUR/USD', 'GBP/USD', 'USD/JPY', 'AUD/USD']\n",
    "\n",
    "ATR_PERIOD = 14\n",
    "MIN_ATR = 1e-5\n",
    "BASE_CAPITAL = 100\n",
    "MAX_POSITION_FRACTION = 0.1\n",
    "\n",
    "POPULATION_SIZE = 12\n",
    "GENERATIONS = 10\n",
    "MUTATION_RATE = 0.2\n",
    "EARLY_STOPPING = 5\n",
    "TOURNAMENT_SIZE = 3\n",
    "EPS = 1e-8\n",
    "\n",
    "# -----------------------------\n",
    "# Gmail App password\n",
    "# -----------------------------\n",
    "GMAIL_USER = \"nakatonabira3@gmail.com\"\n",
    "GMAIL_APP_PASSWORD = \"gmwohahtltmcewug\"  # <-- your App password\n",
    "\n",
    "# -----------------------------\n",
    "# Browserless fetch\n",
    "# -----------------------------\n",
    "def fetch_live_rate(pair: str, timeout: int = 8) -> float:\n",
    "    from_currency, to_currency = pair.split('/')\n",
    "    token = os.environ.get('BROWSERLESS_TOKEN')\n",
    "    if not token: return 0.0\n",
    "    url = f\"https://production-sfo.browserless.io/content?token={token}\"\n",
    "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
    "    try:\n",
    "        res = requests.post(url, json=payload, timeout=timeout)\n",
    "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
    "        return float(match.group(1).replace(',', '')) if match else 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ fetch_live_rate error for {pair}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# -----------------------------\n",
    "# Data prep\n",
    "# -----------------------------\n",
    "def make_index_tz_naive(df):\n",
    "    if isinstance(df.index, pd.DatetimeIndex) and df.index.tz is not None:\n",
    "        return df.tz_convert(None)\n",
    "    return df\n",
    "\n",
    "def seed_hybrid_signal_if_needed(df):\n",
    "    if 'hybrid_signal' not in df.columns: df['hybrid_signal'] = 0.0\n",
    "    if df['hybrid_signal'].abs().sum() == 0:\n",
    "        fast = df['close'].rolling(10, min_periods=1).mean()\n",
    "        slow = df['close'].rolling(50, min_periods=1).mean()\n",
    "        df['hybrid_signal'] = (fast - slow).fillna(0)\n",
    "    df['hybrid_signal'] = df['hybrid_signal'].fillna(0).astype(float)\n",
    "    return df\n",
    "\n",
    "def ensure_atr(df):\n",
    "    if 'atr' in df.columns and not df['atr'].isnull().all():\n",
    "        df['atr'] = df['atr'].fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
    "        return df\n",
    "    high, low, close = df['high'].values, df['low'].values, df['close'].values\n",
    "    tr = np.maximum.reduce([\n",
    "        high - low,\n",
    "        np.abs(high - np.roll(close, 1)),\n",
    "        np.abs(low - np.roll(close, 1))\n",
    "    ])\n",
    "    tr[0] = (high[0]-low[0]) if len(tr)>0 else MIN_ATR\n",
    "    df['atr'] = pd.Series(tr, index=df.index).rolling(ATR_PERIOD, min_periods=1).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
    "    return df\n",
    "\n",
    "def load_combined_data(folder):\n",
    "    combined_data = {}\n",
    "    precomputed = {}\n",
    "    for pair in pairs:\n",
    "        combined_data[pair] = {}\n",
    "        prefix = pair.replace('/','_')\n",
    "        for p in Path(folder).glob(f\"{prefix}_*_combined.pkl\"):\n",
    "            tf_name = p.name.replace(f\"{prefix}_\",\"\").replace(\"_combined.pkl\",\"\")\n",
    "            try:\n",
    "                df = pd.read_pickle(p)\n",
    "                df = make_index_tz_naive(df)\n",
    "                if not all(c in df.columns for c in ['open','high','low','close']): continue\n",
    "                df = seed_hybrid_signal_if_needed(df)\n",
    "                df = ensure_atr(df)\n",
    "                combined_data[pair][tf_name] = df\n",
    "            except: pass\n",
    "    return combined_data\n",
    "\n",
    "# -----------------------------\n",
    "# Vectorized Backtest\n",
    "# -----------------------------\n",
    "def run_vector_backtest_vectorized(combined_data, capital, base_risk, atr_sl, atr_tp, conf_mult, tf_weights_per_pair, trade_memory=None):\n",
    "    if trade_memory is None: trade_memory = {pair: [] for pair in combined_data.keys()}\n",
    "    results = {}\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # Precompute signals, prices, atr for all pairs\n",
    "    precomputed = {}\n",
    "    for pair, tfs in combined_data.items():\n",
    "        if not tfs:\n",
    "            results[pair] = {'equity_curve': pd.Series([capital]), 'total_pnl':0,'max_drawdown':0}\n",
    "            continue\n",
    "        all_idxs = sorted(set().union(*[set(df.index) for df in tfs.values()]))\n",
    "        df_all = pd.DataFrame(index=all_idxs)\n",
    "        for tf_name, df in tfs.items():\n",
    "            df_all[f'close_{tf_name}'] = df['close'].reindex(df_all.index).ffill()\n",
    "            df_all[f'signal_{tf_name}'] = df['hybrid_signal'].reindex(df_all.index).ffill().fillna(0.0)\n",
    "            df_all[f'atr_{tf_name}'] = df['atr'].reindex(df_all.index).ffill().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
    "        df_all['price'] = df_all[[c for c in df_all.columns if c.startswith('close_')]].mean(axis=1).clip(lower=epsilon)\n",
    "        df_all['atr'] = df_all[[c for c in df_all.columns if c.startswith('atr_')]].mean(axis=1).clip(lower=MIN_ATR)\n",
    "        precomputed[pair] = df_all\n",
    "\n",
    "    # Evaluate\n",
    "    for pair, df_all in precomputed.items():\n",
    "        tfs = combined_data.get(pair, {})\n",
    "        if not tfs:\n",
    "            continue\n",
    "        agg_signal = sum([df_all[f'signal_{tf}']*tf_weights_per_pair.get(pair,{}).get(tf,0.0) for tf in tfs.keys()])\n",
    "        mean_abs_signal = np.mean([df_all[f'signal_{tf}'].abs().mean() for tf in tfs.keys()]) if tfs else 0.0\n",
    "        conf_threshold = conf_mult * (mean_abs_signal + EPS)\n",
    "        df_all['agg_signal'] = np.where(np.abs(agg_signal)>=conf_threshold, agg_signal, 0.0)\n",
    "\n",
    "        price, atr, agg_signal = df_all['price'].values, df_all['atr'].values, df_all['agg_signal'].values\n",
    "        n = len(price)\n",
    "        if n<=1:\n",
    "            results[pair] = {'equity_curve': pd.Series([capital]), 'total_pnl':0,'max_drawdown':0}\n",
    "            continue\n",
    "\n",
    "        memory_factor = 1.0\n",
    "        if trade_memory.get(pair):\n",
    "            total_prev_pnl = sum([float(tr.get('pnl',0)) for tr in trade_memory[pair]])\n",
    "            memory_factor = max(0.2, 1.0 + total_prev_pnl / max(1.0, capital*10.0))\n",
    "\n",
    "        size = (capital*base_risk*np.abs(agg_signal))/(atr_sl*(atr/price)+EPS)\n",
    "        size = np.minimum(size*memory_factor, capital*MAX_POSITION_FRACTION)\n",
    "        size = np.nan_to_num(size,nan=0.0,posinf=capital*MAX_POSITION_FRACTION)\n",
    "        direction = np.sign(agg_signal)\n",
    "        pnl = direction*size*(atr_tp*atr/price)\n",
    "        equity = np.zeros(n,dtype=float)\n",
    "        equity[0]=capital\n",
    "        for i in range(1,n): equity[i] = equity[i-1]+float(pnl[i])\n",
    "        trade_memory.setdefault(pair,[]).append({'equity':float(equity[-1]), 'pnl':float(equity[-1]-capital)})\n",
    "        if len(trade_memory[pair])>200: trade_memory[pair] = trade_memory[pair][-200:]\n",
    "        equity_series = pd.Series(equity,index=df_all.index)\n",
    "        results[pair] = {'equity_curve': equity_series,'total_pnl':float(equity[-1]-capital),'max_drawdown':float((equity_series.cummax()-equity_series).max())}\n",
    "\n",
    "    total_pnl = sum([r['total_pnl'] for r in results.values()])\n",
    "    max_dd = max([r['max_drawdown'] for r in results.values()] or [0.0])\n",
    "    score = total_pnl/(1.0+max_dd) if (1.0+max_dd)!=0 else total_pnl\n",
    "    return score, results, trade_memory\n",
    "\n",
    "# -----------------------------\n",
    "# GA functions\n",
    "# -----------------------------\n",
    "def build_tf_names(combined_data): return {pair: sorted(list(combined_data[pair].keys())) for pair in pairs}\n",
    "\n",
    "def create_chromosome(tf_names_map):\n",
    "    chrom = [random.uniform(1.0,2.0), random.uniform(2.0,4.0), random.uniform(0.005,0.02), random.uniform(0.3,0.7)]\n",
    "    for pair in pairs:\n",
    "        n=max(1,len(tf_names_map.get(pair,[])))\n",
    "        w=np.random.dirichlet(np.ones(n)).tolist()\n",
    "        chrom.extend(w)\n",
    "    return chrom\n",
    "\n",
    "def decode_chromosome(chrom,tf_names_map):\n",
    "    atr_sl, atr_tp, base_risk, conf = chrom[:4]\n",
    "    tf_weights_per_pair={}\n",
    "    idx=4\n",
    "    for pair in pairs:\n",
    "        n=max(1,len(tf_names_map.get(pair,[])))\n",
    "        w=np.array(chrom[idx:idx+n],dtype=float)\n",
    "        if w.sum()<=0: w=np.ones_like(w)/float(len(w))\n",
    "        else: w=w/(w.sum()+EPS)\n",
    "        tf_list=tf_names_map.get(pair,[])\n",
    "        tf_weights_per_pair[pair]={tf: float(weight) for tf,weight in zip(tf_list,w)} if tf_list else {}\n",
    "        idx+=n\n",
    "    return atr_sl, atr_tp, base_risk, conf, tf_weights_per_pair\n",
    "\n",
    "def tournament_selection(scored_population,k=TOURNAMENT_SIZE):\n",
    "    selected=random.sample(scored_population,k)\n",
    "    selected.sort(reverse=True,key=lambda x:x[0])\n",
    "    return selected[0][1]\n",
    "\n",
    "# -----------------------------\n",
    "# GA runner (parallelized)\n",
    "# -----------------------------\n",
    "def run_ga_vectorized_parallel(combined_data, generations=GENERATIONS, population_size=POPULATION_SIZE, mutation_rate=MUTATION_RATE):\n",
    "    tf_names_map=build_tf_names(combined_data)\n",
    "    if os.path.exists(POPULATION_FILE):\n",
    "        try: population=pickle.load(open(POPULATION_FILE,'rb'))\n",
    "        except: population=[create_chromosome(tf_names_map) for _ in range(population_size)]\n",
    "    else: population=[create_chromosome(tf_names_map) for _ in range(population_size)]\n",
    "    trade_memory={}\n",
    "    if os.path.exists(TRADE_MEMORY_FILE):\n",
    "        try: trade_memory=pickle.load(open(TRADE_MEMORY_FILE,'rb'))\n",
    "        except: trade_memory={}\n",
    "    last_gen=0\n",
    "    if os.path.exists(GEN_COUNT_FILE):\n",
    "        try: last_gen=pickle.load(open(GEN_COUNT_FILE,'rb'))\n",
    "        except: last_gen=0\n",
    "\n",
    "    best_score_ever=-np.inf\n",
    "    best_chrom_ever=None\n",
    "    early_stop_counter=0\n",
    "\n",
    "    def evaluate_chrom(chrom):\n",
    "        atr_sl, atr_tp, base_risk, conf, tf_weights_per_pair = decode_chromosome(chrom, tf_names_map)\n",
    "        score, _, _ = run_vector_backtest_vectorized(combined_data, BASE_CAPITAL, base_risk, atr_sl, atr_tp, conf, tf_weights_per_pair, trade_memory)\n",
    "        return score, chrom\n",
    "\n",
    "    for gen in range(last_gen+1,last_gen+1+generations):\n",
    "        scored_population = Parallel(n_jobs=-1)(delayed(evaluate_chrom)(c) for c in population)\n",
    "        scored_population.sort(reverse=True,key=lambda x:x[0])\n",
    "        best_score,best_chrom=scored_population[0]\n",
    "        if best_score<best_score_ever:\n",
    "            best_score=best_score_ever\n",
    "            best_chrom=best_chrom_ever\n",
    "\n",
    "        print(f\"=== Generation {gen} === Best Score: {best_score:.2f}\")\n",
    "        if best_score>best_score_ever:\n",
    "            best_score_ever=best_score\n",
    "            best_chrom_ever=best_chrom\n",
    "            early_stop_counter=0\n",
    "        else:\n",
    "            early_stop_counter+=1\n",
    "            if early_stop_counter>=EARLY_STOPPING:\n",
    "                print(\"⚠️ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        # Generate next population\n",
    "        next_population=[best_chrom]\n",
    "        while len(next_population)<population_size:\n",
    "            parent1=tournament_selection(scored_population)\n",
    "            parent2=tournament_selection(scored_population)\n",
    "            child=[(p1+p2)/2.0 for p1,p2 in zip(parent1,parent2)]\n",
    "            child=[c*random.uniform(0.95,1.05) if random.random()<mutation_rate else c for c in child]\n",
    "            next_population.append(child)\n",
    "        population=next_population\n",
    "\n",
    "        # Save state\n",
    "        pickle.dump(population,open(POPULATION_FILE,'wb'))\n",
    "        pickle.dump(trade_memory,open(TRADE_MEMORY_FILE,'wb'))\n",
    "        pickle.dump(best_chrom_ever,open(BEST_CHROM_FILE,'wb'))\n",
    "        pickle.dump(gen,open(GEN_COUNT_FILE,'wb'))\n",
    "\n",
    "    print(\"✅ GA complete. Best chromosome saved.\")\n",
    "    return best_chrom_ever, trade_memory\n",
    "\n",
    "# -----------------------------\n",
    "# Live signals with high-confidence flag\n",
    "# -----------------------------\n",
    "def generate_live_signals_with_sl_tp(best_chrom, combined_data):\n",
    "    tf_names_map = build_tf_names(combined_data)\n",
    "    atr_sl, atr_tp, base_risk, conf, tf_weights_per_pair = decode_chromosome(best_chrom, tf_names_map)\n",
    "\n",
    "    live_signals = {}\n",
    "    prev_signals = {}\n",
    "\n",
    "    if os.path.exists(SIGNALS_JSON_PATH):\n",
    "        try:\n",
    "            prev_data = json.load(open(SIGNALS_JSON_PATH, 'r'))\n",
    "            prev_signals = {pair: data.get('strength', 0.0) for pair, data in prev_data.get(\"pairs\", {}).items()}\n",
    "        except:\n",
    "            prev_signals = {}\n",
    "\n",
    "    for pair in pairs:\n",
    "        tfs = combined_data.get(pair, {})\n",
    "\n",
    "        price = fetch_live_rate(pair)\n",
    "        if price <= 0:\n",
    "            price = np.mean([df['close'].iloc[-1] for df in tfs.values()]) if tfs else 1.0\n",
    "\n",
    "        signal_strength = sum([\n",
    "            tf_weights_per_pair.get(pair, {}).get(tf, 0.0) * tfs[tf]['hybrid_signal'].iloc[-1]\n",
    "            for tf in tf_names_map.get(pair, [])\n",
    "        ])\n",
    "\n",
    "        prev_strength = prev_signals.get(pair, 0.0)\n",
    "        if np.sign(signal_strength) != np.sign(prev_strength):\n",
    "            signal_strength = 0.7 * prev_strength + 0.3 * signal_strength\n",
    "\n",
    "        direction = \"BUY\" if signal_strength > 0 else \"SELL\" if signal_strength < 0 else \"HOLD\"\n",
    "\n",
    "        recent_atr = np.mean([tfs[tf]['atr'].iloc[-1] for tf in tf_names_map.get(pair, [])]) if tfs else 1.0\n",
    "\n",
    "        score_100 = min(max(int(100*(abs(signal_strength)/(recent_atr+EPS))**0.5), 1), 100)\n",
    "\n",
    "        sl_multiplier = 0.5\n",
    "        tp_multiplier = 1.0\n",
    "        SL = price - atr_sl * recent_atr * sl_multiplier if direction == \"BUY\" else price + atr_sl * recent_atr * sl_multiplier\n",
    "        TP = price + atr_tp * recent_atr * tp_multiplier if direction == \"BUY\" else price - atr_tp * recent_atr * tp_multiplier\n",
    "\n",
    "        # High-confidence flag\n",
    "        high_confidence = score_100 >= 80\n",
    "\n",
    "        live_signals[pair] = {\n",
    "            \"direction\": direction,\n",
    "            \"strength\": float(signal_strength),\n",
    "            \"score_1_100\": score_100,\n",
    "            \"last_price\": float(price),\n",
    "            \"SL\": float(SL),\n",
    "            \"TP\": float(TP),\n",
    "            \"high_confidence\": high_confidence\n",
    "        }\n",
    "\n",
    "    with open(SIGNALS_JSON_PATH, 'w') as f:\n",
    "        json.dump({\"timestamp\": pd.Timestamp.now().isoformat(), \"pairs\": live_signals}, f, indent=2)\n",
    "\n",
    "    print(f\"📡 Live signals with SL/TP generated and saved to {SIGNALS_JSON_PATH}\")\n",
    "    return live_signals\n",
    "\n",
    "# -----------------------------\n",
    "# Email builder with high-confidence visual flag\n",
    "# -----------------------------\n",
    "def send_forex_email(signals, recipient=\"nakatonabira3@gmail.com\"):\n",
    "    def format_price(price, pair=\"\"):\n",
    "        decimals = 3 if \"JPY\" in pair else 4\n",
    "        return f\"{price:.{decimals}f}\" if price else \"-\"\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    table_rows = \"\"\n",
    "    flags_map = {\"USD\":\"🇺🇸\",\"EUR\":\"🇪🇺\",\"GBP\":\"🇬🇧\",\"JPY\":\"🇯🇵\",\"AUD\":\"🇦🇺\"}\n",
    "    for pair, data in signals.items():\n",
    "        f1,f2 = pair.split(\"/\")\n",
    "        flag_str = f\"{flags_map.get(f1,'')} {flags_map.get(f2,'')}\"\n",
    "        high_conf = \"🔥\" if data.get(\"high_confidence\") else \"\"\n",
    "        table_rows += f\"\"\"\n",
    "        <tr>\n",
    "          <td>{flag_str} {pair}</td>\n",
    "          <td>{format_price(data['last_price'], pair)}</td>\n",
    "          <td>{data['direction']}</td>\n",
    "          <td>{data['score_1_100']} {high_conf}</td>\n",
    "          <td>SL:{format_price(data['SL'], pair)} | TP:{format_price(data['TP'], pair)}</td>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "      <body>\n",
    "        <h2>Forex Signals - {today}</h2>\n",
    "        <table border=\"1\" style=\"border-collapse:collapse;text-align:center;\">\n",
    "          <tr><th>Instrument</th><th>Price</th><th>Signal</th><th>Score</th><th>SL/TP</th></tr>\n",
    "          {table_rows}\n",
    "        </table>\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    msg = MIMEMultipart(\"alternative\")\n",
    "    msg['From'] = f\"Forex Bot <{GMAIL_USER}>\"\n",
    "    msg['To'] = recipient\n",
    "    msg['Subject'] = f\"Forex Signals - {today}\"\n",
    "    msg.attach(MIMEText(html, \"html\"))\n",
    "    try:\n",
    "        with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\n",
    "            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)\n",
    "            server.sendmail(GMAIL_USER, recipient, msg.as_string())\n",
    "        print(f\"📧 Email sent to {recipient}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Email send failed: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN\n",
    "# -----------------------------\n",
    "if __name__==\"__main__\":\n",
    "    print(\"Loading combined data...\")\n",
    "    combined_data = load_combined_data(SAVE_FOLDER)\n",
    "\n",
    "    print(\"🎯 Running GA + vectorized backtest (parallelized)...\")\n",
    "    best_chrom, trade_memory = run_ga_vectorized_parallel(combined_data)\n",
    "\n",
    "    print(\"📡 Generating live signals with SL/TP, 1-100 score, and high-confidence flag...\")\n",
    "    signals = generate_live_signals_with_sl_tp(best_chrom, combined_data)\n",
    "\n",
    "    print(json.dumps(signals, indent=2))\n",
    "\n",
    "    print(\"📨 Sending email with signals...\")\n",
    "    send_forex_email(signals, recipient=\"nakatonabira3@gmail.com\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 83.047719,
   "end_time": "2025-10-05T15:16:46.593304",
   "environment_variables": {},
   "exception": true,
   "input_path": "AI_Forex_Brain 2.ipynb",
   "output_path": "output/AI_Forex_Brain_Executed.ipynb",
   "parameters": {},
   "start_time": "2025-10-05T15:15:23.545585",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}