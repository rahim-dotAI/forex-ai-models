{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438d4eb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:42:11.717146Z",
     "iopub.status.busy": "2025-10-06T02:42:11.716899Z",
     "iopub.status.idle": "2025-10-06T02:42:11.731527Z",
     "shell.execute_reply": "2025-10-06T02:42:11.730968Z"
    },
    "id": "h1ckIvw5mGi3",
    "papermill": {
     "duration": 0.019875,
     "end_time": "2025-10-06T02:42:11.732487",
     "exception": false,
     "start_time": "2025-10-06T02:42:11.712612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Working directory set to: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models/forex-alpha-models\n",
      "✅ Git configured: Rahim AI Bot <nakatonabira3@gmail.com>\n",
      "✅ Output folders ready: forex-alpha-models/pickles, forex-alpha-models/csvs\n",
      "Python version: 3.11.13 (main, Jun  4 2025, 04:12:12) [GCC 13.3.0]\n",
      "Current directory: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Notebook Initialization - Safe for Colab & GitHub Actions\n",
    "# ======================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Detect environment\n",
    "# -----------------------------\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Safe working folder\n",
    "# -----------------------------\n",
    "if IN_COLAB:\n",
    "    SAVE_FOLDER = Path(\"/content/forex-alpha-models\")\n",
    "else:\n",
    "    # GitHub Actions or local\n",
    "    SAVE_FOLDER = Path(\"./forex-alpha-models\")\n",
    "\n",
    "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(SAVE_FOLDER)\n",
    "print(f\"✅ Working directory set to: {SAVE_FOLDER.resolve()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Git config (headless-safe)\n",
    "# -----------------------------\n",
    "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
    "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
    "\n",
    "os.system(f'git config --global user.name \"{GIT_NAME}\"')\n",
    "os.system(f'git config --global user.email \"{GIT_EMAIL}\"')\n",
    "print(f\"✅ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Read tokens safely\n",
    "# -----------------------------\n",
    "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
    "BROWSERLESS_TOKEN = os.environ.get(\"BROWSERLESS_TOKEN\")\n",
    "\n",
    "if not FOREX_PAT and IN_GHA:\n",
    "    print(\"⚠️ Warning: FOREX_PAT not found in GitHub Actions secrets\")\n",
    "if not BROWSERLESS_TOKEN:\n",
    "    print(\"⚠️ Warning: BROWSERLESS_TOKEN not found\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Optional: safe repo paths for outputs & pickles\n",
    "# -----------------------------\n",
    "PICKLE_FOLDER = SAVE_FOLDER / \"pickles\"\n",
    "PICKLE_FOLDER.mkdir(parents=True, exist_ok=True)  # ✅ parents=True fixes FileNotFoundError\n",
    "\n",
    "CSV_FOLDER = SAVE_FOLDER / \"csvs\"\n",
    "CSV_FOLDER.mkdir(parents=True, exist_ok=True)     # ✅ same here\n",
    "\n",
    "print(f\"✅ Output folders ready: {PICKLE_FOLDER}, {CSV_FOLDER}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Python environment info (debug)\n",
    "# -----------------------------\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10002ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:42:11.739248Z",
     "iopub.status.busy": "2025-10-06T02:42:11.739084Z",
     "iopub.status.idle": "2025-10-06T02:42:19.741135Z",
     "shell.execute_reply": "2025-10-06T02:42:19.740383Z"
    },
    "id": "N8Oonb6gP_ho",
    "papermill": {
     "duration": 8.00663,
     "end_time": "2025-10-06T02:42:19.742340",
     "exception": false,
     "start_time": "2025-10-06T02:42:11.735710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mplfinance\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading mplfinance-0.12.10b0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting firebase-admin\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading firebase_admin-7.1.0-py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Collecting dropbox\r\n",
      "  Downloading dropbox-12.0.2-py3-none-any.whl.metadata (4.3 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (2.32.5)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (4.14.2)\r\n",
      "Requirement already satisfied: pandas in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: numpy in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: ta in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (0.11.0)\r\n",
      "Requirement already satisfied: yfinance in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (0.2.66)\r\n",
      "Collecting pyppeteer\r\n",
      "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: nest_asyncio in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (1.6.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\r\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\r\n",
      "Requirement already satisfied: joblib in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (1.5.2)\r\n",
      "Requirement already satisfied: matplotlib in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (3.10.6)\r\n",
      "Collecting alpha_vantage\r\n",
      "  Downloading alpha_vantage-3.0.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: tqdm in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (4.67.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (1.7.2)\r\n",
      "Collecting cachecontrol>=0.14.3 (from firebase-admin)\r\n",
      "  Downloading cachecontrol-0.14.3-py3-none-any.whl.metadata (3.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-core<3.0.0dev,>=2.25.1 (from google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading google_api_core-2.25.2-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting google-cloud-firestore>=2.21.0 (from firebase-admin)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading google_cloud_firestore-2.21.0-py3-none-any.whl.metadata (9.9 kB)\r\n",
      "Collecting google-cloud-storage>=3.1.1 (from firebase-admin)\r\n",
      "  Downloading google_cloud_storage-3.4.0-py3-none-any.whl.metadata (13 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyjwt>=2.10.1 (from pyjwt[crypto]>=2.10.1->firebase-admin)\r\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Requirement already satisfied: httpx==0.28.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpx[http2]==0.28.1->firebase-admin) (0.28.1)\r\n",
      "Requirement already satisfied: anyio in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (4.11.0)\r\n",
      "Requirement already satisfied: certifi in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (2025.10.5)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (1.0.9)\r\n",
      "Requirement already satisfied: idna in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (3.10)\r\n",
      "Collecting h2<5,>=3 (from httpx[http2]==0.28.1->firebase-admin)\r\n",
      "  Downloading h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\r\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin) (6.32.1)\r\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading google_auth-2.41.1-py2.py3-none-any.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from requests) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from requests) (2.5.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading grpcio-1.75.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions~=4.12 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.15.0)\r\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]==0.28.1->firebase-admin)\r\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\r\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]==0.28.1->firebase-admin)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Requirement already satisfied: h11>=0.16 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from httpcore==1.*->httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (0.16.0)\r\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=2.25.1->google-api-core[grpc]<3.0.0dev,>=2.25.1; platform_python_implementation != \"PyPy\"->firebase-admin)\r\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from dropbox) (1.17.0)\r\n",
      "Collecting stone<3.3.3,>=2 (from dropbox)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading stone-3.3.1-py3-none-any.whl.metadata (8.0 kB)\r\n",
      "Collecting ply>=3.4 (from stone<3.3.3,>=2->dropbox)\r\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from beautifulsoup4) (2.8)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (0.0.12)\r\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (4.4.0)\r\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (2.4.6)\r\n",
      "Requirement already satisfied: peewee>=3.16.2 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (3.18.2)\r\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (0.13.0)\r\n",
      "Requirement already satisfied: websockets>=13.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from yfinance) (15.0.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer)\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\r\n",
      "Collecting importlib-metadata>=1.4 (from pyppeteer)\r\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer)\r\n",
      "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: pip is looking at multiple versions of pyppeteer to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting pyppeteer\r\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Collecting pyppeteer\r\n",
      "  Downloading pyppeteer-1.0.1-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "  Downloading pyppeteer-1.0.0-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "  Downloading pyppeteer-0.2.6-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "  Downloading pyppeteer-0.2.5-py3-none-any.whl.metadata (6.9 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyppeteer-0.2.4-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "  Downloading pyppeteer-0.2.3-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting pyee<8.0.0,>=7.0.1 (from pyppeteer)\r\n",
      "  Downloading pyee-7.0.4-py2.py3-none-any.whl.metadata (1.8 kB)\r\n",
      "INFO: pip is still looking at multiple versions of pyppeteer to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting pyppeteer\r\n",
      "  Downloading pyppeteer-0.2.2-py3-none-any.whl.metadata (6.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyppeteer-0.0.25.tar.gz (1.2 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n",
      "\u001b[?25hCollecting pyee (from pyppeteer)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: scipy in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from lightgbm) (1.16.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (1.3.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (4.60.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (1.4.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (25.0)\r\n",
      "Requirement already satisfied: pillow>=8 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (11.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from matplotlib) (3.2.5)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp (from alpha_vantage)\r\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting msgpack<2.0.0,>=0.5.2 (from cachecontrol>=0.14.3->firebase-admin)\r\n",
      "  Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\r\n",
      "Requirement already satisfied: cffi>=1.12.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-core<3.0.0,>=1.4.1 (from google-cloud-firestore>=2.21.0->firebase-admin)\r\n",
      "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-resumable-media<3.0.0,>=2.7.2 (from google-cloud-storage>=3.1.1->firebase-admin)\r\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-crc32c<2.0.0,>=1.1.3 (from google-cloud-storage>=3.1.1->firebase-admin)\r\n",
      "  Downloading google_crc32c-1.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cryptography>=3.4.0 (from pyjwt[crypto]>=2.10.1->firebase-admin)\r\n",
      "  Downloading cryptography-46.0.2-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->alpha_vantage)\r\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->alpha_vantage)\r\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (25.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting frozenlist>=1.1.1 (from aiohttp->alpha_vantage)\r\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multidict<7.0,>=4.5 (from aiohttp->alpha_vantage)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\r\n",
      "Collecting propcache>=0.2.0 (from aiohttp->alpha_vantage)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading propcache-0.4.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->alpha_vantage)\r\n",
      "  Downloading yarl-1.21.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (74 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sniffio>=1.1 in /opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages (from anyio->httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (1.3.1)\r\n",
      "Downloading mplfinance-0.12.10b0-py3-none-any.whl (75 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading firebase_admin-7.1.0-py3-none-any.whl (137 kB)\r\n",
      "Downloading google_api_core-2.25.2-py3-none-any.whl (162 kB)\r\n",
      "Downloading google_auth-2.41.1-py2.py3-none-any.whl (221 kB)\r\n",
      "Downloading cachetools-6.2.0-py3-none-any.whl (11 kB)\r\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\r\n",
      "Downloading grpcio-1.75.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.5 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m231.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading grpcio_status-1.75.1-py3-none-any.whl (14 kB)\r\n",
      "Downloading h2-4.3.0-py3-none-any.whl (61 kB)\r\n",
      "Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\r\n",
      "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\r\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\r\n",
      "Downloading dropbox-12.0.2-py3-none-any.whl (572 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/572.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.1/572.1 kB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading stone-3.3.1-py3-none-any.whl (162 kB)\r\n",
      "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m234.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading alpha_vantage-3.0.0-py3-none-any.whl (35 kB)\r\n",
      "Downloading cachecontrol-0.14.3-py3-none-any.whl (21 kB)\r\n",
      "Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (429 kB)\r\n",
      "Downloading google_cloud_firestore-2.21.0-py3-none-any.whl (368 kB)\r\n",
      "Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\r\n",
      "Downloading google_cloud_storage-3.4.0-py3-none-any.whl (278 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading google_crc32c-1.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\r\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\r\n",
      "Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\r\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\r\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\r\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\r\n",
      "Downloading cryptography-46.0.2-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m224.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m199.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\r\n",
      "Downloading yarl-1.21.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\r\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\r\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading propcache-0.4.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (209 kB)\r\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Downloading pyee-13.0.0-py3-none-any.whl (15 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: pyppeteer\r\n",
      "\u001b[33m  DEPRECATION: Building 'pyppeteer' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyppeteer'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for pyppeteer (setup.py) ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \b\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyppeteer: filename=pyppeteer-0.0.25-py3-none-any.whl size=78442 sha256=40d82e0eb996c2f802e3ecef45ccb06ab0f45a9c75464e0d69e9184d6eb4e3e6\r\n",
      "  Stored in directory: /home/runner/.cache/pip/wheels/73/a3/1e/3a15782836222d82b917f1ebf652f9db54eec93f3268a42bcf\r\n",
      "Successfully built pyppeteer\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: ply, appdirs, stone, pyjwt, pyee, pyasn1, proto-plus, propcache, multidict, msgpack, hyperframe, hpack, grpcio, googleapis-common-protos, google-crc32c, frozenlist, cachetools, aiohappyeyeballs, yarl, rsa, pyppeteer, pyasn1-modules, lightgbm, h2, grpcio-status, google-resumable-media, dropbox, cryptography, cachecontrol, aiosignal, mplfinance, google-auth, aiohttp, google-api-core, alpha_vantage, google-cloud-core, google-cloud-storage, google-cloud-firestore, firebase-admin\r\n",
      "\u001b[?25l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/39\u001b[0m [pyee]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/39\u001b[0m [hpack]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/39\u001b[0m [googleapis-common-protos]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/39\u001b[0m [pyppeteer]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/39\u001b[0m [lightgbm]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m26/39\u001b[0m [dropbox]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m26/39\u001b[0m [dropbox]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m26/39\u001b[0m [dropbox]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m27/39\u001b[0m [cryptography]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m32/39\u001b[0m [aiohttp]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m33/39\u001b[0m [google-api-core]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m36/39\u001b[0m [google-cloud-storage]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m37/39\u001b[0m [google-cloud-firestore]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39/39\u001b[0m [firebase-admin]\r\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 alpha_vantage-3.0.0 appdirs-1.4.4 cachecontrol-0.14.3 cachetools-6.2.0 cryptography-46.0.2 dropbox-12.0.2 firebase-admin-7.1.0 frozenlist-1.7.0 google-api-core-2.25.2 google-auth-2.41.1 google-cloud-core-2.4.3 google-cloud-firestore-2.21.0 google-cloud-storage-3.4.0 google-crc32c-1.7.1 google-resumable-media-2.7.2 googleapis-common-protos-1.70.0 grpcio-1.75.1 grpcio-status-1.75.1 h2-4.3.0 hpack-4.1.0 hyperframe-6.1.0 lightgbm-4.6.0 mplfinance-0.12.10b0 msgpack-1.1.1 multidict-6.6.4 ply-3.11 propcache-0.4.0 proto-plus-1.26.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pyee-13.0.0 pyjwt-2.10.1 pyppeteer-0.0.25 rsa-4.9.1 stone-3.3.1 yarl-1.21.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mplfinance firebase-admin dropbox requests beautifulsoup4 pandas numpy ta yfinance pyppeteer nest_asyncio lightgbm joblib matplotlib alpha_vantage tqdm scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb7b3f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:42:19.755827Z",
     "iopub.status.busy": "2025-10-06T02:42:19.755648Z",
     "iopub.status.idle": "2025-10-06T02:42:23.681141Z",
     "shell.execute_reply": "2025-10-06T02:42:23.680537Z"
    },
    "id": "kAsEJB-A5k3O",
    "papermill": {
     "duration": 3.933682,
     "end_time": "2025-10-06T02:42:23.682109",
     "exception": false,
     "start_time": "2025-10-06T02:42:19.748427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Git already installed.\n",
      "✅ Git LFS already installed.\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "🔧 Configuring Git identity...\n",
      "📥 Cloning repo 'forex-ai-models' from GitHub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'forex-ai-models'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:   5% (9/152)\r",
      "Filtering content:   6% (10/152)\r",
      "Filtering content:   7% (11/152)\r",
      "Filtering content:   8% (13/152)\r",
      "Filtering content:   9% (14/152)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  10% (16/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  11% (17/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  12% (19/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  13% (20/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  14% (22/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  15% (23/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  16% (25/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  17% (26/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  18% (28/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  19% (29/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  20% (31/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  21% (32/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  22% (34/152), 53.61 MiB | 89.03 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  23% (35/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  24% (37/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  25% (38/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  26% (40/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  27% (42/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  28% (43/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  29% (45/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  30% (46/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  31% (48/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  32% (49/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  33% (51/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  34% (52/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  35% (54/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  36% (55/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  37% (57/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  38% (58/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  39% (60/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  40% (61/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  41% (63/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  42% (64/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  43% (66/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  44% (67/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  45% (69/152), 53.61 MiB | 89.03 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  46% (70/152), 53.61 MiB | 89.03 MiB/s\r",
      "Filtering content:  47% (72/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  48% (73/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  49% (75/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  50% (76/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  51% (78/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  52% (80/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  53% (81/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  54% (83/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  55% (84/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  56% (86/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  57% (87/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  58% (89/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  59% (90/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  60% (92/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  61% (93/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  62% (95/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  63% (96/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  64% (98/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  65% (99/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  66% (101/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  67% (102/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  68% (104/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  69% (105/152), 129.78 MiB | 118.00 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  70% (107/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  71% (108/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  72% (110/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  73% (111/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  74% (113/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  75% (114/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  76% (116/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  77% (118/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  78% (119/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  78% (120/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  79% (121/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  80% (122/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  81% (124/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  82% (125/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  83% (127/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  84% (128/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  85% (130/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  86% (131/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  87% (133/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  88% (134/152), 129.78 MiB | 118.00 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  89% (136/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  90% (137/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  91% (139/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  92% (140/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  93% (142/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  94% (143/152), 129.78 MiB | 118.00 MiB/s\r",
      "Filtering content:  95% (145/152), 141.53 MiB | 86.94 MiB/s \r",
      "Filtering content:  96% (146/152), 141.53 MiB | 86.94 MiB/s\r",
      "Filtering content:  97% (148/152), 141.53 MiB | 86.94 MiB/s\r",
      "Filtering content:  98% (149/152), 141.53 MiB | 86.94 MiB/s\r",
      "Filtering content:  99% (151/152), 141.53 MiB | 86.94 MiB/s\r",
      "Filtering content: 100% (152/152), 141.53 MiB | 86.94 MiB/s\r",
      "Filtering content: 100% (152/152), 141.54 MiB | 59.70 MiB/s, done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Pulling Git LFS-tracked files...\n",
      "📌 Tracking CSV/PKL files with Git LFS...\n",
      "\"*.csv\" already supported\n",
      "\"*.pkl\" already supported\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "No .gitattributes changes\n",
      "📂 Staging all new/modified files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No changes detected, skipping commit/push.\n",
      "📋 LFS-tracked files:\n",
      "d507ed8551 * AUD_USD.csv\n",
      "ee81642fe3 * AUD_USD_15m_60d.csv\n",
      "ee81642fe3 * AUD_USD_15m_60d_combined.csv\n",
      "89b218254d * AUD_USD_1d_5y.csv\n",
      "89b218254d * AUD_USD_1d_5y_combined.csv\n",
      "ee81642fe3 * AUD_USD_1h_2y.csv\n",
      "ee81642fe3 * AUD_USD_1h_2y_combined.csv\n",
      "b24a2f6b4d * AUD_USD_1m_7d.csv\n",
      "b24a2f6b4d * AUD_USD_1m_7d_combined.csv\n",
      "ee81642fe3 * AUD_USD_5m_1mo.csv\n",
      "ee81642fe3 * AUD_USD_5m_1mo_combined.csv\n",
      "747ba3d4b8 * EUR_USD.csv\n",
      "03019d6062 * EUR_USD_15m_60d.csv\n",
      "03019d6062 * EUR_USD_15m_60d_combined.csv\n",
      "8169d3624f * EUR_USD_1d_5y.csv\n",
      "8169d3624f * EUR_USD_1d_5y_combined.csv\n",
      "03019d6062 * EUR_USD_1h_2y.csv\n",
      "03019d6062 * EUR_USD_1h_2y_combined.csv\n",
      "d747e163e2 * EUR_USD_1m_7d.csv\n",
      "d747e163e2 * EUR_USD_1m_7d_combined.csv\n",
      "d747e163e2 * EUR_USD_5m_1mo.csv\n",
      "d747e163e2 * EUR_USD_5m_1mo_combined.csv\n",
      "7f9a90e54d * GBP_USD.csv\n",
      "03019d6062 * GBP_USD_15m_60d.csv\n",
      "03019d6062 * GBP_USD_15m_60d_combined.csv\n",
      "a12f1034cd * GBP_USD_1d_5y.csv\n",
      "a12f1034cd * GBP_USD_1d_5y_combined.csv\n",
      "b24a2f6b4d * GBP_USD_1h_2y.csv\n",
      "b24a2f6b4d * GBP_USD_1h_2y_combined.csv\n",
      "03019d6062 * GBP_USD_1m_7d.csv\n",
      "03019d6062 * GBP_USD_1m_7d_combined.csv\n",
      "03019d6062 * GBP_USD_5m_1mo.csv\n",
      "03019d6062 * GBP_USD_5m_1mo_combined.csv\n",
      "482f48267d * USD_JPY.csv\n",
      "b24a2f6b4d * USD_JPY_15m_60d.csv\n",
      "b24a2f6b4d * USD_JPY_15m_60d_combined.csv\n",
      "a12f1034cd * USD_JPY_1d_5y.csv\n",
      "a12f1034cd * USD_JPY_1d_5y_combined.csv\n",
      "b24a2f6b4d * USD_JPY_1h_2y.csv\n",
      "b24a2f6b4d * USD_JPY_1h_2y_combined.csv\n",
      "b24a2f6b4d * USD_JPY_1m_7d.csv\n",
      "b24a2f6b4d * USD_JPY_1m_7d_combined.csv\n",
      "b24a2f6b4d * USD_JPY_5m_1mo.csv\n",
      "b24a2f6b4d * USD_JPY_5m_1mo_combined.csv\n",
      "1411d03052 * best_chromosome.pkl\n",
      "4c1ed93439 * broker_signals_log.csv\n",
      "530fd64279 * combined_data/AUD_USD_15m_60d_combined.pkl\n",
      "294388c07b * combined_data/AUD_USD_1d_5y_combined.pkl\n",
      "1329ef90a8 * combined_data/AUD_USD_1h_2y_combined.pkl\n",
      "c847a1510d * combined_data/AUD_USD_1m_7d_combined.pkl\n",
      "530fd64279 * combined_data/AUD_USD_5m_1mo_combined.pkl\n",
      "a00d5cde1a * combined_data/EUR_USD_15m_60d_combined.pkl\n",
      "daa21c075e * combined_data/EUR_USD_1d_5y_combined.pkl\n",
      "c586a988d4 * combined_data/EUR_USD_1h_2y_combined.pkl\n",
      "c586a988d4 * combined_data/EUR_USD_1m_7d_combined.pkl\n",
      "c586a988d4 * combined_data/EUR_USD_5m_1mo_combined.pkl\n",
      "c586a988d4 * combined_data/GBP_USD_15m_60d_combined.pkl\n",
      "0369c29448 * combined_data/GBP_USD_1d_5y_combined.pkl\n",
      "0efd067b50 * combined_data/GBP_USD_1h_2y_combined.pkl\n",
      "c586a988d4 * combined_data/GBP_USD_1m_7d_combined.pkl\n",
      "a00d5cde1a * combined_data/GBP_USD_5m_1mo_combined.pkl\n",
      "0efd067b50 * combined_data/USD_JPY_15m_60d_combined.pkl\n",
      "0369c29448 * combined_data/USD_JPY_1d_5y_combined.pkl\n",
      "0efd067b50 * combined_data/USD_JPY_1h_2y_combined.pkl\n",
      "0efd067b50 * combined_data/USD_JPY_1m_7d_combined.pkl\n",
      "0efd067b50 * combined_data/USD_JPY_5m_1mo_combined.pkl\n",
      "8b9683b7b3 * combined_with_indicators/AUD_USD_15m_60d_combined.pkl\n",
      "55ca4426a5 * combined_with_indicators/AUD_USD_15m_60d_combined_combined.pkl\n",
      "9ee08a9bb1 * combined_with_indicators/AUD_USD_1d_5y_combined.pkl\n",
      "8b252f0b90 * combined_with_indicators/AUD_USD_1d_5y_combined_combined.pkl\n",
      "f9b05fac88 * combined_with_indicators/AUD_USD_1h_2y_combined.pkl\n",
      "3adb3ab338 * combined_with_indicators/AUD_USD_1h_2y_combined_combined.pkl\n",
      "4dcf63be33 * combined_with_indicators/AUD_USD_1m_7d_combined.pkl\n",
      "99f9d845ea * combined_with_indicators/AUD_USD_1m_7d_combined_combined.pkl\n",
      "f2165c4f1c * combined_with_indicators/AUD_USD_5m_1mo_combined.pkl\n",
      "55ca4426a5 * combined_with_indicators/AUD_USD_5m_1mo_combined_combined.pkl\n",
      "02e8216f9a * combined_with_indicators/AUD_USD_AUD_USD_15m_60d_combined.pkl\n",
      "94d7c482d1 * combined_with_indicators/AUD_USD_AUD_USD_1d_5y_combined.pkl\n",
      "02e8216f9a * combined_with_indicators/AUD_USD_AUD_USD_1h_2y_combined.pkl\n",
      "05be10aaf6 * combined_with_indicators/AUD_USD_AUD_USD_1m_7d_combined.pkl\n",
      "02e8216f9a * combined_with_indicators/AUD_USD_AUD_USD_5m_1mo_combined.pkl\n",
      "c6abe92aea * combined_with_indicators/AUD_USD_AUD_USD_combined.pkl\n",
      "debc2271d0 * combined_with_indicators/EUR_USD_15m_60d_combined.pkl\n",
      "8edcaa12ce * combined_with_indicators/EUR_USD_15m_60d_combined_combined.pkl\n",
      "bbdb7e7abc * combined_with_indicators/EUR_USD_1d_5y_combined.pkl\n",
      "8d328d50ad * combined_with_indicators/EUR_USD_1d_5y_combined_combined.pkl\n",
      "4455792a8e * combined_with_indicators/EUR_USD_1h_2y_combined.pkl\n",
      "8b938c03b4 * combined_with_indicators/EUR_USD_1h_2y_combined_combined.pkl\n",
      "fe89f585bc * combined_with_indicators/EUR_USD_1m_7d_combined.pkl\n",
      "8b938c03b4 * combined_with_indicators/EUR_USD_1m_7d_combined_combined.pkl\n",
      "fe89f585bc * combined_with_indicators/EUR_USD_5m_1mo_combined.pkl\n",
      "8b938c03b4 * combined_with_indicators/EUR_USD_5m_1mo_combined_combined.pkl\n",
      "979b95e6ce * combined_with_indicators/EUR_USD_EUR_USD_15m_60d_combined.pkl\n",
      "b46b3dca53 * combined_with_indicators/EUR_USD_EUR_USD_1d_5y_combined.pkl\n",
      "979b95e6ce * combined_with_indicators/EUR_USD_EUR_USD_1h_2y_combined.pkl\n",
      "2795d857b0 * combined_with_indicators/EUR_USD_EUR_USD_1m_7d_combined.pkl\n",
      "2795d857b0 * combined_with_indicators/EUR_USD_EUR_USD_5m_1mo_combined.pkl\n",
      "ebad9bd837 * combined_with_indicators/EUR_USD_EUR_USD_combined.pkl\n",
      "31d7e9fa88 * combined_with_indicators/GBP_USD_15m_60d_combined.pkl\n",
      "8b938c03b4 * combined_with_indicators/GBP_USD_15m_60d_combined_combined.pkl\n",
      "84fe2d65b8 * combined_with_indicators/GBP_USD_1d_5y_combined.pkl\n",
      "fbecbb6da0 * combined_with_indicators/GBP_USD_1d_5y_combined_combined.pkl\n",
      "8dc7a979bf * combined_with_indicators/GBP_USD_1h_2y_combined.pkl\n",
      "bb4e780d62 * combined_with_indicators/GBP_USD_1h_2y_combined_combined.pkl\n",
      "4455792a8e * combined_with_indicators/GBP_USD_1m_7d_combined.pkl\n",
      "8b938c03b4 * combined_with_indicators/GBP_USD_1m_7d_combined_combined.pkl\n",
      "debc2271d0 * combined_with_indicators/GBP_USD_5m_1mo_combined.pkl\n",
      "8edcaa12ce * combined_with_indicators/GBP_USD_5m_1mo_combined_combined.pkl\n",
      "979b95e6ce * combined_with_indicators/GBP_USD_GBP_USD_15m_60d_combined.pkl\n",
      "847fa2c358 * combined_with_indicators/GBP_USD_GBP_USD_1d_5y_combined.pkl\n",
      "05be10aaf6 * combined_with_indicators/GBP_USD_GBP_USD_1h_2y_combined.pkl\n",
      "979b95e6ce * combined_with_indicators/GBP_USD_GBP_USD_1m_7d_combined.pkl\n",
      "979b95e6ce * combined_with_indicators/GBP_USD_GBP_USD_5m_1mo_combined.pkl\n",
      "48c0a88f92 * combined_with_indicators/GBP_USD_GBP_USD_combined.pkl\n",
      "5940e1ebbf * combined_with_indicators/USD_JPY_15m_60d_combined.pkl\n",
      "bb4e780d62 * combined_with_indicators/USD_JPY_15m_60d_combined_combined.pkl\n",
      "29a4402069 * combined_with_indicators/USD_JPY_1d_5y_combined.pkl\n",
      "fbecbb6da0 * combined_with_indicators/USD_JPY_1d_5y_combined_combined.pkl\n",
      "41f8570556 * combined_with_indicators/USD_JPY_1h_2y_combined.pkl\n",
      "bb4e780d62 * combined_with_indicators/USD_JPY_1h_2y_combined_combined.pkl\n",
      "b48e432d7b * combined_with_indicators/USD_JPY_1m_7d_combined.pkl\n",
      "bb4e780d62 * combined_with_indicators/USD_JPY_1m_7d_combined_combined.pkl\n",
      "92822f0e15 * combined_with_indicators/USD_JPY_5m_1mo_combined.pkl\n",
      "bb4e780d62 * combined_with_indicators/USD_JPY_5m_1mo_combined_combined.pkl\n",
      "05be10aaf6 * combined_with_indicators/USD_JPY_USD_JPY_15m_60d_combined.pkl\n",
      "847fa2c358 * combined_with_indicators/USD_JPY_USD_JPY_1d_5y_combined.pkl\n",
      "05be10aaf6 * combined_with_indicators/USD_JPY_USD_JPY_1h_2y_combined.pkl\n",
      "05be10aaf6 * combined_with_indicators/USD_JPY_USD_JPY_1m_7d_combined.pkl\n",
      "05be10aaf6 * combined_with_indicators/USD_JPY_USD_JPY_5m_1mo_combined.pkl\n",
      "5b2e5617ee * combined_with_indicators/USD_JPY_USD_JPY_combined.pkl\n",
      "4f85dea4e0 * generation_count.pkl\n",
      "162aca1140 * models/AUD_USD_15m_60d.pkl\n",
      "e5ab4d3664 * models/AUD_USD_1d_5y.pkl\n",
      "3bbb2cd486 * models/AUD_USD_1m_7d.pkl\n",
      "4f555c9467 * models/AUD_USD_5m_1mo.pkl\n",
      "c7120b1fa7 * models/EUR_USD_15m_60d.pkl\n",
      "bfde667918 * models/EUR_USD_1d_5y.pkl\n",
      "f81697d5a9 * models/EUR_USD_1h_2y.pkl\n",
      "f80548f6b9 * models/EUR_USD_1m_7d.pkl\n",
      "b722890a14 * models/EUR_USD_5m_1mo.pkl\n",
      "5c470acfa4 * models/GBP_USD_15m_60d.pkl\n",
      "91299cdf09 * models/GBP_USD_1d_5y.pkl\n",
      "afdf742f1d * models/GBP_USD_1h_2y.pkl\n",
      "c540c22a8a * models/GBP_USD_1m_7d.pkl\n",
      "f92222e036 * models/GBP_USD_5m_1mo.pkl\n",
      "63afe3b11e * models/USD_JPY_15m_60d.pkl\n",
      "61b7d377fa * models/USD_JPY_1d_5y.pkl\n",
      "31ee5ad988 * models/USD_JPY_1h_2y.pkl\n",
      "9b9b37bda1 * models/USD_JPY_1m_7d.pkl\n",
      "4e694667e1 * models/USD_JPY_5m_1mo.pkl\n",
      "84ad53ab35 * population.pkl\n",
      "926248e52d * trade_memory.pkl\n",
      "✅ Fresh-run GitHub repo workflow complete!\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Fully Automatic Fresh-Run GitHub Workflow (PAT-safe)\n",
    "# =========================================\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# -----------------------------\n",
    "# 0️⃣ User Config\n",
    "# -----------------------------\n",
    "GITHUB_USERNAME = \"rahim-dotAI\"\n",
    "GITHUB_REPO = \"forex-ai-models\"\n",
    "REPO_FOLDER = GITHUB_REPO  # Local folder\n",
    "GIT_USER_EMAIL = \"nakatonabira3@gmail.com\"\n",
    "\n",
    "# Use environment variable for token\n",
    "GITHUB_PAT = os.environ.get(\"FOREX_PAT\")\n",
    "if not GITHUB_PAT:\n",
    "    raise ValueError(\"❌ Token not set! Define environment variable FOREX_PAT in Colab or GitHub Actions secret.\")\n",
    "\n",
    "BRANCH = \"main\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Helper function to run shell safely\n",
    "# -----------------------------\n",
    "def safe_run(cmd, shell=True, check=True):\n",
    "    \"\"\"Run shell command safely with logging.\"\"\"\n",
    "    try:\n",
    "        subprocess.run(cmd, shell=shell, check=check)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"⚠️ Command failed: {cmd}\\n   Reason: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Detect sudo availability\n",
    "# -----------------------------\n",
    "USE_SUDO = shutil.which(\"sudo\") is not None\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Install Git and Git LFS if missing\n",
    "# -----------------------------\n",
    "if shutil.which(\"git\") is None:\n",
    "    cmd = \"apt-get update -qq && apt-get install -y git\"\n",
    "    if USE_SUDO:\n",
    "        cmd = \"sudo \" + cmd\n",
    "    safe_run(cmd)\n",
    "else:\n",
    "    print(\"✅ Git already installed.\")\n",
    "\n",
    "if shutil.which(\"git-lfs\") is None:\n",
    "    cmd = \"apt-get install -y git-lfs\"\n",
    "    if USE_SUDO:\n",
    "        cmd = \"sudo \" + cmd\n",
    "    safe_run(cmd)\n",
    "else:\n",
    "    print(\"✅ Git LFS already installed.\")\n",
    "\n",
    "safe_run(\"git lfs install\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Remove local repo for fresh run\n",
    "# -----------------------------\n",
    "if os.path.exists(REPO_FOLDER):\n",
    "    print(f\"🗑️ Removing existing local repo '{REPO_FOLDER}' for a fresh run...\")\n",
    "    shutil.rmtree(REPO_FOLDER)\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Configure Git identity\n",
    "# -----------------------------\n",
    "print(\"🔧 Configuring Git identity...\")\n",
    "safe_run(f'git config --global user.name \"{GITHUB_USERNAME}\"')\n",
    "safe_run(f'git config --global user.email \"{GIT_USER_EMAIL}\"')\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Clone repo fresh (PAT-safe)\n",
    "# -----------------------------\n",
    "REPO_URL = f\"https://{GITHUB_USERNAME}:{GITHUB_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
    "print(f\"📥 Cloning repo '{REPO_FOLDER}' from GitHub...\")\n",
    "safe_run(f\"git clone {REPO_URL}\")\n",
    "\n",
    "orig_dir = os.getcwd()\n",
    "os.chdir(REPO_FOLDER)\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣a Pull LFS-tracked files\n",
    "# -----------------------------\n",
    "print(\"📦 Pulling Git LFS-tracked files...\")\n",
    "safe_run(f\"git lfs pull https://{GITHUB_USERNAME}:{GITHUB_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ Track CSV/PKL files with Git LFS\n",
    "# -----------------------------\n",
    "print(\"📌 Tracking CSV/PKL files with Git LFS...\")\n",
    "safe_run(\"git lfs track '*.csv'\")\n",
    "safe_run(\"git lfs track '*.pkl'\")\n",
    "safe_run(\"git add .gitattributes\")\n",
    "safe_run('git commit -m \"Track CSV/PKL files with Git LFS\" || echo \"No .gitattributes changes\"')\n",
    "\n",
    "# -----------------------------\n",
    "# 8️⃣ Stage, commit, and push changes safely (PAT-safe)\n",
    "# -----------------------------\n",
    "print(\"📂 Staging all new/modified files...\")\n",
    "safe_run(\"git add -A\")\n",
    "\n",
    "# Only commit if there are changes\n",
    "status_result = subprocess.run(\"git status --porcelain\", shell=True, capture_output=True, text=True)\n",
    "if status_result.stdout.strip():\n",
    "    safe_run('git commit -m \"Auto-update: new or modified files\"')\n",
    "    print(\"🚀 Pushing changes to GitHub (PAT-safe)...\")\n",
    "    safe_run(f\"git push https://{GITHUB_USERNAME}:{GITHUB_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git {BRANCH}\")\n",
    "else:\n",
    "    print(\"✅ No changes detected, skipping commit/push.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9️⃣ List LFS-tracked files\n",
    "# -----------------------------\n",
    "print(\"📋 LFS-tracked files:\")\n",
    "safe_run(\"git lfs ls-files\")\n",
    "\n",
    "# -----------------------------\n",
    "# 10️⃣ Return to original directory\n",
    "# -----------------------------\n",
    "os.chdir(orig_dir)\n",
    "print(\"✅ Fresh-run GitHub repo workflow complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792803b8",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-06T02:42:23.695709Z",
     "iopub.status.busy": "2025-10-06T02:42:23.695483Z",
     "iopub.status.idle": "2025-10-06T02:42:23.699298Z",
     "shell.execute_reply": "2025-10-06T02:42:23.698713Z"
    },
    "id": "JS9qXRF_JXJO",
    "papermill": {
     "duration": 0.011526,
     "end_time": "2025-10-06T02:42:23.700268",
     "exception": false,
     "start_time": "2025-10-06T02:42:23.688742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Vantage Key: 1W58NPZXOG5SLHZ6\n",
      "Browserless Token: 2St0qUktyKsA0Bsb5b510553885cae26942e44c26c0f19c3d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set your keys (only for this session)\n",
    "os.environ['ALPHA_VANTAGE_KEY'] = '1W58NPZXOG5SLHZ6'\n",
    "os.environ['BROWSERLESS_TOKEN'] = '2St0qUktyKsA0Bsb5b510553885cae26942e44c26c0f19c3d'\n",
    "\n",
    "# Test if they work\n",
    "print(\"Alpha Vantage Key:\", os.environ.get('ALPHA_VANTAGE_KEY'))\n",
    "print(\"Browserless Token:\", os.environ.get('BROWSERLESS_TOKEN'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c88f9f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:42:23.713631Z",
     "iopub.status.busy": "2025-10-06T02:42:23.713465Z",
     "iopub.status.idle": "2025-10-06T02:42:28.039005Z",
     "shell.execute_reply": "2025-10-06T02:42:28.038501Z"
    },
    "id": "ScZwa5zIQDpD",
    "papermill": {
     "duration": 4.333304,
     "end_time": "2025-10-06T02:42:28.039931",
     "exception": false,
     "start_time": "2025-10-06T02:42:23.706627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "📥 Cloning repo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'forex-alpha-models'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  15% (24/152)\r",
      "Filtering content:  16% (25/152)\r",
      "Filtering content:  17% (26/152)\r",
      "Filtering content:  18% (28/152)\r",
      "Filtering content:  19% (29/152)\r",
      "Filtering content:  20% (31/152)\r",
      "Filtering content:  21% (32/152)\r",
      "Filtering content:  22% (34/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  23% (35/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  24% (37/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  25% (38/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  26% (40/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  27% (42/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  28% (43/152), 92.92 MiB | 175.38 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  29% (45/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  30% (46/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  31% (48/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  32% (49/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  33% (51/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  34% (52/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  35% (54/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  36% (55/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  37% (57/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  38% (58/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  39% (60/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  40% (61/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  41% (63/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  42% (64/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  43% (66/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  44% (67/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  45% (69/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  46% (70/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  47% (72/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  48% (73/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  49% (75/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  50% (76/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  51% (78/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  52% (80/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  53% (81/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  54% (83/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  55% (84/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  56% (86/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  57% (87/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  58% (89/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  59% (90/152), 92.92 MiB | 175.38 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  60% (92/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  61% (93/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  62% (95/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  63% (96/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  64% (98/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  65% (99/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  66% (101/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  67% (102/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  68% (104/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  69% (105/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  70% (107/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  71% (108/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  72% (110/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  73% (111/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  74% (113/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  75% (114/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  76% (116/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  77% (118/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  78% (119/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  79% (121/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  80% (122/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  81% (124/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  82% (125/152), 92.92 MiB | 175.38 MiB/s\r",
      "Filtering content:  83% (127/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  84% (128/152), 141.49 MiB | 135.44 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  85% (130/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  86% (131/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  87% (133/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  88% (134/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  89% (136/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  90% (137/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  91% (139/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  92% (140/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  93% (142/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  94% (143/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  95% (145/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  96% (146/152), 141.49 MiB | 135.44 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  97% (148/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  98% (149/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content:  99% (151/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content: 100% (152/152), 141.49 MiB | 135.44 MiB/s\r",
      "Filtering content: 100% (152/152), 141.54 MiB | 73.61 MiB/s, done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUD/USD no changes\n",
      "GBP/USD no changes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR/USD no changes\n",
      "USD/JPY no changes\n",
      "✅ No changes to commit.\n",
      "✅ All FX pairs processed, saved, and pushed (LFS-ready, 403-proof).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# ======================================================\n",
    "# CONFIGURATION\n",
    "# ======================================================\n",
    "SAVE_FOLDER = Path(\"/content/forex-alpha-models\") if \"google.colab\" in str(os.sys.modules) else Path(\"./forex-alpha-models\")\n",
    "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PICKLE_FOLDER = SAVE_FOLDER / \"pickles\"\n",
    "PICKLE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "CSV_FOLDER = SAVE_FOLDER / \"csvs\"\n",
    "CSV_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Abdul Rahim\")\n",
    "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
    "GITHUB_USERNAME = \"rahim-dotAI\"\n",
    "GITHUB_REPO = \"forex-ai-models\"\n",
    "GITHUB_TOKEN = os.environ.get(\"FOREX_PAT\")\n",
    "BRANCH = \"main\"\n",
    "\n",
    "ALPHA_VANTAGE_KEY = os.environ.get(\"ALPHA_VANTAGE_KEY\")\n",
    "if not ALPHA_VANTAGE_KEY:\n",
    "    raise ValueError(\"❌ Missing ALPHA_VANTAGE_KEY environment variable!\")\n",
    "\n",
    "# ======================================================\n",
    "# SAFE GIT + LFS SETUP\n",
    "# ======================================================\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
    "\n",
    "cred_file = Path.home() / \".git-credentials\"\n",
    "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com\\n\")\n",
    "\n",
    "subprocess.run([\"git\", \"lfs\", \"install\"], check=False)\n",
    "\n",
    "# ======================================================\n",
    "# HELPERS\n",
    "# ======================================================\n",
    "def ensure_tz_naive(df):\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "    if df.index.tz is not None:\n",
    "        df.index = df.index.tz_convert(None)\n",
    "    return df\n",
    "\n",
    "def file_hash(filepath, chunk_size=8192):\n",
    "    if not filepath.exists():\n",
    "        return None\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "def fetch_alpha_vantage_fx(pair, outputsize='compact', max_retries=3, retry_delay=5):\n",
    "    base_url = 'https://www.alphavantage.co/query'\n",
    "    from_currency, to_currency = pair.split('/')\n",
    "    params = {\n",
    "        'function': 'FX_DAILY',\n",
    "        'from_symbol': from_currency,\n",
    "        'to_symbol': to_currency,\n",
    "        'outputsize': outputsize,\n",
    "        'datatype': 'json',\n",
    "        'apikey': ALPHA_VANTAGE_KEY\n",
    "    }\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = requests.get(base_url, params=params, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            if 'Time Series FX (Daily)' not in data:\n",
    "                raise ValueError(f\"Unexpected API response: {data}\")\n",
    "            ts = data['Time Series FX (Daily)']\n",
    "            df = pd.DataFrame(ts).T\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            df = df.sort_index()\n",
    "            df = df.rename(columns={\n",
    "                '1. open':'open',\n",
    "                '2. high':'high',\n",
    "                '3. low':'low',\n",
    "                '4. close':'close'\n",
    "            }).astype(float)\n",
    "            return ensure_tz_naive(df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Attempt {attempt+1} failed fetching {pair}: {e}\")\n",
    "            time.sleep(retry_delay)\n",
    "    print(f\"❌ Failed to fetch {pair} after {max_retries} retries\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# ======================================================\n",
    "# REPO ENSURE\n",
    "# ======================================================\n",
    "def ensure_repo_cloned(repo_url, repo_folder, branch=\"main\"):\n",
    "    repo_folder = Path(repo_folder)\n",
    "    if not (repo_folder / \".git\").exists():\n",
    "        if repo_folder.exists():\n",
    "            subprocess.run([\"rm\", \"-rf\", str(repo_folder)], check=True)\n",
    "        print(\"📥 Cloning repo...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"-b\", branch, repo_url, str(repo_folder)], check=True)\n",
    "    else:\n",
    "        print(\"🔄 Repo exists, pulling latest...\")\n",
    "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"fetch\", \"origin\"], check=True)\n",
    "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"checkout\", branch], check=False)\n",
    "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"pull\", \"origin\", branch], check=False)\n",
    "        subprocess.run([\"git\", \"-C\", str(repo_folder), \"lfs\", \"pull\"], check=False)\n",
    "\n",
    "# ======================================================\n",
    "# THREAD-SAFE FX PROCESSING\n",
    "# ======================================================\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_pair(pair):\n",
    "    filename = f\"{pair.replace('/', '_')}.csv\"\n",
    "    filepath = SAVE_FOLDER / filename\n",
    "\n",
    "    existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
    "    old_hash = file_hash(filepath)\n",
    "    new_df = fetch_alpha_vantage_fx(pair)\n",
    "    if new_df.empty:\n",
    "        return None, f\"No new data for {pair}\"\n",
    "\n",
    "    combined_df = pd.concat([existing_df, new_df]) if not existing_df.empty else new_df\n",
    "    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
    "    combined_df.sort_index(inplace=True)\n",
    "\n",
    "    with lock:\n",
    "        combined_df.to_csv(filepath)\n",
    "\n",
    "    new_hash = file_hash(filepath)\n",
    "    changed = old_hash != new_hash\n",
    "    return str(filepath) if changed else None, f\"{pair} {'updated' if changed else 'no changes'}\"\n",
    "\n",
    "# ======================================================\n",
    "# MAIN EXECUTION\n",
    "# ======================================================\n",
    "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "REPO_URL = f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
    "\n",
    "ensure_repo_cloned(REPO_URL, SAVE_FOLDER, BRANCH)\n",
    "\n",
    "changed_files = []\n",
    "tasks = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for pair in pairs:\n",
    "        tasks.append(executor.submit(process_pair, pair))\n",
    "    for future in as_completed(tasks):\n",
    "        filepath, msg = future.result()\n",
    "        print(msg)\n",
    "        if filepath:\n",
    "            changed_files.append(filepath)\n",
    "\n",
    "if changed_files:\n",
    "    print(f\"🚀 Committing {len(changed_files)} updated files...\")\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"add\"] + changed_files, check=False)\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"commit\", \"-m\", \"Update Alpha Vantage FX data\"], check=False)\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"push\", \"origin\", BRANCH], check=False)\n",
    "else:\n",
    "    print(\"✅ No changes to commit.\")\n",
    "\n",
    "print(\"✅ All FX pairs processed, saved, and pushed (LFS-ready, 403-proof).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980853b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:42:28.054450Z",
     "iopub.status.busy": "2025-10-06T02:42:28.054281Z",
     "iopub.status.idle": "2025-10-06T02:42:35.255339Z",
     "shell.execute_reply": "2025-10-06T02:42:35.254690Z"
    },
    "id": "Vh2hht7KPGNC",
    "papermill": {
     "duration": 7.209348,
     "end_time": "2025-10-06T02:42:35.256287",
     "exception": false,
     "start_time": "2025-10-06T02:42:28.046939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Working directory: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models/forex-alpha-models/forex-alpha-models\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "📥 Cloning fresh repo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'forex-alpha-models'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:   7% (11/152)\r",
      "Filtering content:   8% (13/152)\r",
      "Filtering content:   9% (14/152)\r",
      "Filtering content:  10% (16/152)\r",
      "Filtering content:  11% (17/152)\r",
      "Filtering content:  12% (19/152)\r",
      "Filtering content:  13% (20/152)\r",
      "Filtering content:  14% (22/152)\r",
      "Filtering content:  15% (23/152)\r",
      "Filtering content:  16% (25/152)\r",
      "Filtering content:  17% (26/152)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  18% (28/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  19% (29/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  20% (31/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  21% (32/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  22% (34/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  23% (35/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  24% (37/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  25% (38/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  26% (40/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  27% (42/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  28% (43/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  29% (45/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  30% (46/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  31% (48/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  32% (49/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  33% (51/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  34% (52/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  35% (54/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  36% (55/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  37% (57/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  38% (58/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  39% (60/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  40% (61/152), 82.32 MiB | 153.84 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  41% (63/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  42% (64/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  43% (66/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  44% (67/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  45% (69/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  46% (70/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  47% (72/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  48% (73/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  49% (75/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  50% (76/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  51% (78/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  52% (80/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  53% (81/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  54% (83/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  55% (84/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  56% (86/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  57% (87/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  58% (89/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  59% (90/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  60% (92/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  61% (93/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  62% (95/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  63% (96/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  64% (98/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  65% (99/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  66% (101/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  67% (102/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  68% (104/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  69% (105/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  70% (107/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  71% (108/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  72% (110/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  73% (111/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  74% (113/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  75% (114/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  76% (116/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  77% (118/152), 82.32 MiB | 153.84 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  78% (119/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  79% (121/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  80% (122/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  81% (124/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  82% (125/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  83% (127/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  84% (128/152), 82.32 MiB | 153.84 MiB/s\r",
      "Filtering content:  85% (130/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  86% (131/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  87% (133/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  88% (134/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  89% (136/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  90% (137/152), 141.50 MiB | 134.95 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  91% (139/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  92% (140/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  93% (142/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  94% (143/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  95% (145/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  96% (146/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  96% (147/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  97% (148/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  98% (149/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content:  99% (151/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content: 100% (152/152), 141.50 MiB | 134.95 MiB/s\r",
      "Filtering content: 100% (152/152), 141.54 MiB | 68.92 MiB/s, done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n",
      "/tmp/ipykernel_2893/1684014659.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Updated USD/JPY 5m_1mo\n",
      "📈 Updated AUD/USD 1d_5y\n",
      "📈 Updated EUR/USD 5m_1mo\n",
      "📈 Updated EUR/USD 1h_2y\n",
      "📈 Updated EUR/USD 1d_5y\n",
      "📈 Updated AUD/USD 1h_2y\n",
      "📈 Updated GBP/USD 5m_1mo\n",
      "📈 Updated USD/JPY 15m_60d\n",
      "📈 Updated EUR/USD 1m_7d\n",
      "📈 Updated AUD/USD 15m_60d\n",
      "📈 Updated GBP/USD 1h_2y\n",
      "📈 Updated AUD/USD 5m_1mo\n",
      "📈 Updated USD/JPY 1m_7d\n",
      "📈 Updated GBP/USD 1d_5y\n",
      "📈 Updated AUD/USD 1m_7d\n",
      "📈 Updated GBP/USD 1m_7d\n",
      "📈 Updated USD/JPY 1d_5y\n",
      "📈 Updated EUR/USD 15m_60d\n",
      "📈 Updated USD/JPY 1h_2y\n",
      "📈 Updated GBP/USD 15m_60d\n",
      "🚀 Committing 20 updated files...\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   AUD_USD_15m_60d.csv\n",
      "\tmodified:   AUD_USD_1d_5y.csv\n",
      "\tmodified:   AUD_USD_1h_2y.csv\n",
      "\tmodified:   AUD_USD_1m_7d.csv\n",
      "\tmodified:   AUD_USD_5m_1mo.csv\n",
      "\tmodified:   EUR_USD_15m_60d.csv\n",
      "\tmodified:   EUR_USD_1d_5y.csv\n",
      "\tmodified:   EUR_USD_1h_2y.csv\n",
      "\tmodified:   EUR_USD_1m_7d.csv\n",
      "\tmodified:   EUR_USD_5m_1mo.csv\n",
      "\tmodified:   GBP_USD_15m_60d.csv\n",
      "\tmodified:   GBP_USD_1d_5y.csv\n",
      "\tmodified:   GBP_USD_1h_2y.csv\n",
      "\tmodified:   GBP_USD_1m_7d.csv\n",
      "\tmodified:   GBP_USD_5m_1mo.csv\n",
      "\tmodified:   USD_JPY_15m_60d.csv\n",
      "\tmodified:   USD_JPY_1d_5y.csv\n",
      "\tmodified:   USD_JPY_1h_2y.csv\n",
      "\tmodified:   USD_JPY_1m_7d.csv\n",
      "\tmodified:   USD_JPY_5m_1mo.csv\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'forex-alpha-models/USD_JPY_5m_1mo.csv' did not match any files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 All FX pairs & timeframes processed safely (Colab & GHA compatible, LFS & 403-proof).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# ============================================================\n",
    "# 1️⃣ SAFE WORKING FOLDER (Colab & GitHub Actions)\n",
    "# ============================================================\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
    "\n",
    "SAVE_FOLDER = Path(\"/content/forex-alpha-models\") if IN_COLAB else Path(\"./forex-alpha-models\")\n",
    "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(SAVE_FOLDER)\n",
    "print(f\"✅ Working directory: {SAVE_FOLDER.resolve()}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣ GIT CONFIG\n",
    "# ============================================================\n",
    "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Abdul Rahim\")\n",
    "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
    "GITHUB_USERNAME = \"rahim-dotAI\"\n",
    "GITHUB_REPO = \"forex-ai-models\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=False)\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣ TOKENS\n",
    "# ============================================================\n",
    "GITHUB_TOKEN = os.environ.get(\"FOREX_PAT\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"❌ FOREX_PAT missing! Set environment variable.\")\n",
    "\n",
    "cred_file = Path.home() / \".git-credentials\"\n",
    "cred_file.write_text(f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com\\n\")\n",
    "\n",
    "subprocess.run([\"git\", \"lfs\", \"install\"], check=False)\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣ REPO ENSURE\n",
    "# ============================================================\n",
    "def ensure_repo():\n",
    "    if not (SAVE_FOLDER / \".git\").exists():\n",
    "        if SAVE_FOLDER.exists():\n",
    "            subprocess.run([\"rm\", \"-rf\", str(SAVE_FOLDER)], check=True)\n",
    "        print(\"📥 Cloning fresh repo...\")\n",
    "        subprocess.run([\n",
    "            \"git\", \"clone\", \"-b\", BRANCH,\n",
    "            f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\",\n",
    "            str(SAVE_FOLDER)\n",
    "        ], check=True)\n",
    "    else:\n",
    "        print(\"🔄 Repo exists, pulling latest...\")\n",
    "        subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"fetch\", \"origin\"], check=True)\n",
    "        subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"checkout\", BRANCH], check=False)\n",
    "        subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"pull\", \"origin\", BRANCH], check=False)\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"lfs\", \"pull\"], check=False)\n",
    "\n",
    "ensure_repo()\n",
    "\n",
    "# ============================================================\n",
    "# 5️⃣ FX CONFIG\n",
    "# ============================================================\n",
    "FX_PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "TIMEFRAMES = {\n",
    "    \"1m_7d\": (\"1m\", \"7d\"),\n",
    "    \"5m_1mo\": (\"5m\", \"1mo\"),\n",
    "    \"15m_60d\": (\"15m\", \"60d\"),\n",
    "    \"1h_2y\": (\"1h\", \"2y\"),\n",
    "    \"1d_5y\": (\"1d\", \"5y\")\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# 6️⃣ HELPERS\n",
    "# ============================================================\n",
    "def file_hash(filepath, chunk_size=8192):\n",
    "    if not filepath.exists():\n",
    "        return None\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "def ensure_tz_naive(df):\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "    if df.index.tz is not None:\n",
    "        df.index = df.index.tz_convert(None)\n",
    "    return df\n",
    "\n",
    "def merge_data(existing_df, new_df):\n",
    "    existing_df = ensure_tz_naive(existing_df)\n",
    "    new_df = ensure_tz_naive(new_df)\n",
    "    if existing_df.empty:\n",
    "        return new_df\n",
    "    if new_df.empty:\n",
    "        return existing_df\n",
    "    combined_df = pd.concat([existing_df, new_df])\n",
    "    combined_df = combined_df[~combined_df.index.duplicated(keep=\"last\")]\n",
    "    combined_df.sort_index(inplace=True)\n",
    "    return combined_df\n",
    "\n",
    "# ============================================================\n",
    "# 7️⃣ WORKER FUNCTION\n",
    "# ============================================================\n",
    "def process_pair_tf(pair, tf_name, interval, period, max_retries=3, retry_delay=5):\n",
    "    symbol = pair.replace(\"/\", \"\") + \"=X\"\n",
    "    filename = f\"{pair.replace('/', '_')}_{tf_name}.csv\"\n",
    "    filepath = SAVE_FOLDER / filename\n",
    "\n",
    "    existing_df = pd.read_csv(filepath, index_col=0, parse_dates=True) if filepath.exists() else pd.DataFrame()\n",
    "    old_hash = file_hash(filepath)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            df = yf.download(symbol, interval=interval, period=period, progress=False, auto_adjust=False, threads=True)\n",
    "            if df.empty:\n",
    "                raise ValueError(\"No data returned\")\n",
    "            df = df[[c for c in ['Open','High','Low','Close','Volume'] if c in df.columns]].copy()\n",
    "            df.rename(columns=lambda x: x.lower(), inplace=True)\n",
    "            df = ensure_tz_naive(df)\n",
    "\n",
    "            combined_df = merge_data(existing_df, df)\n",
    "            combined_df.to_csv(filepath)\n",
    "            if old_hash != file_hash(filepath):\n",
    "                return f\"📈 Updated {pair} {tf_name}\", str(filepath)\n",
    "            else:\n",
    "                return f\"✅ No changes {pair} {tf_name}\", None\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Attempt {attempt+1}/{max_retries} failed for {pair} {tf_name}: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                return f\"❌ Failed {pair} {tf_name}\", None\n",
    "\n",
    "# ============================================================\n",
    "# 8️⃣ PARALLEL EXECUTION (Snake-safe with glob)\n",
    "# ============================================================\n",
    "changed_files = []\n",
    "tasks = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for pair in FX_PAIRS:\n",
    "        for tf_name, (interval, period) in TIMEFRAMES.items():\n",
    "            tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period))\n",
    "\n",
    "for future in as_completed(tasks):\n",
    "    msg, filename = future.result()\n",
    "    print(msg)\n",
    "    if filename:\n",
    "        changed_files.append(filename)\n",
    "\n",
    "# ============================================================\n",
    "# 9️⃣ COMMIT & PUSH\n",
    "# ============================================================\n",
    "if changed_files:\n",
    "    print(f\"🚀 Committing {len(changed_files)} updated files...\")\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"add\"] + changed_files, check=False)\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"commit\", \"-m\", \"Update multiple FX files\"], check=False)\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"push\", \"origin\", BRANCH], check=False)\n",
    "else:\n",
    "    print(\"✅ No changes detected, nothing to push.\")\n",
    "\n",
    "print(\"🎯 All FX pairs & timeframes processed safely (Colab & GHA compatible, LFS & 403-proof).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29c5a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:42:35.273758Z",
     "iopub.status.busy": "2025-10-06T02:42:35.273558Z",
     "iopub.status.idle": "2025-10-06T02:42:38.485470Z",
     "shell.execute_reply": "2025-10-06T02:42:38.484836Z"
    },
    "id": "1tPwXDNssbcf",
    "papermill": {
     "duration": 3.221643,
     "end_time": "2025-10-06T02:42:38.486375",
     "exception": false,
     "start_time": "2025-10-06T02:42:35.264732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'forex-alpha-models'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Working directory: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models\n",
      "✅ Git configured: Rahim AI Bot <nakatonabira3@gmail.com>\n",
      "📥 Cloning repo (attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  14% (22/152)\r",
      "Filtering content:  15% (23/152)\r",
      "Filtering content:  16% (25/152)\r",
      "Filtering content:  17% (26/152)\r",
      "Filtering content:  18% (28/152)\r",
      "Filtering content:  19% (29/152)\r",
      "Filtering content:  20% (31/152)\r",
      "Filtering content:  21% (32/152)\r",
      "Filtering content:  22% (34/152)\r",
      "Filtering content:  23% (35/152)\r",
      "Filtering content:  24% (37/152)\r",
      "Filtering content:  25% (38/152)\r",
      "Filtering content:  26% (40/152)\r",
      "Filtering content:  27% (42/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  28% (43/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  29% (45/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  30% (46/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  31% (48/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  32% (49/152), 108.37 MiB | 206.54 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  33% (51/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  34% (52/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  35% (54/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  36% (55/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  37% (57/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  38% (58/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  39% (60/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  40% (61/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  41% (63/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  42% (64/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  43% (66/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  44% (67/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  45% (69/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  46% (70/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  47% (72/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  48% (73/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  49% (75/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  50% (76/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  51% (78/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  52% (80/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  53% (81/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  54% (83/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  55% (84/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  56% (86/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  57% (87/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  58% (89/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  59% (90/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  60% (92/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  61% (93/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  62% (95/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  63% (96/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  64% (98/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  65% (99/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  66% (101/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  67% (102/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  68% (104/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  69% (105/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  70% (107/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  71% (108/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  72% (110/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  73% (111/152), 108.37 MiB | 206.54 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  74% (113/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  75% (114/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  76% (116/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  77% (118/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  78% (119/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  79% (121/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  80% (122/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  81% (124/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  82% (125/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  83% (127/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  84% (128/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  85% (130/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  86% (131/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  87% (133/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  88% (134/152), 108.37 MiB | 206.54 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  89% (136/152), 108.37 MiB | 206.54 MiB/s\r",
      "Filtering content:  90% (137/152), 141.52 MiB | 133.17 MiB/s\r",
      "Filtering content:  91% (139/152), 141.52 MiB | 133.17 MiB/s\r",
      "Filtering content:  92% (140/152), 141.52 MiB | 133.17 MiB/s\r",
      "Filtering content:  93% (142/152), 141.52 MiB | 133.17 MiB/s\r",
      "Filtering content:  94% (143/152), 141.52 MiB | 133.17 MiB/s\r",
      "Filtering content:  95% (145/152), 141.52 MiB | 133.17 MiB/s\r",
      "Filtering content:  96% (146/152), 141.52 MiB | 133.17 MiB/s\r",
      "Filtering content:  97% (148/152), 141.52 MiB | 133.17 MiB/s\r",
      "Filtering content:  98% (149/152), 141.52 MiB | 133.17 MiB/s\r",
      "Filtering content:  99% (151/152), 141.52 MiB | 133.17 MiB/s\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "\"*_combined.csv\" already supported\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "No changes\n",
      "✅ Output folders ready: forex-alpha-models/pickles, forex-alpha-models/csvs\n",
      "⚪ No data to combine for GBP/USD 5m_1mo\n",
      "⚪ No data to combine for AUD/USD 15m_60d\n",
      "⚪ No data to combine for GBP/USD 1d_5y\n",
      "⚪ No data to combine for USD/JPY 5m_1mo\n",
      "⚪ No data to combine for EUR/USD 1h_2y\n",
      "⚪ No data to combine for USD/JPY 1h_2y\n",
      "⚪ No data to combine for AUD/USD 1d_5y\n",
      "⚪ No data to combine for USD/JPY 1m_7d\n",
      "⚪ No data to combine for EUR/USD 5m_1mo\n",
      "⚪ No data to combine for GBP/USD 1m_7d\n",
      "⚪ No data to combine for USD/JPY 15m_60d\n",
      "⚪ No data to combine for EUR/USD 1d_5y\n",
      "⚪ No data to combine for GBP/USD 1h_2y\n",
      "⚪ No data to combine for GBP/USD 15m_60d\n",
      "⚪ No data to combine for USD/JPY 1d_5y\n",
      "⚪ No data to combine for EUR/USD 1m_7d\n",
      "⚪ No data to combine for EUR/USD 15m_60d\n",
      "⚪ No data to combine for AUD/USD 1h_2y\n",
      "⚪ No data to combine for AUD/USD 5m_1mo\n",
      "⚪ No data to combine for AUD/USD 1m_7d\n",
      "✅ No combined files changed, nothing to push.\n",
      "🎯 All FX pairs processed and combined (parallel, single push).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content: 100% (152/152), 141.52 MiB | 133.17 MiB/s\r",
      "Filtering content: 100% (152/152), 141.54 MiB | 75.21 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# FX Data Processing & Git Automation - Updated\n",
    "# ======================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# -----------------------------\n",
    "# 0️⃣ Environment Detection & Safe Folder\n",
    "# -----------------------------\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    SAVE_FOLDER = Path(\"/content/forex-alpha-models\")\n",
    "else:\n",
    "    SAVE_FOLDER = Path(\"./forex-alpha-models\")\n",
    "\n",
    "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(SAVE_FOLDER)\n",
    "print(f\"✅ Working directory: {SAVE_FOLDER.resolve()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Git Config & Repo Info\n",
    "# -----------------------------\n",
    "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Abdul Rahim\")\n",
    "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
    "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
    "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
    "GITHUB_TOKEN = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
    "\n",
    "if not GITHUB_TOKEN and IN_GHA:\n",
    "    raise ValueError(\"❌ Token not set! Define environment variable FOREX_PAT.\")\n",
    "\n",
    "BRANCH = \"main\"\n",
    "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
    "\n",
    "# Configure Git\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
    "print(f\"✅ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Ensure Repo & Git LFS\n",
    "# -----------------------------\n",
    "def ensure_repo():\n",
    "    \"\"\"Clone or update repo safely with retries and Git LFS.\"\"\"\n",
    "    if not (SAVE_FOLDER / \".git\").exists():\n",
    "        if SAVE_FOLDER.exists():\n",
    "            subprocess.run([\"rm\", \"-rf\", str(SAVE_FOLDER)], check=True)\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                print(f\"📥 Cloning repo (attempt {attempt+1})...\")\n",
    "                subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(SAVE_FOLDER)], check=True)\n",
    "                break\n",
    "            except subprocess.CalledProcessError:\n",
    "                time.sleep(5)\n",
    "    else:\n",
    "        print(\"🔄 Repo exists, pulling latest changes...\")\n",
    "        subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"fetch\", \"origin\"], check=True)\n",
    "        subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"checkout\", BRANCH], check=False)\n",
    "        subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"pull\", \"origin\", BRANCH], check=False)\n",
    "\n",
    "    # Git LFS setup\n",
    "    if IN_COLAB:\n",
    "        subprocess.run(\"apt-get update && apt-get install git-lfs -y\", shell=True)\n",
    "    subprocess.run([\"git\", \"lfs\", \"install\"], check=False)\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"lfs\", \"track\", \"*_combined.csv\"], check=False)\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"add\", \".gitattributes\"], check=False)\n",
    "    subprocess.run(f'git -C {SAVE_FOLDER} commit -m \"Track combined CSVs with Git LFS\" || echo \"No changes\"', shell=True)\n",
    "\n",
    "ensure_repo()\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Output Folders\n",
    "# -----------------------------\n",
    "PICKLE_FOLDER = SAVE_FOLDER / \"pickles\"\n",
    "CSV_FOLDER = SAVE_FOLDER / \"csvs\"\n",
    "for folder in [PICKLE_FOLDER, CSV_FOLDER]:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"✅ Output folders ready: {PICKLE_FOLDER}, {CSV_FOLDER}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Helper Functions\n",
    "# -----------------------------\n",
    "def ensure_tz_naive(df):\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def file_hash(filepath, chunk_size=8192):\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "def combine_fx_data(av_df, yf_df):\n",
    "    av_df = ensure_tz_naive(av_df)\n",
    "    yf_df = ensure_tz_naive(yf_df)\n",
    "    if av_df.empty and yf_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    combined_df = pd.merge(\n",
    "        yf_df, av_df[['open','high','low','close']],\n",
    "        left_index=True, right_index=True, how='outer', suffixes=('','_av')\n",
    "    )\n",
    "    for col in ['open','high','low','close']:\n",
    "        combined_df[col] = combined_df[col].fillna(combined_df[f'{col}_av'])\n",
    "    combined_df.drop(columns=[f'{col}_av' for col in ['open','high','low','close']], errors='ignore', inplace=True)\n",
    "    combined_df.sort_index(inplace=True)\n",
    "    combined_df.dropna(subset=['open','high','low','close'], inplace=True)\n",
    "    combined_df = combined_df[~combined_df.index.duplicated(keep=\"last\")]\n",
    "    return combined_df\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Worker Function\n",
    "# -----------------------------\n",
    "def process_pair_tf(pair, tf_name, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            av_file = CSV_FOLDER / f\"{pair.replace('/','_')}_daily.csv\"\n",
    "            yf_file = CSV_FOLDER / f\"{pair.replace('/','_')}_{tf_name}.csv\"\n",
    "\n",
    "            av_df = ensure_tz_naive(pd.read_csv(av_file, index_col=0, parse_dates=True)) if av_file.exists() else pd.DataFrame()\n",
    "            yf_df = ensure_tz_naive(pd.read_csv(yf_file, index_col=0, parse_dates=True)) if yf_file.exists() else pd.DataFrame()\n",
    "\n",
    "            combined_df = combine_fx_data(av_df, yf_df)\n",
    "            if combined_df.empty:\n",
    "                return f\"⚪ No data to combine for {pair} {tf_name}\", None\n",
    "\n",
    "            combined_file = CSV_FOLDER / f\"{pair.replace('/','_')}_{tf_name}_combined.csv\"\n",
    "            old_hash = file_hash(combined_file)\n",
    "\n",
    "            with lock:\n",
    "                combined_df.to_csv(combined_file)\n",
    "\n",
    "            new_hash = file_hash(combined_file)\n",
    "            if old_hash != new_hash:\n",
    "                return f\"📌 Updated {pair} {tf_name}\", combined_file\n",
    "            else:\n",
    "                return f\"✅ No changes for {pair} {tf_name}\", None\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Attempt {attempt+1} failed for {pair} {tf_name}: {e}\")\n",
    "            time.sleep(3)\n",
    "    return f\"❌ Failed to combine {pair} {tf_name}\", None\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Parallel Execution\n",
    "# -----------------------------\n",
    "pairs = [\"EUR/USD\",\"GBP/USD\",\"USD/JPY\",\"AUD/USD\"]\n",
    "timeframes = [\"1m_7d\",\"5m_1mo\",\"15m_60d\",\"1h_2y\",\"1d_5y\"]\n",
    "\n",
    "changed_files = []\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = [executor.submit(process_pair_tf, p, tf) for p in pairs for tf in timeframes]\n",
    "    for future in as_completed(futures):\n",
    "        msg, file = future.result()\n",
    "        print(msg)\n",
    "        if file:\n",
    "            changed_files.append(str(file))\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ Commit & Push Changes\n",
    "# -----------------------------\n",
    "if changed_files:\n",
    "    print(f\"🚀 Committing {len(changed_files)} files...\")\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"add\"] + changed_files, check=False)\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"commit\", \"-m\", \"Update combined FX data\"], check=False)\n",
    "\n",
    "    push_cmd = f\"git -C {SAVE_FOLDER} push {REPO_URL} {BRANCH}\"\n",
    "    for attempt in range(3):\n",
    "        if subprocess.run(push_cmd, shell=True).returncode == 0:\n",
    "            print(\"✅ Push successful.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"⚠️ Push attempt {attempt+1} failed, retrying...\")\n",
    "            time.sleep(5)\n",
    "else:\n",
    "    print(\"✅ No combined files changed, nothing to push.\")\n",
    "\n",
    "print(\"🎯 All FX pairs processed and combined (parallel, single push).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac8ebf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:42:38.504000Z",
     "iopub.status.busy": "2025-10-06T02:42:38.503825Z",
     "iopub.status.idle": "2025-10-06T02:42:46.628563Z",
     "shell.execute_reply": "2025-10-06T02:42:46.628073Z"
    },
    "id": "z96OOkCQSzof",
    "papermill": {
     "duration": 8.13423,
     "end_time": "2025-10-06T02:42:46.629418",
     "exception": false,
     "start_time": "2025-10-06T02:42:38.495188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR/USD: 1.172\n",
      "GBP/USD: 1.345\n",
      "USD/JPY: 149.89\n",
      "AUD/USD: 0.6607\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def fetch_live_rate(pair):\n",
    "    \"\"\"\n",
    "    Fetch live FX rate from X-Rates using Browserless.\n",
    "    \"\"\"\n",
    "    from_currency, to_currency = pair.split('/')\n",
    "    browserless_token = os.environ.get('BROWSERLESS_TOKEN')\n",
    "    if not browserless_token:\n",
    "        raise ValueError(\"Set BROWSERLESS_TOKEN in your environment variables\")\n",
    "\n",
    "    url = f\"https://production-sfo.browserless.io/content?token={browserless_token}\"\n",
    "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
    "\n",
    "    try:\n",
    "        res = requests.post(url, json=payload)\n",
    "        # Regex to extract the FX value\n",
    "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
    "        return float(match.group(1).replace(',', '')) if match else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {pair}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# --- Fetch live prices for all pairs ---\n",
    "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "live_prices = {pair: fetch_live_rate(pair) for pair in pairs}\n",
    "\n",
    "for pair, price in live_prices.items():\n",
    "    print(f\"{pair}: {price}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67ce7b55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:42:46.647277Z",
     "iopub.status.busy": "2025-10-06T02:42:46.647103Z",
     "iopub.status.idle": "2025-10-06T02:42:54.312474Z",
     "shell.execute_reply": "2025-10-06T02:42:54.311884Z"
    },
    "id": "h6oAPmA8OWsG",
    "papermill": {
     "duration": 7.675076,
     "end_time": "2025-10-06T02:42:54.313312",
     "exception": false,
     "start_time": "2025-10-06T02:42:46.638236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Working directory: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models\n",
      "✅ Git configured: Rahim AI Bot <nakatonabira3@gmail.com>\n",
      "📥 Cloning fresh repository...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'forex-alpha-models'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  13% (20/152)\r",
      "Filtering content:  14% (22/152)\r",
      "Filtering content:  15% (23/152)\r",
      "Filtering content:  16% (25/152)\r",
      "Filtering content:  17% (26/152)\r",
      "Filtering content:  18% (28/152)\r",
      "Filtering content:  19% (29/152)\r",
      "Filtering content:  20% (31/152)\r",
      "Filtering content:  21% (32/152)\r",
      "Filtering content:  22% (34/152)\r",
      "Filtering content:  23% (35/152)\r",
      "Filtering content:  24% (37/152)\r",
      "Filtering content:  25% (38/152)\r",
      "Filtering content:  26% (40/152)\r",
      "Filtering content:  27% (42/152)\r",
      "Filtering content:  28% (43/152)\r",
      "Filtering content:  29% (45/152)\r",
      "Filtering content:  30% (46/152)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  31% (48/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  32% (49/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  33% (51/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  34% (52/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  35% (54/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  36% (55/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  37% (57/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  38% (58/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  39% (60/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  40% (61/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  41% (63/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  42% (64/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  43% (66/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  44% (67/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  45% (69/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  46% (70/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  47% (72/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  48% (73/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  49% (75/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  50% (76/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  51% (78/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  52% (80/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  53% (81/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  54% (83/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  55% (84/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  56% (86/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  57% (87/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  58% (89/152), 115.24 MiB | 223.62 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  59% (90/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  60% (92/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  61% (93/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  62% (95/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  63% (96/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  64% (98/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  65% (99/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  66% (101/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  67% (102/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  68% (104/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  69% (105/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  70% (107/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  71% (108/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  72% (110/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  73% (111/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  74% (113/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  75% (114/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  76% (116/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  77% (118/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  78% (119/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  79% (121/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  80% (122/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  81% (124/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  82% (125/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  83% (127/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  84% (128/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  85% (130/152), 115.24 MiB | 223.62 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  86% (131/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  87% (133/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  88% (134/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  89% (136/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  90% (137/152), 115.24 MiB | 223.62 MiB/s\r",
      "Filtering content:  91% (139/152), 141.52 MiB | 137.90 MiB/s\r",
      "Filtering content:  92% (140/152), 141.52 MiB | 137.90 MiB/s\r",
      "Filtering content:  93% (142/152), 141.52 MiB | 137.90 MiB/s\r",
      "Filtering content:  94% (143/152), 141.52 MiB | 137.90 MiB/s\r",
      "Filtering content:  95% (145/152), 141.52 MiB | 137.90 MiB/s\r",
      "Filtering content:  96% (146/152), 141.52 MiB | 137.90 MiB/s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering content:  97% (148/152), 141.52 MiB | 137.90 MiB/s\r",
      "Filtering content:  98% (149/152), 141.52 MiB | 137.90 MiB/s\r",
      "Filtering content:  99% (151/152), 141.52 MiB | 137.90 MiB/s\r",
      "Filtering content: 100% (152/152), 141.52 MiB | 137.90 MiB/s\r",
      "Filtering content: 100% (152/152), 141.54 MiB | 74.90 MiB/s, done.\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR/USD (EUR_USD.csv) updated\n",
      "GBP/USD (GBP_USD.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP/USD (GBP_USD_1d_5y.csv) updated\n",
      "GBP/USD (GBP_USD_1h_2y.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR/USD (EUR_USD_5m_1mo.csv) updated\n",
      "EUR/USD (EUR_USD_1d_5y.csv) updated\n",
      "EUR/USD (EUR_USD_1m_7d.csv) updated\n",
      "GBP/USD (GBP_USD_5m_1mo.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP/USD (GBP_USD_15m_60d.csv) updated\n",
      "EUR/USD (EUR_USD_15m_60d.csv) updated\n",
      "USD/JPY (USD_JPY.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR/USD (EUR_USD_1h_2y.csv) updated\n",
      "USD/JPY (USD_JPY_5m_1mo.csv) updated\n",
      "USD/JPY (USD_JPY_1m_7d.csv) updated\n",
      "USD/JPY (USD_JPY_15m_60d.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUD/USD (AUD_USD.csv) updated\n",
      "USD/JPY (USD_JPY_1h_2y.csv) updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893/3299132034.py:214: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
      "/tmp/ipykernel_2893/3299132034.py:94: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USD/JPY (USD_JPY_1d_5y.csv) updated\n",
      "AUD/USD (AUD_USD_1m_7d.csv) updated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP/USD (GBP_USD_1m_7d.csv) updated\n",
      "AUD/USD (AUD_USD_5m_1mo.csv) updated\n",
      "AUD/USD (AUD_USD_15m_60d.csv) updated\n",
      "AUD/USD (AUD_USD_1d_5y.csv) updated\n",
      "AUD/USD (AUD_USD_1h_2y.csv) updated\n",
      "🚀 Committing 24 modified files...\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   combined_with_indicators/AUD_USD_AUD_USD_15m_60d_combined.pkl\n",
      "\tmodified:   combined_with_indicators/AUD_USD_AUD_USD_1d_5y_combined.pkl\n",
      "\tmodified:   combined_with_indicators/AUD_USD_AUD_USD_1h_2y_combined.pkl\n",
      "\tmodified:   combined_with_indicators/AUD_USD_AUD_USD_1m_7d_combined.pkl\n",
      "\tmodified:   combined_with_indicators/AUD_USD_AUD_USD_5m_1mo_combined.pkl\n",
      "\tmodified:   combined_with_indicators/AUD_USD_AUD_USD_combined.pkl\n",
      "\tmodified:   combined_with_indicators/EUR_USD_EUR_USD_15m_60d_combined.pkl\n",
      "\tmodified:   combined_with_indicators/EUR_USD_EUR_USD_1d_5y_combined.pkl\n",
      "\tmodified:   combined_with_indicators/EUR_USD_EUR_USD_1h_2y_combined.pkl\n",
      "\tmodified:   combined_with_indicators/EUR_USD_EUR_USD_1m_7d_combined.pkl\n",
      "\tmodified:   combined_with_indicators/EUR_USD_EUR_USD_5m_1mo_combined.pkl\n",
      "\tmodified:   combined_with_indicators/EUR_USD_EUR_USD_combined.pkl\n",
      "\tmodified:   combined_with_indicators/GBP_USD_GBP_USD_15m_60d_combined.pkl\n",
      "\tmodified:   combined_with_indicators/GBP_USD_GBP_USD_1d_5y_combined.pkl\n",
      "\tmodified:   combined_with_indicators/GBP_USD_GBP_USD_1h_2y_combined.pkl\n",
      "\tmodified:   combined_with_indicators/GBP_USD_GBP_USD_1m_7d_combined.pkl\n",
      "\tmodified:   combined_with_indicators/GBP_USD_GBP_USD_5m_1mo_combined.pkl\n",
      "\tmodified:   combined_with_indicators/GBP_USD_GBP_USD_combined.pkl\n",
      "\tmodified:   combined_with_indicators/USD_JPY_USD_JPY_15m_60d_combined.pkl\n",
      "\tmodified:   combined_with_indicators/USD_JPY_USD_JPY_1d_5y_combined.pkl\n",
      "\tmodified:   combined_with_indicators/USD_JPY_USD_JPY_1h_2y_combined.pkl\n",
      "\tmodified:   combined_with_indicators/USD_JPY_USD_JPY_1m_7d_combined.pkl\n",
      "\tmodified:   combined_with_indicators/USD_JPY_USD_JPY_5m_1mo_combined.pkl\n",
      "\tmodified:   combined_with_indicators/USD_JPY_USD_JPY_combined.pkl\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'forex-alpha-models/combined_with_indicators/EUR_USD_EUR_USD_combined.pkl' did not match any files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Push complete.\n",
      "✅ All FX pairs processed and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# FX Data + Indicators Processing - Updated for Environment Awareness\n",
    "# ======================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import ta\n",
    "from ta.momentum import WilliamsRIndicator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# -----------------------------\n",
    "# 0️⃣ Environment Detection & Safe Folders\n",
    "# -----------------------------\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    SAVE_FOLDER = Path(\"/content/forex-alpha-models\")\n",
    "else:\n",
    "    SAVE_FOLDER = Path(\"./forex-alpha-models\")\n",
    "\n",
    "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(SAVE_FOLDER)\n",
    "print(f\"✅ Working directory: {SAVE_FOLDER.resolve()}\")\n",
    "\n",
    "combined_save_path = SAVE_FOLDER / \"combined_with_indicators\"\n",
    "combined_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Git Config & Repo Info\n",
    "# -----------------------------\n",
    "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Abdul Rahim\")\n",
    "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
    "GITHUB_USERNAME = os.environ.get(\"GITHUB_USERNAME\", \"rahim-dotAI\")\n",
    "GITHUB_REPO = os.environ.get(\"GITHUB_REPO\", \"forex-ai-models\")\n",
    "GITHUB_TOKEN = os.environ.get(\"FOREX_PAT\", \"\").strip()\n",
    "\n",
    "if not GITHUB_TOKEN and IN_GHA:\n",
    "    raise ValueError(\"❌ FOREX_PAT environment variable not found\")\n",
    "\n",
    "BRANCH = \"main\"\n",
    "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
    "\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", GIT_NAME], check=False)\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", GIT_EMAIL], check=False)\n",
    "print(f\"✅ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Ensure Repo Exists or Synced\n",
    "# -----------------------------\n",
    "def ensure_repo():\n",
    "    if not (SAVE_FOLDER / \".git\").exists():\n",
    "        print(\"📥 Cloning fresh repository...\")\n",
    "        if SAVE_FOLDER.exists():\n",
    "            subprocess.run([\"rm\", \"-rf\", str(SAVE_FOLDER)], check=False)\n",
    "        subprocess.run([\"git\", \"clone\", \"-b\", BRANCH, REPO_URL, str(SAVE_FOLDER)], check=True)\n",
    "    else:\n",
    "        print(\"🔄 Repo found, updating...\")\n",
    "        subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"fetch\", \"origin\"], check=False)\n",
    "        subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"checkout\", BRANCH], check=False)\n",
    "        subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"pull\", \"origin\", BRANCH], check=False)\n",
    "\n",
    "ensure_repo()\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Utilities\n",
    "# -----------------------------\n",
    "def file_hash(filepath, chunk_size=8192):\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "def ensure_tz_naive(df):\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def safe_numeric(df, columns=None):\n",
    "    if columns is None:\n",
    "        columns = ['open','high','low','close']\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(subset=columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "def combine_fx_data(av_df, yf_df):\n",
    "    av_df = ensure_tz_naive(av_df)\n",
    "    yf_df = ensure_tz_naive(yf_df)\n",
    "    if av_df is None or av_df.empty:\n",
    "        return safe_numeric(yf_df)\n",
    "    if yf_df is None or yf_df.empty:\n",
    "        return safe_numeric(av_df)\n",
    "    combined_df = pd.merge(\n",
    "        yf_df,\n",
    "        av_df[['open','high','low','close']],\n",
    "        left_index=True, right_index=True,\n",
    "        how='outer',\n",
    "        suffixes=('','_av')\n",
    "    )\n",
    "    for col in ['open','high','low','close']:\n",
    "        combined_df[col] = combined_df[col].fillna(combined_df[f'{col}_av'])\n",
    "    combined_df.drop(columns=[f'{col}_av' for col in ['open','high','low','close']], errors='ignore', inplace=True)\n",
    "    combined_df.sort_index(inplace=True)\n",
    "    combined_df = safe_numeric(combined_df)\n",
    "    combined_df = ensure_tz_naive(combined_df)\n",
    "    return combined_df\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Add Indicators\n",
    "# -----------------------------\n",
    "def add_all_indicators(df):\n",
    "    df = ensure_tz_naive(df)\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    df = safe_numeric(df.copy())\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Trend indicators\n",
    "    trend = {\n",
    "        'SMA_10': lambda d: ta.trend.sma_indicator(d['close'],10),\n",
    "        'SMA_50': lambda d: ta.trend.sma_indicator(d['close'],50),\n",
    "        'SMA_200': lambda d: ta.trend.sma_indicator(d['close'],200),\n",
    "        'EMA_10': lambda d: ta.trend.ema_indicator(d['close'],10),\n",
    "        'EMA_50': lambda d: ta.trend.ema_indicator(d['close'],50),\n",
    "        'EMA_200': lambda d: ta.trend.ema_indicator(d['close'],200),\n",
    "        'MACD': lambda d: ta.trend.macd(d['close']),\n",
    "        'MACD_signal': lambda d: ta.trend.macd_signal(d['close']),\n",
    "        'ADX': lambda d: ta.trend.adx(d['high'], d['low'], d['close'],14)\n",
    "    }\n",
    "\n",
    "    # Momentum indicators\n",
    "    momentum = {\n",
    "        'RSI_14': lambda d: ta.momentum.rsi(d['close'],14),\n",
    "        'StochRSI': lambda d: ta.momentum.stochrsi(d['close'],14),\n",
    "        'CCI': lambda d: ta.trend.cci(d['high'],d['low'],d['close'],20),\n",
    "        'ROC': lambda d: ta.momentum.roc(d['close'],12),\n",
    "        'Williams_%R': lambda d: WilliamsRIndicator(d['high'],d['low'],d['close'],14).williams_r()\n",
    "    }\n",
    "\n",
    "    # Volatility indicators\n",
    "    volatility = {\n",
    "        'Bollinger_High': lambda d: ta.volatility.bollinger_hband(d['close'],20,2),\n",
    "        'Bollinger_Low': lambda d: ta.volatility.bollinger_lband(d['close'],20,2),\n",
    "        'ATR': lambda d: ta.volatility.average_true_range(d['high'],d['low'],d['close'],14),\n",
    "        'STDDEV_20': lambda d: d['close'].rolling(20).std()\n",
    "    }\n",
    "\n",
    "    # Volume indicators\n",
    "    volume = {}\n",
    "    if 'volume' in df.columns:\n",
    "        volume = {\n",
    "            'OBV': lambda d: ta.volume.on_balance_volume(d['close'],d['volume']),\n",
    "            'MFI': lambda d: ta.volume.money_flow_index(d['high'],d['low'],d['close'],d['volume'],14)\n",
    "        }\n",
    "\n",
    "    indicators = {**trend,**momentum,**volatility,**volume}\n",
    "    for name, func in indicators.items():\n",
    "        try:\n",
    "            df[name] = func(df)\n",
    "        except Exception:\n",
    "            df[name] = np.nan\n",
    "\n",
    "    # Derived signals\n",
    "    df['EMA_10_cross_EMA_50'] = (df['EMA_10'] > df['EMA_50']).astype(int)\n",
    "    df['EMA_50_cross_EMA_200'] = (df['EMA_50'] > df['EMA_200']).astype(int)\n",
    "    df['SMA_10_cross_SMA_50'] = (df['SMA_10'] > df['SMA_50']).astype(int)\n",
    "    df['SMA_50_cross_SMA_200'] = (df['SMA_50'] > df['SMA_200']).astype(int)\n",
    "\n",
    "    df.replace([np.inf,-np.inf], np.nan, inplace=True)\n",
    "    df.ffill(inplace=True)\n",
    "    df.bfill(inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Worker Function\n",
    "# -----------------------------\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_pair_file(pair, tf_file, max_retries=3):\n",
    "    av_file = SAVE_FOLDER / f\"{pair.replace('/','_')}_daily.csv\"\n",
    "\n",
    "    def safe_read_csv(path):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                if path.exists():\n",
    "                    return ensure_tz_naive(pd.read_csv(path, index_col=0, parse_dates=True))\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Retry {attempt+1} reading {path}: {e}\")\n",
    "                time.sleep(2)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    av_df = safe_read_csv(av_file)\n",
    "    yf_path = SAVE_FOLDER / tf_file\n",
    "    yf_df = safe_read_csv(yf_path)\n",
    "\n",
    "    if yf_df.empty and av_df.empty:\n",
    "        return None, f\"{pair} ({tf_file}) skipped - no data\"\n",
    "\n",
    "    combined_df = combine_fx_data(av_df, yf_df)\n",
    "    combined_df = add_all_indicators(combined_df)\n",
    "\n",
    "    save_file = combined_save_path / f\"{pair.replace('/','_')}_{tf_file.replace('.csv','')}_combined.pkl\"\n",
    "    old_hash = file_hash(save_file)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with lock:\n",
    "                combined_df.to_pickle(save_file, protocol=4)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Retry {attempt+1} writing {save_file}: {e}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    new_hash = file_hash(save_file)\n",
    "    changed = old_hash != new_hash\n",
    "    return save_file if changed else None, f\"{pair} ({tf_file}) {'updated' if changed else 'no change'}\"\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Execute in Parallel\n",
    "# -----------------------------\n",
    "changed_files = []\n",
    "tasks = []\n",
    "files_to_process = []\n",
    "\n",
    "for pair in pairs:\n",
    "    for tf_file in list(SAVE_FOLDER.glob(f\"{pair.replace('/','_')}*.csv\")):\n",
    "        tf_file_name = tf_file.name\n",
    "        if any(tf_file_name.endswith(x) for x in [\"daily.csv\", \"_combined.csv\", \"_combined.pkl\"]):\n",
    "            continue\n",
    "        files_to_process.append((pair, tf_file_name))\n",
    "\n",
    "max_workers = max(1, min(8, len(files_to_process), (os.cpu_count() or 4)*2))\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    for pair, tf_file in files_to_process:\n",
    "        tasks.append(executor.submit(process_pair_file, pair, tf_file))\n",
    "    for future in as_completed(tasks):\n",
    "        changed_file, msg = future.result()\n",
    "        print(msg)\n",
    "        if changed_file:\n",
    "            changed_files.append(str(changed_file))\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ Commit & Push Changes\n",
    "# -----------------------------\n",
    "if changed_files:\n",
    "    print(f\"🚀 Committing {len(changed_files)} modified files...\")\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"add\"] + changed_files, check=False)\n",
    "    subprocess.run([\"git\", \"-C\", str(SAVE_FOLDER), \"commit\", \"-m\", \"📈 Auto update combined indicators\"], check=False)\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            f\"git -C {SAVE_FOLDER} push https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git {BRANCH}\",\n",
    "            shell=True, check=False\n",
    "        )\n",
    "        print(\"✅ Push complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Push failed: {e}\")\n",
    "else:\n",
    "    print(\"✅ No data changes detected — skipping push.\")\n",
    "\n",
    "print(\"✅ All FX pairs processed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd80c4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:42:54.333653Z",
     "iopub.status.busy": "2025-10-06T02:42:54.333458Z",
     "iopub.status.idle": "2025-10-06T02:43:05.967417Z",
     "shell.execute_reply": "2025-10-06T02:43:05.966862Z"
    },
    "id": "PGknUXgSWFi6",
    "papermill": {
     "duration": 11.645036,
     "end_time": "2025-10-06T02:43:05.968330",
     "exception": false,
     "start_time": "2025-10-06T02:42:54.323294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Working directory: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models\n",
      "✅ Git configured: Rahim AI Bot <nakatonabira3@gmail.com>\n",
      "✅ Output folders ready: forex-alpha-models/models, forex-alpha-models/pickles, forex-alpha-models/csvs\n",
      "Python version: 3.11.13 (main, Jun  4 2025, 04:12:12) [GCC 13.3.0]\n",
      "Current directory: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Broker JSON saved: forex-alpha-models/broker_signals.json\n",
      "📑 Signals logged to CSV: forex-alpha-models/broker_signals_log.csv\n",
      "\n",
      "📊 Latest Signals JSON:\n",
      " {\n",
      "  \"timestamp\": \"2025-10-06T02:43:02.452204+00:00\",\n",
      "  \"pairs\": {\n",
      "    \"EUR/USD\": {\n",
      "      \"1m_7d\": {\n",
      "        \"long\": 433,\n",
      "        \"short\": 5254,\n",
      "        \"hold\": 45,\n",
      "        \"latest_signal\": -1,\n",
      "        \"live_price\": 1.172\n",
      "      },\n",
      "      \"5m_1mo\": {\n",
      "        \"long\": 465,\n",
      "        \"short\": 4057,\n",
      "        \"hold\": 1210,\n",
      "        \"latest_signal\": -1,\n",
      "        \"live_price\": 1.172\n",
      "      },\n",
      "      \"1d_5y\": {\n",
      "        \"long\": 1722,\n",
      "        \"short\": 4001,\n",
      "        \"hold\": 9,\n",
      "        \"latest_signal\": 1,\n",
      "        \"live_price\": 1.172\n",
      "      },\n",
      "      \"15m_60d\": {\n",
      "        \"long\": 678,\n",
      "        \"short\": 3186,\n",
      "        \"hold\": 1868,\n",
      "        \"latest_signal\": -1,\n",
      "        \"live_price\": 1.172\n",
      "      },\n",
      "      \"1h_2y\": {\n",
      "        \"long\": 2648,\n",
      "        \"short\": 5,\n",
      "        \"hold\": 3079,\n",
      "        \"latest_signal\": 1,\n",
      "        \"live_price\": 1.172\n",
      "      }\n",
      "    },\n",
      "    \"GBP/USD\": {\n",
      "      \"1m_7d\": {\n",
      "        \"long\": 211,\n",
      "        \"short\": 4502,\n",
      "        \"hold\": 1019,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 1.345\n",
      "      },\n",
      "      \"1d_5y\": {\n",
      "        \"long\": 5724,\n",
      "        \"short\": 8,\n",
      "        \"hold\": 0,\n",
      "        \"latest_signal\": 1,\n",
      "        \"live_price\": 1.345\n",
      "      },\n",
      "      \"1h_2y\": {\n",
      "        \"long\": 1262,\n",
      "        \"short\": 4470,\n",
      "        \"hold\": 0,\n",
      "        \"latest_signal\": -1,\n",
      "        \"live_price\": 1.345\n",
      "      },\n",
      "      \"5m_1mo\": {\n",
      "        \"long\": 159,\n",
      "        \"short\": 5573,\n",
      "        \"hold\": 0,\n",
      "        \"latest_signal\": -1,\n",
      "        \"live_price\": 1.345\n",
      "      }\n",
      "    },\n",
      "    \"USD/JPY\": {\n",
      "      \"15m_60d\": {\n",
      "        \"long\": 1041,\n",
      "        \"short\": 256,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 149.89\n",
      "      },\n",
      "      \"5m_1mo\": {\n",
      "        \"long\": 5578,\n",
      "        \"short\": 150,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 149.89\n",
      "      },\n",
      "      \"1m_7d\": {\n",
      "        \"long\": 5718,\n",
      "        \"short\": 10,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 149.89\n",
      "      },\n",
      "      \"1h_2y\": {\n",
      "        \"long\": 0,\n",
      "        \"short\": 5603,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 149.89\n",
      "      },\n",
      "      \"1d_5y\": {\n",
      "        \"long\": 5498,\n",
      "        \"short\": 210,\n",
      "        \"hold\": 0,\n",
      "        \"latest_signal\": 1,\n",
      "        \"live_price\": 149.89\n",
      "      }\n",
      "    },\n",
      "    \"AUD/USD\": {\n",
      "      \"5m_1mo\": {\n",
      "        \"long\": 5514,\n",
      "        \"short\": 190,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 0.6607\n",
      "      },\n",
      "      \"1m_7d\": {\n",
      "        \"long\": 0,\n",
      "        \"short\": 5704,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 0.6607\n",
      "      },\n",
      "      \"1h_2y\": {\n",
      "        \"long\": 2629,\n",
      "        \"short\": 3075,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 0.6607\n",
      "      },\n",
      "      \"15m_60d\": {\n",
      "        \"long\": 5514,\n",
      "        \"short\": 190,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 0.6607\n",
      "      },\n",
      "      \"1d_5y\": {\n",
      "        \"long\": 5570,\n",
      "        \"short\": 9,\n",
      "        \"hold\": 4,\n",
      "        \"latest_signal\": 0,\n",
      "        \"live_price\": 0.6607\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Cell 10 - Hybrid Signal Pipeline + Safe Environment\n",
    "# ======================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import ta\n",
    "from ta.momentum import WilliamsRIndicator\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import datetime as dt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Detect environment\n",
    "# -----------------------------\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Safe working folder\n",
    "# -----------------------------\n",
    "if IN_COLAB:\n",
    "    SAVE_FOLDER = Path(\"/content/forex-alpha-models\")\n",
    "else:\n",
    "    SAVE_FOLDER = Path(\"./forex-alpha-models\")\n",
    "\n",
    "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(SAVE_FOLDER)\n",
    "print(f\"✅ Working directory: {SAVE_FOLDER.resolve()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Git config\n",
    "# -----------------------------\n",
    "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
    "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
    "os.system(f'git config --global user.name \"{GIT_NAME}\"')\n",
    "os.system(f'git config --global user.email \"{GIT_EMAIL}\"')\n",
    "print(f\"✅ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Read tokens\n",
    "# -----------------------------\n",
    "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
    "BROWSERLESS_TOKEN = os.environ.get(\"BROWSERLESS_TOKEN\")\n",
    "\n",
    "if not FOREX_PAT and IN_GHA:\n",
    "    print(\"⚠️ Warning: FOREX_PAT not found in GitHub Actions secrets\")\n",
    "if not BROWSERLESS_TOKEN:\n",
    "    print(\"⚠️ Warning: BROWSERLESS_TOKEN not found\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Safe subfolders for outputs\n",
    "# -----------------------------\n",
    "MODEL_DIR = SAVE_FOLDER / \"models\"\n",
    "PICKLE_FOLDER = SAVE_FOLDER / \"pickles\"\n",
    "CSV_FOLDER = SAVE_FOLDER / \"csvs\"\n",
    "\n",
    "for folder in [MODEL_DIR, PICKLE_FOLDER, CSV_FOLDER]:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BROKER_JSON = SAVE_FOLDER / \"broker_signals.json\"\n",
    "BROKER_LOG = SAVE_FOLDER / \"broker_signals_log.csv\"\n",
    "\n",
    "print(f\"✅ Output folders ready: {MODEL_DIR}, {PICKLE_FOLDER}, {CSV_FOLDER}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Python environment info\n",
    "# -----------------------------\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ CONFIG\n",
    "# -----------------------------\n",
    "PAIRS = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "TIMEFRAMES = {\n",
    "    \"1m_7d\": (\"1m\", \"7d\"),\n",
    "    \"5m_1mo\": (\"5m\", \"1mo\"),\n",
    "    \"15m_60d\": (\"15m\", \"60d\"),\n",
    "    \"1h_2y\": (\"1h\", \"2y\"),\n",
    "    \"1d_5y\": (\"1d\", \"5y\"),\n",
    "}\n",
    "INJECT_CANDLES = 5\n",
    "\n",
    "# Delete old models\n",
    "for f in MODEL_DIR.glob(\"*.pkl\"):\n",
    "    f.unlink()\n",
    "\n",
    "# -----------------------------\n",
    "# 8️⃣ Live price fetch\n",
    "# -----------------------------\n",
    "def fetch_live_rate(pair):\n",
    "    from_currency, to_currency = pair.split('/')\n",
    "    if not BROWSERLESS_TOKEN:\n",
    "        print(f\"⚠️ BROWSERLESS_TOKEN not set, skipping live rates for {pair}\")\n",
    "        return 0\n",
    "    url = f\"https://production-sfo.browserless.io/content?token={BROWSERLESS_TOKEN}\"\n",
    "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
    "    try:\n",
    "        res = requests.post(url, json=payload)\n",
    "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
    "        return float(match.group(1).replace(',', '')) if match else 0\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Live price fetch failed for {pair}: {e}\")\n",
    "        return 0\n",
    "\n",
    "live_prices = {pair: fetch_live_rate(pair) for pair in PAIRS}\n",
    "\n",
    "# -----------------------------\n",
    "# 9️⃣ Data & feature functions\n",
    "# -----------------------------\n",
    "def fetch_data(symbol, interval, period):\n",
    "    df = yf.download(symbol.replace('/', '') + \"=X\", interval=interval, period=period,\n",
    "                     progress=False, auto_adjust=True)\n",
    "    df.dropna(inplace=True)\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [col[0] for col in df.columns]\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def inject_live_price(df, live_price, n_candles=INJECT_CANDLES):\n",
    "    df_copy = df.copy()\n",
    "    n_inject = min(n_candles, len(df_copy))\n",
    "    for col in [\"Open\",\"High\",\"Low\",\"Close\"]:\n",
    "        if col in df_copy.columns:\n",
    "            df_copy.iloc[-n_inject:, df_copy.columns.get_loc(col)] = live_price\n",
    "    df_copy.index = pd.to_datetime(df_copy.index, errors='coerce').tz_localize(None)\n",
    "    return df_copy\n",
    "\n",
    "def add_all_indicators(df):\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
    "\n",
    "    df[\"SMA_10\"] = ta.trend.sma_indicator(df[\"Close\"], 10)\n",
    "    df[\"SMA_50\"] = ta.trend.sma_indicator(df[\"Close\"], 50)\n",
    "    df[\"EMA_10\"] = ta.trend.ema_indicator(df[\"Close\"], 10)\n",
    "    df[\"EMA_50\"] = ta.trend.ema_indicator(df[\"Close\"], 50)\n",
    "    df[\"EMA_200\"] = ta.trend.ema_indicator(df[\"Close\"], 200)\n",
    "    df[\"MACD\"] = ta.trend.macd(df[\"Close\"])\n",
    "    df[\"MACD_signal\"] = ta.trend.macd_signal(df[\"Close\"])\n",
    "    df[\"ADX\"] = ta.trend.adx(df[\"High\"], df[\"Low\"], df[\"Close\"], window=14)\n",
    "    df[\"WilliamsR\"] = WilliamsRIndicator(df[\"High\"], df[\"Low\"], df[\"Close\"], lbp=14).williams_r()\n",
    "    df[\"Bollinger_High\"] = ta.volatility.bollinger_hband(df[\"Close\"], window=20)\n",
    "    df[\"Bollinger_Low\"] = ta.volatility.bollinger_lband(df[\"Close\"], window=20)\n",
    "    df[\"ATR\"] = ta.volatility.average_true_range(df[\"High\"], df[\"Low\"], df[\"Close\"], window=14)\n",
    "\n",
    "    close = df[\"Close\"].values\n",
    "    df[\"return\"] = np.concatenate([[0], (close[1:] - close[:-1])/close[:-1]])\n",
    "    df[\"ma_fast\"] = pd.Series(close).rolling(5).mean()\n",
    "    df[\"ma_slow\"] = pd.Series(close).rolling(20).mean()\n",
    "    delta = np.diff(close, prepend=close[0])\n",
    "    gain = np.where(delta>0, delta, 0)\n",
    "    loss = np.where(delta<0, -delta, 0)\n",
    "    avg_gain = pd.Series(gain).rolling(14).mean()\n",
    "    avg_loss = pd.Series(loss).rolling(14).mean()\n",
    "    rs = avg_gain / avg_loss.replace(0, 1e-9)\n",
    "    df[\"rsi\"] = 100 - (100/(1+rs))\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "def generate_features(df):\n",
    "    return add_all_indicators(df).select_dtypes(include=[np.number])\n",
    "\n",
    "def make_labels(df):\n",
    "    future = df[\"Close\"].shift(-1)\n",
    "    signal = np.where(future>df[\"Close\"], 1, np.where(future<df[\"Close\"], -1, 0))\n",
    "    return signal[:-1]\n",
    "\n",
    "def train_or_load_model(features, labels, model_path):\n",
    "    if features.empty or len(labels)==0:\n",
    "        return None, []\n",
    "    model = SGDClassifier(loss=\"log_loss\", max_iter=1000, tol=1e-3)\n",
    "    model.fit(features, labels)\n",
    "    joblib.dump({\"model\": model, \"features\": list(features.columns)}, model_path)\n",
    "    return model, list(features.columns)\n",
    "\n",
    "def hybrid_signal(model, features, trained_features):\n",
    "    if model is None or features.empty:\n",
    "        return pd.Series([0]*len(features), index=features.index)\n",
    "    for col in trained_features:\n",
    "        if col not in features.columns:\n",
    "            features[col] = 0\n",
    "    features_aligned = features[trained_features].copy()\n",
    "    features_aligned.fillna(0, inplace=True)\n",
    "    preds = model.predict(features_aligned)\n",
    "    return pd.Series(preds, index=features.index)\n",
    "\n",
    "# -----------------------------\n",
    "# 10️⃣ Process one pair/timeframe\n",
    "# -----------------------------\n",
    "def process_pair_tf(pair, tf_name, interval, period, live_price):\n",
    "    df = fetch_data(pair, interval, period)\n",
    "    if len(df)<2:\n",
    "        return None, None, None\n",
    "    df_live = inject_live_price(df, live_price)\n",
    "    features = generate_features(df_live)\n",
    "    labels = make_labels(df_live)\n",
    "    features = features.iloc[:len(labels)]\n",
    "    model_path = MODEL_DIR / f\"{pair.replace('/','_')}_{tf_name}.pkl\"\n",
    "    model, trained_features = train_or_load_model(features, labels, model_path)\n",
    "    signals = hybrid_signal(model, features, trained_features)\n",
    "    df_live = df_live.iloc[:len(signals)].copy()\n",
    "    df_live[\"hybrid_signal\"] = signals.values\n",
    "    df_live.index = pd.to_datetime(df_live.index, errors='coerce').tz_localize(None)\n",
    "    latest_signal = int(df_live[\"hybrid_signal\"].iloc[-1])\n",
    "    long_count = int((df_live[\"hybrid_signal\"]==1).sum())\n",
    "    short_count = int((df_live[\"hybrid_signal\"]==-1).sum())\n",
    "    hold_count = int((df_live[\"hybrid_signal\"]==0).sum())\n",
    "    result = {\n",
    "        \"pair\": pair,\n",
    "        \"timeframe\": tf_name,\n",
    "        \"long\": long_count,\n",
    "        \"short\": short_count,\n",
    "        \"hold\": hold_count,\n",
    "        \"latest_signal\": latest_signal,\n",
    "        \"live_price\": live_price\n",
    "    }\n",
    "    return result, df, df_live\n",
    "\n",
    "# -----------------------------\n",
    "# 11️⃣ Run hybrid pipeline\n",
    "# -----------------------------\n",
    "def run_hybrid():\n",
    "    broker_output = {\"timestamp\": dt.datetime.now(dt.timezone.utc).isoformat(), \"pairs\": {}}\n",
    "    log_rows = []\n",
    "    tasks = []\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        for pair in PAIRS:\n",
    "            live_price = live_prices[pair]\n",
    "            broker_output[\"pairs\"][pair] = {}\n",
    "            for tf_name, (interval, period) in TIMEFRAMES.items():\n",
    "                tasks.append(executor.submit(process_pair_tf, pair, tf_name, interval, period, live_price))\n",
    "        for future in as_completed(tasks):\n",
    "            result, df, df_live = future.result()\n",
    "            if result is None:\n",
    "                continue\n",
    "            pair, tf_name = result[\"pair\"], result[\"timeframe\"]\n",
    "            broker_output[\"pairs\"][pair][tf_name] = {\n",
    "                \"long\": result[\"long\"],\n",
    "                \"short\": result[\"short\"],\n",
    "                \"hold\": result[\"hold\"],\n",
    "                \"latest_signal\": result[\"latest_signal\"],\n",
    "                \"live_price\": float(result[\"live_price\"])\n",
    "            }\n",
    "            log_rows.append({\n",
    "                \"timestamp\": dt.datetime.now(dt.timezone.utc),\n",
    "                \"pair\": pair,\n",
    "                \"timeframe\": tf_name,\n",
    "                **{k: result[k] for k in [\"long\",\"short\",\"hold\",\"latest_signal\"]},\n",
    "                \"live_price\": result[\"live_price\"]\n",
    "            })\n",
    "    with open(BROKER_JSON, \"w\") as f:\n",
    "        json.dump(broker_output, f, indent=2)\n",
    "    log_df = pd.DataFrame(log_rows)\n",
    "    if not BROKER_LOG.exists():\n",
    "        log_df.to_csv(BROKER_LOG, index=False)\n",
    "    else:\n",
    "        log_df.to_csv(BROKER_LOG, mode=\"a\", header=False, index=False)\n",
    "    print(f\"💾 Broker JSON saved: {BROKER_JSON}\")\n",
    "    print(f\"📑 Signals logged to CSV: {BROKER_LOG}\")\n",
    "    return broker_output\n",
    "\n",
    "# -----------------------------\n",
    "# 12️⃣ Run script\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    signals = run_hybrid()\n",
    "    print(\"\\n📊 Latest Signals JSON:\\n\", json.dumps(signals, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e489ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:43:05.988768Z",
     "iopub.status.busy": "2025-10-06T02:43:05.988577Z",
     "iopub.status.idle": "2025-10-06T02:43:06.005702Z",
     "shell.execute_reply": "2025-10-06T02:43:06.005139Z"
    },
    "id": "7pYp31gCRzAX",
    "papermill": {
     "duration": 0.028189,
     "end_time": "2025-10-06T02:43:06.006617",
     "exception": false,
     "start_time": "2025-10-06T02:43:05.978428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV folder: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models\n",
      "✅ Save folder: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/combined_data\n",
      "✅ Git configured: Rahim AI Bot <nakatonabira3@gmail.com>\n",
      "⚠️ No CSV files found for EUR/USD, skipping.\n",
      "⚠️ No CSV files found for GBP/USD, skipping.\n",
      "⚠️ No CSV files found for USD/JPY, skipping.\n",
      "⚠️ No CSV files found for AUD/USD, skipping.\n",
      "🎯 All CSVs processed, hybrid_signal generated, and converted to .pkl for backtest.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Cell 11 - Initialization & CSV → PKL with Hybrid Signals\n",
    "# ======================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Detect environment\n",
    "# -----------------------------\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Safe working folders\n",
    "# -----------------------------\n",
    "if IN_COLAB:\n",
    "    CSV_FOLDER = Path(\"/content/forex-alpha-models\")  # folder where fetched CSVs are\n",
    "    SAVE_FOLDER = Path(\"/content/combined_data\")       # folder for backtest-ready .pkl files\n",
    "else:\n",
    "    CSV_FOLDER = Path(\"./forex-alpha-models\")\n",
    "    SAVE_FOLDER = Path(\"./combined_data\")\n",
    "\n",
    "CSV_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✅ CSV folder: {CSV_FOLDER.resolve()}\")\n",
    "print(f\"✅ Save folder: {SAVE_FOLDER.resolve()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Git config (optional)\n",
    "# -----------------------------\n",
    "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
    "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
    "\n",
    "os.system(f'git config --global user.name \"{GIT_NAME}\"')\n",
    "os.system(f'git config --global user.email \"{GIT_EMAIL}\"')\n",
    "print(f\"✅ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Tokens check\n",
    "# -----------------------------\n",
    "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
    "BROWSERLESS_TOKEN = os.environ.get(\"BROWSERLESS_TOKEN\")\n",
    "\n",
    "if not FOREX_PAT and IN_GHA:\n",
    "    print(\"⚠️ Warning: FOREX_PAT not found in GitHub Actions secrets\")\n",
    "if not BROWSERLESS_TOKEN:\n",
    "    print(\"⚠️ Warning: BROWSERLESS_TOKEN not found\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ FX Pairs\n",
    "# -----------------------------\n",
    "pairs = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"AUD/USD\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Helper: temporary hybrid signal\n",
    "# -----------------------------\n",
    "def generate_temp_signal(df, fast=5, slow=20):\n",
    "    if len(df) < slow:\n",
    "        return pd.Series([0]*len(df), index=df.index)\n",
    "    fast_ma = df['close'].rolling(fast).mean()\n",
    "    slow_ma = df['close'].rolling(slow).mean()\n",
    "    signal = (fast_ma - slow_ma).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    return signal.fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ Process all CSVs\n",
    "# -----------------------------\n",
    "for pair in pairs:\n",
    "    csv_files = list(CSV_FOLDER.glob(f\"{pair.replace('/','_')}_*_combined.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\"⚠️ No CSV files found for {pair}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Load CSV\n",
    "            df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
    "\n",
    "            # Normalize timestamps\n",
    "            df.index = pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n",
    "\n",
    "            # Check required OHLC columns\n",
    "            if not all(col in df.columns for col in ['open', 'high', 'low', 'close']):\n",
    "                print(f\"⚠️ {csv_file} missing OHLC columns, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Generate temporary hybrid_signal\n",
    "            df['hybrid_signal'] = generate_temp_signal(df)\n",
    "\n",
    "            # Compute ATR if missing\n",
    "            if 'atr' not in df.columns:\n",
    "                high, low, close = df['high'].values, df['low'].values, df['close'].values\n",
    "                tr = pd.Series(\n",
    "                    [max(h-l, abs(h-close[i-1]), abs(l-close[i-1])) if i>0 else h-l\n",
    "                     for i, (h, l) in enumerate(zip(high, low))],\n",
    "                    index=df.index\n",
    "                )\n",
    "                df['atr'] = tr.rolling(14).mean().fillna(1e-5).clip(lower=1e-5)\n",
    "\n",
    "            # Save as pickle\n",
    "            pkl_file = SAVE_FOLDER / f\"{csv_file.stem}.pkl\"\n",
    "            df.to_pickle(pkl_file)\n",
    "            print(f\"✅ Saved {pkl_file} with temporary hybrid_signal\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to process {csv_file}: {e}\")\n",
    "\n",
    "print(\"🎯 All CSVs processed, hybrid_signal generated, and converted to .pkl for backtest.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0c13d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T02:43:06.027345Z",
     "iopub.status.busy": "2025-10-06T02:43:06.027179Z",
     "iopub.status.idle": "2025-10-06T02:43:15.874016Z",
     "shell.execute_reply": "2025-10-06T02:43:15.873433Z"
    },
    "id": "aaxRl4eFL33C",
    "papermill": {
     "duration": 9.858303,
     "end_time": "2025-10-06T02:43:15.874969",
     "exception": false,
     "start_time": "2025-10-06T02:43:06.016666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Working directory: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/combined_with_indicators/forex-alpha-models/combined_with_indicators\n",
      "✅ Git configured: Rahim AI Bot <nakatonabira3@gmail.com>\n",
      "✅ Output paths ready: forex-alpha-models/combined_with_indicators\n",
      "Python version: 3.11.13 (main, Jun  4 2025, 04:12:12) [GCC 13.3.0]\n",
      "Current directory: /home/runner/work/forex-ai-models/forex-ai-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/forex-alpha-models/combined_with_indicators\n",
      "Loading combined data...\n",
      "🎯 Running GA optimization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generation 1 === Best Score: 0.00\n",
      "=== Generation 2 === Best Score: 0.00\n",
      "=== Generation 3 === Best Score: 0.00\n",
      "=== Generation 4 === Best Score: 0.00\n",
      "=== Generation 5 === Best Score: 0.00\n",
      "=== Generation 6 === Best Score: 0.00\n",
      "⚠️ Early stopping triggered.\n",
      "✅ GA complete. Best chromosome saved.\n",
      "📡 Generating live signals...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 Live signals saved to forex-alpha-models/combined_with_indicators/broker_signals.json\n",
      "{\n",
      "  \"EUR/USD\": {\n",
      "    \"direction\": \"HOLD\",\n",
      "    \"strength\": 0.0,\n",
      "    \"score_1_100\": 1,\n",
      "    \"last_price\": 1.172,\n",
      "    \"SL\": 1.7284264087570127,\n",
      "    \"TP\": -1.427248808508346,\n",
      "    \"high_confidence\": false\n",
      "  },\n",
      "  \"GBP/USD\": {\n",
      "    \"direction\": \"HOLD\",\n",
      "    \"strength\": 0.0,\n",
      "    \"score_1_100\": 1,\n",
      "    \"last_price\": 1.345,\n",
      "    \"SL\": 1.9014264087570127,\n",
      "    \"TP\": -1.2542488085083459,\n",
      "    \"high_confidence\": false\n",
      "  },\n",
      "  \"USD/JPY\": {\n",
      "    \"direction\": \"HOLD\",\n",
      "    \"strength\": 0.0,\n",
      "    \"score_1_100\": 1,\n",
      "    \"last_price\": 149.88,\n",
      "    \"SL\": 150.436426408757,\n",
      "    \"TP\": 147.28075119149165,\n",
      "    \"high_confidence\": false\n",
      "  },\n",
      "  \"AUD/USD\": {\n",
      "    \"direction\": \"HOLD\",\n",
      "    \"strength\": 0.0,\n",
      "    \"score_1_100\": 1,\n",
      "    \"last_price\": 0.6607,\n",
      "    \"SL\": 1.2171264087570126,\n",
      "    \"TP\": -1.938548808508346,\n",
      "    \"high_confidence\": false\n",
      "  }\n",
      "}\n",
      "📨 Sending email...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📧 Email sent to nakatonabira3@gmail.com\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Hybrid Vectorized Backtest + GA + Momentum-aware Live Browserless Signals + Email\n",
    "- Multi-timeframe calculations preserved\n",
    "- GA loads previous population and continues from last generation\n",
    "- Live signals with SL, TP, and 1-100 scoring\n",
    "- High-confidence trade flags included\n",
    "- Signals sent via Gmail (hardcoded App password)\n",
    "- GA evaluation fully parallelized\n",
    "- Safe for GitHub Actions and Colab (path-agnostic)\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================\n",
    "# Initialization\n",
    "# ======================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from joblib import Parallel, delayed\n",
    "import ta  # technical indicators\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Detect environment\n",
    "# -----------------------------\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "IN_GHA = \"GITHUB_ACTIONS\" in os.environ\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Safe working folder\n",
    "# -----------------------------\n",
    "ROOT_PATH = Path(\"/content\") if IN_COLAB else Path(\".\")\n",
    "SAVE_FOLDER = ROOT_PATH / \"forex-alpha-models\" / \"combined_with_indicators\"\n",
    "SAVE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(SAVE_FOLDER)\n",
    "print(f\"✅ Working directory: {SAVE_FOLDER.resolve()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Git config\n",
    "# -----------------------------\n",
    "GIT_NAME = os.environ.get(\"GIT_USER_NAME\", \"Forex AI Bot\")\n",
    "GIT_EMAIL = os.environ.get(\"GIT_USER_EMAIL\", \"nakatonabira3@gmail.com\")\n",
    "os.system(f'git config --global user.name \"{GIT_NAME}\"')\n",
    "os.system(f'git config --global user.email \"{GIT_EMAIL}\"')\n",
    "print(f\"✅ Git configured: {GIT_NAME} <{GIT_EMAIL}>\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Read tokens\n",
    "# -----------------------------\n",
    "FOREX_PAT = os.environ.get(\"FOREX_PAT\")\n",
    "BROWSERLESS_TOKEN = os.environ.get(\"BROWSERLESS_TOKEN\")\n",
    "\n",
    "if not FOREX_PAT and IN_GHA:\n",
    "    print(\"⚠️ Warning: FOREX_PAT not found in GitHub Actions secrets\")\n",
    "if not BROWSERLESS_TOKEN:\n",
    "    print(\"⚠️ Warning: BROWSERLESS_TOKEN not found\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Safe subfolders for outputs\n",
    "# -----------------------------\n",
    "BEST_CHROM_FILE = SAVE_FOLDER / \"best_chromosome.pkl\"\n",
    "TRADE_MEMORY_FILE = SAVE_FOLDER / \"trade_memory.pkl\"\n",
    "POPULATION_FILE = SAVE_FOLDER / \"population.pkl\"\n",
    "GEN_COUNT_FILE = SAVE_FOLDER / \"generation_count.pkl\"\n",
    "SIGNALS_JSON_PATH = SAVE_FOLDER / \"broker_signals.json\"\n",
    "\n",
    "for path in [SAVE_FOLDER, BEST_CHROM_FILE, TRADE_MEMORY_FILE, POPULATION_FILE, GEN_COUNT_FILE, SIGNALS_JSON_PATH]:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Output paths ready: {SAVE_FOLDER}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Python environment info\n",
    "# -----------------------------\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "pairs = ['EUR/USD', 'GBP/USD', 'USD/JPY', 'AUD/USD']\n",
    "ATR_PERIOD = 14\n",
    "MIN_ATR = 1e-5\n",
    "BASE_CAPITAL = 100\n",
    "MAX_POSITION_FRACTION = 0.1\n",
    "POPULATION_SIZE = 12\n",
    "GENERATIONS = 10\n",
    "MUTATION_RATE = 0.2\n",
    "EARLY_STOPPING = 5\n",
    "TOURNAMENT_SIZE = 3\n",
    "EPS = 1e-8\n",
    "\n",
    "# -----------------------------\n",
    "# Gmail Config\n",
    "# -----------------------------\n",
    "GMAIL_USER = \"nakatonabira3@gmail.com\"\n",
    "GMAIL_APP_PASSWORD = \"gmwohahtltmcewug\"\n",
    "\n",
    "# -----------------------------\n",
    "# Browserless Fetch\n",
    "# -----------------------------\n",
    "def fetch_live_rate(pair: str, timeout: int = 8) -> float:\n",
    "    from_currency, to_currency = pair.split('/')\n",
    "    token = BROWSERLESS_TOKEN\n",
    "    if not token:\n",
    "        print(f\"⚠️ No BROWSERLESS_TOKEN found for {pair}.\")\n",
    "        return 0.0\n",
    "    url = f\"https://production-sfo.browserless.io/content?token={token}\"\n",
    "    payload = {\"url\": f\"https://www.x-rates.com/calculator/?from={from_currency}&to={to_currency}&amount=1\"}\n",
    "    try:\n",
    "        res = requests.post(url, json=payload, timeout=timeout)\n",
    "        match = re.search(r'ccOutputRslt[^>]*>([\\d,.]+)', res.text)\n",
    "        return float(match.group(1).replace(',', '')) if match else 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ fetch_live_rate error for {pair}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# -----------------------------\n",
    "# Data Preparation Helpers\n",
    "# -----------------------------\n",
    "def make_index_tz_naive(df):\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "        if df.index.tz is not None:\n",
    "            df.index = df.index.tz_convert(None)\n",
    "    return df\n",
    "\n",
    "def seed_hybrid_signal_if_needed(df):\n",
    "    if 'hybrid_signal' not in df.columns:\n",
    "        df['hybrid_signal'] = 0.0\n",
    "    # Stronger seeding with RSI + SMA\n",
    "    if df['hybrid_signal'].abs().sum() < 1e-6:\n",
    "        rsi = ta.momentum.rsi(df['close'], window=14)\n",
    "        sma_fast = df['close'].rolling(10, min_periods=1).mean()\n",
    "        sma_slow = df['close'].rolling(50, min_periods=1).mean()\n",
    "        df['hybrid_signal'] = (np.sign(sma_fast - sma_slow) + np.sign(rsi - 50)) / 2.0\n",
    "    df['hybrid_signal'] = df['hybrid_signal'].fillna(0).astype(float)\n",
    "    return df\n",
    "\n",
    "def ensure_atr(df):\n",
    "    if 'atr' in df.columns and not df['atr'].isnull().all():\n",
    "        df['atr'] = df['atr'].fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
    "        return df\n",
    "    high, low, close = df['high'].values, df['low'].values, df['close'].values\n",
    "    tr = np.maximum.reduce([\n",
    "        high - low,\n",
    "        np.abs(high - np.roll(close, 1)),\n",
    "        np.abs(low - np.roll(close, 1))\n",
    "    ])\n",
    "    tr[0] = high[0] - low[0] if len(tr) > 0 else MIN_ATR\n",
    "    df['atr'] = pd.Series(tr, index=df.index).rolling(ATR_PERIOD, min_periods=1).mean().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
    "    return df\n",
    "\n",
    "def load_combined_data(folder):\n",
    "    combined_data = {}\n",
    "    for pair in pairs:\n",
    "        combined_data[pair] = {}\n",
    "        prefix = pair.replace('/', '_')\n",
    "        for p in Path(folder).glob(f\"{prefix}_*_combined.pkl\"):\n",
    "            tf_name = p.name.replace(f\"{prefix}_\", \"\").replace(\"_combined.pkl\", \"\")\n",
    "            try:\n",
    "                df = pd.read_pickle(p)\n",
    "                df = make_index_tz_naive(df)\n",
    "                if not all(c in df.columns for c in ['open', 'high', 'low', 'close']):\n",
    "                    continue\n",
    "                df = seed_hybrid_signal_if_needed(df)\n",
    "                df = ensure_atr(df)\n",
    "                combined_data[pair][tf_name] = df\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to load {p}: {e}\")\n",
    "    return combined_data\n",
    "\n",
    "# -----------------------------\n",
    "# Vectorized Backtest\n",
    "# -----------------------------\n",
    "def run_vector_backtest_vectorized(combined_data, capital, base_risk, atr_sl, atr_tp, conf_mult, tf_weights_per_pair, trade_memory=None):\n",
    "    if trade_memory is None:\n",
    "        trade_memory = {pair: [] for pair in combined_data.keys()}\n",
    "    results = {}\n",
    "    precomputed = {}\n",
    "    for pair, tfs in combined_data.items():\n",
    "        if not tfs:\n",
    "            results[pair] = {'equity_curve': pd.Series([capital]), 'total_pnl': 0, 'max_drawdown': 0}\n",
    "            continue\n",
    "        all_idxs = sorted(set().union(*[set(pd.to_datetime(df.index, errors='coerce')) for df in tfs.values()]))\n",
    "        df_all = pd.DataFrame(index=all_idxs)\n",
    "        for tf_name, df in tfs.items():\n",
    "            df_all[f'close_{tf_name}'] = df['close'].reindex(df_all.index).ffill()\n",
    "            df_all[f'signal_{tf_name}'] = df['hybrid_signal'].reindex(df_all.index).ffill().fillna(0.0)\n",
    "            df_all[f'atr_{tf_name}'] = df['atr'].reindex(df_all.index).ffill().fillna(MIN_ATR).clip(lower=MIN_ATR)\n",
    "        df_all['price'] = df_all[[c for c in df_all.columns if c.startswith('close_')]].mean(axis=1).clip(lower=EPS)\n",
    "        df_all['atr'] = df_all[[c for c in df_all.columns if c.startswith('atr_')]].mean(axis=1).clip(lower=MIN_ATR)\n",
    "        precomputed[pair] = df_all\n",
    "\n",
    "    for pair, df_all in precomputed.items():\n",
    "        tfs = combined_data.get(pair, {})\n",
    "        if not tfs:\n",
    "            continue\n",
    "        agg_signal = sum([df_all[f'signal_{tf}'] * tf_weights_per_pair.get(pair, {}).get(tf, 0.0) for tf in tfs.keys()])\n",
    "        mean_abs_signal = np.mean([df_all[f'signal_{tf}'].abs().mean() for tf in tfs.keys()]) if tfs else 0.0\n",
    "        conf_threshold = conf_mult * (mean_abs_signal + EPS)\n",
    "        df_all['agg_signal'] = np.where(np.abs(agg_signal) >= conf_threshold, agg_signal, 0.0)\n",
    "        price, atr, agg_signal = df_all['price'].values, df_all['atr'].values, df_all['agg_signal'].values\n",
    "        n = len(price)\n",
    "        if n <= 1:\n",
    "            results[pair] = {'equity_curve': pd.Series([capital]), 'total_pnl': 0, 'max_drawdown': 0}\n",
    "            continue\n",
    "        memory_factor = 1.0\n",
    "        if trade_memory.get(pair):\n",
    "            total_prev_pnl = sum([float(tr.get('pnl', 0)) for tr in trade_memory[pair]])\n",
    "            memory_factor = max(0.2, 1.0 + total_prev_pnl / max(1.0, capital * 10.0))\n",
    "        size = (capital * base_risk * np.abs(agg_signal)) / (atr_sl * (atr / price) + EPS)\n",
    "        size = np.minimum(size * memory_factor, capital * MAX_POSITION_FRACTION)\n",
    "        size = np.nan_to_num(size, nan=0.0, posinf=capital * MAX_POSITION_FRACTION)\n",
    "        direction = np.sign(agg_signal)\n",
    "        pnl = direction * size * (atr_tp * atr / price)\n",
    "        equity = np.zeros(n, dtype=float)\n",
    "        equity[0] = capital\n",
    "        for i in range(1, n):\n",
    "            equity[i] = equity[i - 1] + float(pnl[i])\n",
    "        trade_memory.setdefault(pair, []).append({'equity': float(equity[-1]), 'pnl': float(equity[-1] - capital)})\n",
    "        if len(trade_memory[pair]) > 200:\n",
    "            trade_memory[pair] = trade_memory[pair][-200:]\n",
    "        equity_series = pd.Series(equity, index=df_all.index)\n",
    "        results[pair] = {\n",
    "            'equity_curve': equity_series,\n",
    "            'total_pnl': float(equity[-1] - capital),\n",
    "            'max_drawdown': float((equity_series.cummax() - equity_series).max()),\n",
    "        }\n",
    "    total_pnl = sum([r['total_pnl'] for r in results.values()])\n",
    "    max_dd = max([r['max_drawdown'] for r in results.values()] or [0.0])\n",
    "    score = total_pnl / (1.0 + max_dd) if (1.0 + max_dd) != 0 else total_pnl\n",
    "    return score, results, trade_memory\n",
    "\n",
    "# -----------------------------\n",
    "# GA Functions\n",
    "# -----------------------------\n",
    "def build_tf_names(combined_data):\n",
    "    return {pair: sorted(list(combined_data[pair].keys())) for pair in pairs}\n",
    "\n",
    "def create_chromosome(tf_names_map):\n",
    "    chrom = [random.uniform(1.0, 2.0), random.uniform(2.0, 4.0), random.uniform(0.005, 0.02), random.uniform(0.3, 0.7)]\n",
    "    for pair in pairs:\n",
    "        n = max(1, len(tf_names_map.get(pair, [])))\n",
    "        w = np.random.dirichlet(np.ones(n)).tolist()\n",
    "        chrom.extend(w)\n",
    "    return chrom\n",
    "\n",
    "def decode_chromosome(chrom, tf_names_map):\n",
    "    atr_sl, atr_tp, base_risk, conf = chrom[:4]\n",
    "    tf_weights_per_pair = {}\n",
    "    idx = 4\n",
    "    for pair in pairs:\n",
    "        n = max(1, len(tf_names_map.get(pair, [])))\n",
    "        w = np.array(chrom[idx:idx + n], dtype=float)\n",
    "        if w.sum() <= 0:\n",
    "            w = np.ones_like(w) / float(len(w))\n",
    "        else:\n",
    "            w = w / (w.sum() + EPS)\n",
    "        tf_list = tf_names_map.get(pair, [])\n",
    "        tf_weights_per_pair[pair] = {tf: float(weight) for tf, weight in zip(tf_list, w)} if tf_list else {}\n",
    "        idx += n\n",
    "    return atr_sl, atr_tp, base_risk, conf, tf_weights_per_pair\n",
    "\n",
    "def tournament_selection(scored_population, k=TOURNAMENT_SIZE):\n",
    "    selected = random.sample(scored_population, k)\n",
    "    selected.sort(reverse=True, key=lambda x: x[0])\n",
    "    return selected[0][1]\n",
    "\n",
    "def run_ga_vectorized_parallel(combined_data, generations=GENERATIONS, population_size=POPULATION_SIZE, mutation_rate=MUTATION_RATE):\n",
    "    tf_names_map = build_tf_names(combined_data)\n",
    "\n",
    "    if os.path.exists(POPULATION_FILE):\n",
    "        try:\n",
    "            population = pickle.load(open(POPULATION_FILE, 'rb'))\n",
    "        except:\n",
    "            population = [create_chromosome(tf_names_map) for _ in range(population_size)]\n",
    "    else:\n",
    "        population = [create_chromosome(tf_names_map) for _ in range(population_size)]\n",
    "\n",
    "    trade_memory = {}\n",
    "    if os.path.exists(TRADE_MEMORY_FILE):\n",
    "        try:\n",
    "            trade_memory = pickle.load(open(TRADE_MEMORY_FILE, 'rb'))\n",
    "        except:\n",
    "            trade_memory = {}\n",
    "\n",
    "    last_gen = 0\n",
    "    if os.path.exists(GEN_COUNT_FILE):\n",
    "        try:\n",
    "            last_gen = pickle.load(open(GEN_COUNT_FILE, 'rb'))\n",
    "        except:\n",
    "            last_gen = 0\n",
    "\n",
    "    best_score_ever = -np.inf\n",
    "    best_chrom_ever = None\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    def evaluate_chrom(chrom):\n",
    "        atr_sl, atr_tp, base_risk, conf, tf_weights_per_pair = decode_chromosome(chrom, tf_names_map)\n",
    "        score, _, _ = run_vector_backtest_vectorized(combined_data, BASE_CAPITAL, base_risk, atr_sl, atr_tp, conf, tf_weights_per_pair, trade_memory)\n",
    "        return score, chrom\n",
    "\n",
    "    for gen in range(last_gen + 1, last_gen + 1 + generations):\n",
    "        scored_population = Parallel(n_jobs=-1)(delayed(evaluate_chrom)(c) for c in population)\n",
    "        scored_population.sort(reverse=True, key=lambda x: x[0])\n",
    "        best_score, best_chrom = scored_population[0]\n",
    "\n",
    "        if best_score < best_score_ever:\n",
    "            best_score, best_chrom = best_score_ever, best_chrom_ever\n",
    "\n",
    "        print(f\"=== Generation {gen} === Best Score: {best_score:.2f}\")\n",
    "\n",
    "        if best_score > best_score_ever:\n",
    "            best_score_ever, best_chrom_ever, early_stop_counter = best_score, best_chrom, 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= EARLY_STOPPING:\n",
    "                print(\"⚠️ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        next_population = [best_chrom]\n",
    "        while len(next_population) < population_size:\n",
    "            p1, p2 = tournament_selection(scored_population), tournament_selection(scored_population)\n",
    "            child = [(a + b) / 2 for a, b in zip(p1, p2)]\n",
    "            child = [c * random.uniform(0.95, 1.05) if random.random() < mutation_rate else c for c in child]\n",
    "            next_population.append(child)\n",
    "        population = next_population\n",
    "\n",
    "        # Save population\n",
    "        for path, data in [(POPULATION_FILE, population), (TRADE_MEMORY_FILE, trade_memory), (BEST_CHROM_FILE, best_chrom_ever), (GEN_COUNT_FILE, gen)]:\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            pickle.dump(data, open(path, 'wb'))\n",
    "\n",
    "    print(\"✅ GA complete. Best chromosome saved.\")\n",
    "    return best_chrom_ever, trade_memory\n",
    "\n",
    "# -----------------------------\n",
    "# Live Signal Generation\n",
    "# -----------------------------\n",
    "def generate_live_signals_with_sl_tp(best_chrom, combined_data):\n",
    "    tf_names_map = build_tf_names(combined_data)\n",
    "    atr_sl, atr_tp, base_risk, conf, tf_weights_per_pair = decode_chromosome(best_chrom, tf_names_map)\n",
    "    live_signals = {}\n",
    "    prev_signals = {}\n",
    "\n",
    "    if os.path.exists(SIGNALS_JSON_PATH):\n",
    "        try:\n",
    "            prev_data = json.load(open(SIGNALS_JSON_PATH, 'r'))\n",
    "            prev_signals = {pair: data.get('strength', 0.0) for pair, data in prev_data.get(\"pairs\", {}).items()}\n",
    "        except:\n",
    "            prev_signals = {}\n",
    "\n",
    "    for pair in pairs:\n",
    "        tfs = combined_data.get(pair, {})\n",
    "        price = fetch_live_rate(pair)\n",
    "        if price <= 0:\n",
    "            price = np.mean([df['close'].iloc[-1] for df in tfs.values()]) if tfs else 1.0\n",
    "        signal_strength = sum([tf_weights_per_pair.get(pair, {}).get(tf, 0.0) * tfs[tf]['hybrid_signal'].iloc[-1] for tf in tf_names_map.get(pair, [])])\n",
    "        prev_strength = prev_signals.get(pair, 0.0)\n",
    "        if np.sign(signal_strength) != np.sign(prev_strength):\n",
    "            signal_strength = 0.7 * prev_strength + 0.3 * signal_strength\n",
    "        direction = \"BUY\" if signal_strength > 0 else \"SELL\" if signal_strength < 0 else \"HOLD\"\n",
    "        recent_atr = np.mean([tfs[tf]['atr'].iloc[-1] for tf in tf_names_map.get(pair, [])]) if tfs else 1.0\n",
    "        score_100 = min(max(int(100 * (abs(signal_strength) / (recent_atr + EPS)) ** 0.5), 1), 100)\n",
    "        SL = price - atr_sl * recent_atr * 0.5 if direction == \"BUY\" else price + atr_sl * recent_atr * 0.5\n",
    "        TP = price + atr_tp * recent_atr * 1.0 if direction == \"BUY\" else price - atr_tp * recent_atr * 1.0\n",
    "        high_conf = score_100 >= 80\n",
    "        live_signals[pair] = {\"direction\": direction, \"strength\": float(signal_strength), \"score_1_100\": score_100, \"last_price\": float(price), \"SL\": float(SL), \"TP\": float(TP), \"high_confidence\": high_conf}\n",
    "\n",
    "    with open(SIGNALS_JSON_PATH, 'w') as f:\n",
    "        json.dump({\"timestamp\": pd.Timestamp.now().isoformat(), \"pairs\": live_signals}, f, indent=2)\n",
    "\n",
    "    print(f\"📡 Live signals saved to {SIGNALS_JSON_PATH}\")\n",
    "    return live_signals\n",
    "\n",
    "# -----------------------------\n",
    "# Email Function\n",
    "# -----------------------------\n",
    "def send_forex_email(signals, recipient=\"nakatonabira3@gmail.com\"):\n",
    "    def fmt(price, pair=\"\"):\n",
    "        decimals = 3 if \"JPY\" in pair else 4\n",
    "        return f\"{price:.{decimals}f}\" if price else \"-\"\n",
    "\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    flags = {\"USD\": \"🇺🇸\", \"EUR\": \"🇪🇺\", \"GBP\": \"🇬🇧\", \"JPY\": \"🇯🇵\", \"AUD\": \"🇦🇺\"}\n",
    "    rows = \"\"\n",
    "    for pair, d in signals.items():\n",
    "        f1, f2 = pair.split(\"/\")\n",
    "        flag_str = f\"{flags.get(f1, '')} {flags.get(f2, '')}\"\n",
    "        conf = \"🔥\" if d.get(\"high_confidence\") else \"\"\n",
    "        rows += f\"\"\"<tr><td>{flag_str} {pair}</td><td>{fmt(d['last_price'], pair)}</td><td>{d['direction']}</td><td>{d['score_1_100']} {conf}</td><td>SL:{fmt(d['SL'], pair)} | TP:{fmt(d['TP'], pair)}</td></tr>\"\"\"\n",
    "\n",
    "    html = f\"\"\"<html><body><h2>Forex Signals - {today}</h2><table border=\"1\" style=\"border-collapse:collapse;text-align:center;\"><tr><th>Instrument</th><th>Price</th><th>Signal</th><th>Score</th><th>SL/TP</th></tr>{rows}</table></body></html>\"\"\"\n",
    "    msg = MIMEMultipart(\"alternative\")\n",
    "    msg['From'] = f\"Forex Bot <{GMAIL_USER}>\"\n",
    "    msg['To'] = recipient\n",
    "    msg['Subject'] = f\"Forex Signals - {today}\"\n",
    "    msg.attach(MIMEText(html, \"html\"))\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as s:\n",
    "            s.login(GMAIL_USER, GMAIL_APP_PASSWORD)\n",
    "            s.sendmail(GMAIL_USER, recipient, msg.as_string())\n",
    "        print(f\"📧 Email sent to {recipient}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Email send failed: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading combined data...\")\n",
    "    combined_data = load_combined_data(SAVE_FOLDER)\n",
    "\n",
    "    print(\"🎯 Running GA optimization...\")\n",
    "    best_chrom, trade_memory = run_ga_vectorized_parallel(combined_data)\n",
    "\n",
    "    print(\"📡 Generating live signals...\")\n",
    "    signals = generate_live_signals_with_sl_tp(best_chrom, combined_data)\n",
    "\n",
    "    print(json.dumps(signals, indent=2))\n",
    "\n",
    "    print(\"📨 Sending email...\")\n",
    "    send_forex_email(signals)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 68.127378,
   "end_time": "2025-10-06T02:43:18.500878",
   "environment_variables": {},
   "exception": null,
   "input_path": "AI_Forex_Brain 2.ipynb",
   "output_path": "output/AI_Forex_Brain_Executed.ipynb",
   "parameters": {},
   "start_time": "2025-10-06T02:42:10.373500",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}